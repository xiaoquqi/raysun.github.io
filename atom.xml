<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老孙正经胡说</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.site/"/>
  <updated>2021-02-20T23:19:05.257Z</updated>
  <id>http://sunqi.site/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>防止你的钱包掉进Serverless使用的坑</title>
    <link href="http://sunqi.site/2021/02/18/%E9%98%B2%E6%AD%A2%E4%BD%A0%E7%9A%84%E9%92%B1%E5%8C%85%E6%8E%89%E8%BF%9BServerless%E4%BD%BF%E7%94%A8%E7%9A%84%E5%9D%91/"/>
    <id>http://sunqi.site/2021/02/18/%E9%98%B2%E6%AD%A2%E4%BD%A0%E7%9A%84%E9%92%B1%E5%8C%85%E6%8E%89%E8%BF%9BServerless%E4%BD%BF%E7%94%A8%E7%9A%84%E5%9D%91/</id>
    <published>2021-02-18T13:51:50.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>今天读到InfoQ一篇《应用上云2小时烧掉近50万，创始人：差点破产，简直噩梦》讲述了在使用Serverless方式开发时由于程序Bug导致快速的资源消耗，差点破产的经历。特意查询了英文原文，题目叫做《We Burnt $72K testing Firebase - Cloud Run and almost went Bankrupt》。我之前在使用函数计算时也有一次类似经历，所以写出来供大家参考，防止你的钱包掉进Serverless使用的坑。</p><a id="more"></a><h1 id="钱是如何被烧掉的？"><a href="#钱是如何被烧掉的？" class="headerlink" title="钱是如何被烧掉的？"></a>钱是如何被烧掉的？</h1><p>如果要理解原文中为什么出现问题，先要了解一下Google Serverless的产品体系：</p><ul><li>Cloud Functions: 基于事件驱动的函数计算服务</li><li>App Engine: 上一篇讲述Serverless发展过程的时候恰好提到过这个服务，是Google的PaaS平台，开发人员可以直接将代码托管在平台上运行，具有良好的扩展性</li><li>Cloud Run: 无状态的Serverless HTTP容器，口号是 Bringing Serverless to Containers，可以执行任何语言</li><li>Cloud Firestore: 是一种灵活且可扩缩的数据库，适用于在Firebase和Google Cloud Platform上进行移动、Web 和服务器开发。</li></ul><p>从账单里可以看到，用户最消耗资源的费用是来自App Engine对Cloud Firestore读取次数达到惊人的千亿次，按照原文里的说法，这里仅仅是两个个小时的成绩。在惊叹费用的同时，我们不得不感叹云原生服务性能之优异。</p><p><img src="/images/pasted-171.png" alt="upload successful"></p><p>作者针对这一事件前后共发布了三篇博客，第一篇主要还原事情的脉络，以及在使用Google服务时候的坑；第二篇在剖析自身的错误；最后一篇在作者感叹了一下自己的经历被翻译成各种语言，另外写了另外一篇《How to user Cloud without losing Sleep》防止大家踩坑。</p><p>通篇看下来，作者将问题归咎于几个地方：</p><ul><li>核心问题：代码中包含Bug(Deploying flawed algorithm on Cloud)，不恰当的使用递归而形成死循环(Exponential Recursion without Break: The instances wouldn’t know when to break, as there was no break statement.)</li><li>测试环境中，Cloud Run默认并发实例时1000，测试环境中也使用了默认值，如果在测试环境选择并发数是2，那么费用将从72,000美金降到144美金</li><li>在没有完全掌握Firebase时探索性的使用，没想到Firebase性能这么强大（广告嫌疑）</li><li>基于云开发仍然要抱有对技术的敬畏心，云原生服务降低了运维和开发的难度，但是在使用方式、价格、配置等诸多因素的复杂度必然会与传统有很大的区别，正所谓软件工程领域没有银弹</li></ul><h1 id="我在函数计算上踩过的坑"><a href="#我在函数计算上踩过的坑" class="headerlink" title="我在函数计算上踩过的坑"></a>我在函数计算上踩过的坑</h1><p>之前在测试阿里云函数计算过程时，曾经也有过一次入坑经历。当时我写了一个简单的函数，当有.png文件上传至OSS后，就自动进行Resize操作，保存成三种规格16x16, 32x32, 64x64，之后重新传回到OSS。但是我忽略了一点，我将处理好的文件重新传输回了同一个OSS Bucket内，结果造成了一个死循环。</p><p><img src="/images/pasted-172.png" alt="upload successful"></p><p>其实我所犯的错误和上面公司遇到的问题是一样的，不过好在我的这次错误没有导致很严重的后果。由于时间久远，我无法找到当时的截图。我只记得我的函数计算在短时间内调用了70万次，OSS上传了几百万个小文件。还好阿里云100万次内的访问是免费的，OSS价格比较便宜，但是还是惊出了一身冷汗。赶紧把我的函数服务下线，同时写了一个脚本删除小文件，因为没有直接的命令可以删除非空的Bucket，整个善后清理工作持续了几个小时。</p><h1 id="如何防止踩坑"><a href="#如何防止踩坑" class="headerlink" title="如何防止踩坑"></a>如何防止踩坑</h1><p>作为新的技术趋势，在前进的过程中难免遇到各种各样的问题。目前确实还没有一套指导规范帮助研发人员有效的避免踩坑。所以也需要大家在实践中不断总结。</p><ul><li>做技术的人还是要有一颗敬畏的心，任何新技术的引进都会带来风险，所以在你没有十足的把握的时候，还是小心的比较好</li><li>云的并发性能真的太强大了，所以一定要做好发生异常时的应对方案，让并发性在你的可控范围内进行，控制”爆炸半径“</li><li>Serverless的开发模式对研发流程是一个全新的挑战，在上述案例中问题出现在了测试阶段，不同于传统的测试，Serverless与开发更为紧密，对于白盒测试的要求更高，一方面需要做好单元测试，另外一方面对开发者综合考虑问题的素质要求更高，或许DevTest会成为Serverless在落地实践中需要解决的问题之一</li><li>动手与阅读文档并行，云服务的文档就和普通商品的说明书一样，没人会去主动阅读的，只有实在搞不出来的时候才会想着看一看，但是云服务不同于普通的产品，一些细枝末节的选项太多，还是应当在使用一段时间后回过头来通读文档的内容，加深理解</li></ul><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://blog.tomilkieway.com/72k-1/" target="_blank" rel="noopener">https://blog.tomilkieway.com/72k-1/</a></li><li><a href="https://blog.tomilkieway.com/72k-2/" target="_blank" rel="noopener">https://blog.tomilkieway.com/72k-2/</a></li><li><a href="https://sudcha.com/guide-to-cloud/" target="_blank" rel="noopener">https://sudcha.com/guide-to-cloud/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天读到InfoQ一篇《应用上云2小时烧掉近50万，创始人：差点破产，简直噩梦》讲述了在使用Serverless方式开发时由于程序Bug导致快速的资源消耗，差点破产的经历。特意查询了英文原文，题目叫做《We Burnt $72K testing Firebase - Cloud Run and almost went Bankrupt》。我之前在使用函数计算时也有一次类似经历，所以写出来供大家参考，防止你的钱包掉进Serverless使用的坑。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Serverless发展历史</title>
    <link href="http://sunqi.site/2021/02/13/Serverless%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2/"/>
    <id>http://sunqi.site/2021/02/13/Serverless%E5%8F%91%E5%B1%95%E5%8E%86%E5%8F%B2/</id>
    <published>2021-02-13T02:23:04.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>最近看了很多Serverless的文章，关于发展历史看了很多版本，其实对于2010年后的时间点各种国内外说法基本一致，但是在2010年之前就有多种不同的描述方式了，本文结合各种资料对Serverless发展的历史进行一下整理。</p><a id="more"></a><p>从应用开发的历史角度看，我们的应用从运行在传统的物理服务器上，经历了到虚拟化、容器到目前的Serverless，至于Serverless之后会是什么，目前还没有看到。从应用开发角度看，这个过程实际一直在进行的事情就是“解耦”，解除应用与底层之间的关联关系，最终使得应用的开发走向轻量化，用户对于底层无感。</p><p><img src="/images/pasted-166.png" alt="upload successful"></p><p>再从我们看的见的商业维度说，中关村海龙、鼎好的兴衰的过程也在印证了这一切的发展，时间回退到20年前，当初做硬件发家致富的中关村老板大有人在，包括今天的京东也是从中关村卖光盘起家的。但是如今的中关村是什么样的景象呢？这些老牌的商场纷纷面临转型，如今的繁华再也一去不复返了。</p><p>与之相呼应的是，传统的硬件集成商的日子越来越不好过了，靠卖铁度日的利润越来越薄，竞争也越来越激烈，市场也越来越透明。为什么出现这样的局面呢？原因其实就在于应用离底层越来越远，单纯的采购硬件无法解决用户的需求。虽然在传统硬件的销售也一样需要解决方案，但是在新的云计算的态势下，解决方案的复杂程度、技术含量越来越高，并非传统的“攒机”式销售就能搞定的。当然在云计算领域下还是存在这样的简单粗暴的销售产品，就是网络，这个话题并不在我们本文的范畴内。</p><h1 id="Zimki——最早使用Serverless模式的公司"><a href="#Zimki——最早使用Serverless模式的公司" class="headerlink" title="Zimki——最早使用Serverless模式的公司"></a>Zimki——最早使用Serverless模式的公司</h1><p>目前公开资料认为最早使用Serverless模式的公司叫做Zimki，即Pay as you go。他们提供服务端的Javascript程序，理念在2006年被提出。这个团队来自被欧洲佳能收购的Fotango。在slidershare上还能找到他们最早的PPT资料，不过这家公司已经倒闭。</p><p><img src="/images/pasted-163.png" alt="upload successful"></p><p>虽然Zimki是Serverless模式最早的缔造者，但是并不是最早使用Serverless词的公司。</p><p>有兴趣的朋友可以看一下他们早期的一些理念的介绍：</p><ul><li><a href="http://radar.oreilly.com/2006/09/zimki-hosted-javascript-enviro.html" target="_blank" rel="noopener">http://radar.oreilly.com/2006/09/zimki-hosted-javascript-enviro.html</a></li><li><a href="https://www.slideshare.net/swardley/zimki-2006" target="_blank" rel="noopener">https://www.slideshare.net/swardley/zimki-2006</a></li></ul><h1 id="Platform-as-a-Service"><a href="#Platform-as-a-Service" class="headerlink" title="Platform-as-a-Service"></a>Platform-as-a-Service</h1><p>在出现函数计算服务形态之前，还有一种PaaS形态，也是属于Serverless的一种形态，只不过设计的出发点不同。PaaS更注重的是完整的代码托管，开发者只需要上传自己的代码，剩下全部的交给平台。这种形态最早出现于2007年。在最近3年内，特别在容器和K8S出现之后，PaaS平台迎来了新一波的关注热度。</p><p>Heroku这家公司想必早期写博客的朋友都不会感到陌生，在那个计算资源还比较昂贵的时代，Heroku的免费资源还是非常受广大开发人员欢迎的。2007年6月Heroku开始开发，最早只支持Ruby语言(这就是为什么早期的博客工具octopress是基于Ruby开发的)，2011年的时候Ruby的首席设计师日本人松本行弘加盟了这家公司，后续又扩大了对Node.js和Clojure的支持。到目前为止，Heroku几乎覆盖了主流开发语言的。2010年Salesforce斥资两亿多美金收购了Heroku公司。</p><p><img src="/images/pasted-165.png" alt="upload successful"></p><p>2008年，Google推出了Google App Engine的PaaS服务，想必如果你当时对科学上网有所研究的话，对这个平台并不陌生。当时，最著名的开源项目非GoAgent莫属，当然实现这个项目已经无法使用了。但是当时GAE平台有非常大的局限性，无论是开发语言还是开发模式，对于框架支持等都有比较严格的限制；另外由于缺少云原生服务的支持，所以应用场景有限，同时具有非常明显的厂商锁定的特性。</p><p>其实包括AWS(AWS Beanstak)、阿里云(Web应用托管服务)在内的主流公有云厂商，目前均提供了类似GAE便于用户快速构建自己的应用，在很多最佳实践中也都有提到。</p><p>国内最早提供SAE服务的，是新浪云，是的你没有听错新浪也是有云的。新浪早在2009年11月3日推出了Alpha版本的新浪SAE，当时用于支付平台费用的叫做云豆，笔者记得当时注册新浪SAE平台，会赠送相当可观的云豆用于开发和测试。虽然新浪涉及云计算领域较早，但是后劲不足。另外还有一个值得一提的一点，新浪SAE也是国内比较早期使用OpenStack的技术团队。后来，OpenStack在中国得到前所未有的发展，与新浪SAE技术与运维团队的推广有着非常直接的关系。</p><h1 id="开源PaaS平台"><a href="#开源PaaS平台" class="headerlink" title="开源PaaS平台"></a>开源PaaS平台</h1><p>2008年一个基于AWS EC2的PaaS项目开始开发，项目使用Java语言并且基于AWS EC2，名称叫做Cloud Foundry。2009年该项目被SpringSource公司收购，同年SpringSource又被VMWare收购。但是这个Cloud Foundry与我们现在熟知的Cloud Foundry项目完全无关，只是保留了名称。最早的Cloud Foundry项目实际是由VMware内部的一个小团队开发的B29项目。</p><p>2011年4月，Cloud Foundry正式宣布开源。2012年4月，又开源了BOSH项目用于Cloud Foundry基础资源及自身的全生命周期管理。笔者在2011年有机会从事了一部分Cloud Foundry产品开发工作，记得当时BOSH对VMware支持非常完美，但是OpenStack平台基本惨不忍睹，当然这也怪当时的OpenStack自己不够争气。<br>2013年，VMware和EMC正式成立了Pivotal公司，自此包括Cloud Foundry, RabbitMQ和Spring都归属于Pivotal。</p><p>同一时期，与Cloud Foundry同一类型的开源项目就是Redhat OpenShift，目前OpenShift也是国内基于Kubernetes之上的PaaS项目(CloudFoundry也开始这么定义自己了)。当然OpenShift也不是Redhat原生项目，而是在2010年收购的Makara的项目，该公司主要基于Linux Container技术实现PaaS平台。2012年5月的时候，OpenShift正式宣布开源。OpenShift v3版本开始支持Kubernetes作为容器编排引擎，Docker作为底层容器。OpenShift v4版本为了防止Docker锁定，使用CRI-O作为容器Runtime。</p><h1 id="”Serverless“概念的来历"><a href="#”Serverless“概念的来历" class="headerlink" title="”Serverless“概念的来历"></a>”Serverless“概念的来历</h1><p>如果你使用中文搜索引擎搜索Serverless的历史，往往会提到一家公司叫做Iron.io，但是如果你搜索英文资料的时候却发现很少有提及此公司。经过一系列的搜索，终于梳理清楚了Serverless概念和定义的由来。</p><p>2012年10月，时任Iron.io BD副总裁的Ken Fromm在ReadWrite网站(互联网科技博客)上发表了一篇名为《Why The Future Of Software And Apps Is Serverless》的文章，完整的阐述了对Serverless架构的构想，其中开篇的第一句话就是：</p><blockquote><p>Even with the rise of cloud computing, the world still revolves around servers. That won’t last, though. Cloud apps are moving into a serverless world, and that will bring big implications for the creation and distribution of software and applications.</p></blockquote><p>ThoughWorks提出了对Serverless架构的定义：</p><blockquote><p>A serverless architecture approach replaces long-running virtual machines with ephemeral compute power that comes into existence on request and disappears immediately after use.</p></blockquote><blockquote><p>Serverless架构使用临时计算资源替代原有常态化运行的虚拟机，当有请求时资源存在，请求结束后资源自动销毁。</p></blockquote><p>也许你觉得太复杂了，我们来看看Techopedia网站上对Serverless的定义：</p><blockquote><p>Serverless computing is a type of cloud computing where the customer does not have to provision servers for the back-end code to run on, but accesses services as they are needed. Instead, the cloud provider starts and stops a container platform as a service as requests come in and the provider bills accordingly.</p></blockquote><blockquote><p>Serverless是云计算服务的一种，用户只需要将提供服务的代码运行在云上，而无须实现其他后端服务。云商根据访问情况，启动或者停止容器来提供服务，用户只需要根据实际消费付费。</p></blockquote><p>如果你还认为复杂，可以简单的将Serverless理解为“基于事件驱动的计算服务”。</p><h1 id="Function-as-a-Service"><a href="#Function-as-a-Service" class="headerlink" title="Function-as-a-Service"></a>Function-as-a-Service</h1><p>2014年对于Serverless是具备里程碑的一年，AWS发布了Lambda服务——基于事件驱动的函数计算服务。最早发布的Lambda仅支持JavaScript和Node，但是目前几乎涵盖了所有主流编程语言，同时支持自定义方式。</p><p><img src="/images/pasted-167.png" alt="upload successful"></p><p>在接下来的时间里，各大公有云厂商纷纷发布了自己的函数计算服务。从2014年到2018年的四年里，各大主要公有云厂商纷纷发布自己的函数计算服务并不断的迭代、演进。一方面扩大对触发器的支持范围，加强与各个云原生服务的联动性；另外一方面基于函数计算增加编排服务，方便构建更复杂的应用场景。</p><p><img src="/images/pasted-168.png" alt="upload successful"></p><h1 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h1><p>目前，Serverless被各方认为是未来云计算服务发展的重要趋势，也是各大厂商的必争之地。我认为Serverless对于行业的影响是深远的，除了技术层面外，还包括从业者的格局。根据Gartner 2020年6月的成熟度曲线看，Serverless还有一段发展的时间，但是作为云计算行业的从业者，应该着手应对Serverless对未来产业格局的影响。后面的文章，我将继续和大家分享我对目前公有云厂商Serverless发展的看法。</p><p><img src="/images/pasted-170.png" alt="upload successful"></p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li><a href="https://mp.weixin.qq.com/s/1jhLRNaUag-Gp-kbYvzzGA" target="_blank" rel="noopener">Serverless国内发展的纵向观察</a></li><li><a href="https://readwrite.com/2012/10/15/why-the-future-of-software-and-apps-is-serverless/?__cf_chl_jschl_tk__=9d9134331acb78cc239f3e7db934345af67bbdc7-1613485823-0-ARJ7RlgI0rpCz7GIY2DCiOUmXfWwk0bP-j7LmFE25MHdY6rqorQ069DcGqkzOpoxRuF_6QQav0-GxS00_nMmF7lpD2gCs33ZMSva-klU-Dlc9Vg2bMzg9TiW4s4mjpmwpjG4SvaWqwsr0rTe48hjYksKmMwUn9GWWeYRjERPJUvgQ20EVTLysumFK6sOjvEt7-AlesfFVqDeCRFjjpN6-_cbDwyGHGZ-PgAxaWrgy4_dbgDFXiz98GSEb0BBhtdqcWMFpI1qkocucVqWrOwsQdKfwX6_zh_QV1joZDfefFJqKafULTlgJ8bpx7AczZOkheoMZFwMaCXRrCd2jX5SFiv2fkgf5fBq3h71pWKaQsFF_oKHxqzx-NBGidZH22_qSYf5LkbpJdGLJRUNGWURU02GZmSK_HqqPlLhRxNS_pQTHMe2qM-7pSzvMadnDRZafQ" target="_blank" rel="noopener">Why The Future Of Software And Apps Is Serverless</a></li><li><a href="https://www.thoughtworks.com/radar/techniques/serverless-architecture" target="_blank" rel="noopener">ThoughtWorks Serverless architecture</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近看了很多Serverless的文章，关于发展历史看了很多版本，其实对于2010年后的时间点各种国内外说法基本一致，但是在2010年之前就有多种不同的描述方式了，本文结合各种资料对Serverless发展的历史进行一下整理。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>AWS Serverless现状与发展趋势</title>
    <link href="http://sunqi.site/2021/02/11/AWS-Serverless%E7%8E%B0%E7%8A%B6%E4%B8%8E%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/"/>
    <id>http://sunqi.site/2021/02/11/AWS-Serverless%E7%8E%B0%E7%8A%B6%E4%B8%8E%E5%8F%91%E5%B1%95%E8%B6%8B%E5%8A%BF/</id>
    <published>2021-02-11T07:17:12.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>Serverless架构是最近一直非常关注的技术方向，基础架构在应用构建中的地位被被进一步弱化的趋势不可逆转。Serveless让开发者更加关注业务本身逻辑的特性决定了技术演进的趋势。</p><p>目前，很多人可能很难认可这样的观点，大部分人还是认为Kubernetes能够带来更多的灵活性。但是Kubernetes对于应用开发者来说，仍然要顾及底层架构。Serverless的终极目标就是彻底打消这层的关系。但不可否认的一点，Kubernetes、OpenStack等基础平台作为Serverless的底层支撑一定会长期存在，所以二者之间并非竞争关系，而是发展的阶段的不同。从另外一个角度看，在公有云最佳实践中，函数计算已经成为构建应用必不可少的一环。</p><a id="more"></a><p>我大概在2018年左右接触了AWS Lambda服务，后续一直在关注着Serverless趋势，所以本文就结合我的一点粗浅理解来分析一下AWS、阿里云和腾讯云在Serverless架构发展的趋势（分为三篇）。本文中的观点是我自身使用公有云相关服务的经验，难免存在片面性，如有不妥之处，请各位给予批评和指正。</p><p>这里先借用一张InfoQ在《Serverless国内发展纵向观察》的一张图，前面的文章中我也讲到这个问题，对于云来说，Serverless架构很重要的两个部分就是函数即服务（Function-as-a-Service）和后端即服务（Backend-as-a-Service）两部分，这两种服务类型有都属于云原生的范畴。</p><p><img src="/images/pasted-162.png" alt="upload successful"></p><p>由上图可知，FaaS和BaaS是成就Serverless的关键服务能力。函数计算的执行是通过各个云原生服务触发的（触发器执行），所以作为Serverless架构中的业务逻辑实现层，函数计算及其相关服务的发展策略基本代表了一家云商对于Serverless架构的态度。每个云其实都会有自己对Serverless的独特理解，有共性也有差别，同时又都有自身对于Serverless未来场景的理解。</p><h1 id="AWS-Serverless现状"><a href="#AWS-Serverless现状" class="headerlink" title="AWS Serverless现状"></a>AWS Serverless现状</h1><p>我在上一篇关于《Serverless发展历史》的中提到，2014年AWS发布函数计算服务Lambda，开启了新Serverless的篇章，自此以后各大公有云纷纷推出自己的函数计算服务。截止目前为止，AWS仍然是公有云领域的引领者，也是为数不多盈利的公有云公司，其发展方向一直是其他友商追逐的目标。</p><p>从局部角度看，函数计算服务在整个云计算架构中像一个粘合剂，巧妙的串联了各个云原生服务，让“不可变”的云原生服务具备了“可变性”，巧妙的解决了云原生服务之间最后一公里的问题。目前，这种使用方式几乎涵盖了AWS所有的最佳实践。</p><h1 id="AWS-Serverless架构"><a href="#AWS-Serverless架构" class="headerlink" title="AWS Serverless架构"></a>AWS Serverless架构</h1><p>根据AWS的定义，将Serverless架构定义为三层服务类别：计算、应用集成和数据存储。</p><h2 id="计算"><a href="#计算" class="headerlink" title="计算"></a>计算</h2><p>计算层主要包括Lambda和Fargate服务。Lambda是函数计算服务，Fargate是AWS的容器编排引擎，Fargate服务包含两种不同不同的方式：一种是自有的引擎(ECS服务)，另外一种是基于Kubernetes底座的。相较于AWS EC2服务，容器编排层好像并不是AWS关注的重点，这一点从容器服务与其他服务联动性能够隐约感觉出来。但是，AWS Lambda服务绝对足够强大。</p><p><img src="/images/pasted-173.png" alt="upload successful"></p><h2 id="应用集成"><a href="#应用集成" class="headerlink" title="应用集成"></a>应用集成</h2><p>应用集成类主要包含了用于串联函数计算的相关服务。SQS/SNS/API都是非常常用的触发器；AppSync提供了GraphQL的接口，能够从函数计算获取数据；EventBridge提供了事件驱动的架构，扩展了函数计算的事件触发类别，同时也可以进行自定义。</p><p><img src="/images/pasted-177.png" alt="upload successful"></p><p>Step Functions通过状态机的定义，很好的让多个函数计算有序运行，降低Serverless架构控制难度。</p><p><img src="/images/pasted-178.png" alt="upload successful"></p><h2 id="数据存储"><a href="#数据存储" class="headerlink" title="数据存储"></a>数据存储</h2><p>在持久化数据方面，</p><p><img src="/images/pasted-179.png" alt="upload successful"></p><h1 id="函数服务Lambda"><a href="#函数服务Lambda" class="headerlink" title="函数服务Lambda"></a>函数服务Lambda</h1><h2 id="开发语言支持"><a href="#开发语言支持" class="headerlink" title="开发语言支持"></a>开发语言支持</h2><p>目前函数计算服务几乎覆盖了主流的高级开发语言，如果你的语言比较特殊，还可以基于容器自定义，提供了最大灵活度，这也是大部分厂商通用的做法。</p><p><img src="/images/pasted-181.png" alt="upload successful"></p><h2 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h2><p>我们重点来看看Lmabda服务，函数计算是由事件触发的，以目前的认知，AWS Lambda与国内的公有云相比，是触发器最多的云平台(Google和AZure我没有对比过)。能够触发Lambda的，除了传统的数据库、消息队列、对象存储等，还包括了物联网设备，包括loT和Alexa音箱。</p><p><img src="/images/pasted-174.png" alt="upload successful"></p><p>除了云原生服务外，还通过EventBridge支持第三方服务直接触发Lambda，为应用开发提供了极大的便利性。</p><p><img src="/images/pasted-175.png" alt="upload successful"></p><h2 id="目标配置"><a href="#目标配置" class="headerlink" title="目标配置"></a>目标配置</h2><p>另外值得称道的是Lambda除了提供了丰富的触发器资源，在函数执行结束后也提供了异常处理能力以及后续触发调用能力，让基于Lambda开发的应用更加健壮，也更加简单。</p><p><img src="/images/pasted-176.png" alt="upload successful"></p><p>AWS还支持流式调用的映射，方便更实时的利用Lambda处理数据。</p><p><img src="/images/pasted-180.png" alt="upload successful"></p><h2 id="开发与调试"><a href="#开发与调试" class="headerlink" title="开发与调试"></a>开发与调试</h2><p>我在使用函数过程中，最大的感触就是函数计算的开发和调试是比较麻烦的，因为牵扯到各种云服务，所以在本地调试的时候会有很多限制，在线上调试又担心出现问题。AWS除了提供在线的编辑器外，CLI工具包括了AWS CLI和AWS SAM(AWS Serverless Application Model)。如果是开发函数计算，还是推荐SAM，毕竟是面向函数计算开发设计的。</p><h2 id="日志、监控与权限控制"><a href="#日志、监控与权限控制" class="headerlink" title="日志、监控与权限控制"></a>日志、监控与权限控制</h2><p>在实际使用过程中，函数计算追踪往往是初学者遇到的最大的挑战。其实用了这么久的云，在云原生服务关联性方面你是能真真切切感受到AWS的设计感的。函数计算也不例外，与IAM、日志以及CloudWatch监控服务都有比较良好的互动性，包括上面提到的CLI工具，底层也是调用了CloudFormation编排的能力实现的。所以Lambda的整个开发者生态还是非常完善和友好的。</p><h1 id="发展趋势"><a href="#发展趋势" class="headerlink" title="发展趋势"></a>发展趋势</h1><p>AWS Lambda是函数计算的里程碑，也是目前为止生态最为完整的函数计算服务。由于Lambda与其他AWS服务的良好的互动性，所以往往在整个Serverless架构中，Lambda作为连接云原生服务之间的能力体现的更加淋漓尽致一些。特别是在一些最佳实践中，通过Lambda的合理利用，往往可以降低在云架构设计和开发的成本。从另外一个层面上讲，对Lambda的特性使用的越多，对AWS依赖就会越强，对用户形成一定的锁定性。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Serverless架构是最近一直非常关注的技术方向，基础架构在应用构建中的地位被被进一步弱化的趋势不可逆转。Serveless让开发者更加关注业务本身逻辑的特性决定了技术演进的趋势。&lt;/p&gt;
&lt;p&gt;目前，很多人可能很难认可这样的观点，大部分人还是认为Kubernetes能够带来更多的灵活性。但是Kubernetes对于应用开发者来说，仍然要顾及底层架构。Serverless的终极目标就是彻底打消这层的关系。但不可否认的一点，Kubernetes、OpenStack等基础平台作为Serverless的底层支撑一定会长期存在，所以二者之间并非竞争关系，而是发展的阶段的不同。从另外一个角度看，在公有云最佳实践中，函数计算已经成为构建应用必不可少的一环。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>PostgreSQL无法启动“global/pg_control”：Permission denied</title>
    <link href="http://sunqi.site/2021/02/11/PostgreSQL%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E2%80%9Cglobal-pg-control%E2%80%9D%EF%BC%9APermission-denied/"/>
    <id>http://sunqi.site/2021/02/11/PostgreSQL%E6%97%A0%E6%B3%95%E5%90%AF%E5%8A%A8%E2%80%9Cglobal-pg-control%E2%80%9D%EF%BC%9APermission-denied/</id>
    <published>2021-02-11T00:44:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>昨天在为用户进行迁移后，用户Windows 2012系统上PostgreSQL服务无法启动，日志中提示“global/pg_control”：Permission denied，于是上网一顿搜索终于解决了这个问题。</p><a id="more"></a><h1 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h1><p>如果端口没有被占用，那么你可以用PostgreSQL原生的命令启动它。进入postgresql安装路径下的 bin 文件夹，在这里打开命令行，执行下面的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\pg_ctl start -D ..\data</span><br></pre></td></tr></table></figure><p>如果程序报出如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ERROR: could not open control file “global&#x2F;pg_control”: Permission denied</span><br></pre></td></tr></table></figure><p><img src="/images/pasted-159.png" alt="upload successful"></p><p>则说明当前操作系统用户丢失了data文件夹及其内容的权限。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><ol><li>首先，进入postgresql 的安装路径，右键data文件夹，依次点击属性——安全——编辑，你能看到所有用户或用户组的权限。</li></ol><p><img src="/images/pasted-160.png" alt="upload successful"></p><ol start="2"><li>确保System 和 Administrator 拥有“完全控制”权限。Users 用户组默认只拥有“读取和执行”，“列出文件夹内容”和“读取”3种权限。当启动数据库提示“权限不足”时，应再添加“修改”和 “写入”。我这次出现问题就在这里，User没有修改和写入权限，添加后即可启动成功。</li></ol><p><img src="/images/pasted-161.png" alt="upload successful"></p><ol start="3"><li>保存并尝试再次在bin 文件夹下执行：</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\pg_ctl start -D ..\data</span><br></pre></td></tr></table></figure><p>观察PostgreSQL数据库能否启动。</p><h1 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a>参考链接</h1><ul><li><a href="https://blog.csdn.net/international24/article/details/89710703" target="_blank" rel="noopener">https://blog.csdn.net/international24/article/details/89710703</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;昨天在为用户进行迁移后，用户Windows 2012系统上PostgreSQL服务无法启动，日志中提示“global/pg_control”：Permission denied，于是上网一顿搜索终于解决了这个问题。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="数据库" scheme="http://sunqi.site/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>传统应用架构与Serverless架构总体拥有成本(TCO)分析与比较</title>
    <link href="http://sunqi.site/2021/02/10/%E4%BC%A0%E7%BB%9F%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%8EServerless%E6%9E%B6%E6%9E%84%E6%80%BB%E4%BD%93%E6%8B%A5%E6%9C%89%E6%88%90%E6%9C%AC-TCO-%E5%88%86%E6%9E%90%E4%B8%8E%E6%AF%94%E8%BE%83/"/>
    <id>http://sunqi.site/2021/02/10/%E4%BC%A0%E7%BB%9F%E5%BA%94%E7%94%A8%E6%9E%B6%E6%9E%84%E4%B8%8EServerless%E6%9E%B6%E6%9E%84%E6%80%BB%E4%BD%93%E6%8B%A5%E6%9C%89%E6%88%90%E6%9C%AC-TCO-%E5%88%86%E6%9E%90%E4%B8%8E%E6%AF%94%E8%BE%83/</id>
    <published>2021-02-10T13:55:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>这篇报告是2019年9月德勤基于AWS Serverless发布的一篇白皮书，原文叫《Determining the Total<br>Cost of Ownership of Serverless Technologies when compared to Traditional Cloud》。本文就基于这篇文章提供的方法论，来分析一下Serverless模式的开发总成本。顺便比较一下国内各个云在Serverless架构下的成本。我们重点比较国内排名前三的云平台：阿里云、腾讯云和华为云。</p><p>在原文中包含两个案例，我们重点结合第一个案例云主机与Serverless的比较进行详细分析。</p><a id="more"></a><h1 id="如何对Serverless模式成本进行估算"><a href="#如何对Serverless模式成本进行估算" class="headerlink" title="如何对Serverless模式成本进行估算"></a>如何对Serverless模式成本进行估算</h1><p>Serverless模式是一种真正的将云平台的计算、存储和网络按需付费的云计算理想模式。之前的文章中我也提到过，Serverless最大的优势就是降低了基础架构层的可见性，即“零运维”。开发人员可以更关注业务逻辑的开发，而将繁重的运维工作交由云商负责。Serverless为应用带来了良好的可扩展性、敏捷性和弹性，缩短应用上线时间，提高发布频率，加速公司收入增长。</p><p>但是由于使用情况的不确定性，往往很难精准的估算Serverless的成本。在本文中对于Serverless TCO评估框架包含三个关键成本：基础设施、开发和维护。</p><h1 id="案例简述"><a href="#案例简述" class="headerlink" title="案例简述"></a>案例简述</h1><p>这是一家交通运输公司的需求，用户平均在途中的时间要花费两个多小时。交通公司提供了一个应用（可能是手机APP）包含以下功能：在线订票、连接WIFI、还可以实时监控自己行程。每年有数百万乘客、数百个目的地以及数千条线路，这些相乘之后需要一个并发量巨大的系统进行支撑。运输公司应对这种情况需要花费非常昂贵的成本，而且很难预测。这样也影响了与决策相关的数据得不到及时更新，造成机会成本的丧失。所以为了解决这个问题，这家公司在评估将订票系统运行在函数计算服务上还是EC2实例上。根据测算，这套系统需要满足每天150万笔交易。</p><h1 id="基础设施成本"><a href="#基础设施成本" class="headerlink" title="基础设施成本"></a>基础设施成本</h1><p>资源部分应该是最透明也是最容易比较的部分，只要根据应用运行的情况评估价格即可。</p><h2 id="传统方式"><a href="#传统方式" class="headerlink" title="传统方式"></a>传统方式</h2><p>这是原文中给出的系统资源消耗情况，因为没有更细节化的信息，所以只能根据价格进行反推。</p><table><thead><tr><th align="left">云资源名称</th><th>规格</th><th align="center">数量</th></tr></thead><tbody><tr><td align="left">云主机</td><td>m5.large(2vCPU/8G/200G)</td><td align="center">3个</td></tr><tr><td align="left">数据库</td><td>r5.large(2vCPU/16G/500G)</td><td align="center">3个</td></tr><tr><td align="left">云硬盘</td><td>500 IOPS SSD</td><td align="center">1GB</td></tr><tr><td align="left">负载均衡</td><td></td><td align="center">30 GB/小时</td></tr></tbody></table><p>原文案例中并没有改变给出具体的应用架构和构建细节，所以我们无法从技术层面深究架构合理性，所以只能更关注成本因素。</p><h2 id="Serverless模式"><a href="#Serverless模式" class="headerlink" title="Serverless模式"></a>Serverless模式</h2><p>Serverless是按照资源使用时间计费的，所以这里要根据使用时间计算价格。</p><table><thead><tr><th align="left">功能模块</th><th>规格</th><th align="center">执行时间</th><th>每月平均请求次数</th></tr></thead><tbody><tr><td align="left">售票响应函数</td><td>512 MB内存</td><td align="center">3000 ms</td><td>32,400,000</td></tr><tr><td align="left">数据处理</td><td>512 MB内存</td><td align="center">3000 ms</td><td>10,800,000</td></tr></tbody></table><h2 id="对比"><a href="#对比" class="headerlink" title="对比"></a>对比</h2><p>这是原文给出的资源消耗情况比对，但是有一些非常明显的错误，总计部分很明显的对不上。从原文中的描述可以得知这些信息：Webservers三台节点对应了下面处理票务的函数；而数据库加上存储对应了数据处理函数。这里我非常疑惑的一点：就算你把逻辑写在了函数计算里，你最终处理完的数据还是需要用持久化存储呀？所以我认为这个案例在描述上存在很大的纰漏，对于Serverless至少还应该包含一个数据库服务用于存储最终处理的数据。不过这里我们不纠结与这个问题，只从案例的成本分析进行对比。</p><p><img src="/images/pasted-155.png" alt="upload successful"></p><p>出乎意料的是，在这个请求量级上，使用函数计算服务在这个场景下反而要高于应用系统建立在云主机上。函数计算两个非常重要的计量指标：运行时间和请求次数，这两个指标决定了你的账单。</p><p>相同的情况也发生在Kubernetes服务中，我们之前在构建SaaS时，重点评估了阿里云Kubernetes托管版本(用两台云主机作为Worker, Master托管)和Serverless版本(Master和Worker节点均不见，底层直接使用阿里云ECI容器服务)，原本以为Serverless版本要比托管版本便宜，但是由于我们的程序多以Daemon方式运行，轮询任务很多，导致资源使用时间较长，最终的结果是Serverless版本的价格远远高于托管版本的价格。所以在应用Serverless架构时，要从这种架构的特点进行设计，降低上线后的成本。</p><h1 id="开发成本"><a href="#开发成本" class="headerlink" title="开发成本"></a>开发成本</h1><p>开发成本是评估Serverless架构总体成本第二个重要因素，如果使用传统方式开发，当客户量激增后，基础架构瓶颈的问题。当出现问题再去调整架构进行开发，会无形中错失很多机会成本，引起客户不满。如果在前期过度进行开发设计或者投入大量基础架构设施，又会导致前期成本的浪费。所以对于这种场景，Serverless灵活的扩展性就更加适合业务发展的需要，更好的满足客户的需求，同时又避免了资源的浪费。</p><p><img src="/images/pasted-156.png" alt="upload successful"></p><p>虽然云主机在一定程度上降低了传统运维成本，但是相较于Serverless架构，仍然需要去维护网络、安全、负载均衡、自动伸缩策略等。而使用Serverless架构时，由于采用的事件驱动，所以研发人员无须在前期设计阶段将过多精力研究系统健壮性，可以马上进入研发阶段。</p><p><img src="/images/pasted-157.png" alt="upload successful"></p><p>另外一部分成本来自于持久化集成，在传统方式下，DevOps流程往往需要经历多个阶段，从编译到制品库，从打包到上线中间过程相对较长。由于函数计算的特点，可以更快速的发布上线。测试人员也可以根据函数计算针对性的设计测试方法，合理的控制产品出现Bug的“爆炸半径”。</p><h1 id="运维成本"><a href="#运维成本" class="headerlink" title="运维成本"></a>运维成本</h1><p>运维上主要关注以下几个方面：扩展性、安全、补丁、监控验证测试等内容。每一个应用受到应用的范围和组织的影响，在运维成本上不同。但是，根据平均情况，使用传统云主机，运维人员每个月需要花费8-10小时在于加强安全性，额外每月需要40个小时用于监控、日志分析、验证和测试工作。而使用Serverless架构，运维的工作量进一步下降，真正可以做到开发运维一体化的效果，让Developer做Operation成为可能。</p><h1 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h1><p>在达到一定请求数量级时，Serverless架构在基础架构上要高于传统自建方式，但是在开发和维护成本上，Serverless要大幅度优于传统架构。在本案例中，整体成本预估节约40%的TCO。要从架构设计上对函数计算的特点有充分的认知，根据函数计算特点进行设计，才能最大化利用函数计算实现降本增效的结果。</p><p><img src="/images/pasted-158.png" alt="upload successful"></p><p>Serverless非常适合建设业务需求变化较快的业务场景，并且由于拥有一定数量免费额度，当请求量在每月百万级别时，使用Serverless的成本会更低甚至免费。</p><h1 id="参考：国内云平台资源比较"><a href="#参考：国内云平台资源比较" class="headerlink" title="参考：国内云平台资源比较"></a>参考：国内云平台资源比较</h1><p>这里所列的费用，均未包含流量费用。我们可以看到国内三大主流公有云的定价是出奇的一致，但从目录价格角度来说，三朵云基本是一样的。</p><table><thead><tr><th align="left">功能模块</th><th>阿里云</th><th>腾讯云</th><th>华为云</th></tr></thead><tbody><tr><td align="left">售票响应函数</td><td>5372.2964元</td><td>5395.818元</td><td>5396元</td></tr><tr><td align="left">数据处理</td><td>1760.3876元</td><td>1768.098元</td><td>1768元</td></tr></tbody></table><h2 id="链接"><a href="#链接" class="headerlink" title="链接"></a>链接</h2><ul><li>阿里云函数价格计算器：<a href="http://tools.functioncompute.com/?spm=5176.8663048.0.0.42f33edcdlsZnq#/price" target="_blank" rel="noopener">http://tools.functioncompute.com/?spm=5176.8663048.0.0.42f33edcdlsZnq#/price</a></li><li>腾讯云函数价格计算器：<a href="https://buy.cloud.tencent.com/price/scf/calculator?rid=8&amp;invokeCountUnit=1&amp;invokeDurationUnit=1&amp;publicNetOutTrafficUnit=1&amp;timestamp=0" target="_blank" rel="noopener">https://buy.cloud.tencent.com/price/scf/calculator?rid=8&amp;invokeCountUnit=1&amp;invokeDurationUnit=1&amp;publicNetOutTrafficUnit=1&amp;timestamp=0</a></li><li>华为云函数工作流（无计算功能）：<a href="https://www.huaweicloud.com/pricing.html?tab=detail#/function" target="_blank" rel="noopener">https://www.huaweicloud.com/pricing.html?tab=detail#/function</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇报告是2019年9月德勤基于AWS Serverless发布的一篇白皮书，原文叫《Determining the Total&lt;br&gt;Cost of Ownership of Serverless Technologies when compared to Traditional Cloud》。本文就基于这篇文章提供的方法论，来分析一下Serverless模式的开发总成本。顺便比较一下国内各个云在Serverless架构下的成本。我们重点比较国内排名前三的云平台：阿里云、腾讯云和华为云。&lt;/p&gt;
&lt;p&gt;在原文中包含两个案例，我们重点结合第一个案例云主机与Serverless的比较进行详细分析。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用阿里云VPN服务实现HyperMotion SaaS私有云迁移</title>
    <link href="http://sunqi.site/2021/02/10/%E5%88%A9%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91VPN%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0HyperMotion-SaaS%E7%A7%81%E6%9C%89%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    <id>http://sunqi.site/2021/02/10/%E5%88%A9%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91VPN%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0HyperMotion-SaaS%E7%A7%81%E6%9C%89%E4%BA%91%E8%BF%81%E7%A7%BB/</id>
    <published>2021-02-10T02:35:16.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>目前云原生迁移平台HyperMotion SaaS主要应用场景在公有云上，但是在我们平时的测试场景中，由于上行带宽的限制，每次向公有云同步比较消耗时间，特别是在验证启动流程时，需要等待半天到一天的时间进行数据同步，非常不划算。在我们内部环境中，我们经常测试的一种场景是从VMWare迁移到私有化部署的OpenStack上。但是由于网络的限制不可能将OpenStack及Floating IP资源在公网上一一映射（如果是客户场景，通常是私有化部署的HyperMotion解决）。那么，是否可以将线上VPC与本地的机房网络环境利用VPN隧道打通，实现利用HyperMotion SaaS进行私有云环境的迁移呢？本文就为你分享利用阿里云VPN服务实现上述场景的需求。</p><a id="more"></a><h1 id="需求与场景分析"><a href="#需求与场景分析" class="headerlink" title="需求与场景分析"></a>需求与场景分析</h1><p>HyperMotion SaaS是部署在阿里云Kubernetes托管版集群中，即Kubernetes Master节点由阿里云负责，阿里云为我们在指定VPC内启动了两台ECS实例作为Worker节点。在我们自身需求中，需要解决两个流量问题：</p><ul><li>控制流：HyperMotion SaaS每个租户可以添加指定的目标云平台，HyperMotion SaaS后台模块通过VPC关联的NAT网关访问云平台API接口及资源，但是如果添加的是我们内部的OpenStack，则需要SaaS侧与OpenStack控制网络想通；另外HyperMotion会自动利用云平台的云主机资源安装云存储网关，所以也需要访问OpenStack Floating IP的地址（具体看云平台规划，也许是Fixed IP）。</li><li>数据流：在数据层面上，我们仍然希望数据层面通过内网传输，没有必要将数据流入公网，好在HyperMotion SaaS的设计满足了这样的需求</li></ul><p><img src="/images/pasted-133.png" alt="upload successful"></p><p>所以在这个解决方案中，重点是利用阿里云VPN网关和本地打通后（前提是公司出口路由有固定的公网IP），通过合理的设置路由规则实现我们上述的需求。</p><p>注意：文章中使用的截图并非全部都是真实截图，所以在实际配置过程中要根据实际情况进行。</p><h1 id="配置流程"><a href="#配置流程" class="headerlink" title="配置流程"></a>配置流程</h1><p>配置过程中主要涉及阿里云VPN服务和H3C路由器，基本流程如下：</p><ul><li>1、阿里云建立VPN网关，这个最低购买力度是包月</li><li>2、拿到阿里云VPN网关后，在路由器上进行相关配置</li><li>3、回到阿里云配置用户网关及IPsec连接，查看连接是否成功</li><li>4、阿里云侧路由设置</li></ul><h1 id="1、阿里云VPN网关配置"><a href="#1、阿里云VPN网关配置" class="headerlink" title="1、阿里云VPN网关配置"></a>1、阿里云VPN网关配置</h1><p>VPN需要关联到VPC和交换机上，根据带宽的不同，价格也不同，最低是按照包1个月5 Mbps。</p><p><img src="/images/pasted-147.png" alt="upload successful"></p><p>配置好后，会得到一个公网IP，这个公网IP需要在后续配置到路由器上。</p><p><img src="/images/pasted-134.png" alt="upload successful"></p><h1 id="2、H3C路由器设置"><a href="#2、H3C路由器设置" class="headerlink" title="2、H3C路由器设置"></a>2、H3C路由器设置</h1><p>目前我们机房使用的路由器属于非常入门级的企业级路由器（H3C ER3200G2），但是基本能满足我们的需求了，并且支持IPsec VPN方式。之前一直很惧怕配置IPsec VPN，相较于L2TP等简单方案，配置起来太复杂了。但是经过几次折腾，也基本摸清楚是怎么回事了，真是应了那句话：人类的恐惧来自于无知。</p><p>我并不是网络方面的专家，也对IPsec原理没什么研究，我只想记录一下我是怎么配置的。我认为IPsec在配置的时候，最重要的一点是两头配置一样，无法连接往往是由于配置信息不一致导致的。这是一张原理图，加深我们对配置过程的理解。</p><p><img src="/images/pasted-143.png" alt="upload successful"></p><p>H3C配置的基本流程为：虚接口-&gt;IKE安全提议-&gt;IKE对等体-&gt;IPsec安全提议-&gt;IPsec安全策略。</p><h2 id="2-1、虚接口配置"><a href="#2-1、虚接口配置" class="headerlink" title="2.1、虚接口配置"></a>2.1、虚接口配置</h2><p>虚接口应该是定义与外界互连的通道，配置很简单，只要指明对外服务的接口（比如：WAN1）就可以了。</p><p><img src="/images/pasted-134.png" alt="upload successful"></p><h2 id="2-2、IKE安全提议"><a href="#2-2、IKE安全提议" class="headerlink" title="2.2、IKE安全提议"></a>2.2、IKE安全提议</h2><p>IKE是因特网密钥交换的缩写(Internet Key Exchange)，从名字上可以猜出这与互联网进行交换数据时加密有关。验证算法和加密算法一定要与对端配置一致，关于DH组，每一个平台选项不一样，比如截图中叫DH2 modp1024，到了阿里云就叫做group2了，所以也必须要配置一致。</p><p><img src="/images/pasted-139.png" alt="upload successful"></p><p>阿里云侧DH组选项</p><p><img src="/images/pasted-140.png" alt="upload successful"></p><h2 id="2-3、IKE对等体"><a href="#2-3、IKE对等体" class="headerlink" title="2.3、IKE对等体"></a>2.3、IKE对等体</h2><p>和对端的VPN网关进行连接，对端IP是需要首先在对端建立VPN网关后，会得到相应的地址，填入即可。</p><p><img src="/images/pasted-141.png" alt="upload successful"></p><p>协商模式上，阿里云的配置是英文的，主模式叫做main，而野蛮模式被称为aggresive。</p><p><img src="/images/pasted-142.png" alt="upload successful"></p><p>共享密钥是自定义的，两端必须一致，DPD阿里云默认是开启的，而H3C上是关闭的，保持统一即可。</p><h2 id="2-4、IPsec安全提议"><a href="#2-4、IPsec安全提议" class="headerlink" title="2.4、IPsec安全提议"></a>2.4、IPsec安全提议</h2><p>按照我粗浅的认知，IKE主要负责两端连接，同时简化了IPsec交互，而真正的数据交互还是要在IPsec上进行控制。所以要对IPsec也要进行相应的安全配置。安全协议类型，我们选择了默认的ESP，阿里云侧默认也应该采用的是此协议。在配置对端时，仍然是保持一致即可。</p><p><img src="/images/pasted-144.png" alt="upload successful"></p><h2 id="2-5、IPsec安全策略"><a href="#2-5、IPsec安全策略" class="headerlink" title="2.5、IPsec安全策略"></a>2.5、IPsec安全策略</h2><p>这一步最关键的是本地子网IP和对端子网IP及掩码的设置，双方是相反的，如果本地是192.168.0.0/24，源端是172.16.0.0/24。则在阿里云侧的配置就是本地是172.16.0.0/24，远端是192.168.0.0/24。</p><p><img src="/images/pasted-145.png" alt="upload successful"></p><p>还有一个就是PFS的设置，和IKE的DH组是一样的，在阿里云侧也被称为IPsec的DH组。也必须设置一致。</p><p><img src="/images/pasted-146.png" alt="upload successful"></p><h1 id="3、阿里云IPsec连接配置"><a href="#3、阿里云IPsec连接配置" class="headerlink" title="3、阿里云IPsec连接配置"></a>3、阿里云IPsec连接配置</h1><h2 id="3-1、用户网关设置"><a href="#3-1、用户网关设置" class="headerlink" title="3.1、用户网关设置"></a>3.1、用户网关设置</h2><p>用户网关设置比较简单，只要在阿里云测配置你路由的公网IP即可。</p><p><img src="/images/pasted-149.png" alt="upload successful"></p><h2 id="3-2、IPsec连接"><a href="#3-2、IPsec连接" class="headerlink" title="3.2、IPsec连接"></a>3.2、IPsec连接</h2><p>这是最关键的一步，经常在这一步配置失败，提示在第一阶段或者第二阶段失败，目前在我遇到的情况中，基本都是上述配置不一致导致的。配置过程基本分为三个阶段，基本配置、高级配置中的IKE配置和IPsec配置。</p><h3 id="3-2-1-基本配置"><a href="#3-2-1-基本配置" class="headerlink" title="3.2.1 基本配置"></a>3.2.1 基本配置</h3><p>注意图中标出的本端网络、对端网络和预共享密钥的配置，一定要填对。</p><p><img src="/images/pasted-150.png" alt="upload successful"></p><h3 id="3-2-2-IKE配置"><a href="#3-2-2-IKE配置" class="headerlink" title="3.2.2 IKE配置"></a>3.2.2 IKE配置</h3><p>点开下方的高级设置，能够看到IKE和IPsec设置。</p><p>配置只要按照我们在H3C的配置选择相应的内容即可，LocalId和RemoteId都是自动根据VPN填写的，并不需要输入。</p><p><img src="/images/pasted-151.png" alt="upload successful"></p><h3 id="3-2-3-IPsec配置"><a href="#3-2-3-IPsec配置" class="headerlink" title="3.2.3 IPsec配置"></a>3.2.3 IPsec配置</h3><p>图中标注的选项一定要保持一致，提交配置后，等待连接。</p><p><img src="/images/pasted-152.png" alt="upload successful"></p><h2 id="3-3、查看连接状态"><a href="#3-3、查看连接状态" class="headerlink" title="3.3、查看连接状态"></a>3.3、查看连接状态</h2><p>如果连接状态为第二阶段协商成功，就证明VPN已经建立成功，否则请检查配置，多半是由于配置不一致导致的。</p><p><img src="/images/pasted-153.png" alt="upload successful"></p><h1 id="4、设置路由"><a href="#4、设置路由" class="headerlink" title="4、设置路由"></a>4、设置路由</h1><p>我们次此设置路由的目的是为了阿里云侧能够访问我们内网，所以接下来需要在阿里云VPC内设置路由表，当访问我们的内网时，需要使用VPN网关。进入VPC服务的路由表配置中，找到VPC。将目标IP段吓一跳设置为VPN网关。因为阿里云的ACL还处于内测阶段，所以暂时无须考虑ACL的设定。</p><p><img src="/images/pasted-154.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>网络对于未来混合云的场景有至关重要的作用，本文重点描述的是以VPN方式来打通云上和云下环境，但是VPN最大的带宽规格只有200 Mbps，如果真实的需求更大，则需要考虑云联网，通过运营商底层基础设施，实现不同云之间的互联互通。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前云原生迁移平台HyperMotion SaaS主要应用场景在公有云上，但是在我们平时的测试场景中，由于上行带宽的限制，每次向公有云同步比较消耗时间，特别是在验证启动流程时，需要等待半天到一天的时间进行数据同步，非常不划算。在我们内部环境中，我们经常测试的一种场景是从VMWare迁移到私有化部署的OpenStack上。但是由于网络的限制不可能将OpenStack及Floating IP资源在公网上一一映射（如果是客户场景，通常是私有化部署的HyperMotion解决）。那么，是否可以将线上VPC与本地的机房网络环境利用VPN隧道打通，实现利用HyperMotion SaaS进行私有云环境的迁移呢？本文就为你分享利用阿里云VPN服务实现上述场景的需求。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>基于Serverless架构进行应用开发</title>
    <link href="http://sunqi.site/2021/02/06/%E5%9F%BA%E4%BA%8EServerless-Framework%E8%BF%9B%E8%A1%8C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/"/>
    <id>http://sunqi.site/2021/02/06/%E5%9F%BA%E4%BA%8EServerless-Framework%E8%BF%9B%E8%A1%8C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/</id>
    <published>2021-02-06T01:01:03.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Serverless"><a href="#什么是Serverless" class="headerlink" title="什么是Serverless"></a>什么是Serverless</h1><p>从过去20年IT基础架构层的发展过程来看，计算、存储和网络三种基础资源得到了不断的发展和抽象。从物理机到虚拟化，从虚拟化到云计算，再从云计算到云原生，孕育出诸多新生概念，无论如何变化，一定是向着有利于业务创新方向发展，开发人员越来越不需要关注底层的基础架构。</p><p>云原生本身是一个非常广义的概念，主要包含：微服务架构，应用容器化、Serverless化以及敏捷的软件交付流程。所以很多人在谈论云原生时与Kubernetes划等号，是非常片面的。</p><a id="more"></a><p>今天我们重点来说说Serverless，这种技术的本质并不是真的“无服务器”，真正的目的是要帮助应用开发者摆脱应用程序后端所需的服务器资源的管理和运维工作。我们在设计一款产品或应用时，往往要从不同的唯独考虑产品。<br>从开发与测试角度更多的考虑的是开发语言、框架的选择、数据库、如何进行压力测试等；而从实施和运维角度考虑则要考虑环境怎么构建、高可靠如何实现，如何升级、如何扩容等诸多基础架构的问题。从Serverless架构看，更希望你专注于你的开发和测试，而将实施和运维彻底交给云来解决，前提是你需要遵循Serverless架构。</p><p>在构建Serverless架构中，通常包含两种常用的服务类型：</p><p>1、后端即服务（Backend-as-a-Servce，BaaS），例如：数据库、消息队列、身份验证由云商直接以服务方式提供的服务<br>2、函数计算（Function-as-a-Service），主要将业务逻辑代码运行在其中，函数计算通常运行在容器环境内，按照CPU和内存的运行时间来计费。函数计算触发往往与时间相关，例如：定时器、HTTP请求。目前各家云商云商都提供了函数计算服务，而这方面的佼佼者无疑是2014年11月就推出的AWS Lambda服务，在很多AWS的最佳实践中都能看到巧妙利用函数计算服务来解决业务架构问题。</p><p>我们在开发一个业务系统时，通常采用传统的架构思想来规划系统建设。但是随着云原生技术的发展，除了逐步打破传统的运维与开发之间的关系，我们的开发架构也随之发生了改变。未来的应用开发架构，让开发、测试与运维的边界越来越模糊，应用开发的迭代速度进一步提升。</p><p>当然Serverless并不是万能的，很多劣势无法满足所有的场景需求，但是随着新技术的不断迭代，一定会有新的技术出现来填补这些空白。</p><h1 id="从业务视角看Serverless"><a href="#从业务视角看Serverless" class="headerlink" title="从业务视角看Serverless"></a>从业务视角看Serverless</h1><p>记得是在2020年12月的微信小程序峰会上一个分享中看到这一组数据，给我的震撼很大。这是一家专门依托于微信小程序从事线上娱乐化社交电商社区。我们从图中数据可以看到1-10月份销售数据为23,909,022.69元，销售在14万笔。</p><p><img src="/images/pasted-136.png" alt="upload successful"></p><p>那么如果是一个传统电商平台，承载这样的销量需要付出多少资源的代价呢？我们来看看这家公司的运营数据。是的，你没看错仅仅是3000元，而研发人力投入仅仅不到10个人。</p><p><img src="/images/pasted-137.png" alt="upload successful"></p><p>2020年双十一销量数据为2,194,203元，而基础架构层为此付出的额外费用仅为10元钱。</p><p><img src="/images/pasted-138.png" alt="upload successful"></p><p>从交易数据看，虽然从并发性上远不及天猫这样每秒几十万的交易量。但是如果从性价比(销量/基础架构投入)看，这样的数据绝对是可以各位同行参考的。为什么可以得到这样惊人的数据，这离不开以Serverless为理念的云开发。</p><h1 id="从开发者视角看Serverless"><a href="#从开发者视角看Serverless" class="headerlink" title="从开发者视角看Serverless"></a>从开发者视角看Serverless</h1><p>这是一张云开发自身发展的版图，这张图还是与厂商利益之间进行了深度绑定，不过我们重点从技术角度去分析一下。Serverless架构的发展主要集中在平台能力和基础能力两个方面，当然扩展能力也很重要，我们也可以将这些归属为平台能力层，并且可以是多云。这些插件能够给我们应用提供更多的想象空间，例如最近大火的Clubhouse，就是利用了中国声网提供的服务。</p><p><img src="/images/pasted-135.png" alt="upload successful"></p><p>去年的时候我曾经专门录制过阿里云的函数计算课程( <a href="https://edu.51cto.com/course/22144.html" target="_blank" rel="noopener">https://edu.51cto.com/course/22144.html</a> )，在这个课程里，我更多的是将函数计算作为串联云原生服务的纽带进行讲解。但是随着Servless开发框架越来越成熟，函数计算在构建应用的地位发生了变化，上述提到的云开发就是一个典型。通过微信这个入口，快速支撑了业务发展的需要，在2020年初紧急的开发的健康码就是利用了这样的特性实现。</p><p>我们设计一款全新的应用架构时，要从云开发能够提供的整体能力角度出发进行思考。简单说，Serverless框架的核心在于围绕着函数计算来设计业务逻辑，通过使用各个云原生服务的能力，满足业务上的需求。Serverless架构的设计更多的是在改变原有的开发框架和开发模式，将原有以架构为核心的代码组织形式打散在各个函数中。将原有架构层需要考虑的并发、高可用等完全交由底层来支撑，开发可以更专注于业务本身。</p><p>那么构建以函数计算为核心的Serverless框架应用时，应当注意哪些问题呢？</p><p>1、函数计算需要事件驱动，例如一个HTTP请求，或者一个上传对象存储的行为都会产生事件，而这些事件产生都是云原生的，也是最及时的。以最常见的WEB类应用为例，基本就是前端及后端对数据库各种CURD的组合实现。前端的静态文件可以考虑使用对象存储，再使用CDN进行加速，通过API网关来驱动后端的函数计算，如果有需要持久化存储的数据可以选择NAS或对象存储服务。通过这套架构可以很轻松的实现一个高并发的业务系统。<br>2、虽然Serverless架构看上去很美好，但是仍然会面临很多挑战。性能问题就是其中之一，因为函数计算是触发式启动，在初始阶段和并发激增的情况下响应请求时会很慢。所以在实际开发过程中，除了要合理优化自己的初始化代码逻辑外，还要结合性能监控指标，合理利用预留实例的功能，达到性能与价格的最优。</p><p><img src="/images/pasted-132.png" alt="upload successful"></p><p>3、那么用户现有的业务是否有必要改造为Serverless架构呢？我觉得应该取决于需求，因为Serverless的开发模式决定了这个改造一定会产生时间和人力投入的成本。首先研发人员要学习Serverless的理念，熟悉开发模式，梳理出当前系统改造的方式，甚至要重构部分代码。这个过程往往要高于容器改造的成本，但是小于微服务改造的成本。</p><p>4、研发管理问题也是在应用Serverless应用中需要考虑的问题，从代码结构如何组织，到如何进行上线前的测试，再到如何优化原有持久化集成的流程，都对研发管理提出了挑战。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>Serverless架构为应用开发带来了新的活力，让研发人员能够更加的专注于业务逻辑的开发工作。同时，让企业的总体拥有成本（TCO）降低，但是提供服务的能力和灵活度大幅度提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;什么是Serverless&quot;&gt;&lt;a href=&quot;#什么是Serverless&quot; class=&quot;headerlink&quot; title=&quot;什么是Serverless&quot;&gt;&lt;/a&gt;什么是Serverless&lt;/h1&gt;&lt;p&gt;从过去20年IT基础架构层的发展过程来看，计算、存储和网络三种基础资源得到了不断的发展和抽象。从物理机到虚拟化，从虚拟化到云计算，再从云计算到云原生，孕育出诸多新生概念，无论如何变化，一定是向着有利于业务创新方向发展，开发人员越来越不需要关注底层的基础架构。&lt;/p&gt;
&lt;p&gt;云原生本身是一个非常广义的概念，主要包含：微服务架构，应用容器化、Serverless化以及敏捷的软件交付流程。所以很多人在谈论云原生时与Kubernetes划等号，是非常片面的。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>某股权交易中心业务迁移阿里云最佳实践</title>
    <link href="http://sunqi.site/2021/02/05/%E6%9F%90%E8%82%A1%E6%9D%83%E4%BA%A4%E6%98%93%E4%B8%AD%E5%BF%83%E4%B8%9A%E5%8A%A1%E8%BF%81%E7%A7%BB%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <id>http://sunqi.site/2021/02/05/%E6%9F%90%E8%82%A1%E6%9D%83%E4%BA%A4%E6%98%93%E4%B8%AD%E5%BF%83%E4%B8%9A%E5%8A%A1%E8%BF%81%E7%A7%BB%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-02-05T09:35:56.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h1><p>某股权交易中心是在深圳地区建设的市场化运作的区域性交易市场。由于业务发展需要，用户需要将主要业务全面上云。最终用户选择使用万博智云HyperMotion将业务系统迁移至阿里公有云平台，在保障用户业务连续性的前提下，实现业务系统全面上云。</p><h1 id="客户面临的挑战"><a href="#客户面临的挑战" class="headerlink" title="客户面临的挑战"></a>客户面临的挑战</h1><p>用户原有业务系统运行在运营商机房内，该机房将整体清退，主要应用系统已切换至备用机房。但备用机房规模较小、总体运维成本较高。由于公司发展需要，运维团队规模由原来的十人缩减为一人。为满足业务快速发展以及系统业务连续性要求，提升整体运维效率，计划将主要生产系统由本地机房迁移至阿里云。</p><p>用户采用传统的VMware与存储阵列的经典组合，业务系统由100多台虚拟机、10+TB数据量构成，用户需要将这100多台VMware虚拟机平滑的迁移至阿里云平台。</p><a id="more"></a><h1 id="为什么选择阿里云"><a href="#为什么选择阿里云" class="headerlink" title="为什么选择阿里云"></a>为什么选择阿里云</h1><p>用户希望将业务系统部署在领先的、高水准的云平台上，并且一定要求云平台为国产化自主可控的公有云平台。实现“一步到位”的信息化建设高起点，同时作为金融交易平台，用户需要得到全方位的技术保障和极为可靠的安全性和稳定性。</p><p>本次在竞标主要竞争是在阿里云和另外一家云商之间展开，由于是金融级别客户，用户重点从【安全产品】和【业务迁移】两个维度对两朵公有云产品及服务能力进行了深度评估。</p><p>经过深度评估，选定阿里云安骑士的高配版本和万博智云HyperMotion云迁移为解决方案。</p><h1 id="为什么选择万博智云"><a href="#为什么选择万博智云" class="headerlink" title="为什么选择万博智云"></a>为什么选择万博智云</h1><p>万博智云是国内最早且目前最优的云原生迁移工具研发的公司，通过与阿里云API接口及云原生资源高度自动化对接，将迁移缩减为简单的三步，满足用户高度自动化、智能化迁移需求。</p><p>同时通过迁移演练能力，满足了用户在切换至云端前从业务维度对系统进行多次生产演练验证的需求，实现了【灾备演练式的渐进迁移体验】。</p><p>通过HyperMotion云迁移工具，实现了：</p><ul><li>业务连续性迁移</li><li>批量/高效/全程可视化迁移</li><li>迁移后IP地址不变</li><li>上云前多次业务级别演练</li></ul><p>充分保障了用户业务上云后的连续性和可靠性。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="网络解决方案"><a href="#网络解决方案" class="headerlink" title="网络解决方案"></a>网络解决方案</h2><p>用户业务上云后，期望保持与原有数据中心内业务系统IP地址保持一致，这就要求在公有云侧VPC需要使用与原有数据中心相同的IP地址规划。同时在上云后，由于云上的地址通常采用DHCP方式进行分配，这就要求在主机启动前就要将端口及IP进行分配，确保在云端启动的主机获得与原有业务系统完全一样的IP地址。</p><p>在该项目中，我们采用【阿里云云联网服务】将用户原有机房网络与云上VPC进行打通，为了避免地址冲突，用户进行数据同步的网络采用单独的地址段，在启动时，通过【HyperMotion指定IP地址】方式进行启动。</p><h2 id="业务连续性"><a href="#业务连续性" class="headerlink" title="业务连续性"></a>业务连续性</h2><p>根据迁移方法论中提到“6R”理论，【重新托管（Re-Host）】方式是上云的最短最高效路径，同时也是对用户原有业务影响最小的方案。</p><p>HyperMotion采用了块级别同步复制技术来实现“热迁移”：</p><ul><li>★源端无代理模式→在源端VMware环境下无入侵操作（不安装agent），对用户业务侧影响几乎为0，实现用户源端业务零停机或者少停机下实现业务系统上云的效果。</li><li>块级别数据的整体复制→用户的操作系统、应用、数据一起被同步到目标侧，无论是WEB应用、数据库或者中间件，都可以通过这种方式完整的迁移至云端，无需针对单独文件或者数据进行操作和配置。</li><li>★异构平台适配技术→通过异构平台智能适配转换驱动，实现跨平台无缝迁移</li><li>★云原生能力→调度云侧API以及逻辑流程，无须繁琐的人为操作，实现高度自动化的用户体验。</li><li>★灾备演练式渐进迁移</li><li>业务系统上云验证是迁移上云前最后一道防线，在云侧完整的对业务系统进行验证是最准确和有效的手段。在传统容灾场景中，通常以定期的灾备演练方式来保证灾备的可用性。</li></ul><p>在HyperMotion中创新的提供了【★迁移演练的能力】，方便用户在上云前通过灾备演练式的体验进行业务系统切换前的校验。HyperMotion通过对阿里云API进行深度整合，以全自动化的方式解决了主机启动、驱动修复、网络修复等多种上云后复杂的人为操作。同时HyperMotion还可以通过指定IP地址启动方式，保证业务系统与源端地址一致。迁移验证系统启动后，并不影响源端业务运行，同时增量数据可以继续同步至云侧。</p><p>本案例中充分发挥了该功能的优势，最大程度满足客户严苛的业务迁移需求。</p><h2 id="成本"><a href="#成本" class="headerlink" title="成本"></a>成本</h2><p>成本因素是迁移到公有云必须要考虑的问题之一，从开始的业务系统同步到迁移演练到最终的迁移上云，均涉及到资源的成本支出，如果无法做到合理的使用云原生资源就会造成大量的浪费。</p><p>HyperMotion创新的采用了云同步网关的概念，实现了多对一的方式进行同步。数据同步阶段，只利用较少的计算资源，而将数据存储于云硬盘中，降低成本消耗。真正的业务主机只在验证或最终切换阶段启动，实现成本最优的效果。</p><p>客户实际在迁移过程中耗时大概在两个月时间，期间将一百多台多台主机拉起进行了三次验证，每次在业务部门确认后，清理掉资源。最终在第四次拉起后，将业务负载全部切换至云侧。目前用户业务系统已经稳定超过半年以上时间。</p><p>以下是我们就该项目中实际消耗的资源&amp;成本 与 备选方案测算进行的比对：</p><p>用户【原有系统资源】统计：</p><p><img src="/images/pasted-130.png" alt="upload successful"></p><p>迁移【中间资源成本】对比：</p><p>项目迁移周期耗时两个月，通过对阿里云账单进行分析，统计出在迁移中间资源、成本、时间如下：</p><p><img src="/images/pasted-131.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;项目概述&quot;&gt;&lt;a href=&quot;#项目概述&quot; class=&quot;headerlink&quot; title=&quot;项目概述&quot;&gt;&lt;/a&gt;项目概述&lt;/h1&gt;&lt;p&gt;某股权交易中心是在深圳地区建设的市场化运作的区域性交易市场。由于业务发展需要，用户需要将主要业务全面上云。最终用户选择使用万博智云HyperMotion将业务系统迁移至阿里公有云平台，在保障用户业务连续性的前提下，实现业务系统全面上云。&lt;/p&gt;
&lt;h1 id=&quot;客户面临的挑战&quot;&gt;&lt;a href=&quot;#客户面临的挑战&quot; class=&quot;headerlink&quot; title=&quot;客户面临的挑战&quot;&gt;&lt;/a&gt;客户面临的挑战&lt;/h1&gt;&lt;p&gt;用户原有业务系统运行在运营商机房内，该机房将整体清退，主要应用系统已切换至备用机房。但备用机房规模较小、总体运维成本较高。由于公司发展需要，运维团队规模由原来的十人缩减为一人。为满足业务快速发展以及系统业务连续性要求，提升整体运维效率，计划将主要生产系统由本地机房迁移至阿里云。&lt;/p&gt;
&lt;p&gt;用户采用传统的VMware与存储阵列的经典组合，业务系统由100多台虚拟机、10+TB数据量构成，用户需要将这100多台VMware虚拟机平滑的迁移至阿里云平台。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用腾讯云开发免费搭建个人博客</title>
    <link href="http://sunqi.site/2021/02/02/%E5%88%A9%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E5%BC%80%E5%8F%91%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://sunqi.site/2021/02/02/%E5%88%A9%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E5%BC%80%E5%8F%91%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-02-02T23:03:10.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>之前一直使用Github Pages搭建个人博客，随着Github访问越来越困难，个人博客国内访问速度越来越慢。之前也试过用cloudflare进行加速，但是收效甚微，所以才考虑将项目迁回到国内。一个偶然的机会，腾讯云开发进入我的视野，起因是他们的9.9元计划，不过后来由于我配置错误，这部分资源无法使用。但是经过研究，原来腾讯云开发提供了最基础的免费资源，恰好可以让我们搭建个人的Blog，经过将近一年多的使用过程中，非常好用，所以就记录下来，供大家参考。</p><a id="more"></a><h1 id="什么是腾讯云开发"><a href="#什么是腾讯云开发" class="headerlink" title="什么是腾讯云开发"></a>什么是腾讯云开发</h1><p>腾讯云开发是我一种非常推崇的开发理念，简单来说就是将Serverless进行了进一步封装，为开发者提供了更便捷的开发体验。目前云开发将轻量级业务中常见的数据库、存储（包括文件存储、对象存储）、云函数（计算资源）、基础运维（告警、监控、日志）进行了整合，同时云开发与微信小程序之间有个非常紧密的整合，能够快速帮助微信小程序构建服务端程序，基本可以承载很多基于微信场景的业务开发，比如电商等应用的开发，并且基于这样的基础架构，支撑千万级并发的需要。</p><p><img src="/images/pasted-126.png" alt="upload successful"></p><p>目前腾讯云开发，主要在广州和上海区域提供服务。</p><p><img src="/images/pasted-125.png" alt="upload successful"></p><p>腾讯云开发目前提供用户可以创建一个免费的环境，其中包含了存储、数据库等免费资源，但是相对于博客场景，主要还是静态资源的托管，每个月有1GB的存储空间，和5GB/月的流量，如果资源不够还可以购买额外的资源包。</p><p>更多的免费额度请参考<a href="https://cloud.tencent.com/document/product/876/47816" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/876/47816</a></p><p><img src="/images/pasted-127.png" alt="upload successful"></p><h1 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h1><p>先说一下整体的构建思路：</p><ul><li>我们的博客源代码仍然托管在github上，这样不需要破坏现有逻辑</li><li>如果你有自己的域名，最好申请备案，因为云开发在绑定域名的时候必须要求已备案的域名，但是如果你就不想备案，也有一个Work Around方法，就是通过cloudflare进行跳转的方式实现了，后面会简单介绍</li><li>通过Travis CI自动构建，并上传至云开发中，这样就实现我们在提交代码后，自动进行博客发布的效果了</li></ul><h1 id="云开发购买"><a href="#云开发购买" class="headerlink" title="云开发购买"></a>云开发购买</h1><p>云开发购买的过程，这里不再赘述了，只需要在新建时选择免费资源即可。因为我已经购买过资源了，所以提示我再次购买。</p><p><img src="/images/pasted-128.png" alt="upload successful"></p><p>成功构建后，你会得到这样的环境id，这个id作为你后续使用cli命令行更新环境的参数使用。</p><p><img src="/images/pasted-129.png" alt="upload successful"></p><h1 id="Travis-CI配置文件"><a href="#Travis-CI配置文件" class="headerlink" title="Travis CI配置文件"></a>Travis CI配置文件</h1><p>相信很多人都使用Travis CI构建自己的Github Pages，确实非常方便，虽然Github也提供了自己的CI工具，但是我依然保留着使用Travis CI的习惯。我们无须调整之前的Github Pages的配置或者策略，只需要在你的master分支下，增加或者修改你的.travis.yml即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sudo: false</span><br><span class="line">language: node_js</span><br><span class="line">node_js:</span><br><span class="line">  - 10 # use nodejs v10 LTS</span><br><span class="line">cache: npm</span><br><span class="line">branches:</span><br><span class="line">  only:</span><br><span class="line">    - master # build master branch only</span><br><span class="line">before_install:</span><br><span class="line">  - npm i -g @cloudbase&#x2F;cli</span><br><span class="line">  - git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;JoeyBling&#x2F;hexo-theme-yilia-plus.git themes&#x2F;yilia-plus</span><br><span class="line">after_success:</span><br><span class="line">  - cloudbase login --apiKeyId $TECENT_AK --apiKey $TECENT_KS</span><br><span class="line">  - cd public &amp;&amp; echo &#39;y&#39; | tcb hosting deploy -e your-env-id</span><br><span class="line">script:</span><br><span class="line">  - hexo generate # generate static files</span><br><span class="line">deploy:</span><br><span class="line">  repo: xiaoquqi&#x2F;xiaoquqi.github.io</span><br><span class="line">  target_branch: master</span><br><span class="line">  provider: pages</span><br><span class="line">  skip-cleanup: true</span><br><span class="line">  github-token: $GH_TOKEN</span><br><span class="line">  keep-history: true</span><br><span class="line">  on:</span><br><span class="line">    branch: master</span><br><span class="line">  local-dir: public</span><br></pre></td></tr></table></figure><p>在新的配置文件中，我保留了之前deploy到Github Pages的逻辑，主要增加的逻辑是在before_install开始前，安装cloudbase的cli。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">before_install:</span><br><span class="line">  - npm i -g @cloudbase&#x2F;cli</span><br></pre></td></tr></table></figure><p>在hexo generate成功后，增加部署的命令，这里需要在Travis CI中配置腾讯云的鉴权环境变量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">after_success:</span><br><span class="line">  - cloudbase login --apiKeyId $TECENT_AK --apiKey $TECENT_KS</span><br><span class="line">  - cd public &amp;&amp; echo &#39;y&#39; | tcb hosting deploy -e your-env-id</span><br></pre></td></tr></table></figure><p>目前cloudbase cli(简写：tcb)，有一个问题，如果超过1000个文件上传会有个提示，导致Travis CI认为没有返回任务失败，但是实际上已经提交上去了，这里已经给腾讯团队提交了一个需求，在cloudbase cli中增加一个force-yes的选项。</p><p>这样我们在提交代码后，就可以实现在腾讯云开发中自动发布我们博客的效果了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前一直使用Github Pages搭建个人博客，随着Github访问越来越困难，个人博客国内访问速度越来越慢。之前也试过用cloudflare进行加速，但是收效甚微，所以才考虑将项目迁回到国内。一个偶然的机会，腾讯云开发进入我的视野，起因是他们的9.9元计划，不过后来由于我配置错误，这部分资源无法使用。但是经过研究，原来腾讯云开发提供了最基础的免费资源，恰好可以让我们搭建个人的Blog，经过将近一年多的使用过程中，非常好用，所以就记录下来，供大家参考。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CentOS7 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T09:15:02.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：</p><p><img src="/images/pasted-124.png" alt="upload successful"></p><p>原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。</p><p>CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。</p><a id="more"></a><h1 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h1><p>虽然通过yum方式可以安装zsh，但是无法满足powerlevel10k的要求，所以先使用zsh源码进行编译后安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">curl -o zsh.tar.xz https:&#x2F;&#x2F;jaist.dl.sourceforge.net&#x2F;project&#x2F;zsh&#x2F;zsh&#x2F;5.8&#x2F;zsh-5.8.tar.xz</span><br><span class="line">tar -xvf zsh-5.8.tar.xz</span><br><span class="line"></span><br><span class="line">cd zsh</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>zsh会安装在用户目录中/usr/local/bin/zsh中，将zsh设置为默认的系统shell，配置成功后，需要关闭Terminal重新登陆。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chsh -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;zsh</span><br></pre></td></tr></table></figure><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>接下来的流程与MacOS上安装类似，由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-124.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt;原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。&lt;/p&gt;
&lt;p&gt;CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mac iTerm2 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T03:26:14.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。</p><p>我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：</p><p><img src="/images/pasted-123.png" alt="upload successful"></p><a id="more"></a><h1 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h1><p>我们开始配置前，还是有必要讲一下这几个项目的关系，以便了解其工作原理。</p><ul><li>iTerm2不用说了，MacOS上必备的Terminal工具，替代原有系统自带的工具。</li><li>ohmyzsh(<a href="https://github.com/ohmyzsh/ohmyzsh/" target="_blank" rel="noopener">https://github.com/ohmyzsh/ohmyzsh/</a>) 是一套基于zsh深度定制的插件及主题管理的框架，方便定制适合你的zsh环境。</li><li>Nerd Fonts(<a href="https://www.nerdfonts.com/" target="_blank" rel="noopener">https://www.nerdfonts.com/</a>) 我们在截图中看到的那些可爱的小图标就是来自这个项目，让我们原本枯燥的Terminal增添了几分乐趣。</li><li>powerlevel10k(<a href="https://github.com/romkatv/powerlevel10k" target="_blank" rel="noopener">https://github.com/romkatv/powerlevel10k</a>) 是一套zsh皮肤，也是目前我个人比较喜欢的一套皮肤，同时提供了较强的配置能力，包括字体下载，iTerm2的配置都自动完成了，所以也是目前使用最顺手的一套皮肤。</li></ul><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;&#39; &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;&#39; &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。&lt;/p&gt;
&lt;p&gt;我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-123.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何在微信开发者工具中使用vim编辑模式</title>
    <link href="http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/"/>
    <id>http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-01-26T23:52:10.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。</p><p>作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。</p><a id="more"></a><h1 id="在VS-Code安装vim插件"><a href="#在VS-Code安装vim插件" class="headerlink" title="在VS Code安装vim插件"></a>在VS Code安装vim插件</h1><p>首先在VS Code中安装vim模拟器，如图所示，我安装的是1.18.5版本。我使用的是mac系统，安装完成后，插件会存放在用户HOME目录下的$HOME/.vscode/extensions/vscodevim.vim-1.18.5中。</p><p><img src="/images/pasted-116.png" alt="upload successful"></p><h1 id="在微信开发者工具安装VS-Code插件"><a href="#在微信开发者工具安装VS-Code插件" class="headerlink" title="在微信开发者工具安装VS Code插件"></a>在微信开发者工具安装VS Code插件</h1><p>1、在微信开发者工具中点击“设置”-&gt;”扩展设置”</p><p><img src="/images/pasted-117.png" alt="upload successful"></p><p>2、在打开的窗口中选择“编辑器自定义扩展”，因为我已经安装过了，所以截图中已经包含了vscode.vim插件</p><p><img src="/images/pasted-118.png" alt="upload successful"></p><p>3、点击上方的“打开扩展文件夹”，此时会打开微信开发者插件目录，而你要做的就是将vscode插件拷贝过去。</p><p><img src="/images/pasted-119.png" alt="upload successful"></p><p>但是由于从Finder中无法直接访问隐藏目录，先在左侧选择HOME目录。</p><p><img src="/images/pasted-121.png" alt="upload successful"></p><p>使用“前往文件夹”选项，填入.vscode/extensions。将.vscode/extensions/vscodevim.vim-1.18.5拷贝之刚才打开的微信开发者工具的扩展目录中。</p><p><img src="/images/pasted-120.png" alt="upload successful"></p><p>4、重启微信开发者工具后，就能在“编辑器自定义扩展”中看到vim插件，启动插件后，再次退出重启，此时编辑器里已经可以使用vim模式了。</p><p><img src="/images/pasted-122.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。&lt;/p&gt;
&lt;p&gt;作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>使用镜像源加速Github Clone速度</title>
    <link href="http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/"/>
    <id>http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/</id>
    <published>2021-01-25T01:11:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？</p><a id="more"></a><h1 id="设定gitconfig自动实现替换"><a href="#设定gitconfig自动实现替换" class="headerlink" title="设定gitconfig自动实现替换"></a>设定gitconfig自动实现替换</h1><p>通过在HOME目录下的.gitconfig文件可以实现自动的对github.com进行替换的目的，具体的方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global url.&quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;.insteadOf https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><p>在$HOME/.gitconfig会发现增加了如下行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[url &quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;]</span><br><span class="line">insteadOf &#x3D; https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><h1 id="其他镜像源"><a href="#其他镜像源" class="headerlink" title="其他镜像源"></a>其他镜像源</h1><p>目前国内提供github镜像源还包括以下地址，但是通过网站测速（<a href="https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：" target="_blank" rel="noopener">https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：</a></p><ul><li>fastgit.org: <a href="https://doc.fastgit.org/" target="_blank" rel="noopener">https://doc.fastgit.org/</a></li><li>gitclone.com: <a href="https://gitclone.com/" target="_blank" rel="noopener">https://gitclone.com/</a></li><li>gitee: <a href="https://gitee.com/mirrors" target="_blank" rel="noopener">https://gitee.com/mirrors</a></li><li>cnpmjs.org: <a href="https://github.com.cnpmjs.org/" target="_blank" rel="noopener">https://github.com.cnpmjs.org/</a></li></ul><h1 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h1><p>还有一种情况是要从github下载某个文件，由于raw.githubusercontent.com属于长期被屏蔽状态，所以基本通过wget进行下载，比如要下载的文件为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -O https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure><p>可以替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;raw.staticdn.net&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Git" scheme="http://sunqi.site/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>利用Docker快速搭建Prometheus监控及告警平台</title>
    <link href="http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/"/>
    <id>http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/</id>
    <published>2020-12-25T13:32:48.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。</p><p>目前本文涉及的监控内容：</p><ul><li>主机监控</li><li>容器监控</li><li>Ceph监控</li><li>VMware监控</li></ul><a id="more"></a><h1 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h1><p>我们假设读者已经使用CentOS搭建了容器环境，并配置了国内源的前提下。如果没有设置请参考<a href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/">《CentOS 7和Docker初始化安装》</a>。</p><p>Prometheus快速构建的docker compose原始项目来自<a href="https://github.com/stefanprodan/dockprom" target="_blank" rel="noopener">stefanprodan/dockprom</a>，但是由于原项目中的cAdvisor使用了Google源，所以Fork的项目修改为国内源<a href="https://github.com/xiaoquqi/dockprom" target="_blank" rel="noopener">xiaoquqi/dockprom</a>。</p><p>原项目中包含的组件：</p><ul><li>Prometheus (metrics database) http://<host-ip>:9090</li><li>Prometheus-Pushgateway (push acceptor for ephemeral and batch jobs) http://<host-ip>:9091</li><li>AlertManager (alerts management) http://<host-ip>:9093</li><li>Grafana (visualize metrics) http://<host-ip>:3000</li><li>Caddy (reverse proxy and basic auth provider for prometheus and alertmanager)</li></ul><p>默认包含的采集器：</p><ul><li>NodeExporter (host metrics collector)</li><li>cAdvisor (containers metrics collector)</li></ul><p>在此基础上增加的内容：</p><ul><li>Ceph exporter</li><li>VMware exporter</li><li>钉钉告警webhook</li><li>轻量级http服务，用于内网分发docker-compse.exporter.yml</li></ul><h1 id="环境快速构建"><a href="#环境快速构建" class="headerlink" title="环境快速构建"></a>环境快速构建</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;dockprom</span><br><span class="line">cd dockprom</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>启动完成后，用浏览器访问：</p><ul><li>Prometheus: <a href="http://yourip:9090" target="_blank" rel="noopener">http://yourip:9090</a></li><li>Grafana: <a href="http://yourip:3000" target="_blank" rel="noopener">http://yourip:3000</a></li></ul><p>默认的用户名/密码为: admin/admin，如果需要修改可以在启动之前修改docker-compose.yml文件。</p><p>访问Prometheus，查看metrics是否被正确采集。如果有采集器有红色字样，根据提示查看具体的错误原因，大部分的错误都是因为配置问题，或者网络不通造成的。</p><p><img src="/images/pasted-109.png" alt="upload successful"></p><h1 id="Grafana配置"><a href="#Grafana配置" class="headerlink" title="Grafana配置"></a>Grafana配置</h1><p>访问Grafana的控制面板，其中已经内置了一些模板，也可以选择Import导入Grafana模板库的模板，数据源选择已经配置好的Prometheus即可。</p><p><img src="/images/pasted-110.png" alt="upload successful"></p><h1 id="主机监控"><a href="#主机监控" class="headerlink" title="主机监控"></a>主机监控</h1><p>默认安装情况下，主机层面仅监控了本机，如果需要增加新的监控主机，需要进行以下两步：</p><ul><li>为主机安装node exporter</li><li>修改Prometheus配置文件，并重启服务</li></ul><h2 id="1、安装node-exporter"><a href="#1、安装node-exporter" class="headerlink" title="1、安装node exporter"></a>1、安装node exporter</h2><p>在项目中，内置了一个单独的docker-compose.exporters.yml，如果目标主机安装了容器，可以直接将该yaml文件拷贝至目标节点后，启动监控服务即可。当然也可以通过软件包安装方式，本文不再赘述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>安装完成后，访问metrics接口，即代表安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9100&#x2F;metrics</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;14&quot;&#125; 2.8795339e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;15&quot;&#125; 2.3535384e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;16&quot;&#125; 3.4674675e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;17&quot;&#125; 2.5727501e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;18&quot;&#125; 2.5931391e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;19&quot;&#125; 2.67231846e+08</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;2&quot;&#125; 4.3448998e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;20&quot;&#125; 3.0684276e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;21&quot;&#125; 3.0587632e+07</span><br></pre></td></tr></table></figure><h2 id="2、修改Prometheus配置文件"><a href="#2、修改Prometheus配置文件" class="headerlink" title="2、修改Prometheus配置文件"></a>2、修改Prometheus配置文件</h2><p>回到Prometheus节点，找到dockerprom/prometheus/prometheus.yml进行如下修改，在nodeexporter段的targets增加新的监控节点后重启服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># A scrape configuration containing exactly one endpoint to scrape.</span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#39;nodeexporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;nodeexporter:9100&#39;, &#39;newip:9100&#39;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><h1 id="告警配置"><a href="#告警配置" class="headerlink" title="告警配置"></a>告警配置</h1><p>其实监控并不是最终的目的，往往告警才是监控系统成功与否的关键，在实际运维中对于根因分析和告警收敛是有非常强烈的需求的，本文中暂时还没对此做深入的分析，仅仅提供了常规的告警手段。告警的配置方法有两种方式，一种是通过Prometheus AlertManager，另外一种也可以通过在Grafana上直接进行配置。</p><p>对于告警方式支持多种方式，例如我们常用的邮件或者钉钉等，当然你也可以实现你自己的方式，这里我们使用钉钉的WEBHOOK作为告警方式。</p><h2 id="1-钉钉webhook配置"><a href="#1-钉钉webhook配置" class="headerlink" title="1. 钉钉webhook配置"></a>1. 钉钉webhook配置</h2><p>默认已经启动了钉钉容器，只需要修改dingtalk/config.yaml即可。Targets下面有各种示例，比如配置一个最简单的钉钉告警：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">targets:</span><br><span class="line">  devops:</span><br><span class="line">    url: https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;xxxxx</span><br></pre></td></tr></table></figure><p>这里的devops是自定义的，但是和后面要填入alertmanager的链接地址有关，比如本例中alertmanager回调地址就是http://<yourip>:8060/dingtalk/devops/send</p><h2 id="2-修改AlertManager配置"><a href="#2-修改AlertManager配置" class="headerlink" title="2. 修改AlertManager配置"></a>2. 修改AlertManager配置</h2><p>  修改alertmanager/config.yml</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  route:</span><br><span class="line">    receiver: &#39;dingtalk&#39;</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">  - name: &#39;dingtalk&#39;</span><br><span class="line">    webhook_configs:</span><br><span class="line">    - send_resolved: true</span><br><span class="line">      url: http:&#x2F;&#x2F;&lt;yourip&gt;:8060&#x2F;dingtalk&#x2F;devops&#x2F;send</span><br></pre></td></tr></table></figure><p>  这里不要用localhost，因为部署在容器内。</p><h2 id="3-修改Prometheus配置文件"><a href="#3-修改Prometheus配置文件" class="headerlink" title="3. 修改Prometheus配置文件"></a>3. 修改Prometheus配置文件</h2><p>  修改alert.rules，尝试修改一些规则测试告警，例如：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: host</span><br><span class="line">rules:</span><br><span class="line">- alert: high_cpu_load</span><br><span class="line">  expr: node_load1 &gt; 0.2</span><br><span class="line">  for: 1s</span><br><span class="line">  labels:</span><br><span class="line">    severity: warning</span><br><span class="line">  annotations:</span><br><span class="line">    summary: &quot;Server under high load&quot;</span><br><span class="line">    description: &quot;Docker host is under high load, the avg load 1m is at &#123;&#123; $value&#125;&#125;. Reported by instance &#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125;.&quot;</span><br></pre></td></tr></table></figure><p>此时可以通过AlertManager查看http://<yourip>:9093/#/alerts，检查是否有告警产生。</p><p><img src="/images/pasted-111.png" alt="upload successful"></p><p>如果告警产生了，但是无法触发钉钉，可以通过检查alertmanager容器进行debug，例如上述提到的localhost问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">level&#x3D;warn ts&#x3D;2020-12-29T07:21:56.345Z caller&#x3D;notify.go:674 component&#x3D;dispatcher receiver&#x3D;dingtalk integration&#x3D;webhook[0] msg&#x3D;&quot;Notify attempt failed, will retry later&quot; attempts&#x3D;1 err&#x3D;&quot;Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br><span class="line">level&#x3D;error ts&#x3D;2020-12-29T07:26:56.344Z caller&#x3D;dispatch.go:309 component&#x3D;dispatcher msg&#x3D;&quot;Notify for alerts failed&quot; num_alerts&#x3D;1 err&#x3D;&quot;dingtalk&#x2F;webhook[0]: notify retry canceled after 16 attempts: Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br></pre></td></tr></table></figure><h1 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h1><p>确保Ceph配置文件已经在/etc/ceph目录下，并且能够正常访问Ceph集群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.ceph.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9128/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;ceph-exporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    honor_labels: true</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.201:9128&#39;]</span><br><span class="line">      labels:</span><br><span class="line">        instance: Ceph Cluster</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入三个模板：</p><ul><li>Ceph Cluster Overview: <a href="https://grafana.com/dashboards/917" target="_blank" rel="noopener">https://grafana.com/dashboards/917</a></li><li>Ceph Pools Overview: <a href="https://grafana.com/dashboards/926" target="_blank" rel="noopener">https://grafana.com/dashboards/926</a></li><li>Ceph OSD Overview: <a href="https://grafana.com/dashboards/923" target="_blank" rel="noopener">https://grafana.com/dashboards/923</a></li></ul><p>Ceph Cluster效果：</p><p><img src="/images/pasted-113.png" alt="upload successful"></p><p>Ceph Pool效果：</p><p><img src="/images/pasted-114.png" alt="upload successful"></p><p>Ceph OSD效果：</p><p><img src="/images/pasted-115.png" alt="upload successful"></p><h1 id="VMware监控"><a href="#VMware监控" class="headerlink" title="VMware监控"></a>VMware监控</h1><p>首先修改docker-compose.vmware.exporters.yml中vcenter的连接信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">services:</span><br><span class="line">  vmware-exporter:</span><br><span class="line">    image: pryorda&#x2F;vmware_exporter:v0.11.1</span><br><span class="line">    container_name: vmware-exporter</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    ports:</span><br><span class="line">       - &#39;9272:9272&#39;</span><br><span class="line">    expose:</span><br><span class="line">       - 9272</span><br><span class="line">    environment:</span><br><span class="line">      VSPHERE_HOST: &quot;VC_HOST&quot;</span><br><span class="line">      VSPHERE_IGNORE_SSL: &quot;True&quot;</span><br><span class="line">      VSPHERE_USER: &quot;VC_USERNAME&quot;</span><br><span class="line">      VSPHERE_PASSWORD: &quot;VC_PASSWORD&quot;</span><br><span class="line">    labels:</span><br><span class="line">      org.label-schema.group: &quot;monitoring&quot;</span><br></pre></td></tr></table></figure><p>启动VMware exporter：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.vmware.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9272/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;vmware_vcenter&#39;</span><br><span class="line">    metrics_path: &#39;&#x2F;metrics&#39;</span><br><span class="line">    scrape_timeout: 15s</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.13:9272&#39;]</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入模板：<a href="https://grafana.com/grafana/dashboards/11243" target="_blank" rel="noopener">https://grafana.com/grafana/dashboards/11243</a></p><p>效果如下：</p><p><img src="/images/pasted-112.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。&lt;/p&gt;
&lt;p&gt;目前本文涉及的监控内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主机监控&lt;/li&gt;
&lt;li&gt;容器监控&lt;/li&gt;
&lt;li&gt;Ceph监控&lt;/li&gt;
&lt;li&gt;VMware监控&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我需要一款什么样的网盘？</title>
    <link href="http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/"/>
    <id>http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/</id>
    <published>2020-12-14T13:56:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。</p><a id="more"></a><h1 id="阿里云盘or网盘——傻傻分不清楚"><a href="#阿里云盘or网盘——傻傻分不清楚" class="headerlink" title="阿里云盘or网盘——傻傻分不清楚"></a>阿里云盘or网盘——傻傻分不清楚</h1><p>我相信很多关注了阿里网盘的朋友都会有一个问题，到底哪个是阿里真的网盘？我们通过公开渠道，至少能找到阿里云有至少三款“云盘”或者“网盘”。</p><p><img src="/images/pasted-97.png" alt="upload successful"></p><p><img src="/images/pasted-98.png" alt="upload successful"></p><p><img src="/images/pasted-99.png" alt="upload successful"></p><p>幸运的一点，我除了通过阿里MVP渠道获取到Teambition开发的网盘，同时还获取了阿里云盘的内测码，所以有机会对这两款都进行了充分体验。</p><h2 id="Teambiton网盘"><a href="#Teambiton网盘" class="headerlink" title="Teambiton网盘"></a>Teambiton网盘</h2><p>2019年初阿里耗资1亿美金收购了团队协作平台Teambition，通过收购及整合兼并，阿里不断扩大自身在To B领域的影响力。在推出网盘前，Teambition已经推出了代码托管平台codeUp和Flow，为企业构建完整的DevOps流程垫定了基础。网盘也是顺应这一趋势，阿里也继续丰富着自己的To B版图。</p><p>不过与我们理解的传统网盘有所区别，目前网盘是紧耦合在原有的Teambition体系内，更像是Teambition的一个扩展功能，而不能独立使用，这一点从网页版和手机侧的设计就能看出来。所以，也许Teambition网盘的目标客户也许并不是传统的网盘用户，更像是解决企业成员间文件存储和分享的问题。</p><p>目前Teambition暂不支持企业网盘，还需要切换至个人账户。但是在我测试过程中，有个小Bug，我明明在登陆后是个人账户，但是仍然看到的是企业网盘暂未开放的提示信息。需要先切换到企业，再切换回个人后才能看到。</p><p><img src="/images/pasted-100.png" alt="upload successful"></p><p>先来看一下整体的界面风格，从内测版本的截图看，Teambition的网盘目前还处于非常简单的状态，界面上展现的功能并不多，我认为类似最早期对象存储的基本功能。</p><p><img src="/images/pasted-101.png" alt="upload successful"></p><p>由于目前还处于内测阶段，所以基本不会对上传和下载速度做任何限制，不知道在商用化之后免费权益到底如何？我家的宽带是联通300Mbps，上传之前对网速进行了基本测试，上传的时候我使用的是浏览器直接上传的方式，我的iMac使用网线和路由器直接进行连接。</p><p><img src="/images/pasted-103.png" alt="upload successful"></p><p>我是晚上做的测试，上传了一个4.66GB的MP4文件，上传速度基本稳定在了400KB/s。</p><p><img src="/images/pasted-104.png" alt="upload successful"></p><p>这个和我最早期的一次测试结果是有出入的，感觉速度没有这么慢，并且中途失败了好几次，于是第二天早晨的时候，在失败后进行了重传。这时候速度基本上能把网络的上行速度跑满。</p><p><img src="/images/pasted-107.png" alt="upload successful"></p><p>另外，对于网盘很重要的分享功能也并未开放，只能作为用户自有的网络存储空间。</p><p><img src="/images/pasted-105.png" alt="upload successful"></p><p>手机侧的功能也基本与网页版本相似，并且必须要借助Teambition APP使用，暂时没有提供任何自动备份的功能。</p><p><img src="/images/pasted-106.png" alt="upload successful"></p><h2 id="阿里云盘"><a href="#阿里云盘" class="headerlink" title="阿里云盘"></a>阿里云盘</h2><p>经过与阿里内部确认，阿里的云盘和Teambition是完全独立的两个团队，也就是这是两端独立的产品，所以这也让我怀疑我当初看到的文章到底是在讲网盘还是云盘的？</p><p>阿里的云盘更接近传统对网盘的认知，这一点从用户体验上就能感觉到。与网盘不同的是，阿里云盘相对来说比较独立，之前只开放了手机版本，目前已经可以从网页上进行登陆了。目前只能通过网页版进行内测资格的申请，而所有操作也只能在手机侧完成。阿里云盘的域名是aliyundrive.com。</p><p>目前的客户端方面只提供了手机侧的，对我来说比较重要的客户端的还没有看到。从上传的感受来说，云盘的稳定性要好于网盘，网速基本上稳定在5MB/s，中间没有出现任何断线情况。</p><p><img src="/images/pasted-108.png" alt="upload successful"></p><h1 id="我需要什么样的网盘产品？"><a href="#我需要什么样的网盘产品？" class="headerlink" title="我需要什么样的网盘产品？"></a>我需要什么样的网盘产品？</h1><p>回答这个问题，先要从我用了哪两款付费网盘产品说起。目前我使用的两款付费网盘的产品，一款是苹果的iCloud，一款是百度的网盘。iCloud的费用一个月是21元/月200GB家庭共享空间，而百度网盘超级会员一个月是18元/月空间，空间达到了5TB，但是无法家庭共享。</p><p>可能有人会问为什么每个月要花将近40元在付费网盘呢？为什么不使用同一种网盘呢？这主要源自我最主要的几个需求：</p><p>一、解决不同设备之间的文件同步问题。我目前使用的手机是iPhone、办公电脑是Mac Pro，而家里使用的是iMac。因为经常要在家里工作，而又懒着背笔记本回家，所以重要文件在不同电脑的传输对我来说就非常重要了。另外，因为有时候需要出去交流，所以会从手机侧查看文件，那么手机与PC之间的文件互通也变得非常重要了。百度网盘之前在Mac侧会有个同步盘的应用，但是后来不再维护了，并且同步盘之前在同步过程中经常发生同步冲突，而莫名其妙产生了多个文件的问题。后来还试过类似OneDrive等网盘，都有类似的问题。最终发现只有苹果自身的方案才能最完美的符合我的要求。<br>二、费用成本问题。虽然苹果的方案很完美，但是也是最贵的，苹果的付费储存空间方案跨度太大，200GB之后就是2TB，价格直接从21元/月涨到了68元/月。因为会拍摄一些视频资料，所以对空间消耗比加大，无奈之下，只得选择了百度网盘作为补充，一方面是之前有很多文件都存在了百度网盘上，另外一方面付费后也不受下载限速的影响了，算是作为一种二级存储的方案使用。近期，准备购置百度的智能音箱，还能直接播放网盘内宝宝的音频。<br>三、照片、视频自动备份功能。虽然iCloud也能对照片、视频进行自动备份，但是有了孩子之后发现照片和视频成倍增加，真是不禁用，这时百度网盘的照片、视频自动备份就派上了用场。</p><p>其实，百度网盘还有一些类似照片整理、搜索等功能也非常强大，但是相比前两点并非刚需，所以只是偶尔会用到。</p><p>虽然这种组合使用方式在一定程度基本满足了我的日常需求，只是要定期的将部分数据导入百度网盘略显繁琐。但是另外一个网盘的需求依然困扰着我，就是对于微信的备份功能。虽然钉钉在这一点上要明显好于微信，但是谁让微信是第一社交软件呢？平时的沟通还是在微信，经常遇到一些文件不随手存下来就被微信清理掉的情况。虽然腾讯也有自己的微盘，但是就是不支持微信的自动备份功能，如果支持我估计我会立马付费购买。</p><p>总结一下我对网盘的几个需求：第一，空间足够大，上传下载不限速；第二、设备之间能够互相同步；第三、微信聊天记录和附件的自动备份功能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Docker构建服务器空间占满问题</title>
    <link href="http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/"/>
    <id>http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/</id>
    <published>2020-11-13T06:19:41.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h1><p>今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。</p><a id="more"></a><p><img src="/images/pasted-94.png" alt="upload successful"></p><h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><h2 id="检查容器空间"><a href="#检查容器空间" class="headerlink" title="检查容器空间"></a>检查容器空间</h2><p>首先从容器层面检查一下空间占用情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df</span><br></pre></td></tr></table></figure><p>发现有容器的占用空间达到了1个多TB的空间。</p><p><img src="/images/pasted-95.png" alt="upload successful"></p><h2 id="清理无用的容器和镜像"><a href="#清理无用的容器和镜像" class="headerlink" title="清理无用的容器和镜像"></a>清理无用的容器和镜像</h2><p>先用prune进行一下清理，为了保险起见，过滤一下时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system prune -a -f --filter &quot;until &#x3D; 1h&quot;</span><br></pre></td></tr></table></figure><p>清理完成后，空间仍然没有释放，于是继续排查。</p><h2 id="检查-var-lib下的空间占用"><a href="#检查-var-lib下的空间占用" class="headerlink" title="检查/var/lib下的空间占用"></a>检查/var/lib下的空间占用</h2><p>通过检查发现/var/lib/docker/overlay2中的66d44a19ee93a191cc0585efac45e10696edfd0381d0dc96d9646080337f629e目录空间占用巨大，进入后发现其中有tmp目录没有及时清理。由于没有Jenkins任务在执行，所以手动清理了/tmp/tmp*的目录，空间被立即释放了。</p><p><img src="/images/pasted-96.png" alt="upload successful"></p><p>那么此时问题清晰了，这一层属于Jenkins，进入容器后发现Jenkins的/tmp目录没有被及时清理，属于Build逻辑有缺陷造成了，及时修复Pipeline的Jenkinsfile后，该问题不再出现。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;现象描述&quot;&gt;&lt;a href=&quot;#现象描述&quot; class=&quot;headerlink&quot; title=&quot;现象描述&quot;&gt;&lt;/a&gt;现象描述&lt;/h1&gt;&lt;p&gt;今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MacOS VPN拨号后自动设置路由</title>
    <link href="http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/"/>
    <id>http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/</id>
    <published>2020-11-11T12:04:07.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。</p><a id="more"></a><h1 id="允许L2TP共享密钥为空"><a href="#允许L2TP共享密钥为空" class="headerlink" title="允许L2TP共享密钥为空"></a>允许L2TP共享密钥为空</h1><p>公司L2TP共享密钥为空，默认MacOS是不支持的，所以需要在配置文件中特殊设定，在/etc/ppp下生成options文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tee &#x2F;etc&#x2F;ppp&#x2F;options &lt;&lt; EOF</span><br><span class="line">plugin L2TP.ppp</span><br><span class="line">l2tpnoipsec</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="自动路由设置"><a href="#自动路由设置" class="headerlink" title="自动路由设置"></a>自动路由设置</h1><p>原理很简单，在连接VPN后将指定网段的IP经过VPN虚拟接口即可，具体的实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo touch &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br><span class="line">sudo chmod 0755 &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br></pre></td></tr></table></figure><p>ip-up的内容如下，只需要修改SUBNET网段即可，例如：192.168.10.0/24</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">&#x2F;sbin&#x2F;route add &lt;SUBNET&gt; -interface $1</span><br></pre></td></tr></table></figure><p>其余可利用参数如下：</p><ul><li>$1: VPN接口(例如：ppp0)</li><li>$2: 未知</li><li>$3: VPN服务器地址</li><li>$4: VPN网关地址</li><li>$5: 非VPN网关，本地使用</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h1&gt;&lt;p&gt;公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用钉钉通讯录同步构建本地LDAP服务</title>
    <link href="http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/"/>
    <id>http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/</id>
    <published>2020-10-31T10:24:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？</p><a id="more"></a><p>目前，在我们公司使用OpenLDAP服务作为各个服务统一鉴权的入口，使用的应用系统包括：Gerrit/Jenkins/Yapi/Wiki/进度跟踪等，目前所有的系统都支持LDAP鉴权，所以如果能将钉钉的通讯录定期同步至LDAP中就可以实现统一鉴权的需求。但是由于钉钉的密码无法同步回本地，所以密码层面仍然是独立的。</p><p>本文章的实现思路参考了<a href="https://xujiwei.com/blog/2020/02/internal-authorize-based-on-dingtalk-virtual-ldap-keyclaok/" target="_blank" rel="noopener">《基于钉钉 + Virtual-LDAP + KeyCloak 的内网统一认证系统<br>》</a>，感谢原作者的思路及贡献的virtual-ldap模块，本文所有的优化都是基于此文章基础上进行的优化。</p><h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>简单来说，我们希望能通过钉钉提供的LDAP作为统一鉴权方式，但是由于钉钉没有开放这个能力，那么我们需要将钉钉模拟一个LDAP服务。模拟出的LDAP环境，在内网环境中，我们对于LDAP信息的使用基本上围绕着用户名和密码，其他的信息以钉钉为准。所以，除了开放LDAP接口外，我们还需要提供用户界面，允许用户在内网修改密码。</p><p>整体的实现思路如下：</p><ul><li>钉钉开发者平台：需要在钉钉开发者平台新建一个程序，获取鉴权信息后，赋予通讯录同步权限，提供给VirtualLDAP进行数据同步</li><li>VirtualLDAP：该组件是上面提到的作者开发的虚拟LDAP组件，主要功能为同步钉钉通讯录，并以LDAP协议对外提供服务</li><li>KeyCloak：对于这个场景过重，但是暂时没有发现更好的方案，可以触发自动同步并且可以让内网用户进行密码修改</li><li>本地的全部系统按照LDAP配置方式即可实现鉴权</li></ul><p><img src="/images/pasted-90.png" alt="upload successful"></p><h2 id="钉钉开发者平台"><a href="#钉钉开发者平台" class="headerlink" title="钉钉开发者平台"></a>钉钉开发者平台</h2><p>这里我创建的是H5微应用，配置时有几点需要注意：</p><ul><li>IP地址白名单：需要为你未来运行VirtualLDAP配置访问IP白名单，目前钉钉开发者平台对于同一个IP只能给一个应用使用，但是可以通过通配符进行配置，例如：192.168.10.*的方式，那么192.168.10网段所有地址均可以访问</li><li>权限：需要为该应用开放所有通讯录只读权限即可</li></ul><p><img src="/images/pasted-92.png" alt="upload successful"></p><h2 id="VirtualLDAP"><a href="#VirtualLDAP" class="headerlink" title="VirtualLDAP"></a>VirtualLDAP</h2><p>这是基于Node.js开发的一款组件，主要用于同步钉钉通讯录和模拟LDAP协议。基于原作者版本，为了满足自身应用场景，进行了如下修改：</p><ul><li>由于作者没有提供Docker运行方式，所以在github的pull request中有人进行了改造</li><li>仍然是在同一个pull request中，增加了pinyin组件，在LDAP中增加了一个pinyin属性，方便业务系统使用</li><li>登录名和密码：为了防止公司人员重复，所以特别采用了全拼名称+电话号码后四位方式，作为唯一的用户名，而初始密码为全名名称+电话号码前四位，例如：张三的电话号码为13812345678，则登录名称为zhangsan5678，密码为zhangsan1381,</li><li>在使用VirtualLDAP时，需要使用MySQL存储持久化数据，例如用户修改后的密码，所以对鉴权规则进行了修改，先检查数据库密码是否匹配，再检查LDAP</li><li>实现了整体组件的编排，增加了docker-compose.yaml，方便用户使用，该编排文件中包含了KeyCloak、VirtualLDAP和MySQL</li></ul><h2 id="KeyCloak"><a href="#KeyCloak" class="headerlink" title="KeyCloak"></a>KeyCloak</h2><p>KeyCloak两部分需要进行配置：</p><ul><li>管理员在第一次使用时配置VirtualLDAP的信息，用于用户同步，方便新用户修改密码</li><li>新用户自行修改密码</li></ul><p>正如之前所说，KeyCloak功能过于强大，这里用到的功能非常有限，如果有新的应用场景，欢迎留言。</p><p><img src="/images/pasted-93.png" alt="upload successful"></p><h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><p>这里提供了完整的编排文件，直接使用即可完成整套环境的快速建立。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;virtual-ldap</span><br><span class="line">cd virtual-ldap&#x2F;docker-compose</span><br><span class="line">docker-compose up -d0</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><ul><li>VirtualLDAP配置文件修改。所有配置在virtual-ldap/docker-compose/config.js中进行修改，需要修改钉钉的appKey和appSecret，以及root DN的信息，配置文件有比较详细的介绍，所以这里不再赘述。</li><li>KeyCloak的默认密码修改在docker-compose.yaml中</li></ul><h2 id="登录相关信息"><a href="#登录相关信息" class="headerlink" title="登录相关信息"></a>登录相关信息</h2><ul><li>URL: ldap://ip:1389</li><li>ManageDN: cn=admin,dc=oneprocloud,dc=com</li><li>ManagePassword: password</li><li>User Search Base: ou=People,o=department,dc=oneprocloud,dc=com</li><li>User Search Filter: uid={0}</li><li>Display Name LDAP attribute: cn</li><li>Email Address LDAP attribute: mail</li></ul><h1 id="待优化"><a href="#待优化" class="headerlink" title="待优化"></a>待优化</h1><ul><li>目前对于LDAP的组没有充分利用，配置文件中允许创建特定组，并且通过用户email进行匹配，如果需要可以进行配置</li><li>如果有外部用户，暂时无方法进行创建，例如：如果需要在LDAP中增加一个非钉钉用户暂时无法实现，需要进行开发实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Node.js" scheme="http://sunqi.site/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>使用Kolla部署OpenStack Stein版本</title>
    <link href="http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/</id>
    <published>2020-10-30T14:15:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<p>开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。</p><a id="more"></a><h1 id="1、云平台规划"><a href="#1、云平台规划" class="headerlink" title="1、云平台规划"></a>1、云平台规划</h1><p>在实际环境中，我们在一台2U的超微四子星服务器上进行了部署。由于是内部使用的研发环境，为了节约成本，我们并没有部署高可靠方案，而是采用了一台作为控制节点+计算节点+存储节点，另外三台作为计算节点+存储节点的方式进行部署。</p><p>由于OpenStack最新的Ussari在使用Kolla部署时，不再支持CentOS 7版本，所以这里我们选定了上一个稳定版本Stein版本进行部署。</p><h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><table><thead><tr><th>硬件名称</th><th>配置规格</th><th>备注</th></tr></thead><tbody><tr><td>CPU</td><td>Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz    x 2</td><td>共40线程</td></tr><tr><td>内存</td><td>DDR4 2400 MHz 64GB</td><td></td></tr><tr><td>硬盘</td><td>板载64 GB x 1 <br/> 240 GB Intel SSD x 1 <br/> 1.2 TB SAS x 5</td><td>经过测试，由于板载64GB空间过小，在控制节点需要损失一块SAS盘空间用于root分区挂载</td></tr><tr><td>网卡</td><td>千兆 x 4 <br/> 万兆 x 4 <br/> IPMI x 1</td><td></td></tr></tbody></table><h3 id="分区规划"><a href="#分区规划" class="headerlink" title="分区规划"></a>分区规划</h3><table><thead><tr><th>磁盘</th><th>规划</th><th>备注</th></tr></thead><tbody><tr><td>64G</td><td>系统盘</td><td>不要使用LVM分区</td></tr><tr><td>SSD 240G</td><td>Ceph Journal<br></td><td>1块盘</td></tr><tr><td>SAS 1.2 T</td><td>Ceph OSD</td><td>5块盘</td></tr></tbody></table><h2 id="网络规划"><a href="#网络规划" class="headerlink" title="网络规划"></a>网络规划</h2><h3 id="交换机配置"><a href="#交换机配置" class="headerlink" title="交换机配置"></a>交换机配置</h3><ul><li>我们默认采用了VLAN模式，所以无须在交换机上进行Trunk配置</li></ul><h3 id="网络规划-1"><a href="#网络规划-1" class="headerlink" title="网络规划"></a>网络规划</h3><table><thead><tr><th>网卡</th><th>网络类型</th><th>VLAN ID</th><th>网段</th><th>说明</th><th>网关</th><th>备注</th></tr></thead><tbody><tr><td></td><td>管理网络</td><td>3</td><td>192.168.10.0/24</td><td>OpenStack管理</td><td>192.168.10.1</td><td>192.168.10.201 - 204</td></tr><tr><td></td><td>存储网络</td><td></td><td>10.0.100.0/24</td><td>Ceph网络</td><td>无需网关</td><td>10.0.100.201 -&nbsp;204</td></tr><tr><td></td><td>External网络</td><td>3</td><td>192.168.10.0/24</td><td>External网络</td><td>192.168.10.1</td><td>可分配地址192.168.10.100 - 192.168.10.200</td></tr><tr><td></td><td>Tunnel网络</td><td></td><td>172.16.100.0/24</td><td>VxLAN通讯网络</td><td><br data-mce-bogus="1"></td><td>172.16.100.201 - 204</td></tr><tr><td>console</td><td>IPMI</td><td>4</td><td>192.168.10.0/24</td><td></td><td></td><td>与管理网地址一一对应, 192.168.10.201</td></tr></tbody></table><h3 id="网卡配置"><a href="#网卡配置" class="headerlink" title="网卡配置"></a>网卡配置</h3><table><thead><tr><th>主机名</th><th>em1(管理网地址)</th><th>em2(存储网)</th><th>em3(External网络)</th><th>em4(Tunnel网络)</th><th>备注</th></tr></thead><tbody><tr><td>control201</td><td>192.168.10.201</td><td>10.0.100.201</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.201</td><td></td></tr><tr><td>compute202</td><td>192.168.10.202</td><td>10.0.100.202</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.202</td><td></td></tr><tr><td>compute203</td><td>192.168.10.203</td><td>10.0.100.203</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr><tr><td>compute204</td><td>192.168.10.204</td><td>10.10.20.204</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr></tbody></table><h2 id="OpenStack规划"><a href="#OpenStack规划" class="headerlink" title="OpenStack规划"></a>OpenStack规划</h2><h3 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h3><p>Ceph采用单独安装方式，这目前也是Kolla项目主推的方式，在U版本中已经彻底不支持通过Kolla安装Ceph了。我们主要安装OpenStack核心模块，另外安装的是日志收集ELK的相关模块，便于运维。</p><ul><li>Horizon</li><li>Nova</li><li>Keystone</li><li>Cinder</li><li>Glance</li><li>Neutron</li><li>Heat</li></ul><h1 id="2、部署准备"><a href="#2、部署准备" class="headerlink" title="2、部署准备"></a>2、部署准备</h1><h2 id="部署架构图"><a href="#部署架构图" class="headerlink" title="部署架构图"></a>部署架构图</h2><p><img src="/images/pasted-60.png" alt="upload successful"></p><h2 id="服务器前期准备"><a href="#服务器前期准备" class="headerlink" title="服务器前期准备"></a>服务器前期准备</h2><ul><li>BIOS配置：在BIOS中打开VT，并且正确配置IPMI地址，方便远程管理</li><li>RAID配置：所有磁盘需要配置成NON-RAID模式</li><li>操作系统安装：<ul><li>使用CentOS 7光盘进行最小化安装</li><li>不要使用LVM分区</li><li>配置主机名</li><li>配置第一块网卡，并配置自动启动</li></ul></li></ul><h2 id="网卡配置-1"><a href="#网卡配置-1" class="headerlink" title="网卡配置"></a>网卡配置</h2><h3 id="em1"><a href="#em1" class="headerlink" title="em1"></a>em1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em1</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em1</span><br><span class="line">DEVICE&#x3D;em1</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;192.168.10.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">GATEWAY&#x3D;192.168.10.1</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em2"><a href="#em2" class="headerlink" title="em2"></a>em2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em2</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">DEFROUTE&#x3D;yes</span><br><span class="line">NAME&#x3D;em2</span><br><span class="line">DEVICE&#x3D;em2</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;10.0.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em3"><a href="#em3" class="headerlink" title="em3"></a>em3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em3</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em3</span><br><span class="line">DEVICE&#x3D;em3</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;172.16.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em4"><a href="#em4" class="headerlink" title="em4"></a>em4</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em4</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;none</span><br><span class="line">NAME&#x3D;em4</span><br><span class="line">DEVICE&#x3D;em4</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h1><h2 id="3-1-准备部署节点"><a href="#3-1-准备部署节点" class="headerlink" title="3.1 准备部署节点"></a>3.1 准备部署节点</h2><p>该节点承担了后续所有的部署流程，该节点可以作为OpenStack控制节点复用，包括运行OpenStack Kolla和Ceph Deploy。</p><p>注意：节点之间可以通过密码或者密钥方式进行访问，附录中提供了自动上传密钥的方式，建议在正式安装前配置完成，这里不提供自动化配置方法。</p><h3 id="下载初始化脚本"><a href="#下载初始化脚本" class="headerlink" title="下载初始化脚本"></a>下载初始化脚本</h3><p>目前已经将常用的操作写成了Ansible脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;my_ansible_playbooks</span><br><span class="line"></span><br><span class="line">cd my_ansible_playbooks</span><br><span class="line">prepare_on_centos7.sh</span><br></pre></td></tr></table></figure><h3 id="修改hosts-ini文件"><a href="#修改hosts-ini文件" class="headerlink" title="修改hosts.ini文件"></a>修改hosts.ini文件</h3><p>修改hosts.ini文件来初始化所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># my_ansible_playbooks&#x2F;hosts.ini</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.201 ip&#x3D;192.168.10.201 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.202 ansible_user&#x3D;root</span><br><span class="line">compute203 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.203 ansible_user&#x3D;root</span><br><span class="line">compute204 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.204 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="初始化节点"><a href="#初始化节点" class="headerlink" title="初始化节点"></a>初始化节点</h3><p>该步骤主要包含了，更新软件，修改主机名，增加/etc/hosts等操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;bootstrap_centos7.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;change_hostname.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_etc_hosts.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_docker.yml</span><br><span class="line"></span><br><span class="line"># 安装pip和系统环境下的python docker模块，否则在precheck的时候会发现没有安装docker模块</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_pip2_package.yml</span><br></pre></td></tr></table></figure><h3 id="安装Ceph-Deploy"><a href="#安装Ceph-Deploy" class="headerlink" title="安装Ceph Deploy"></a>安装Ceph Deploy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h3 id="安装Kolla"><a href="#安装Kolla" class="headerlink" title="安装Kolla"></a>安装Kolla</h3><p>由于Python Warning的提示信息导致在安装时出现如下错误，需要增加忽略Python Warning的环境变量，具体修复信息如下：<a href="https://bugs.launchpad.net/kolla-ansible/+bug/1888657" target="_blank" rel="noopener">https://bugs.launchpad.net/kolla-ansible/+bug/1888657</a></p><p>目前通过pip方式还没有8.2.1这个release，所以kolla的安装从源代码中进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ansible 2.2.0.0 used in Stein kolla-toolbox requires paramiko (no version</span><br><span class="line">constraints), which installs latest cryptography package. It results in</span><br><span class="line">Python deprecation warning about Python 2:</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x2F;cryptography&#x2F;__init__.py:39: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release.</span><br><span class="line"></span><br><span class="line">This warning breaks kolla_toolbox module.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python-devel libffi-devel gcc openssl-devel libselinux-python</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;openstack&#x2F;kolla-ansible --branch stable&#x2F;stein</span><br><span class="line">cd kolla-ansible</span><br><span class="line">pip install . --ignore-installed PyYAML</span><br><span class="line"></span><br><span class="line"># 虚拟环境还需要再安装一次ansible，否则kolla-ansible会提示</span><br><span class="line"># ERROR: kolla_ansible has to be available in the Ansible PYTHONPATH.</span><br><span class="line"># Please install both in the same (virtual) environment.</span><br><span class="line">pip install &#39;ansible&lt;2.10&#39;</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla</span><br><span class="line">cp -r $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;etc_examples&#x2F;kolla&#x2F;* &#x2F;etc&#x2F;kolla</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;root&#x2F;kolla</span><br><span class="line">cp $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;ansible&#x2F;inventory&#x2F;* &#x2F;root&#x2F;kolla</span><br></pre></td></tr></table></figure><p>生成密码，如果需要指定密码，可以到/etc/kolla/password.yml中修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-genpwd</span><br></pre></td></tr></table></figure><h2 id="3-2-部署Ceph"><a href="#3-2-部署Ceph" class="headerlink" title="3.2 部署Ceph"></a>3.2 部署Ceph</h2><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>之前有一篇软文详细介绍了使用Ceph Deploy部署Ceph的方法，这里不再赘述，下面直接给出部署命令，这里我们只部署块服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;root&#x2F;ceph</span><br><span class="line">cd &#x2F;root&#x2F;ceph</span><br><span class="line"></span><br><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 如果阿里源无法使用，可以使用163源，并且可以通过指定rpm-octopus，指定安装的Ceph版本</span><br><span class="line">#export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">#export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">#</span><br><span class="line"># 如果cluster-network和public-network需要分开，可以这样定义：</span><br><span class="line"># ceph-deploy new --cluster-network 172.31.6.0&#x2F;24 --public-network 192.168.4.0&#x2F;24 node1 node2 node3</span><br><span class="line"></span><br><span class="line">ceph-deploy new --public-network 10.0.100.0&#x2F;24 compute201</span><br><span class="line">ceph-deploy install compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create compute201</span><br><span class="line"></span><br><span class="line"># 需要根据实际情况修改，这里模拟的是将RocksDB存放至单独的SSD磁盘，目前文档中并没有特别指出这部分的分配比例，所以DB和WAL都是分配10G，写入的基本顺序为WAL -&gt; DB -&gt; DATA</span><br><span class="line"></span><br><span class="line">pvcreate &#x2F;dev&#x2F;vdb</span><br><span class="line">vgcreate ceph-pool &#x2F;dev&#x2F;vdb</span><br><span class="line"></span><br><span class="line"># 每个OSD分配</span><br><span class="line">lvcreate -n osd0.wal -L 10G ceph-pool</span><br><span class="line">lvcreate -n osd0.db -L 10G ceph-pool</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdd --block-db ceph-pool&#x2F;osd0.db --block-wal ceph-pool&#x2F;osd0.wal compute201</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><h3 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h3><p>为Glance/Nova/Cinder创建资源池并生成鉴权文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create images 128</span><br><span class="line">ceph auth get-or-create client.glance mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create volumes 128</span><br><span class="line">ceph auth get-or-create client.cinder mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;volumes, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create backups 128</span><br><span class="line">ceph auth get-or-create client.cinder-backup mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;backups&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create vms 128</span><br><span class="line">ceph auth get-or-create client.nova mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;vms, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring</span><br></pre></td></tr></table></figure><p>注意：</p><h2 id="3-3-OpenStack部署"><a href="#3-3-OpenStack部署" class="headerlink" title="3.3 OpenStack部署"></a>3.3 OpenStack部署</h2><h3 id="kolla配置文件"><a href="#kolla配置文件" class="headerlink" title="kolla配置文件"></a>kolla配置文件</h3><h4 id="etc-kolla-globals-yml"><a href="#etc-kolla-globals-yml" class="headerlink" title="/etc/kolla/globals.yml"></a>/etc/kolla/globals.yml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;stein&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.10.123&quot;</span><br><span class="line"></span><br><span class="line">docker_registry: registry.cn-beijing.aliyuncs.com</span><br><span class="line">docker_namespace: &quot;openstack-dockers&quot;</span><br><span class="line"></span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">storage_interface: &quot;eth1&quot;</span><br><span class="line">tunnel_interface: &quot;eth3&quot;</span><br><span class="line">neutron_external_interface: &quot;eth2&quot;</span><br><span class="line"></span><br><span class="line">openstack_logging_debug: &quot;True&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_ceph: &quot;no&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">enable_cinder_backup: &quot;yes&quot;</span><br><span class="line">enable_fluentd: &quot;no&quot;</span><br><span class="line">enable_openstack_core: &quot;yes&quot;</span><br><span class="line">glance_backend_ceph: &quot;yes&quot;</span><br><span class="line">glance_backend_file: &quot;no&quot;</span><br><span class="line">glance_enable_rolling_upgrade: &quot;no&quot;</span><br><span class="line">cinder_backend_ceph: &quot;yes&quot;</span><br><span class="line">nova_backend_ceph: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h4 id="multinode"><a href="#multinode" class="headerlink" title="multinode"></a>multinode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[control]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[network]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[compute]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[monitoring]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[storage]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="定制服务配置文件"><a href="#定制服务配置文件" class="headerlink" title="定制服务配置文件"></a>定制服务配置文件</h3><h4 id="Ceph-Glance"><a href="#Ceph-Glance" class="headerlink" title="Ceph Glance"></a>Ceph Glance</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance</span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;glance-api.conf &lt;&lt; EOF</span><br><span class="line">[glance_store]</span><br><span class="line">stores &#x3D; rbd</span><br><span class="line">default_store &#x3D; rbd</span><br><span class="line">rbd_store_pool &#x3D; images</span><br><span class="line">rbd_store_user &#x3D; glance</span><br><span class="line">rbd_store_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.client.glance.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Cinder"><a href="#Ceph-Cinder" class="headerlink" title="Ceph Cinder"></a>Ceph Cinder</h4><p>cinder_rbd_secret_uuid是在passwords.yml中生成的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup</span><br><span class="line"></span><br><span class="line">export cinder_rbd_secret_uuid&#x3D;$(grep cinder_rbd_secret_uuid &#x2F;etc&#x2F;kolla&#x2F;passwords.yml | awk &#39;&#123;print $2&#125;&#39;)</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends&#x3D;rbd-1</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;cinder</span><br><span class="line">backend_host&#x3D;rbd:volumes</span><br><span class="line">rbd_pool&#x3D;volumes</span><br><span class="line">volume_backend_name&#x3D;rbd-1</span><br><span class="line">volume_driver&#x3D;cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_secret_uuid &#x3D; $cinder_rbd_secret_uuid</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">backup_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">backup_ceph_user&#x3D;cinder-backup</span><br><span class="line">backup_ceph_chunk_size &#x3D; 134217728</span><br><span class="line">backup_ceph_pool&#x3D;backups</span><br><span class="line">backup_driver &#x3D; cinder.backup.drivers.ceph.CephBackupDriver</span><br><span class="line">backup_ceph_stripe_unit &#x3D; 0</span><br><span class="line">backup_ceph_stripe_count &#x3D; 0</span><br><span class="line">restore_discard_excess_bytes &#x3D; true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有文件必须命名为ceph.client*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Nova"><a href="#Ceph-Nova" class="headerlink" title="Ceph Nova"></a>Ceph Nova</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;nova-compute.conf &lt;&lt; EOF</span><br><span class="line">[libvirt]</span><br><span class="line">images_rbd_pool&#x3D;vms</span><br><span class="line">images_type&#x3D;rbd</span><br><span class="line">images_rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;nova</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.nova.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.cinder.keyring</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 初始化节点，与上述我们自己的初始化有重复之处</span><br><span class="line">kolla-ansible -i multinode bootstrap-servers</span><br><span class="line"></span><br><span class="line">kolla-ansible -i multinode prechecks</span><br><span class="line"></span><br><span class="line"># 拉取所有镜像</span><br><span class="line">kolla-ansible -i multinode pull</span><br></pre></td></tr></table></figure><h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -i multinode deploy</span><br><span class="line">kolla-ansible -i multinode post-deploy</span><br></pre></td></tr></table></figure><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="节点互信"><a href="#节点互信" class="headerlink" title="节点互信"></a>节点互信</h2><p>节点之间互信建议采用key方式，这里并没有实现完全自动化手段，需要首先在控制节点上生成公钥和私钥。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>然后将~/.ssh/id_rsa.pub文件拷贝至可以正常访问两台节点的环境中的playbooks/keys目录下，再更新所有节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_authorized_keys.yml</span><br></pre></td></tr></table></figure><h2 id="部署出错如何调试"><a href="#部署出错如何调试" class="headerlink" title="部署出错如何调试"></a>部署出错如何调试</h2><p>如果在部署中出现任何错误，可以添加更多的Verbose来判断具体问题，有可能是kolla自身bug，也有可能是配置的问题，具体可以根据详细输出进行判断。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -vvv -i multinode deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>云原生趋势下的迁移与容灾思考</title>
    <link href="http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/"/>
    <id>http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/</id>
    <published>2020-10-18T12:05:00.000Z</published>
    <updated>2021-02-20T23:19:05.257Z</updated>
    
    <content type="html"><![CDATA[<h1 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h1><h2 id="云原生发展趋势"><a href="#云原生发展趋势" class="headerlink" title="云原生发展趋势"></a>云原生发展趋势</h2><p>云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。</p><p>2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？</p><a id="more"></a><h2 id="为什么云上需要全新的迁移和容灾"><a href="#为什么云上需要全新的迁移和容灾" class="headerlink" title="为什么云上需要全新的迁移和容灾"></a>为什么云上需要全新的迁移和容灾</h2><h3 id="1、传统方案的局限性"><a href="#1、传统方案的局限性" class="headerlink" title="1、传统方案的局限性"></a>1、传统方案的局限性</h3><p>在这种大的趋势下，传统的迁移和容灾仍然停留在数据搬运的层次上，而忽略了面向云的特性和用户业务重新思考和构建。云计算的愿景是让云资源像水、电一样按需使用，所以基于云上的迁移和容灾也理应顺应这样的历史潮流。Snowflake也是通过这种商业模式的创新，成功打破旧的竞争格局。</p><p>为什么传统容灾的手段无法满足云原生需求呢？简单来说，二者关注的核心不同。传统的容灾往往以存储为核心，拥有对存储的至高无上的控制权。并且在物理时代，对于计算、存储和网络等基础架构层也没有有效的调度方法，无法实现高度自动化的编排。而基于云原生构建的应用，核心变成了云原生服务本身。当用户业务系统全面上云后，用户不再享有对底层存储的绝对控制权，所以传统的容灾手段，就风光不在了。</p><p><img src="/images/pasted-88.png" alt="upload successful"></p><p>我认为在构建云原生容灾的解决方案上，要以业务为核心去思考构建方法，利用云原生服务的编排能力实现业务系统的连续性。</p><h3 id="2、数据安全性"><a href="#2、数据安全性" class="headerlink" title="2、数据安全性"></a>2、数据安全性</h3><p>AWS CTO Werner Vogels曾经说过：Everything fails, all the time。通过AWS的责任共担模型，我们不难发现云商对底层基础架构负责，用户仍然要对自身自身数据安全性和业务连续性负责。</p><p><img src="/images/pasted-74.png" alt="upload successful"></p><p>我认为在云原生趋势下，用户最直接诉求的来自数据安全性即备份，而迁移、恢复、高可靠等都是基于备份表现出的业务形态，而备份能力可能是由云原生能力提供的，也有可能是第三方能力提供的，但最终实现业务形态，是由编排产生的。</p><p>用户上云并不等于高枕无忧，相反用户要学习云的正确打开方式，才能最大程度来保证业务的连续性。虽然云在底层设计上上是高可靠的，但是仍然避免不了外力造成的影响，例如：光缆被挖断、断电、人为误操作导致的云平台可用区无法使用，所以才有了类似“蓝翔决定了中国云计算稳定性”的调侃。我认为用户决定将业务迁移到云上的那一刻开始，备份、迁移、恢复、高可靠是一个连续的过程，如何合理利用云原生服务的特性实现业务连续性，同时进行成本优化，降低总体拥有成本（TCO）。</p><h3 id="3、防止厂商锁定"><a href="#3、防止厂商锁定" class="headerlink" title="3、防止厂商锁定"></a>3、防止厂商锁定</h3><p>某种意义上说，云原生的方向是新一轮厂商锁定，就像当年盛极一时的IOE架构一样，只不过现在换成了云厂商作为底座承载应用。在IOE时代，用户很难找到完美的替代品，但是在云时代，这种差异并不那么明显。所以大部分的客户通常选用混合云作为云建设策略，为了让应用在不同云之间能够平滑移动，利用容灾技术的迁移一定是作为一个常态化需求存在的。Gartnar也在多云管平台定义中，将迁移和DR作为单独的一项能力。充分说明迁移与容灾在多云环境的的常态化趋势。</p><p><img src="/images/pasted-82.png" alt="upload successful"></p><h1 id="云迁移与云容灾的关系"><a href="#云迁移与云容灾的关系" class="headerlink" title="云迁移与云容灾的关系"></a>云迁移与云容灾的关系</h1><h2 id="云迁移需求的产生"><a href="#云迁移需求的产生" class="headerlink" title="云迁移需求的产生"></a>云迁移需求的产生</h2><p>在传统环境下，迁移的需求并不十分突出，除非是遇到机房搬迁或者硬件升级，才会想到迁移，但这里的迁移更像是搬铁，迁移工具化与自动化的需求并不明显。当VMware出现后，从物理环境到虚拟化的迁移需求被放大，但由于是单一的虚拟化平台，基本上虚拟化厂商自身的工具就完全能够满足需求了。在虚拟化平台上，大家突然发现原来只能人工操作的物理环境一下子轻盈起来，简单来说，我们的传统服务器从一堆铁变成了一个文件，并且这个文件还能够被来回移动、复制。再后来，进入云时代，各家云平台风生水起，国内云计算市场更是百家争鸣，上云更是成为了一种刚性需求。随着时间的推移，出于对成本、厂商锁定等诸多因素的影响，在不同云之间的互相迁移更是会成为一种常态化的需求。</p><h2 id="底层技术一致"><a href="#底层技术一致" class="headerlink" title="底层技术一致"></a>底层技术一致</h2><p>这里提到的云迁移和容灾，并不是堆人提供的迁移服务，而是强调的高度自动化的手段。目标就是在迁移过程中保证业务连续性，缩短停机时间甚至不停机的效果。这里就借助了容灾的存储级别同步技术来实现在异构环境下的的“热迁移”。现有解决方案里，既有传统物理机搬迁时代的迁移软件，也有基于云原生开发的工具。但无论何种形式，都在不同程度上都解决了用户上云的基本诉求。最大的区别在于人效比，这一点与你的利益直接相关。</p><p>从另外一个角度也不难发现，所谓的迁移在正式切换之前实质上就是容灾的中间过程。同时，业务系统迁移到云平台后，灾备是一个连续的动作，这里既包含了传统的备份和容灾，还应该包含云上高可靠的概念。这样，用户业务系统在上云后，才能摆脱传统基础架构的负担，做到“零运维”，真正享受到云所带来的的红利。所以，我认为在云原生状态下，云迁移、云容灾、云备份本质上就是一种业务形态，底层采用的技术手段可以是完全一致的。</p><h2 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h2><p>在上述的痛点和趋势下，必然会出现一种全新的平台来帮助客户解决数据的安全性和业务连续性问题，今天就从这个角度来分析一下，在云原生的趋势下如何构建应用系统的迁移与容灾方案。</p><h1 id="云迁移发展趋势"><a href="#云迁移发展趋势" class="headerlink" title="云迁移发展趋势"></a>云迁移发展趋势</h1><h2 id="云迁移方式"><a href="#云迁移方式" class="headerlink" title="云迁移方式"></a>云迁移方式</h2><p>迁移是一项重度的咨询业务，网上各家云商、MSP都有自己的方法论，其实看下来差别都不大，之前也有很多人在分享相关话题，本文就不再赘述。这里我们重点讨论，在实际落地过程中到底该采用哪种工具，哪种方式的效率最高。所谓云迁移工具，就是将源端迁移至目标端，保证源端在目标端正确运行。常见的方式包括：物理机到虚拟化、虚拟化到虚拟化、物理机到云平台、虚拟化到云平台等。</p><p><img src="/images/pasted-62.png" alt="upload successful"></p><p>这是经典的6R迁移理论（现在已经升级为了7R，多了VMware出来搅局），在这个图中与真正迁移相关的其实只有Rehosting, Replatforming, Repurchasing和Refactoring，但是在这4R中，Refactoring明显是一个长期的迭代过程，需要用户和软件开发商共同参与解决，Repurchasing基本上与人为重新部署没有太大的区别。所以真正由用户或MSP在短期完成的只剩下Rehosting和Replatofrming。</p><p>与上面这张经典的迁移理论相比，我更喜欢下面这张图，这张图更能反应一个传统应用到云原生成长的全过程。与上述的结论相似，我们在真正拥抱云的时候，路径基本为上述的三条</p><ul><li>Lift &amp; Shift是Rehost方式的另一种称呼，这种方式路面最宽，寓意这条路是上云的最短路径，应用不需要任何改造直接上云使用</li><li>Evolve和Go Native都属于较窄的路径，寓意为相对于Rehost方式，这两条路径所消耗的时间更久，难度更高</li><li>在图的最右侧，三种形态是存在互相转换的可能，最终演进为彻底的云原生，寓意为迁移并不是一蹴而就，需要循序渐进完成</li></ul><p><img src="/images/pasted-61.png" alt="upload successful"></p><h2 id="重新托管（Rehost）方式"><a href="#重新托管（Rehost）方式" class="headerlink" title="重新托管（Rehost）方式"></a>重新托管（Rehost）方式</h2><p>常用的重新托管方式为冷迁移和热迁移，冷迁移往往涉及到步骤比较繁琐，需要大量人力投入，并且容易出错效率低，对业务连续性有较大的影响，不适合生产系统迁移。而热迁移方案基本都是商用化的解决方案，这里又分为块级别和文件级别，再细分为传统方案与云原生方案。</p><h3 id="冷迁移"><a href="#冷迁移" class="headerlink" title="冷迁移"></a>冷迁移</h3><p>我们先来看一下冷迁移的手动方案，以VMware到OpenStack为例，最简单的方式就是将VMware虚拟机文件(VMDK)通过qemu-img工具进行格式转换，转换为QCOW2或者RAW格式，上传至OpenStack Glance服务，再重新在云平台上进行启动。当然这里面需要进行virtio驱动注入，否则主机无法正常在云平台启动。这个过程中最耗时的应该是虚拟机文件上传至OpenStack Glance服务的过程，在我们最早期的实践中，一台主机从开始迁移到启动完成足足花了24小时。同时，在你迁移这段时间的数据是有增量产生的，除非你将源端关机等待迁移完成，否则，你还要将上述步骤重新来一遍。所以说这种方式真的不适合有业务连续性的生产系统进行迁移。</p><p>那如果是物理机的冷迁移方案怎么做呢？经过我们的最佳实践，这里为大家推荐的是老牌的备份工具CloneZilla，中文名为再生龙。是一款非常老牌的备份软件，常用于进行整机备份与恢复，与我们常见的Norton Ghost原理非常相似。CloneZilla从底层的块级别进行复制，可以进行整盘的备份，并且支持多种目标端，例如我们将磁盘保存至移动硬盘，实际格式就是RAW，你只需要重复上述的方案即可完成迁移。但是在使用CloneZilla过程中，需要使用Live CD方式进行引导，同样会面临长时间业务系统中断的问题，这也是上面我们提到的冷迁移并不适合生产环境迁移的原因。</p><p><img src="/images/pasted-63.png" alt="upload successful"></p><p><img src="/images/pasted-64.png" alt="upload successful"></p><h3 id="传统热迁移方案"><a href="#传统热迁移方案" class="headerlink" title="传统热迁移方案"></a>传统热迁移方案</h3><p>传统的热迁移方案基本分为块级别和文件级别，两者相似之处都是利用差量同步技术进行实现，即全量和增量交叉同步方式。</p><p>文件级别的热迁移方案往往局限性较大，并不能算真正的ReHost方式，因为前期需要准备于源端完全一样的操作系统，无法实现整机搬迁，从操作的复杂性更大和迁移的稳定性来说都不高。我们在Linux上常用的Rsync其实可以作为文件级别热迁移的一种解决方案。</p><p>真正可以实现热迁移的方案，还要使用块级别同步，降低对底层操作系统依赖，实现整机的搬迁效果。传统的块级别热迁移方案基本上来自于传统容灾方案的变种，利用内存操作系统WIN PE或其他Live CD实现，基本原理和过程如下图所示。从过程中我们不难发现这种方式虽然在一定程度解决了迁移的目标，但是作为未来混合云常态化迁移需求来说，仍然有以下几点不足：</p><ul><li>由于传统热迁移方案是基于物理环境构建的，所以我们发现在整个过程中人为介入非常多，对于使用者的技能要求比较高</li><li>无法满足云原生时代多租户、自服务的需求</li><li>安装代理是用户心中永远的芥蒂</li><li>一比一同步方式，从成本角度来说不够经济</li><li>最好的迁移验证方式，就是将业务系统集群在云端完全恢复，但是手动验证的方式，对迁移人力成本是再一次增加</li></ul><p><img src="/images/pasted-67.png" alt="upload successful"></p><h3 id="云原生热迁移方案"><a href="#云原生热迁移方案" class="headerlink" title="云原生热迁移方案"></a>云原生热迁移方案</h3><p>正是由于传统迁移方案的弊端，应运而生了云原生的热迁移方案，这一方面的代表厂商当属AWS在2019年以2.5亿美金击败Google Cloud收购的以色列云原生容灾、迁移厂商CloudEndure。</p><p>云原生热迁移方案是指利用块级别差量同步技术结合云原生API接口和资源实现高度自动化迁移效果，同时提供多租户、API接口满足混合云租户自服务的需求。我们先从原理角度分析一下，为什么相对于传统方案，云原生的方式能够满足高度自动化、用户自服务的用户体验。通过两个方案对比，我们不难发现云原生方式的几个优势：</p><ul><li>利用云原生API接口和资源，操作简便，完全取代了传统方案大量繁琐的人为操作，对使用者技术要求降低，学习陡峭程度大幅度降低</li><li>由于操作简便，迁移效率提高，有效提高迁移实施的人效比</li><li>一对多的同步方式，大幅度降低计算资源使用，计算资源只在验证和最终切换时使用</li><li>能够满足多租户、自服务的要求</li><li>源端也可以支持无代理方式，打消用户疑虑，并且适合大规模批量迁移</li><li>高度自动化的验证手段，在完成迁移切换前，能够反复进行验证</li></ul><p><img src="/images/pasted-69.png" alt="upload successful"></p><p>这是CloudEndure的架构图，当然你也可以利用CloudEndure实现跨区域的容灾。</p><p><img src="/images/pasted-70.png" alt="upload successful"></p><p>不过可惜的一点是由于被AWS收购，CloudEndure目前只能支持迁移至AWS，无法满足国内各种云迁移的需求。所以这里为大家推荐一款纯国产化的迁移平台——万博智云的HyperMotion( <a href="https://hypermotion.oneprocloud.com/" target="_blank" rel="noopener">https://hypermotion.oneprocloud.com/</a> )，从原理上与CloudEndure非常相似，同时支持了VMware及OpenStack无代理的迁移，更重要的是覆盖了国内主流的公有云、专有云和私有云的迁移。</p><p><img src="/images/pasted-71.png" alt="upload successful"></p><h2 id="平台重建（Replatforming）方式"><a href="#平台重建（Replatforming）方式" class="headerlink" title="平台重建（Replatforming）方式"></a>平台重建（Replatforming）方式</h2><p>随着云原生提供越来越多的服务，降低了应用架构的复杂度，使得企业能够更专注自己的业务本身开发。但是研发侧工作量的减少意味着这部分成本被转嫁到部署及运维环节，所以DevOps成为在云原生运用中比不可少的一个缓解，也让企业能够更敏捷的应对业务上的复杂变化。</p><p>正如上面所提到的，用户通过少量的改造可以优先使用一部分云原生服务，这种迁移方式我们成为平台重建（Replatforming），目前选择平台重建方式的迁移，多以与用户数据相关的服务为主。常见的包括：数据库服务RDS、对象存储服务、消息队列服务、容器服务等。这些云原生服务的引入，降低了用户运维成本。但是由于云原生服务自身封装非常严密，底层的基础架构层对于用户完全不可见，所以无法用上述Rehost方式进行迁移，必须采用其他的辅助手段完成。</p><p>以关系型数据库为例，每一种云几乎都提供了迁移工具，像AWS DMS，阿里云的DTS，腾讯云的数据传输服务DTS，这些云原生工具都可以支持 MySQL、MariaDB、PostgreSQL、Redis、MongoDB 等多种关系型数据库及 NoSQL 数据库迁移。以MySQL为例，这些服务都巧妙的利用了binlog复制的方式，实现了数据库的在线迁移。</p><p>再以对象存储为例，几乎每一种云都提供了自己的迁移工具，像阿里云的ossimport，腾讯云COS Migration工具，都可以实现本地到云端对象存储的增量迁移。但是在实际迁移时，还应考虑成本问题，公有云的对象存储在存储数据上比较便宜，但是在读出数据时是要根据网络流量和请求次数进行收费的，这就要求我们在设计迁移方案时，充分考虑成本因素。如果数据量过大，还可以考虑采用离线设备方式，例如：AWS的Snowball，阿里云的闪电立方等。这部分就不展开介绍，以后有机会再单独为大家介绍。</p><p><img src="/images/pasted-72.png" alt="upload successful"></p><p>如果选择平台重建方式上云，除了要进行必要的应用改造，还需要选择一款适合你的迁移工具，保证数据能够平滑上云。结合上面的Rehost方式迁移，能够实现业务系统的整体上云效果。由于涉及的服务较多，这里为大家提供一张迁移工具表格供大家参考。</p><p><img src="/images/pasted-89.png" alt="upload successful"></p><h1 id="云原生下的容灾发展趋势"><a href="#云原生下的容灾发展趋势" class="headerlink" title="云原生下的容灾发展趋势"></a>云原生下的容灾发展趋势</h1><p>目前为止，还没有一套平台能够完全满足云原生状态下的统一容灾需求，我们通过以下场景来分析一下，如何才能构建一套统一的容灾平台满足云原生的需求。</p><h2 id="传统架构"><a href="#传统架构" class="headerlink" title="传统架构"></a>传统架构</h2><p>我们以一个简单的Wordpress + MySQL环境为例，传统下的部署环境一般是这样架构的：</p><p><img src="/images/pasted-58.png" alt="upload successful"></p><p>如果为这套应用架构设计一套容灾方案，可以采用以下的方式：</p><ul><li>负载均衡节点容灾：负载均衡分为硬件和软件层面，硬件负载均衡高可靠和容灾往往通过自身的解决方案实现。如果是软件负载均衡，往往需要安装在基础操作系统上，而同城的容灾可以使用软件高可靠的方式实现，而异地的容灾往往是通过提前建立对等节点，或者干脆采用容灾软件的块或者文件级别容灾实现。是容灾切换（Failover）很重要的一个环节。</li><li>Web Server的容灾：Wordpress的运行环境无非是Apache + PHP，由于分离了用于存放用户上传的文件系统，所以该节点几乎是无状态的，通过扩展节点即可实现高可靠，而异地容灾也比较简单，传统的块级别和文件级别都可以满足容灾的需求</li><li>共享文件系统的容灾，图中采用了Gluster的文件系统，由于分布式系统的一致性通常由内部维护，单纯使用块级别很难保证节点的一致性，所以这里面使用文件级别容灾更为精确</li><li>数据库的容灾，单纯依靠存储层面是无法根本实现数据库0丢失数据的，所以一般采用从数据库层面实现，当然如果为了降低成本，数据库的容灾可以简单的使用周期Dump数据库的方式实现，当然如果对可靠性要求较高，还可以使用CDP方式实现</li></ul><p>从以上的案例分析不难看出，传统基础架构下的容灾往往以存储为核心，无论是磁盘阵列的存储镜像，还是基于I/O数据块、字节级的捕获技术，结合网络、数据库和集群的应用级别技术完成高可靠和容灾体系的构建。在整个容灾过程的参与者主要为：主机、存储、网络和应用软件，相对来说比较单一。所以在传统容灾方案中，如何正确解决存储的容灾也就成为了解决问题的关键。</p><h2 id="混合云容灾"><a href="#混合云容灾" class="headerlink" title="混合云容灾"></a>混合云容灾</h2><p>这应该是目前最常见的混合云的方案，也是各大容灾厂商主推的一种方式。这里我们相当于将云平台当成了一套虚拟化平台，几乎没有利用云平台任何特性。在恢复过程中，需要大量人为的接入才能将业务系统恢复到可用状态。这样的架构并不符合云上的最佳实践，但的确是很多业务系统备份或迁移上云后真实的写照。</p><p><img src="/images/pasted-83.png" alt="upload successful"></p><p>这样的架构确实能解决容灾的问题，但是从成本上来说很高，现在我们来换一种方式。我们利用了对象存储和数据库进行一次优化。我们将原有存储服务存放至对象存储中，而使用数据传输服务来进行实时的数据库复制。云主机仍然采用传统的块级别进行同步。一旦出现故障，则需要自动化编排能力，重新将备份进行恢复，在最短时间内根据我们预设的方案进行恢复，完成容灾。</p><p><img src="/images/pasted-84.png" alt="upload successful"></p><h2 id="云上同城容灾架构"><a href="#云上同城容灾架构" class="headerlink" title="云上同城容灾架构"></a>云上同城容灾架构</h2><p>上述的备份方式，实质上就是利用平台重建的方式进行的迁移，既然已经利用迁移进行了备份，那完全可以对架构进行如下改造，形成同城的容灾架构。我们根据云平台的最佳实践，对架构进行了如下调整：</p><p><img src="/images/pasted-85.png" alt="upload successful"></p><p>这个架构不仅实现了应用级高可靠，还能够支撑一定的高并发性，用户在最少改造代价下就能够在同城实现双活的效果。我们来分析一下在云上利用了多少云原生的服务：</p><ul><li>域名解析服务</li><li>VPC服务</li><li>负载均衡服务</li><li>自动伸缩服务</li><li>云主机服务</li><li>对象存储服务</li><li>关系型数据库RDS服务</li></ul><p>除了云主机外，其他服务均是天然就支持跨可用区的高可用特性，对于云主机我们可以制作镜像方式，由自动伸缩服务负责实例的状态。由于云上可用区就是同城容灾的概念，这里我们就实现了同城的业务系统容灾。</p><p>经过调整的架构在一定程度上满足了业务连续性的要求，但是对于数据的安全性仍然缺乏保障。近几年，勒索病毒横行，大量企业为此蒙受巨大损失，所以数据备份是上云后必须实施的。云原生服务本身提供了备份方案，例如云主机的定期快照等，但往往服务比较分散，不容易统一进行管理。同时，在恢复时往往也是只能每一个服务进行恢复，如果业务系统规模较大，也会增加大量的恢复成本。虽然云原生服务解决了自身备份问题，但是将备份重新组织成应用是需要利用自动化的编排能力实现。</p><h2 id="同云异地容灾架构"><a href="#同云异地容灾架构" class="headerlink" title="同云异地容灾架构"></a>同云异地容灾架构</h2><p>大部分的云原生服务都在可用区内，提供了高可靠能力，但是对于跨区域上通常提供的是备份能力。例如：可以将云主机变为镜像，将镜像复制到其他区域内；关系型数据库和对象存储也具备跨域的备份能力。利用这些组件自身的备份能力，外加上云自身资源的编排能力，我们可以实现在容灾可用域将系统恢复至可用状态。那如何触发切换呢？</p><p>这里我们根据业务系统的特点，在云原生的监控上定制告警，利用告警平台的触发能力触发函数计算，完成业务系统的跨域切换，形成异地容灾的效果。</p><p><img src="/images/pasted-86.png" alt="upload successful"></p><h2 id="跨云容灾"><a href="#跨云容灾" class="headerlink" title="跨云容灾"></a>跨云容灾</h2><p>但跨云容灾不像同云容灾时，在不同的可用区之间至少服务是一致的，那么此时，在同云上使用的方法基本失效，完全需要目标云平台的能力或者中立的第三方的解决方案。这里除了数据的备份，还有一点是服务配置的互相匹配。才能完全满足跨云容灾恢复的需求。另外需要考虑的一点就是成本为例，以对象存储为例，是典型的的“上云容易下云难”。所以如何利用云原生资源特性合理设计容灾方案是对成本的极大考验。</p><p><img src="/images/pasted-87.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>云原生容灾还处于早期阶段，目前尚没有完整的平台能够支持以上各种场景的容灾需求，是值得持续探索的话题。云原生容灾以备份为核心，以迁移、恢复和高可靠为业务场景，实现多云之间的自由流转，最终满足用户的业务需求。</p><p>所以，作为面向云原生的容灾平台要解决好三方面的能力：</p><p>一、以数据为核心，让数据在多云之间互相流转。数据是用户核心价值，所以无论底层基础架构如何变化，数据备份一定是用户的刚醒需求。对于不同云原生服务如何解决好数据备份，是数据流转的必要基础。</p><p>二、利用云原生编排能力，实现高度自动化，在数据基础上构建业务场景。利用自动化编排能力实现更多的基于数据层的应用，帮助用户完成更多的业务创新。</p><p>三、灵活运用云原生资源特点，降低总体拥有成本。解决传统容灾投入巨大的问题，让用户的成本真的能像水、电一样按需付费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;趋势&quot;&gt;&lt;a href=&quot;#趋势&quot; class=&quot;headerlink&quot; title=&quot;趋势&quot;&gt;&lt;/a&gt;趋势&lt;/h1&gt;&lt;h2 id=&quot;云原生发展趋势&quot;&gt;&lt;a href=&quot;#云原生发展趋势&quot; class=&quot;headerlink&quot; title=&quot;云原生发展趋势&quot;&gt;&lt;/a&gt;云原生发展趋势&lt;/h2&gt;&lt;p&gt;云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。&lt;/p&gt;
&lt;p&gt;2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
      <category term="云原生" scheme="http://sunqi.site/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
      <category term="云迁移" scheme="http://sunqi.site/tags/%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    
      <category term="云容灾" scheme="http://sunqi.site/tags/%E4%BA%91%E5%AE%B9%E7%81%BE/"/>
    
      <category term="Cloud Native" scheme="http://sunqi.site/tags/Cloud-Native/"/>
    
  </entry>
  
</feed>
