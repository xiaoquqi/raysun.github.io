<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ray&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.me/"/>
  <updated>2020-07-21T13:52:56.872Z</updated>
  <id>http://sunqi.me/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>OpenStack没落了吗？</title>
    <link href="http://sunqi.me/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/"/>
    <id>http://sunqi.me/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/</id>
    <published>2020-04-29T13:20:37.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OpenStack回顾"><a href="#OpenStack回顾" class="headerlink" title="OpenStack回顾"></a>OpenStack回顾</h1><p>OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。</p><p><img src="/images/pasted-42.png" alt="upload successful"></p><h1 id="OpenStack没落了吗？"><a href="#OpenStack没落了吗？" class="headerlink" title="OpenStack没落了吗？"></a>OpenStack没落了吗？</h1><p>我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。</p><a id="more"></a><p>随着容器、K8S等新兴技术的崛起，OpenStack无疑受到了很大的冲击，在之前两年经常看到一些唱衰OpenStack的文章。但是不可否认，目前OpenStack已经进入到了一个稳定阶段，很多私有云、专有云项目都是基于OpenStack提供解决方案。所以我认为并不存在OpenStack没落一说，只是技术发展的必经阶段：当底层逐渐稳定后，关注度往上发展。</p><p>同时，我们也看到，OpenStack基金会也在通过吸纳更多的项目来维持自身的影响力，比如：安全容器项目Kata Container，边缘计算项目StarlingX。</p><p>这是我对目前国内云计算市场的一张不完全总结，从这张图中我们应该可以很清晰的看到OpenStack对国内云计算市场深远的影响。同时，大家也能看出来谁才是真正的OpenStack这个开源项目的既得利益者。</p><p><img src="/images/pasted-44.png" alt="upload successful"></p><h1 id="OpenStack社区大数据"><a href="#OpenStack社区大数据" class="headerlink" title="OpenStack社区大数据"></a>OpenStack社区大数据</h1><p>从A版本开始到今天（2020年4月29日），总共有442家公司为OpenStack社区贡献过代码。排名前三位的分别是：Red Hat, Rackspace和Mirantis。中国唯一入选前十的是华为。</p><p><img src="/images/pasted-45.png" alt="upload successful"></p><p>OpenStack总共出现了706个Official项目，提交代码次数最多的是nova, neutron和cinder项目。</p><p><img src="/images/pasted-46.png" alt="upload successful"></p><p>总共有8523名开发者成功提交过1个以上的commits。从名字分析，前十名中有两位中国人：Zhong Shengping（麒麟云，主要贡献在自动化安装OpenStack相关项目puppet和ansible）和Qiming Teng（IBM 滕启明博士，主要贡献在senlin项目）。当然，我知道国内为OpenStack项目贡献的人很多，在这就不一一列举了。</p><p><img src="/images/pasted-47.png" alt="upload successful"></p><h1 id="OpenStack社区贡献变化趋势"><a href="#OpenStack社区贡献变化趋势" class="headerlink" title="OpenStack社区贡献变化趋势"></a>OpenStack社区贡献变化趋势</h1><p>参与贡献的公司已经呈现明显下降趋势，从国内情况来看，很多OpenStack初创公司也在积极投身K8S相关项目的研究，产品上提供基于容器的PaaS平台，丰富自己的解决方案。<br>从图中可以看到，OpenStack参与公司最多的是在Ocata Release中，参与公司达到了210家，从时间看是在2016年到2017年之间的时间点。这也是国内客户对OpenStack普遍接受的时间点。<br>另外从C版本开始一直到O版本（2011年到2017年）基本每个版本迭代维持20%以上的增长，可见在这个阶段绝大多数公司都看好OpenStack的未来。国内开源领域在这个阶段感觉也是最活跃的，毕竟只有当商业利益和开源目标相吻合时，这个开源项目才能得到最大的支持力度。<br>从O版本之后，参与的公司呈现小幅度下降趋势，不是很明显，大概在10%以内，下跌最明显的阶段是在S版本到T版本，也就是2019年。S版本有161家公司提交代码，而T版本只有126家，而目前U版本已经下降到了119家公司。</p><p><img src="/images/pasted-48.png" alt="upload successful"></p><p>从开发者数量看也呈现出相同的趋势，参与人数最多的是N版本，有2422人提交了commit。而到了S版本开发者仅为1189人，下降了一半还多。</p><p><img src="/images/pasted-49.png" alt="upload successful"></p><p>最后一张图，我们来看一下OpenStack模块数量。在早期OpenStack中一个新的项目获得批准是需要技术委员会批准的，也就是TC Approved，这样的项目到今天为止一共只有20个，主要是OpenStack基础的计算、存储和网络服务，包括：Nova, Neutron, Cinder, Heat, Horizon, Keystone, Ironic, Swift, Ceilometer, Glance, Sahara, Trove, Designate, neutron-lib, sahara的各种插件。<br>但是在2015年，社区决定采用Big Tent模式。Big Tent模式本意是基于OpenStack底层的计算、存储和网络等基础组件，构建更庞大的云原生应用场景，类似AWS。但是由于OpenStack自身部署、升级的复杂性，是社区力量更加分散，这样的设计并没有带来意料之中的效果。我个人理解，这样的生态建设更适合K8S。<br>在A版本中仅有8个模块，到了最新的T版本中，模块数量变为609个，还没有Release的U版本中，模块数量增长为627个。</p><p><img src="/images/pasted-50.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于OpenStack提供的服务属于基础架构层，从生态角度看，团结了各个层面的公司。从硬件的服务器、处理器、网络、存储厂商，到操作系统厂商，再到OpenStack创业公司，应用厂商，直到最终用户。<br>之前我们总说OpenStack是仅次于Linux的世界上第二大开源社区，不知道现在这种说法是否还准确。但是不可否认，OpenStack出现给了原来默默耕耘的开发者们走到前台充分展现的机会，也将国内开源的热潮推向了一个新的高度。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;OpenStack回顾&quot;&gt;&lt;a href=&quot;#OpenStack回顾&quot; class=&quot;headerlink&quot; title=&quot;OpenStack回顾&quot;&gt;&lt;/a&gt;OpenStack回顾&lt;/h1&gt;&lt;p&gt;OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-42.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;OpenStack没落了吗？&quot;&gt;&lt;a href=&quot;#OpenStack没落了吗？&quot; class=&quot;headerlink&quot; title=&quot;OpenStack没落了吗？&quot;&gt;&lt;/a&gt;OpenStack没落了吗？&lt;/h1&gt;&lt;p&gt;我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[阿里云]使用DataQuotient 画像分析筛选优质基金</title>
    <link href="http://sunqi.me/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/"/>
    <id>http://sunqi.me/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/</id>
    <published>2020-04-14T03:10:17.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>该教程是阿里云帮助文档一部分，这里做了进一步完善：<a href="https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK</a></p><h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。</p><pre>4433法则如下：4：代表近一年收益率排名前1/4的基金。4：代表近两年、三年、五年以来，收益率排名前1/4的基金。3：指近六个月收益率排名前1/3的基金。3：指近三个月以来收益率排名前1/3的基金。</pre><p>本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。</p><a id="more"></a><h1 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h1><p><img src="/images/pasted-0.png" alt="upload successful"></p><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul><li>获取基金数据，从天天基金网接口获取股票型基金的不同时间区间的收益率数据</li><li>数据输入MaxCompute</li></ul><h2 id="创建基金信息标签系统"><a href="#创建基金信息标签系统" class="headerlink" title="创建基金信息标签系统"></a>创建基金信息标签系统</h2><ul><li>新建实体，用于将数源和待分析的对象绑定在一起</li><li>绑定标签，将数据源与标签进行绑定</li></ul><h2 id="筛选并导出优质基金群体"><a href="#筛选并导出优质基金群体" class="headerlink" title="筛选并导出优质基金群体"></a>筛选并导出优质基金群体</h2><ul><li>同步标签至RDS，这个例子中因为只是从MaxCompute到RDS，真实环境中有可能从多个数据源同步至目标RDS中，该服务支持数据的合并等</li><li>根据上述预先建立好的模型，新建长期和短期表现较好群体</li><li>计算群体</li><li>导出优质基金列表</li></ul><h1 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h1><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>本教程基于DataQuotient 画像分析、MaxCompute和RDS产品，请确保您已购买该产品。</p><h2 id="获取基础数据"><a href="#获取基础数据" class="headerlink" title="获取基础数据"></a>获取基础数据</h2><p>根据教程提供的线索，从天天基金网获取了全部股票型基金的数据，脚本已经提交到Github上，有需要的可以直接拿过去用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;fund</span><br><span class="line">cd fund</span><br><span class="line">python fund-cli.py -d -v</span><br></pre></td></tr></table></figure><h2 id="在MaxCompute导入数据"><a href="#在MaxCompute导入数据" class="headerlink" title="在MaxCompute导入数据"></a>在MaxCompute导入数据</h2><h3 id="开通MaxCompute服务"><a href="#开通MaxCompute服务" class="headerlink" title="开通MaxCompute服务"></a>开通MaxCompute服务</h3><p>之前我并没有开通过MaxCompute服务，所以需要开通一下DataWorks，才能使用MaxCompute服务。</p><p><img src="/images/pasted-1.png" alt="upload successful"></p><p>这个就是DataWorks控制台，从操作上看局限性很高（比如不支持删除表<br><img src="/images/pasted-2.png" alt="upload successful"></p><p>），所以建议采用IntelliJ IDEA中的MaxCompute Studio插件，安装方式见：<a href="https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1</a><br><img src="/images/pasted-41.png" alt="upload successful"></p><h3 id="创建MaxCompute表"><a href="#创建MaxCompute表" class="headerlink" title="创建MaxCompute表"></a>创建MaxCompute表</h3><p>由于做的时候才发现数据源并没有近五年的数据，所以修改了一下创建语句，先把表名建立起来，之后进入DDL模式进行表创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists fund_profit_stocktype</span><br><span class="line">( &#96;fundid&#96; bigint comment &#39;基金编号&#39;,</span><br><span class="line"> &#96;fundname&#96; string comment &#39;基金名称&#39;,</span><br><span class="line"> &#96;latest3months&#96; double comment &#39;近三月&#39;,</span><br><span class="line"> &#96;latest6months&#96; double comment &#39;近六月&#39;,</span><br><span class="line"> &#96;latest1year&#96; double comment &#39;近一年&#39;,</span><br><span class="line"> &#96;latest2years&#96; double comment &#39;近两年&#39;,</span><br><span class="line"> &#96;latest3years&#96; double comment &#39;近三年&#39;,</span><br><span class="line"> &#96;currentyear&#96; double comment &#39;今年来&#39;,  </span><br><span class="line"> &#96;fromcreated&#96; double comment &#39;成立来&#39;,</span><br><span class="line">) comment &#39;基金信息&#39; ;</span><br></pre></td></tr></table></figure><p><img src="/images/pasted-3.png" alt="upload successful"></p><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>原有文档是通过DDL去创建的数据，由于我们已经将数据保存在CSV文件中，所以我们选择导入方式试一下。</p><p><img src="/images/pasted-4.png" alt="upload successful"></p><p><img src="/images/pasted-5.png" alt="upload successful"></p><p><img src="/images/pasted-7.png" alt="upload successful"></p><p>不知道什么原因，下拉菜单选择并不生效，于是我决定将原有CSV无用字段全部删除，便于后续测试。删除字段的时候，发现我获取的数据并不包含近五年的项，但是包含成立以来的数据，所以对字段进行一下修改。上面的SQL是我修改过的。页面好像并没有删除表的功能，所以为了不影响测试，我重新建了一张表来导入数据。</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><h2 id="关联云计算资源"><a href="#关联云计算资源" class="headerlink" title="关联云计算资源"></a>关联云计算资源</h2><p>回到画像服务，继续进行配置</p><p><img src="/images/pasted-13.png" alt="upload successful"></p><p><img src="/images/pasted-14.png" alt="upload successful"></p><p>我用的主账号的AK/KS竟然提示我权限不足，于是我去查了半天RAM文档，以为MaxCompute需要更精确的授权，结果发现是提示信息误导了我，只是我的project写错了，最后project信息还是回到DataWorks控制台才找到，如下图所示。</p><p>需要添加这么多信息感觉不是特别方便，都是阿里云的资源感觉没有必要使用AK/KS去通讯吧？</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p><img src="/images/pasted-16.png" alt="upload successful"></p><p>配置好后的云计算资源</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><h2 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h2><p>配置好后，可以继续进行标签配置。</p><p><img src="/images/pasted-9.png" alt="upload successful"></p><p><img src="/images/pasted-10.png" alt="upload successful"></p><h3 id="关联MaxCompute表"><a href="#关联MaxCompute表" class="headerlink" title="关联MaxCompute表"></a>关联MaxCompute表</h3><p><img src="/images/pasted-11.png" alt="upload successful"></p><p>如果你没有正确配置云计算资源，是无法看到MaxCompute下的表。<br><img src="/images/pasted-12.png" alt="upload successful"></p><p>绑定表和字段<br><img src="/images/pasted-18.png" alt="upload successful"></p><p><img src="/images/pasted-19.png" alt="upload successful"></p><p><img src="/images/pasted-20.png" alt="upload successful"></p><p>字段绑定后无法删除，需要到详情中删除<br><img src="/images/pasted-21.png" alt="upload successful"></p><p><img src="/images/pasted-22.png" alt="upload successful"></p><p>最开始显示为0，后来又显示出数据总量，但是根据上方提示，MaxCompute是不支持显示的。<br><img src="/images/pasted-23.png" alt="upload successful"></p><p>过了一会又出现了1185的数据总量，不知道为什么<br><img src="/images/pasted-33.png" alt="upload successful"></p><p><img src="/images/pasted-25.png" alt="upload successful"></p><h2 id="筛选出优质基金群体"><a href="#筛选出优质基金群体" class="headerlink" title="筛选出优质基金群体"></a>筛选出优质基金群体</h2><h3 id="同步标签至RDS"><a href="#同步标签至RDS" class="headerlink" title="同步标签至RDS"></a>同步标签至RDS</h3><p><img src="/images/pasted-24.png" alt="upload successful"></p><p><img src="/images/pasted-26.png" alt="upload successful"></p><p><img src="/images/pasted-27.png" alt="upload successful"></p><p><img src="/images/pasted-28.png" alt="upload successful"></p><p><img src="/images/pasted-29.png" alt="upload successful"></p><p>显示同步成功，但是发现输出的信息里有红色的信息，以为是错误，但是仔细一看又是INFO级别的日志，而且显示的太长了。<br><img src="/images/pasted-31.png" alt="upload successful"></p><p><img src="/images/pasted-32.png" alt="upload successful"></p><h3 id="群体画像"><a href="#群体画像" class="headerlink" title="群体画像"></a>群体画像</h3><p>需要切换至群体画像的工作空间。<br><img src="/images/pasted-30.png" alt="upload successful"></p><p>新建短期表现优质的基金。<br><img src="/images/pasted-34.png" alt="upload successful"></p><p>如果配置正确，可以从结果中看出筛选出的结果。<br><img src="/images/pasted-35.png" alt="upload successful"></p><p><img src="/images/pasted-36.png" alt="upload successful"></p><p>新建长期表现优质的基金。<br><img src="/images/pasted-37.png" alt="upload successful"></p><p>使用群体计算，找出二者的交集。这个就是我们希望得到的结果。<br><img src="/images/pasted-38.png" alt="upload successful"></p><p><img src="/images/pasted-39.png" alt="upload successful"></p><p>利用下载功能，可以下载我们筛选出来的基金。<br><img src="/images/pasted-40.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>使用阿里云DataQuotient服务可以快速帮助用户构建用户画像，能够满足多种应用场景，利用函数计算进行数据爬取定期导入到MaxCompute，可以很容易定制出优质基金筛选功能的API接口，供上层业务场景使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该教程是阿里云帮助文档一部分，这里做了进一步完善：&lt;a href=&quot;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h1&gt;&lt;p&gt;4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。&lt;/p&gt;
&lt;pre&gt;
4433法则如下：
4：代表近一年收益率排名前1/4的基金。
4：代表近两年、三年、五年以来，收益率排名前1/4的基金。
3：指近六个月收益率排名前1/3的基金。
3：指近三个月以来收益率排名前1/3的基金。
&lt;/pre&gt;

&lt;p&gt;本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[Python]利用ZooKeeper构建分布式定时任务</title>
    <link href="http://sunqi.me/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/"/>
    <id>http://sunqi.me/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/</id>
    <published>2020-03-24T12:39:06.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>本文涉及的源代码路径：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz</a></p><h1 id="一、目前现状及存在的问题"><a href="#一、目前现状及存在的问题" class="headerlink" title="一、目前现状及存在的问题"></a>一、目前现状及存在的问题</h1><p>在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。</p><img src="/images/blogs/2020-03-24/1.png" class=""><a id="more"></a><h2 id="软件架构设计"><a href="#软件架构设计" class="headerlink" title="软件架构设计"></a>软件架构设计</h2><p>目前云迁移平台的各个服务模块在设计上使用了OpenStack方式，即大部分模块复用了类似Nova的实现框架。即API层直接集成oslo.service中定义好的WSGI Service基类，Worker采用了olso.service中定义好的Service基类，即Eventlet协程方式，API与Worker通讯使用RabbitMQ，API南向接口除少量直接更新数据库操作采用同步接口外，其余所有接口全部使用异步方式。API发送请求后，得到202 Accepted回复，后续通过GET接口不断轮询任务接口等到任务完成。</p><h2 id="高可靠部署"><a href="#高可靠部署" class="headerlink" title="高可靠部署"></a>高可靠部署</h2><p>根据OpenStack官方的HA部署文档（<a href="https://docs.openstack.org/ha-guide/" target="_blank" rel="noopener">https://docs.openstack.org/ha-guide/</a>），将服务分为无状态和有状态两种。无服务状态只需要直接部署多份即可，有状态服务往往需要通过Pacemaker控制副本数量，来保证高可靠。在云迁移平台部署中，我们将全部服务部署于K8S集群中，所以并不需要Pacemaker+Corosync这样的组件（Pacemaker节点上线为16）。但是，由于需要保持定时任务在单一节点被触发（避免任务被重复执行），所以承载定时快照的模块只能同时存在一个容器在运行，无法构成Active-Active（简称AA方式）模式。这样的部署方式，也造成了上述提到的AP模式对扩展性的瓶颈。</p><h1 id="二、问题思路及解决方案"><a href="#二、问题思路及解决方案" class="headerlink" title="二、问题思路及解决方案"></a>二、问题思路及解决方案</h1><h2 id="思路一、利用消息队列解耦任务分配与任务执行"><a href="#思路一、利用消息队列解耦任务分配与任务执行" class="headerlink" title="思路一、利用消息队列解耦任务分配与任务执行"></a>思路一、利用消息队列解耦任务分配与任务执行</h2><p>从上述对现状的描述，我们不难看出，现有任务分配与任务执行是在同一个任务中执行的，当存在大量任务时，任务执行会对任务产生产生很大的影响。同时，由于任务执行唯一性的需要，在部署上只能采用上述的AP模式，导致任务无法由多个任务同时执行。<br />所以，我们可以将任务分解为分配和执行两个阶段。任务分配上，单纯的进行任务生成，由于任务生成相对较快，生成后的任务发送至消息队列，由无状态性的Worker接收后执行。这样就解决了单点执行的效率低下问题。<br />但是这样的解决方案仍然存在缺陷，我们在任务生成的模块仍然必须需要采用AP模式部署，来保证任务的唯一性。如果在任务数量非常庞大时，该部分仍然是一个瓶颈；另外一方面这样的实现方式，我们需要将任务生成部分单独拆分出一个模块，同时增加了开发和部署上的复杂度，所以我们来看一下第二种解决思路。</p><img src="/images/blogs/2020-03-24/2.png" class=""><h2 id="思路二、利用Zookeeper构建可扩展的分布式定时任务"><a href="#思路二、利用Zookeeper构建可扩展的分布式定时任务" class="headerlink" title="思路二、利用Zookeeper构建可扩展的分布式定时任务"></a>思路二、利用Zookeeper构建可扩展的分布式定时任务</h2><p>为了解决思路一的局限性，我们本质上要解决的是任务执行的分布式问题，即如何让Worker不重复的判定任务的归属后再执行，由被动改为主动。</p><p>我们来看以下几种场景：<br />1、假定我们现在有3个Worker可以用于任务生成，在某一个时间点，将同时产生100个任务。如何由这3个Worker主动产生属于自身负责的任务？<br />2、我们知道大部分云平台目前都有云原生的弹性扩展服务，如果我们结合云平台的弹性扩展服务自动将我们用于任务生成的Worker动态进行调整时，例如变为6个时，还能保证这100个任务能够被自动的由6个节点不重复的产生呢？<br />3、当负载降低后，节点数量由6个变为3个后，如何恢复场景1的状态呢？保证任务不漏生成呢？</p><img src="/images/blogs/2020-03-24/3.png" class=""><p>如果想达到以上场景需求，需要以下几个条件：<br />1、节点之间能够准确知道其他节点的存在——利用Zookeeper进行服务发现<br />2、尽量合理的进行任务（对象）分布，同时兼顾节点增加和减少时，降低对象分配时的位移——利用一致性哈希环</p><h1 id="三、技术要点"><a href="#三、技术要点" class="headerlink" title="三、技术要点"></a>三、技术要点</h1><h2 id="1、Zookeeper"><a href="#1、Zookeeper" class="headerlink" title="1、Zookeeper"></a>1、Zookeeper</h2><p>对于Zookeeper的解释网络上有各种各样的详细集成，这里就不再赘述了，这里我直接引用了这篇文章（<a href="https://www.jianshu.com/p/50becf121c66）中开头的内容：" target="_blank" rel="noopener">https://www.jianshu.com/p/50becf121c66）中开头的内容：</a></p><blockquote><p>官方文档上这么解释zookeeper，它是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。<br>上面的解释有点抽象，简单来说zookeeper=文件系统+监听通知机制。</p></blockquote><img src="/images/blogs/2020-03-24/4.png" class=""><p>从我们应用场景的角度看，Zookeeper帮我们解决了Worker之间相互认识的过程，及时、准确的告诉我们：到底现在有多少个和我相同的活跃节点存在。至于底层是如何实现的，感兴趣的同学可以查看具体的Zookeeper实现原理文档，这里只介绍与我们实现相关的内容。</p><h2 id="2、一致性Hash"><a href="#2、一致性Hash" class="headerlink" title="2、一致性Hash"></a>2、一致性Hash</h2><p>又是一个经典的算法，相关的文章也很多，这里推荐大家几篇，这里摘抄出对理解我们实现有价值的内容。 参考文档：</p><ul><li>《面试必备：什么是一致性Hash算法？》<a href="https://zhuanlan.zhihu.com/p/34985026" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34985026</a></li><li>《五分钟看懂一致性哈希算法》<a href="https://juejin.im/post/5ae1476ef265da0b8d419ef2" target="_blank" rel="noopener">https://juejin.im/post/5ae1476ef265da0b8d419ef2</a></li><li>《一致性hash在分布式系统中的应用》<a href="http://www.firefoxbug.com/index.php/archives/2791/" target="_blank" rel="noopener">http://www.firefoxbug.com/index.php/archives/2791/</a></li></ul><h3 id="2-1-关于一致性哈希算法"><a href="#2-1-关于一致性哈希算法" class="headerlink" title="2.1 关于一致性哈希算法"></a>2.1 关于一致性哈希算法</h3><blockquote><p>一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。但现在一致性hash算法在分布式系统中也得到了广泛应用。</p></blockquote><h3 id="2-2-一致性哈希算法在缓存技术中的应用"><a href="#2-2-一致性哈希算法在缓存技术中的应用" class="headerlink" title="2.2 一致性哈希算法在缓存技术中的应用"></a>2.2 一致性哈希算法在缓存技术中的应用</h3><img src="/images/blogs/2020-03-24/5.png" class=""><blockquote><p>上述的方式虽然提升了性能，我们不再需要对整个Redis服务器进行遍历！但是，使用上述Hash算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！<br>试想一下，如果4台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？很简单，多增加几台缓存服务器不就行了！假设：我们增加了一台缓存服务器，那么缓存服务器的数量就由4台变成了5台。那么原本hash(a.png) % 4 = 2 的公式就变成了hash(a.png) % 5 = ？ ， 可想而知这个结果肯定不是2的，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变！换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据（还记得上一篇的《缓存雪崩》吗？）！<br>同样的，假设4台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从4台变为3台，也是会出现上述的问题！<br>所以，我们应该想办法不让这种情况发生，但是由于上述Hash算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，Hash一致性算法（一致性Hash算法）诞生了！</p></blockquote><h3 id="2-3-一致性哈希在缓存中的应用"><a href="#2-3-一致性哈希在缓存中的应用" class="headerlink" title="2.3 一致性哈希在缓存中的应用"></a>2.3 一致性哈希在缓存中的应用</h3><p>初始状态，将节点映射到哈希环中</p><img src="/images/blogs/2020-03-24/6.png" class=""><p>将对象映射到换后，找到负责处理的Node节点。</p><img src="/images/blogs/2020-03-24/7.png" class=""><p>容错性，Node C出现故障后，只需要将Object C迁移到Node D上。</p><img src="/images/blogs/2020-03-24/8.png" class=""><p>增加节点，此时增加了Node X，在Node C右侧，那么此时只有Object C需要移动到Node X节点。</p><img src="/images/blogs/2020-03-24/9.png" class=""><h2 id="3、tooz和kazoo"><a href="#3、tooz和kazoo" class="headerlink" title="3、tooz和kazoo"></a>3、tooz和kazoo</h2><p>Python中操作zookeeper的项目叫kazoo（<a href="https://kazoo.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://kazoo.readthedocs.io/en/latest/</a>）。<br />tooz是OpenStack中为简化开发人员操作分布式系统一致性所开发的组件，利用底层组件抽象出一致性组成员管理、分布式锁、选举、构建哈希环等。tooz除支持zookeeper作为后端，还可以支持Memcached、Redis、IPC、File、PostgreSQL、MySQL、Etcd、Consul等。<br />有关于tooz的发展历史可以参考：<a href="https://julien.danjou.info/python-distributed-membership-lock-with-tooz/" target="_blank" rel="noopener">https://julien.danjou.info/python-distributed-membership-lock-with-tooz/</a><br />这里我们主要使用tooz操作zookeeper实现我们的一致性组及一致性哈希。</p><h2 id="4、oslo相关项目"><a href="#4、oslo相关项目" class="headerlink" title="4、oslo相关项目"></a>4、oslo相关项目</h2><p>这几年一直在做OpenStack项目，从OpenStack项目中学习到很多设计、架构、研发管理等各种新知识、新理念。oslo项目就是在OpenStack不断的迭代中产生的公共项目库，这些库可以让你非常轻松的构建基于Python的构建近似于OpenStack的分布式、可扩展的微服务系统。<br />之前在从事OpenStack开发培训过程中，有专门的一节课去讲解OpenStack中用到的公共库，其中oslo相关项目就是非常重要的一部分内容。olso项目设计的库非常多，在这个内容中会涉及到oslo.config、oslo.log、oslo.service、oslo.utils和oslo.messaging项目。严格意义上来说，为了更精准控制任务，我们还应该引入oslo.db项目由数据库持久化的维护任务运行状态，包括任务回收等工作，但是本次内容主要讲解的是zookeeper，所以这部分的内容需要开发者在实际项目中去实现。<br />关于olso开发的内容，我会以视频课程的形式为大家讲解，敬请期待。</p><h1 id="四、实现过程"><a href="#四、实现过程" class="headerlink" title="四、实现过程"></a>四、实现过程</h1><h2 id="1、Zookeeper部署"><a href="#1、Zookeeper部署" class="headerlink" title="1、Zookeeper部署"></a>1、Zookeeper部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f zookeeper.yml -d up</span><br></pre></td></tr></table></figure><p>启动完成后，将使用本地的三个容器作为zookeeper的三个节点和三个不同的端口（2181/2182/2183）便于zookeeper连接。如果在生产环境中部署时，可以使用云原生服务或部署在多个可用区的方式，保证高可靠。</p><img src="/images/blogs/2020-03-24/10.png" class=""><h3 id="Zookeeper常用命令行"><a href="#Zookeeper常用命令行" class="headerlink" title="Zookeeper常用命令行"></a>Zookeeper常用命令行</h3><p>进入容器，就可以使用zkCli.sh进入zookeeper的CLI模式。如果是初次接触zookeeper，可以把zookeeper理解成一个文件系统，这里我们常用的命令就是ls。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it zookeeper_zoo1_1 bash</span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">zkCli.sh</span><br></pre></td></tr></table></figure><p>看到这样的提示，就表示连接成功了。</p><img src="/images/blogs/2020-03-24/11.png" class=""><p>如上面提到的zookeeper的存储结构所示，我们先从根节点（/）进行获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/12.png" class=""><p>这里zookeeper目录属于保留的目录，我们来看一下tooz的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/13.png" class=""><p>如果我们想继续查看distribution_tasks的内容，可以继续使用ls命令获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz/distribution_tasks</span><br></pre></td></tr></table></figure><p>通常我们会为每一个加入的节点取一个唯一的标识，当节点加入后我们使用ls命令就可以看到，如果离开了，则返回为空。</p><img src="/images/blogs/2020-03-24/14.png" class=""><p>zookeeper常用的命令还包括get，stat等获取value和更详细的信息，还包含更新节点操作set和删除节点rm。这里面就不做一一介绍了，我们直接操作zookeeper主要是为了帮助大家更好的理解程序逻辑。<br />具体的命令行信息可以参考：<a href="https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm</a></p><h2 id="2、tooz基本使用方法"><a href="#2、tooz基本使用方法" class="headerlink" title="2、tooz基本使用方法"></a>2、tooz基本使用方法</h2><p>关于tooz的两个示例主要来自于这篇博客：<a href="https://dzone.com/articles/scaling-a-polling-python-application-with-tooz" target="_blank" rel="noopener">https://dzone.com/articles/scaling-a-polling-python-application-with-tooz</a><br />原文中的例子是有些Bug的，这里面进行重新进行了优化和整理，并且使用zookeeper替代etcd3驱动。</p><h3 id="2-1-组成员（tooz-test-tooz-test-group-members-py）"><a href="#2-1-组成员（tooz-test-tooz-test-group-members-py）" class="headerlink" title="2.1 组成员（tooz/test_tooz/test_group_members.py）"></a>2.1 组成员（tooz/test_tooz/test_group_members.py）</h3><p>在这个例子中，我们主要为大家演示tooz如何进行组成员的管理。结合我们自身的需求，这里的成员就是每一个Worker。通过这个列子我们将观察三种不同场景的变化：<br />1、初始状态下，我们只能看到一个成员；<br />2、当启动了一个新的进程时，第一个成员马上会发现有第二个成员的加入；<br />3、同时，当我们用CTRL + C结束某一个进程时，另外一个活着的进程会立即发现组成员的变化。</p><h4 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h4><p>这里为了更直观表达，用时序图来说明程序的运行逻辑。</p><img src="/images/blogs/2020-03-24/15.png" class=""><h4 id="完整的代码"><a href="#完整的代码" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the group</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    c.create_group(group).get()</span><br><span class="line"><span class="keyword">except</span> coordination.GroupAlreadyExist:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the group</span></span><br><span class="line">c.join_group(group).get()</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Print the members list</span></span><br><span class="line">        <span class="comment">#c.run_watchers()</span></span><br><span class="line">        members = c.get_members(group).get()</span><br><span class="line">        print(<span class="string">"[%s]Current nodes in cluster: %s"</span> % (</span><br><span class="line">            current_time(), members))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line">    print(<span class="string">"[%s]After leave cluster nodes: %s"</span> % (</span><br><span class="line">        current_time(), c.get_members(group).get()))</span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果"><a href="#执行效果" class="headerlink" title="执行效果"></a>执行效果</h4><p>第一个成员</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_group_members.py client1 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/16.png" class=""><p>第二个成员加入，观察第一个成员的标准输出，为了观察加入集群的时间，我们加入了date</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date &amp;&amp; python test_group_members.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/17.png" class=""><p>第一个脚本的标准输出，在16:07:27秒的时候加入了集群:</p><img src="/images/blogs/2020-03-24/18.png" class=""><p>将第二个成员关闭，直接在第二个成员脚本按CTRL + C，首先观察第二个成员的输出：</p><img src="/images/blogs/2020-03-24/19.png" class=""><p>第一个成员的输出，在16:08:51分时，集群中已经没有了第二个成员了：</p><img src="/images/blogs/2020-03-24/20.png" class=""><h3 id="2-2-一致性哈希（tooz-test-tooz-test-ping-py）"><a href="#2-2-一致性哈希（tooz-test-tooz-test-ping-py）" class="headerlink" title="2.2 一致性哈希（tooz/test_tooz/test_ping.py）"></a>2.2 一致性哈希（tooz/test_tooz/test_ping.py）</h3><p>这个模拟测试中，使用分布式任务去ping某一个C类网段(255个IP地址)中的全部IP地址，如果由一个任务去完成，那么只能顺序执行，无法满足并发需求，这里采用一致性哈希算法，让任务分布在各个Worker上。为了节省时间，我们将原有程序中的实际ping换成了time.sleep等待方式。<br />另外在程序启动后，我们默认等待10秒等待其他成员(member)加入，在实际开发过程中，还需要对任务的状态进行严格控制，防止同一任务重复被执行，在演示代码中主要偏重演示分布式，所以并没有在任务状态上增加过多处理。</p><h4 id="时序图-1"><a href="#时序图-1" class="headerlink" title="时序图"></a>时序图</h4><img src="/images/blogs/2020-03-24/21.png" class=""><p>代码需要说明的几点：<br />0、在程序开始时，我们默认等待了10秒，等待其他节点加入，如果在循环开始后，再有新加入的节点时，由于并不知道第一个节点已经处理过的任务，所以在第二个Worker加入后根据当时哈希环对之前的任务重新分配并执行，造成了重复执行，这个问题需要通过额外的手段（例如数据库记录先前执行的任务状态）监控任务状态来防止任务重新执行。<br />1、代码中使用了tooz内置的Hash环，但是也可以在外部自己构建哈希环，我们在后续最终的例子中还是采用了外部构建哈希环的方法。<br />2、Tooz partitioner依赖于watchers，所以在每次循环的时候必须要调用run_watchers即使获取成员的加入和离开。<br />3、无论是group还是member在变量传递时都要变成bytes类型，这样可以确保对象的唯一性，所以在代码处理上都用到了encode()方法。<br />4、<strong>tooz_hash</strong>方法需要在使用Partition时自己实现，能够唯一标识出对象的方法，例如ID、名称等信息。</p><h4 id="完整的代码-1"><a href="#完整的代码-1" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the partitioned group</span></span><br><span class="line">p = c.join_partitioned_group(group)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Host</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hostname)</span>:</span></span><br><span class="line">        self.hostname = hostname</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tooz_hash__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a unique byte identifier so Tooz</span></span><br><span class="line"><span class="string">           can distribute this object."""</span></span><br><span class="line">        <span class="keyword">return</span> self.hostname.encode()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&lt;%s: %s&gt;"</span> % (self.__class__.__name__, self.hostname)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ping</span><span class="params">(self)</span>:</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">hosts_to_ping = [Host(<span class="string">"192.168.10.%d"</span> % i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>)]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"[%s]Waiting 10 seconds for other members..."</span> % current_time())</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"[%s]Current members: %s"</span> % (</span><br><span class="line">    current_time(), c.get_members(group).get()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> host <span class="keyword">in</span> hosts_to_ping:</span><br><span class="line">            c.run_watchers()</span><br><span class="line">            <span class="keyword">if</span> p.belongs_to_self(host):</span><br><span class="line">                print(<span class="string">"[%s]%s belongs to %s"</span> % (</span><br><span class="line">                    current_time(), host, p.members_for_object(host)))</span><br><span class="line">                <span class="keyword">if</span> host.ping():</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        print(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">        print(<span class="string">"Waiting for next loop..."</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果-1"><a href="#执行效果-1" class="headerlink" title="执行效果"></a>执行效果</h4><p>我们分别使用两个不同的窗口，同时启动两个Worker，我们可以很明显的看到主机被分配到两个不同的Worker中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client1 group1</span><br><span class="line">python test_pring.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/22.png" class=""><p>加入第三个Worker，可以看到一部分任务又被分配给了第三个Worker上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client3 group2</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/23.png" class=""><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>暂停第二个Worker，我们看到第二个Worker被停止后，任务重新被平衡到Worker1和Worker2上。</p><img src="/images/blogs/2020-03-24/24.png" class=""><h2 id="3、构建分布式定时任务"><a href="#3、构建分布式定时任务" class="headerlink" title="3、构建分布式定时任务"></a>3、构建分布式定时任务</h2><p>为了保持代码的兼容性，所以这里的实现是基于目前OpenStack体系的实现。另外，将任务发送给消息的部分在这个例子中并没有体现。示例代码仍然重复实现上述ping的例子，部分代码参考于Sahara项目的实现。</p><p>由于代码量较大，这里不贴出全部代码，仅仅对核心实现进行分析，完整代码请参考：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks</a></p><h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── coordinator.py -&gt; 一致性哈希的实现，该类中并没有直接使用上述tooz的partition，而是自己重新实现了HashRing</span><br><span class="line">├── periodic.py -&gt; 定时任务，基于oslo_service的PeriodicTasks基类</span><br><span class="line">├── service.py -&gt; Service类，继承于oslo.service的Service基类</span><br><span class="line">└── test_periodic_task.py -&gt; 程序入口</span><br></pre></td></tr></table></figure><h3 id="coordinator-py"><a href="#coordinator-py" class="headerlink" title="coordinator.py"></a>coordinator.py</h3><p>Coordinator是关键实现，所以这里重点对该类进行解释，在period task中需要调用coordinator即可实现分布式触发定时任务。</p><p>在coordinator.py中共实现了两个类，Coordinator和HashRing。<br />1、Coordinator类主要是针对tooz中对group members相关操作的封装，类似我们在tooz中的第一个例子；<br />2、HashRing是继承于Coordinator类，在功能上接近于tooz中Hash和Partition的实现，但是更简洁，tooz构建HashRing的用的PartitionNumber是32(2^5)，而我们用的是40，更大的数字会带来更均匀的分布但是会导致构建成本增加<br />3、HashRing中最重要的方法就是get_subset，通过映射到HashRing上的ID来判断Object的归属Worker</p><img src="/images/blogs/2020-03-24/25.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashRing</span><span class="params">(Coordinator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backend_url, group_id)</span>:</span></span><br><span class="line">        self.group_id = group_id</span><br><span class="line">        self.replicas = CONF.hash_ring_replicas_count</span><br><span class="line">        super(HashRing, self).__init__(backend_url)</span><br><span class="line">        self.join_group(group_id)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_hash</span><span class="params">(key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> int(</span><br><span class="line">            hashlib.md5(str(key).encode(<span class="string">'utf-8'</span>)).hexdigest(), <span class="number">16</span>)  <span class="comment"># nosec</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_ring</span><span class="params">(self)</span>:</span></span><br><span class="line">        ring = &#123;&#125;</span><br><span class="line">        members = self.get_members(self.group_id)</span><br><span class="line">        LOG.info(<span class="string">"Coordinator members: %s"</span> % members)</span><br><span class="line">        <span class="keyword">for</span> member <span class="keyword">in</span> members:</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> range(self.replicas):</span><br><span class="line">                hashed_key = self._hash(<span class="string">'%s:%s'</span> % (member, r))</span><br><span class="line">                ring[hashed_key] = member</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ring, sorted(ring.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_object</span><span class="params">(self, object, ring, sorted_keys)</span>:</span></span><br><span class="line">        <span class="string">"""Checks if this object belongs to this member or not"""</span></span><br><span class="line">        hashed_key = self._hash(object.id)</span><br><span class="line">        position = bisect.bisect(sorted_keys, hashed_key)</span><br><span class="line">        position = position <span class="keyword">if</span> position &lt; len(sorted_keys) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ring[sorted_keys[position]] == self.member_id</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_subset</span><span class="params">(self, objects)</span>:</span></span><br><span class="line">        <span class="string">"""Returns subset that belongs to this member"""</span></span><br><span class="line">        <span class="keyword">if</span> self.coordinator:</span><br><span class="line">            ring, keys = self._build_ring()</span><br><span class="line">            <span class="keyword">if</span> ring:</span><br><span class="line">                <span class="keyword">return</span> [obj <span class="keyword">for</span> obj <span class="keyword">in</span> objects <span class="keyword">if</span> self._check_object(</span><br><span class="line">                    obj, ring, keys)]</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">return</span> objects</span><br></pre></td></tr></table></figure><h3 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h3><p>分别在两个Terminal中运行脚本，可以看到Host被均匀的分布在两个Worker中执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_periodic_task.py</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/26.png" class=""><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>通过以上实例，我们了解了如何通过zookeeper构建分布式系统并进行任务调度，当然zookeeper在分布式系统还有更多的应用场景值得我们去学习。另外，OpenStack中很多抽象出来的模块对快速构建Python分布式系统是非常有帮助的，值得我们学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文涉及的源代码路径：&lt;a href=&quot;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、目前现状及存在的问题&quot;&gt;&lt;a href=&quot;#一、目前现状及存在的问题&quot; class=&quot;headerlink&quot; title=&quot;一、目前现状及存在的问题&quot;&gt;&lt;/a&gt;一、目前现状及存在的问题&lt;/h1&gt;&lt;p&gt;在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。&lt;/p&gt;
&lt;img src=&quot;/images/blogs/2020-03-24/1.png&quot; class=&quot;&quot;&gt;
    
    </summary>
    
    
    
      <category term="Zookeeper" scheme="http://sunqi.me/tags/Zookeeper/"/>
    
      <category term="微服务" scheme="http://sunqi.me/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="分布式" scheme="http://sunqi.me/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="Python" scheme="http://sunqi.me/tags/Python/"/>
    
      <category term="tooz" scheme="http://sunqi.me/tags/tooz/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architecture Associate Practice</title>
    <link href="http://sunqi.me/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/"/>
    <id>http://sunqi.me/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/</id>
    <published>2020-01-30T13:35:00.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。</p><p>总得分: 76%<br>主题得分:<br>1.0. Design Resilient Architectures 89%<br>2.0. Define Performant Architectures 71%<br>3.0. Specify Secure Applications and Architectures 50%<br>4.0. Design Cost-Optimized Architectures 100%<br>5.0. Define Operationally Excellent Architectures 100%</p><p>个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。</p><a id="more"></a><h1 id="您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"><a href="#您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。" class="headerlink" title="(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"></a>(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。</h1><p>该区域有三个可用区（us-west-2a，us-west-2b和us-west-2c)可以使用，如果us-west-2中的任何可用区变得不可用，以下哪种部署可以提供容错功能？（选择两顶◊)<br>A. 在us-west-2a中有2个EC2实例，在us-west-2b中有2个EC2实例，在us-west-2c中有2个EC2实例<br>B. 在 us- west- 2 a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中没有EC2实例<br>C. 在us-west-2a中有4个 EC2实例，在us-west-2b中有2个 EC2实例，在us-west-2c中有2个 EC2实例<br>D. 在 us- west- 2 a中有6个 EC2实例，在us-west-2b中有6个 EC2实例，在us-west-2c中没有EC2实例<br>E. 在us-west-2a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中有3个 EC2实例</p><p>Answer: DE</p><p>该道题的重点是始终运行6个EC2实例，所以当一个区Down掉，仍然能保证有6台实例的答案为正确答案。</p><h1 id="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处"><a href="#一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处" class="headerlink" title="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?"></a>一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?</h1><p>A. Elastic Beanstalk<br>B. CloudFormation<br>C. AMS CodeBuild<br>D. AMS CodeDeploy</p><p>Answer: B</p><h1 id="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"><a href="#解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。" class="headerlink" title="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"></a>解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。</h1><p>Amazon Glacier中的哪种功能可以帮助满足该要求？<br>A. 文件库锁定<br>B. 加速检索<br>c. 批量检索<br>D. 标准检索</p><p>Answer: B</p><blockquote><p>问：如何从该服务检索数据？</p><p>当您请求从 S3 Glacier 检索数据时，即表示您启动了一个存档检索作业。当检索作业完成后，您的数据将在 24 小时内可供下载或通过 Amazon Elastic Compute Cloud (Amazon EC2) 访问。有三种方式可以检索数据，每种具有不同的访问时间和成本：加急、标准和批量检索。</p><p>问：什么是加急检索？</p><p>当您偶尔需要加急请求档案子集时，可以使用加急检索来快速访问您的数据。除了最大的存档 (250MB+) 以外，对于使用加急检索方式访问的所有数据，通常在 1-5 分钟内即可使用。有两种加急检索：按需和预置。当我们可以在 1-5 分钟内完成检索时，就可以实施按需检索。所提供的请求将确保在您需要时能够获得加急检索的能力。</p></blockquote><h1 id="一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"><a href="#一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。" class="headerlink" title="(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"></a>(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。</h1><p>该组织应使用哪种解决方案以满足该要求？<br>A. AMS KMS API<br>B. AMS Certificate Manager<br>C. 具有 STS 的 API Gateway<br>D. IAM访问密钥</p><p>Answer: A</p><blockquote><p>问：什么是 Amazon EBS 加密？</p><p>Amazon EBS 加密提供 EBS 数据卷、引导卷和快照的无缝加密，无需构建和维护安全密钥管理基础设施。EBS 加密可使用 Amazon 托管的密钥或您使用 AWS Key Management Service (KMS) 创建和管理的密钥来给您的数据加密，从而保障静态数据的安全性。加密还发生在托管 EC2 实例的服务器上，当数据在 EC2 实例和 EBS 存储之间移动时提供数据加密。有关详细信息，请参阅 Amazon EC2 用户指南中的“Amazon EBS”加密。</p><p>问：什么是 AWS Key Management Service (KMS)？</p><p>AWS KMS 是一项托管服务，可让您轻松创建和控制加密数据所用的加密密钥。AWS Key Management Service 可与其他 AWS 服务集成，包括 Amazon EBS、Amazon S3 和 Amazon Redshift，可让您轻松使用您管理的加密密钥加密您的数据。AWS Key Management Service 还能与 AWS CloudTrail 集成，从而为您提供所有密钥的使用记录，帮助您满足监管和合规性要求。要了解有关 KMS 的更多信息，请访问 AWS Key Management Service 产品页面。</p></blockquote><h1 id="一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"><a href="#一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。" class="headerlink" title="(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"></a>(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。</h1><p>以下哪种步骤组合满足安全策略要求？（选择两顶)<br>A. 启用 Amazon Redshift 增强 VPC 路由。<br>B. 创建集群安全组以允许Amazon Redshift集群访问Amazon S3<br>C. 在公有子网中创建NAT网关以允许Amazon Redshift集群访问Amazon S3<br>D. 创建并配置Amazon S3 VPC终端节点<br>E. 在私有子网中设置NAT网关以允许Amazon Redshift集群访问AmazonS3</p><p>Answer: AD</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html</a></p><p>使用 VPC 终端节点<br>可以使用 VPC 终端节点创建 VPC 中的 Amazon Redshift 集群与 Amazon Simple Storage Service (Amazon S3) 之间的托管连接。在执行此操作时，您的集群与 Amazon S3 数据之间的 COPY 和 UNLOAD 流量将保留在您的 Amazon VPC 中。可以将终端节点策略附加到您的终端节点，以便更严格地管理对数据的访问。例如，可以向 VPC 终端节点添加策略以仅允许将数据上传到您账户中的特定 Amazon S3 存储桶。</p><p>重要<br>目前，Amazon Redshift 仅支持连接到 Amazon S3 的 VPC 终端节点。当 Amazon VPC 添加对其他 AWS 服务的支持以使用 VPC 终端节点时，Amazon Redshift 也将支持这些 VPC 终端节点连接。要使用 VPC 终端节点连接到 Amazon S3 存储桶，Amazon Redshift 集群与其连接到的 Amazon S3 存储桶必须在同一个 AWS 区域中。</p><p>要使用 VPC 终端节点，请为集群所在的 VPC 创建 VPC 终端节点，然后为集群启用增强型 VPC 路由。可以在 VPC 中创建集群时启用增强型 VPC 路由，也可以修改 VPC 中的集群以使用增强型 VPC 路由。</p><p>VPC 终端节点使用路由表来控制 VPC 中的集群和 Amazon S3 之间的流量路由。与指定路由表关联的子网中的所有集群会自动使用该终端节点来访问服务。</p><p>您的 VPC 使用与集群流量匹配的最具体的/最严格的路由来决定路由流量的方式。例如，假设路由表中有一条路由用于所有指向 Internet 网关和 Amazon S3 终端节点的 Internet 流量 (0.0.0.0/0)。在这种情况下，对所有传送到 Amazon S3 的流量优先使用终端节点路由。这是因为 Amazon S3 服务的 IP 地址范围比 0.0.0.0/0 更具体。在此示例中，所有其他 Internet 流量（包括定位到其他 AWS 区域内的 Amazon S3 存储桶的流量）将流向 Internet 网关。</p><p>有关创建终端节点的更多信息，请参阅 Amazon VPC 用户指南 中的 VPC 终端节点。</p><p>您使用终端节点策略控制从集群到包含数据文件的 Amazon S3 存储桶的访问。默认情况下，创建终端节点向导会附加一个终端节点策略，该策略不会进一步限制来自 VPC 中的任何用户或服务的访问。要实现更具体的控制，您可以选择附加一个自定义终端节点策略。有关更多信息，请参阅 Amazon VPC 用户指南 中的使用终端节点策略。</p><p>使用终端节点不收取任何额外费用。采用标准的数据传输和资源使用计费方式。有关定价的更多信息，请参阅 Amazon EC2 定价。</p></blockquote><blockquote><p><a href="https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html" target="_blank" rel="noopener">https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html</a></p><p>Amazon Redshift 增强型 VPC 路由</p><p>在使用 Amazon Redshift 增强型 VPC 路由时，Amazon Redshift 会强制通过您的 Amazon VPC 路由集群和数据存储库之间的所有 COPY 和 UNLOAD 流量。通过使用增强型 VPC 路由，您可以使用标准 VPC 功能，例如 VPC 安全组、网络访问控制列表 (ACL)、VPC 终端节点、VPC 终端节点策略、Internet 网关和域名系统 (DNS) 服务器，如 Amazon VPC 用户指南 中所述。 您可以使用这些功能来严格管理 Amazon Redshift 集群与其他资源之间的数据流。在使用增强型 VPC 路由通过您的 VPC 路由流量时，也可以使用 VPC 流日志来监视 COPY 和 UNLOAD 流量。</p><p>如果未启用增强型 VPC 路由，则 Amazon Redshift 会通过 Internet 路由流量，包括至 AWS 网络中的其他服务的流量。</p><p>重要<br>由于增强型 VPC 路由影响了 Amazon Redshift 访问其他资源的方式，因此，除非您正确配置 VPC，否则 COPY 和 UNLOAD 命令可能会失败。您必须专门在集群的 VPC 和数据资源之间创建网络路径，如下所述。</p><p>在对已启用增强型 VPC 路由的集群执行 COPY 或 UNLOAD 命令时，您的 VPC 会使用最严格 或最具体的可用网络路径来将流量路由到指定资源。</p></blockquote><h1 id="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"><a href="#一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。" class="headerlink" title="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"></a>一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。</h1><p>哪种存储服务可满足这些要求？<br>A. Amazon Redshift<br>B. Amazon RDS<br>C. ElastiCache<br>D. DynamoDB</p><p>Answer: A</p><h1 id="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"><a href="#解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。" class="headerlink" title="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"></a>解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。</h1><p>架构师应使用哪种AWS服务来实现该目的？<br>A. Amazon STS<br>B. Amazon SQS<br>C. Lambda<br>D. Amazon SNS</p><p>Answer: D</p><blockquote><p>Amazon Simple Notification Service (SNS) 是一种高度可用、持久、安全、完全托管的发布/订阅消息收发服务，可以轻松分离微服务、分布式系统和无服务器应用程序。Amazon SNS 提供了面向高吞吐量、多对多推送式消息收发的主题。借助 Amazon SNS 主题，发布系统可以向大量订阅终端节点（包括 Amazon SQS 队列、AWS Lambda 函数和 HTTP/S Webhook 等）扇出消息，从而实现并行处理。此外，SNS 可用于使用移动推送、短信和电子邮件向最终用户扇出通知。</p></blockquote><h1 id="解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"><a href="#解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。" class="headerlink" title="(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"></a>(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。</h1><p>架构师应使用哪种解决方案以满足这些要求？<br>A. 任务的IAM角色<br>B. EC2实例的 IAM角色<br>C. EC2实例的 IAM实例配置文件<br>D. 安全组规则</p><p>Answer: A</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p>借助 Amazon ECS 任务的 IAM 角色，您可以指定一个可由任务中的容器使用的 IAM 角色。应用程序必须使用 AWS 凭证签署其 AWS API 请求，并且此功能提供了一个管理凭证的策略以供应用程序使用，类似于 Amazon EC2 实例配置文件为 EC2 实例提供凭证的方式。您可以将 IAM 角色与 ECS 任务定义或 RunTask API 操作关联，而不是为容器创建和分配 AWS 凭证或使用 EC2 实例的角色。之后，任务的容器中的应用程序可以使用 AWS 开发工具包或 CLI 向授权的 AWS 服务发出 API 请求。</p></blockquote><h1 id="一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"><a href="#一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。" class="headerlink" title="一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"></a>一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。</h1><p>哪种Amazon RDS引擎满足这些要求？<br>A. MySQL<br>B. Microsoft SQL Server<br>C. Oracle<br>D. Amazon Aurora</p><p>Answer: D</p><blockquote><p><a href="https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6" target="_blank" rel="noopener">https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6</a></p><p>Amazon Aurora 副本复制是毫秒级别，而MySQL是秒级别</p></blockquote><h1 id="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"><a href="#管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。" class="headerlink" title="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"></a>管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。</h1><p>哪种AMS服务可以执行该操作？<br>A. Amazon EBS<br>B. Amazon EFS<br>C. Amazon S3<br>D. Amazon EC2实例存储</p><p>Answer: B</p><h1 id="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"><a href="#一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。" class="headerlink" title="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"></a>一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。</h1><p>哪种解决方案可满足这些要求而不会影响其他运行的应用程序？（选择两顶)<br>A. 在Web服务器的子网上创建一个网络ACL，允许HTTPS端口 443入站流量，并将源指定为0.0.0.0/0。<br>B. 创建一个允许来自Anywhere (0.0.0.0/0)的 HTTPS端口 443入站流量的Web服务器安全组，并将其应用于Web服务器。<br>C. 创建一个允许MySQL端口 3306入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。<br>D. 在数据库子网上创建一个网络ACL，允许Web服务器的MySQL端口 3306入站流量，并拒绝所有出站流量。<br>E. 创建一个允许HTTPS端口 443入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。</p><p>Answer: BC</p><h1 id="一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"><a href="#一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。" class="headerlink" title="一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"></a>一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。</h1><p>备份这些卷的最灵活方法是什么？<br>A. 定期创建EBS快照。<br>B. 启用EBS卷加密。<br>C. 创建脚本以将数据复制到EC2实例存储。<br>D. 在两个EBS卷之间镜像数据。</p><p>Answer: A</p><h1 id="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"><a href="#解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。" class="headerlink" title="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"></a>解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。</h1><p>哪种AMS服务可满足这些要求？<br>A. Amazon S3<br>B. Amazon EBS<br>C. Amazon EFS<br>D. Amazon Storage Gateway VTL</p><p>Answer: A</p><h1 id="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"><a href="#AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。" class="headerlink" title="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"></a>AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。</h1><p>哪种解决方案能满足这些要求？<br>A. 启动一个NAT网关并为0.0.0.0/0添加路由<br>B. 附加一个VPC终端节点并为0.0.0.0/0添加路由<br>C. 附加一个Internet网关并为0.0.0.0/0添加路由<br>D. 在公有子网中部署NAT实例并为0.0.0.0/0添加路由</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p><p>Internet 网关是一种横向扩展、支持冗余且高度可用的 VPC 组件，可实现 VPC 中的实例与 Internet 之间的通信。因此它不会对网络流量造成可用性风险或带宽限制。</p><p>NAT 网关<br>您可以使用网络地址转换 (NAT) 网关允许私有子网中的实例连接到 Internet 或其他 AWS 服务，但阻止 Internet 发起与这些实例的连接。有关 NAT 的更多信息，请参阅NAT。<br>您在账户中创建和使用 NAT 网关会产生费用。NAT 网关小时使用费率和数据处理费率适用于此。Amazon EC2 数据传输费同样适用。有关更多信息，请参阅 Amazon VPC 定价。</p></blockquote><h1 id="待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"><a href="#待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。" class="headerlink" title="(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"></a>(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。</h1><p>您可以选择哪种设计选顶以在保持弹性的同时优化成本？（选择两顶)<br>A. 修改Auto Scaling组终止策略以先终止最老的实例。<br>B. 修改Auto Scaling组终止策略以先终止最新的实例。<br>C. 修改Auto Scaling组冷却计时器。<br>D. 修改Auto Scaling策略以使用计划的缩放操作。<br>E. 修改触发Auto Scaling缩减策略的CloudWatch警报周期。</p><p>Answer: BC</p><p>这道题目前纠结点在于AB两个选项，从文档中可知有一种策略结束类型叫ClosestToNextInstanceHour类型更适合该题目。如果从这个角度说，新的实例好像更靠近最近计费时间点这个选项。</p><h1 id="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶"><a href="#对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶" class="headerlink" title="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)"></a>对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)</h1><p>A. 使用Amazon RDS的Web应用程序<br>B. 企业数据仓库<br>C. 长时间运行的工作进程<br>D. 静态网站<br>E. 每晚运行一次的管理任务</p><p>Answer: AD</p><h1 id="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"><a href="#一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。" class="headerlink" title="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"></a>一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。</h1><p>哪种是满足这些要求的最经济高效的存储选顶？<br>A. 具有加速检索的Amazon Glacier<br>B. Amazon S3标准-低频率访问<br>C. Amazon EFS<br>D. Amazon S3 标准</p><p>Answer: B</p><h1 id="一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"><a href="#一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。" class="headerlink" title="一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"></a>一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。</h1><p>最经济高效的存储类型是什么？<br>A. Amazon EBS 预置 IOPS SSD<br>B. Amazon EBS吞吐量优化HDD<br>C. Amazon EBS 通用型 SSD<br>D. Amazon EFS</p><p>Answer: C</p><h1 id="一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"><a href="#一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。" class="headerlink" title="一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"></a>一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。</h1><p>哪种架构能以最经济高效的方式处理该工作负载？<br>A. 使用Kinesis数据传输流存储文件，并使用Uirtoda进行处理。<br>B. 使用SQS队列存储文件，然后，一组EC2实例访问该文件。<br>C. 将文件存储在EBS卷中，然后，其他EC2实例可以访问该文件以进行处理。<br>D. 将文件存储在S3存储捅中，并使用Amazon S3事件通知调用Lambda函数以处理该文件。</p><p>Answer: D</p><h1 id="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"><a href="#一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。" class="headerlink" title="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"></a>一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。</h1><p>该公司应釆取哪些措施以改进网站的用户体验？<br>A. 将数字资产移到到Amazon Glacier中。<br>B. 使用CloudFront缓存静态内容。<br>C. 调整图像以使其变小。<br>D. 使用保留的EC2实例。</p><p>Answer: B</p><h1 id="您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。"><a href="#您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。" class="headerlink" title="您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。"></a>您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。</h1><p>应使用哪种方法可将凭证安全地传送到该应用程序？<br>A. 使用实例用户数据将API凭证传送到实例。<br>B. 将API凭证作为对象存储在Amazon S3中。<br>C. 将API凭证嵌入到JAR文件中。<br>D. 将IAM角色分配给EC2实例。</p><p>Answer: D</p><h1 id="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的"><a href="#一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的" class="headerlink" title="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:"></a>一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:</h1><p>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=de" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=de</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=en" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=en</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=es" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=es</a><br>应如何配置CloudFront以使用正确的语言提供缓存的数据？<br>A. 将Cookie转发到原始地址。<br>B. 基于查询字符串参数。<br>C. 在原始地址中缓存对象。<br>D. 提供动态内容。</p><p>Answer: B</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html</a></p><p>一些 Web 应用程序使用查询字符串将信息发送到源。查询字符串是 Web 请求的一部分，显示在 ? 字符之后；该字符串可以包含一个或多个使用 &amp; 字符分隔的参数。在以下示例中，查询字符串包括两个参数 color=red 和 size=large：</p></blockquote><h1 id="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"><a href="#解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。" class="headerlink" title="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"></a>解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。</h1><p>最适合该使用案例的决策是什么？<br>A. 将文件存储在Amazon EBS上，并创建一个生命周期策略以在三个月后删除这些文件。<br>B. 将文件存储在Amazon S3中，并创建一个生命周期策略以在三个月后删除这些文件。<br>C. 将文件存储在Amazon Glacier中，并创建一个生命周期策略以在三个月后删除这些文件。<br>D. 将文件存储在Amazon EFS上，并创建一个生命周期策略以在三个月后删除这些文件。</p><p>Answer: B</p><h1 id="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"><a href="#一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。" class="headerlink" title="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"></a>一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。</h1><p>该团队应该选择哪种服务？<br>A. Amazon EFS<br>B. Amazon RedShift<br>C. DynamoDB<br>D. AWS CloudFormation</p><p>Answer: C</p><h1 id="一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。"><a href="#一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。" class="headerlink" title="(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。"></a>(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。</h1><p>架构师应如何最大限度地提高Amazon S3性能？<br>A. 在每个对象名称前面添加随机字符串。<br>B. 使用STANDARD_IA存储类<br>C. 在每个对象名称前面添加当前日期。<br>D. 在S3存储桶上启用版本控制。</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>当从 Amazon S3 上传和检索存储时，您的应用程序可以轻松地实现每秒数千个事务的请求性能。Amazon S3 会自动扩展至高请求速率。例如，您的应用程序可以在存储桶中实现至少每秒每个前缀 3,500 个 PUT/COPY/POST/DELETE 请求和 5,500 个 GET/HEAD 请求。对存储桶中的前缀数量没有限制。您可以通过并行读取来增加读取或写入性能。例如，如果您在 Amazon S3 存储桶中创建 10 个前缀以并行处理读取，则可以将读取性能扩展到每秒 55,000 个读取请求。<br>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><blockquote><p>2018年7月17日 Amazon S3 宣布提高请求速率性能(<a href="https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a>)</p><p>Amazon S3 现在提供了更高的性能，支持每秒至少 3500 个数据添加请求、每秒 5500 个数据检索请求，而且无需额外费用，这可以节省大量处理时间。每个 S3 前缀均支持这些请求速率，因此可以轻松实现显著的性能提升。</p><p>目前在 Amazon S3 上运行的应用程序均可享受此性能改进，而无需实施任何更改；在 S3 上构建新应用程序的客户无需进行任何应用程序自定义即可享受此性能。Amazon S3 对并行请求的支持意味着您可以按照计算集群的系数扩展 S3 性能，而无需对应用程序进行任何自定义。性能按前缀扩展，因此您可以并行使用尽可能多的前缀，从而实现所需的吞吐量。前缀的数量没有限制。</p><p>在这种 S3 请求速率性能提升推出后，先前任何为加速性能而随机化对象前缀的指南均被淘汰。也就是说，您现在可以在 S3 对象命名中使用逻辑或顺序命名模式，而不会产生任何性能影响。所有 AWS 区域现在均已提供此改进。有关更多信息，请访问 Amazon S3 开发人员指南。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。&lt;/p&gt;
&lt;p&gt;总得分: 76%&lt;br&gt;主题得分:&lt;br&gt;1.0. Design Resilient Architectures 89%&lt;br&gt;2.0. Define Performant Architectures 71%&lt;br&gt;3.0. Specify Secure Applications and Architectures 50%&lt;br&gt;4.0. Design Cost-Optimized Architectures 100%&lt;br&gt;5.0. Define Operationally Excellent Architectures 100%&lt;/p&gt;
&lt;p&gt;个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Practice" scheme="http://sunqi.me/tags/ACA-Practice/"/>
    
  </entry>
  
  <entry>
    <title>[Digitalcloud.Training]AWS CERTIFIED SOLUTIONS ARCHITECT ASSOCIATE</title>
    <link href="http://sunqi.me/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/"/>
    <id>http://sunqi.me/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/</id>
    <published>2020-01-14T08:49:16.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>该模拟题出自DigitalCloud Training的模拟题，一共20道题，相对来说答案比较准确，第一次答正确率只有60%，看起来还有不太扎实的知识点，并且英文多了不太爱仔细阅读也是准确率低的原因。</p><p>另外，今天在AWS培训官网上看到ACA考试在3月份会推出全新的试题，所以还需要抓紧时间考过。</p><a id="more"></a><h1 id="答题情况统计"><a href="#答题情况统计" class="headerlink" title="答题情况统计"></a>答题情况统计</h1><p>Categories<br>AWS Analytics 100%<br>AWS Application Integration 100%<br>AWS Compute 33.33%<br>AWS Database 66.67%<br>AWS Management &amp; Governance 100%<br>AWS Networking &amp; Content Delivery 33.33%<br>AWS Security, Identity, &amp; Compliance 50%<br>AWS Storage 33.33%</p><h1 id="1-Question"><a href="#1-Question" class="headerlink" title="1. Question"></a>1. Question</h1><p>A Solutions Architect has been asked to suggest a solution for analyzing data in S3 using standard SQL queries. The solution should use a serverless technology.<br>Which AWS service can the Architect use?</p><p>A. Amazon RedShift<br>B. AWS Data Pipeline<br>C. AWS Glue<br>D. Amazon Athena</p><p>Answer: D</p><p>Correct<br>Explanation:<br>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run<br>Amazon RedShift is used for analytics but cannot analyze data in S3<br>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. It is not used for analyzing data in S3<br>AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals</p><p>References:<br><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a></p><h1 id="2-Question"><a href="#2-Question" class="headerlink" title="2. Question"></a>2. Question</h1><p>A systems integration company that helps customers migrate into AWS repeatedly build large, standardized architectures using several AWS services. The Solutions Architects have documented the architectural blueprints for these solutions and are looking for a method of automating the provisioning of the resources.<br>Which AWS service would satisfy this requirement?</p><p>A. AWS OpsWorks<br>B. AWS CloudFormation<br>C. AWS CodeDeploy<br>D. Elastic Beanstalk</p><p>Answer: B</p><p>Correct<br>Explanation:<br>CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts<br>Elastic Beanstalk is a PaaS service that helps you to build and manage web applications<br>AWS OpsWorks is a configuration management service that helps you build and operate highly dynamic applications, and propagate changes instantly<br>AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a></p><h1 id="3-Question"><a href="#3-Question" class="headerlink" title="3. Question"></a>3. Question</h1><p>A data-processing application runs on an i3.large EC2 instance with a single 100 GB EBS gp2 volume. The application stores temporary data in a small database (less than 30 GB) located on the EBS root volume. The application is struggling to process the data fast enough, and a Solutions Architect has determined that the I/O speed of the temporary database is the bottleneck.<br>What is the MOST cost-efficient way to improve the database response times?</p><p>A. Enable EBS optimization on the instance and keep the temporary files on the existing volume<br>B. Put the temporary database on a new 50-GB EBS gp2 volume<br>C. Move the temporary database onto instance storage<br>D. Put the temporary database on a new 50-GB EBS io1 volume with a 3000 IOPS allocation</p><p>Answer: C</p><p>Incorrect<br>Explanation:<br>EC2 Instance Stores are high-speed ephemeral storage that is physically attached to the EC2 instance. The i3.large instance type comes with a single 475GB NVMe SSD instance store so it would be a good way to lower cost and improve performance by using the attached instance store. As the files are temporary, it can be assumed that ephemeral storage (which means the data is lost when the instance is stopped) is sufficient.<br>Enabling EBS optimization will not lower cost. Also, EBS Optimization is a network traffic optimization, it does not change the I/O speed of the volume.<br>Moving the DB to a new 50-GB EBS gp2 volume will not result in a performance improvement as you get IOPS allocated per GB so a smaller volume will have lower performance.<br>Moving the DB to a new 50-GB EBS io1 volume with a 3000 IOPS allocation will improve performance but is more expensive so will not be the most cost-efficient solution.</p><p>References:<br><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/</a></p><h1 id="4-Question"><a href="#4-Question" class="headerlink" title="4. Question"></a>4. Question</h1><p>An application you are designing receives and processes files. The files are typically around 4GB in size and the application extracts metadata from the files which typically takes a few seconds for each file. The pattern of updates is highly dynamic with times of little activity and then multiple uploads within a short period of time.<br>What architecture will address this workload the most cost efficiently?</p><p>A. Upload files into an S3 bucket, and use the Amazon S3 event notification to invoke a Lambda function to extract the metadata<br>B. Place the files in an SQS queue, and use a fleet of EC2 instances to extract the metadata<br>C. Store the file in an EBS volume which can then be accessed by another EC2 instance for processing<br>D. Use a Kinesis data stream to store the file, and use Lambda for processing</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Storing the file in an S3 bucket is the most cost-efficient solution, and using S3 event notifications to invoke a Lambda function works well for this unpredictable workload<br>Kinesis data streams runs on EC2 instances and you must therefore provision some capacity even when the application is not receiving files. This is not as cost-efficient as storing them in an S3 bucket prior to using Lambda for the processing<br>SQS queues have a maximum message size of 256KB. You can use the extended client library for Java to use pointers to a payload on S3 but the maximum payload size is 2GB<br>Storing the file in an EBS volume and using EC2 instances for processing is not cost efficient</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/</a><br><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><h1 id="5-Question"><a href="#5-Question" class="headerlink" title="5. Question"></a>5. Question</h1><p>A Solutions Architect needs to deploy an HTTP/HTTPS service on Amazon EC2 instances that will be placed behind an Elastic Load Balancer. The ELB must support WebSockets.<br>How can the Architect meet these requirements?</p><p>A. Launch an Application Load Balancer (ALB)<br>B. Launch a Network Load Balancer (NLB)<br>C. Launch a Classic Load Balancer (CLB)<br>D. Launch a Layer-4 Load Balancer</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Both the ALB and NLB support WebSockets. However, only the ALB supports HTTP/HTTPS listeners. The NLB only supports TCP, TLS, UDP, TCP_UDP.<br>The CLB does not support WebSockets.<br>A “Layer-4 Load Balancer” is not suitable, we need a layer 7 load balancer for HTTP/HTTPS.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a><br><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a><br><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html</a></p><h1 id="6-Question"><a href="#6-Question" class="headerlink" title="6. Question"></a>6. Question</h1><p>You are building an application that will collect information about user behavior. The application will rapidly ingest large amounts of dynamic data and requires very low latency. The database must be scalable without incurring downtime. Which database would you recommend for this scenario?</p><p>A. RDS with Microsoft SQL<br>B. RedShift<br>C. DynamoDB<br>D. RDS with MySQL</p><p>Answer: C</p><p>Correct<br>Explanation:<br>Amazon Dynamo DB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability<br>Push button scaling means that you can scale the DB at any time without incurring downtime<br>DynamoDB provides low read and write latency<br>RDS uses EC2 instances so you have to change your instance type/size in order to scale compute vertically<br>RedShift uses EC2 instances as well, so you need to choose your instance type/size for scaling compute vertically, but you can also scale horizontally by adding more nodes to the cluster<br>Rapid ingestion of dynamic data is not an ideal use case for RDS or RedShift</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a></p><h1 id="7-Question"><a href="#7-Question" class="headerlink" title="7. Question"></a>7. Question</h1><p>You are running an Auto Scaling Group (ASG) with an Elastic Load Balancer (ELB) and a fleet of EC2 instances. Health checks are configured on the ASG to use EC2 status checks. The ELB has determined that an EC2 instance is unhealthy and has removed it from service. However, you noticed that the instance is still running and has not been terminated by the ASG.<br>What would be an explanation for this behavior?</p><p>A. The health check grace period has not yet expired<br>B. The ELB health check type has not been selected for the ASG and so it is unaware that the instance has been determined to be unhealthy by the ELB and has been removed from service<br>C. Connection draining is enabled and the ASG is waiting for in-flight requests to complete<br>D. The ASG is waiting for the cooldown timer to expire before terminating the instance</p><p>Answer: B</p><p>Incorrect<br>Explanation:<br>If using an ELB it is best to enable ELB health checks as otherwise EC2 status checks may show an instance as being healthy that the ELB has determined is unhealthy. In this case the instance will be removed from service by the ELB but will not be terminated by Auto Scaling<br>Connection draining is not the correct answer as the ELB has taken the instance out of service so there are no active connections<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check<br>More information on ASG health checks:<br>By default uses EC2 status checks<br>Can also use ELB health checks and custom health checks<br>ELB health checks are in addition to the EC2 status checks<br>If any health check returns an unhealthy status the instance will be terminated<br>With ELB an instance is marked as unhealthy if ELB reports it as OutOfService<br>A healthy instance enters the InService state<br>If an instance is marked as unhealthy it will be scheduled for replacement<br>If connection draining is enabled, Auto Scaling waits for in-flight requests to complete or timeout before terminating instances<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check (300 seconds by default)</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/</a></p><h1 id="8-Question"><a href="#8-Question" class="headerlink" title="8. Question"></a>8. Question</h1><p>A solutions Architect is designing a new workload where an AWS Lambda function will access an Amazon DynamoDB table.<br>What is the MOST secure means of granting the Lambda function access to the DynamoDB table?</p><p>A. Create an identity and access management (IAM) role allowing access from AWS Lambda and assign the role to the DynamoDB table<br>B. Create an identity and access management (IAM) role with the necessary permissions to access the DynamoDB table, and assign the role to the Lambda function<br>C. Create a DynamoDB username and password and give them to the Developer to use in the Lambda function<br>D. Create an identity and access management (IAM) user and create access and secret keys for the user. Give the user the necessary permissions to access the DynamoDB table. Have the Developer use these keys to access the resources</p><p>Answer: B</p><p>Correct<br>Explanation:<br>The most secure method is to use an IAM role so you don’t need to embed any credentials in code and can tightly control the services that your Lambda function can access. You need to assign the role to the Lambda function, NOT to the DynamoDB table<br>You should not provide a username and password to the Developer to use with the function. This is insecure – always avoid using credentials in code!<br>You should not use an access key and secret ID to access DynamoDB. Again, this means embedding credentials in code which should be avoided.</p><p>References:<br><a href="https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/</a></p><h1 id="9-Question"><a href="#9-Question" class="headerlink" title="9. Question"></a>9. Question</h1><p>Your company has offices in several locations around the world. Each office utilizes resources deployed in the geographically closest AWS region. You would like to implement connectivity between all of the VPCs so that you can provide full access to each other’s resources. As you are security conscious you would like to ensure the traffic is encrypted and does not traverse the public Internet. The topology should be many-to-many to enable all VPCs to access the resources in all other VPCs.<br>How can you successfully implement this connectivity using only AWS services? (choose 2)</p><p>A. Use inter-region VPC peering<br>B. Use software VPN appliances running on EC2 instances<br>C. Use VPC endpoints between VPCs<br>D. Implement a fully meshed architecture<br>E. Implement a hub and spoke architecture</p><p>Answer: AD</p><p>Incorrect<br>Explanation:<br>Peering connections can be created with VPCs in different regions (available in most regions now)<br>Data sent between VPCs in different regions is encrypted (traffic charges apply)<br>You cannot do transitive peering so a hub and spoke architecture would not allow all VPCs to communicate directly with each other. For this you need to establish a mesh topology<br>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services, it does not provide full VPC to VPC connectivity<br>Using software VPN appliances to connect VPCs together is not the best solution as it is cumbersome, expensive and would introduce bandwidth and latency constraints (amongst other problems)</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a></p><h1 id="10-Question"><a href="#10-Question" class="headerlink" title="10. Question"></a>10. Question</h1><p>A research company is developing a data lake solution in Amazon S3 to analyze huge datasets. The solution makes infrequent SQL queries only. In addition, the company wants to minimize infrastructure costs.<br>Which AWS service should be used to meet these requirements?</p><p>A. Amazon Athena<br>B. Amazon Redshift Spectrum<br>C. Amazon Aurora<br>D. Amazon RDS for MySQL</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run – this satisfies the requirement to minimize infrastructure costs for infrequent queries.<br>Amazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. However, RedShift nodes run on EC2 instances, so for infrequent queries this will not minimize infrastructure costs.<br>Amazon RDS and Aurora are not suitable solutions for analyzing datasets on S3 – these are both relational databases typically used for transactional (not analytical) workloads.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/</a><br><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><h1 id="11-Question"><a href="#11-Question" class="headerlink" title="11. Question"></a>11. Question</h1><p>An Architect is designing a serverless application that will accept images uploaded by users from around the world. The application will make API calls to back-end services and save the session state data of the user to a database.<br>Which combination of services would provide a solution that is cost-effective while delivering the least latency?</p><p>A. Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>B. Amazon S3, API Gateway, AWS Lambda, Amazon RDS<br>C. API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>D. Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, Amazon RDS</p><p>Answer: A</p><p>Incorrect<br>Explanation:<br>Amazon CloudFront caches content closer to users at Edge locations around the world. This is the lowest latency option for uploading content. API Gateway and AWS Lambda are present in all options. DynamoDB can be used for storing session state data<br>The option that presents API Gateway first does not offer a front-end for users to upload content to<br>Amazon RDS is not a serverless service so this option can be ruled out<br>Amazon S3 alone will not provide the least latency for users around the world unless you have many buckets in different regions and a way of directing users to the closest bucket (such as Route 3 latency based routing). However, you would then need to manage replicating the data</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/</a><br><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p><h1 id="12-Question"><a href="#12-Question" class="headerlink" title="12. Question"></a>12. Question</h1><p>A training provider hosts a website using Amazon API Gateway on the front end. Recently, there has been heavy traffic on the website and the company wants to control access by allowing authenticated traffic from paying students only.<br>How should the company limit access to authenticated users only? (choose 2)</p><p>A. Deploy AWS KMS to identify users<br>B. Allow X.509 certificates to authenticate traffic<br>C. Assign permissions in AWS IAM to allow users<br>D. Limit traffic through API Gateway<br>E. Allow users that are authenticated through Amazon Cognito</p><p>Answer: CE<br>Incorrect<br>Explanation:</p><p>API Gateway supports multiple mechanisms for controlling and managing access to your API. These include resource policies, standard IAM roles and policies, Lambda authorizers, and Amazon Cognito user pools.<br>Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.<br>IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs as well as who can invoke them.<br>Limiting traffic through the API Gateway will not filter authenticated traffic, it will just limit overall invocations. This may prevent users from connecting who have a legitimate need.<br>X.509 certificates are not a method of authentication you can use with API Gateway.<br>AWS KMS is used for key management not user identification.</p><p>References:<br><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/</a></p><h1 id="13-Question"><a href="#13-Question" class="headerlink" title="13. Question"></a>13. Question</h1><p>An Auto Scaling Group is unable to respond quickly enough to load changes resulting in lost messages from another application tier. The messages are typically around 128KB in size.<br>What is the best design option to prevent the messages from being lost?</p><p>A. Launch an Elastic Load Balancer<br>B. Store the messages on Amazon S3<br>C. Use larger EC2 instance sizes<br>D. Store the messages on an SQS queue</p><p>Answer: D</p><p>Correct<br>Explanation:<br>In this circumstance the ASG cannot launch EC2 instances fast enough. You need to be able to store the messages somewhere so they don’t get lost whilst the EC2 instances are launched. This is a classic use case for decoupling and SQS is designed for exactly this purpose<br>Amazon Simple Queue Service (Amazon SQS) is a web service that gives you access to message queues that store messages waiting to be processed. SQS offers a reliable, highly-scalable, hosted queue for storing messages in transit between computers. An SQS queue can be used to create distributed/decoupled applications<br>Storing the messages on S3 is potentially feasible but SQS is the preferred solution as it is designed for decoupling. If the messages are over 256KB and therefore cannot be stored in SQS, you may want to consider using S3 and it can be used in combination with SQS by using the Amazon SQS Extended Client Library for Java<br>An ELB can help to distribute incoming connections to the back-end EC2 instances however if the ASG is not scaling fast enough then there aren’t enough resources for the ELB to distributed traffic to</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/</a></p><h1 id="14-Question"><a href="#14-Question" class="headerlink" title="14. Question"></a>14. Question</h1><p>Your company would like to restrict the ability of most users to change their own passwords whilst continuing to allow a select group of users within specific user groups.<br>What is the best way to achieve this? (choose 2)</p><p>A. Under the IAM Password Policy deselect the option to allow users to change their own passwords<br>B. Create an IAM Policy that grants users the ability to change their own password and attach it to the individual user accounts<br>C. Create an IAM Policy that grants users the ability to change their own password and attach it to the groups that contain the users<br>D. Create an IAM Role that grants users the ability to change their own password and attach it to the groups that contain the users<br>E. Disable the ability for all users to change their own passwords using the AWS Security Token Service</p><p>Answer:AC</p><p>Incorrect<br>Explanation:<br>A password policy can be defined for enforcing password length, complexity etc. (applies to all users)<br>You can allow or disallow the ability to change passwords using an IAM policy and you should attach this to the group that contains the users, not to the individual users themselves<br>You cannot use an IAM role to perform this function<br>The AWS STS is not used for controlling password policies</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/</a></p><h1 id="15-Question"><a href="#15-Question" class="headerlink" title="15. Question"></a>15. Question</h1><p>A Solutions Architect has created a VPC design that meets the security requirements of their organization. Any new applications that are deployed must use this VPC design.<br>How can project teams deploy, manage, and delete VPCs that meet this design with the LEAST administrative effort?</p><p>A. Deploy an AWS CloudFormation template that defines components of the VPC<br>B. Use AWS Elastic Beanstalk to deploy both the VPC and the application<br>C. Clone the existing authorized VPC for each new project<br>D. Run a script that uses the AWS Command Line interface to deploy the VPC</p><p>Answer: A</p><p>Correct<br>Explanation:<br>CloudFormation allows you to define your infrastructure through code and securely and repeatably deploy the infrastructure with minimal administrative effort. This is a perfect use case for CloudFormation.<br>You can use a script to create the VPCs using the AWS CLI however this would be a lot more work to create and manage the scripts.<br>You cannot clone VPCs.<br>You cannot deploy the VPC through Elastic Beanstalk – you need to deploy the VPC first and then deploy your application using Beanstalk.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a><br><a href="https://aws.amazon.com/cloudformation/" target="_blank" rel="noopener">https://aws.amazon.com/cloudformation/</a></p><h1 id="16-Question"><a href="#16-Question" class="headerlink" title="16. Question"></a>16. Question</h1><p>Your organization has a data lake on S3 and you need to find a solution for performing in-place queries of the data assets in the data lake. The requirement is to perform both data discovery and SQL querying, and complex queries from a large number of concurrent users using BI tools.<br>What is the BEST combination of AWS services to use in this situation? (choose 2)</p><p>A. AWS Glue for the ad hoc SQL querying<br>B. AWS Lambda for the complex queries<br>C. RedShift Spectrum for the complex queries<br>D. Amazon Athena for the ad hoc SQL querying</p><p>Answer: CD</p><p>Incorrect<br>Explanation:<br>Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a data warehouse<br>You can use both Athena and Redshift Spectrum against the same data assets. You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads<br>AWS Lambda is a serverless technology for running functions, it is not the best solution for running analytics queries<br>AWS Glue is an ETL service</p><p>References:<br><a href="https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html</a><br><a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">https://aws.amazon.com/redshift/</a><br><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a></p><h1 id="17-Question"><a href="#17-Question" class="headerlink" title="17. Question"></a>17. Question</h1><p>You have recently enabled Access Logs on your Application Load Balancer (ALB). One of your colleagues would like to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?</p><p>A. Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files<br>B. Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files<br>C. Configure Access Logs to be delivered to S3 and use EMR for processing the log files<br>D. Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files</p><p>Answer: C</p><p>Correct<br>Explanation:<br>Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon S3<br>Neither Kinesis or EC2 provide a hosted Hadoop service<br>You cannot configure access logs to be delivered to DynamoDB</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a></p><h1 id="18-Question"><a href="#18-Question" class="headerlink" title="18. Question"></a>18. Question</h1><p>A company is deploying a big data and analytics workload. The analytics will be run from a fleet of thousands of EC2 instances across multiple AZs. Data needs to be stored on a shared storage layer that can be mounted and accessed concurrently by all EC2 instances. Latency is not a concern however extremely high throughput is required.<br>What storage layer would be most suitable for this requirement?</p><p>A. Amazon EFS in General Purpose mode<br>B. Amazon EFS in Max I/O mode<br>C. Amazon S3<br>D. Amazon EBS PIOPS</p><p>Answer: B</p><p>Correct<br>Explanation:<br>Amazon EFS file systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations<br>Amazon S3 is not a storage layer that can be mounted and accessed concurrently<br>Amazon EBS volumes cannot be shared between instances</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/</a><br><a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><h1 id="19-Question"><a href="#19-Question" class="headerlink" title="19. Question"></a>19. Question</h1><p>An application launched on Amazon EC2 instances needs to publish personally identifiable information (PII) about customers using Amazon SNS. The application is launched in private subnets within an Amazon VPC.<br>Which is the MOST secure way to allow the application to access service endpoints in the same region?</p><p>A. Use a proxy instance<br>B. Use a NAT gateway<br>C. Use AWS PrivateLink<br>D. Use an Internet Gateway</p><p>Answer: C</p><p>Correct<br>Explanation:<br>To publish messages to Amazon SNS topics from an Amazon VPC, create an interface VPC endpoint. Then, you can publish messages to SNS topics while keeping the traffic within the network that you manage with the VPC. This is the most secure option as traffic does not need to traverse the Internet.<br>Internet Gateways are used by instances in public subnets to access the Internet and this is less secure than an VPC endpoint.<br>A NAT Gateway is used by instances in private subnets to access the Internet and this is less secure than an VPC endpoint.<br>A proxy instance will also use the public Internet and so is less secure than a VPC endpoint.</p><p>References:<br><a href="https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a></p><h1 id="20-Question"><a href="#20-Question" class="headerlink" title="20. Question"></a>20. Question</h1><p>A retail organization is deploying a new application that will read and write data to a database. The company wants to deploy the application in three different AWS Regions in an active-active configuration. The databases need to replicate to keep information in sync.<br>Which solution best meets these requirements?</p><p>A. Amazon Aurora Global Database<br>B. Amazon DynamoDB with global tables<br>C. Amazon Athena with Amazon S3 cross-region replication<br>D. AWS Database Migration Service with change data capture</p><p>Answer: B</p><p>Incorrect<br>Explanation:<br>Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution presented that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.<br>Amazon Athena with S3 cross-region replication is not suitable. This is not a solution that provides a transactional database solution (Athena is used for analytics), or active-active synchronization.<br>Amazon Aurora Global Database provides read access to a database in multiple regions – it does not provide active-active configuration with bi-directional synchronization (though you can failover to your read-only DBs and promote them to writable).</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a><br><a href="https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该模拟题出自DigitalCloud Training的模拟题，一共20道题，相对来说答案比较准确，第一次答正确率只有60%，看起来还有不太扎实的知识点，并且英文多了不太爱仔细阅读也是准确率低的原因。&lt;/p&gt;
&lt;p&gt;另外，今天在AWS培训官网上看到ACA考试在3月份会推出全新的试题，所以还需要抓紧时间考过。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q101-Q200)</title>
    <link href="http://sunqi.me/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/"/>
    <id>http://sunqi.me/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/</id>
    <published>2020-01-08T07:36:23.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。</p><a id="more"></a><h2 id="争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment"><a href="#争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment" class="headerlink" title="(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?"></a>(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?</h2><p>A. Continuously replicate the production database server to Amazon RDS. Use AWS CloudFormation to deploy the application and any additional servers if necessary.<br>B. Continuously replicate the production database server to Amazon RDS. Create one application load balancer and register on-premises servers. Configure ELB Application Load Balancer to automatically deploy Amazon EC2 instances for application and additional servers if the on-premises application is down.<br>C. Use a scheduled Lambda function to replicate the production database to AWS. Use Amazon Route 53 health checks to deploy the application automatically to Amazon S3 if production is unhealthy.<br>D. Use a scheduled Lambda function to replicate the production database to AWS. Register on-premises servers to an Auto Scaling group and deploy the application and additional servers if production is unavailable.</p><p>Answer: B</p><ul><li>分析：有人说答案是A，因为题目中的这个词pilot light(A pilot light is a small gas flame)，准确的翻译没查到，从字面理解应该就是简单轻量级的意思。A选项的方式是当出现灾难时，使用CloudFormation进行除数据库外的重建。所以很多人认为这种方式更符合题目的要求。但是从B选项看，更符合一个容灾的场景，当发生灾难时，通过B中的配置，可以做到马上接管的效果，比A选项更像是一个容灾的解决方案。</li></ul><h2 id="A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation"><a href="#A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation" class="headerlink" title="A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?"></a>A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?</h2><p>A. Change the RDS instance type to get more RAM.<br>B. Change the storage type to Provisioned IOPS.<br>C. Scale the web server tier horizontally.<br>D. Split the DB layer into separate RDS instances.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and"><a href="#A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and" class="headerlink" title="A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and"></a>A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and</h2><p>Windows. Which solution meets this requirement?</p><p>A. Default Amazon CloudWatch metrics.<br>B. Custom Amazon CloudWatch metrics.<br>C. Amazon Inspector resource monitoring.<br>D. Default monitoring of Amazon EC2 instances.</p><p>Answer: B</p><ul><li>分析：这道题又是原网站给出的错题，原来给出的答案是A。我曾经对AWS两个行为比较纳闷：一个是为什么没有VNC，另外一个是为什么不提供内存监控，直到又一次和AWS的架构师聊才理解了AWS的良苦用心。AWS始终把用户安全放在第一位，但凡用户的东西我是坚决不能碰的，而无论是VNC还是内存监控无疑与这一原则相违背的。所以内存不可能是默认监控的范畴，必须通过custom脚本完成，同样磁盘利用率也是类似的方式。</li></ul><h2 id="A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use"><a href="#A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use" class="headerlink" title="A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?"></a>A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?</h2><p>A. Amazon Aurora<br>B. Amazon RDS MySQL with Multi-AZ enabled<br>C. Amazon DynamoDB<br>D. Amazon ElastiCache</p><p>Answer: A</p><blockquote><p>问：Amazon Aurora 如何提高我的数据库对磁盘故障的容错能力？</p><p>Amazon Aurora 会将您的数据库卷分成分散在很多个磁盘上的 10GB 的区段。每 10GB 的数据库卷组块都能在三个可用区间用六种方法进行复制。Amazon Aurora 的设计可透明应对多达两个数据副本的损失，而不会影响数据库写入可用性，还能在不影响读取可用性的情况下应对多达三个副本。Amazon Aurora 存储还具有自我修复能力。可连续扫描数据块和磁盘有无出错并自动修复之。</p></blockquote><h2 id="A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design"><a href="#A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design" class="headerlink" title="A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?"></a>A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?</h2><p>A. Add an S3 lifecycle rule to move any files from the bucket in us-east-1 to the bucket in ap-southeast-2.<br>B. Create a Lambda function to be triggered for every new file in us-east-1 that copies the file to the bucket in ap-southeast-2.<br>C. Use SNS to notify the bucket in ap-southeast-2 to create a file whenever the file is created in the bucket in us-east-1.<br>D. Enable versioning and configure cross-region replication from the bucket in us-east-1 to the bucket in ap-southeast-2.</p><p>Answer: D</p><ul><li>分析：这道题要求的是最简单的方法，B理论上是可以的，但是与D相比过于复杂。</li></ul><h2 id="An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met"><a href="#An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met" class="headerlink" title="An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?"></a>An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?</h2><p>A. Use Amazon CloudWatch Events to invoke an AWS Lambda function that can launch On-Demand Instances.<br>B. Regularly store data from the application on Amazon DynamoDB. Increase the maximum number of instances in the AWS Auto Scaling group.<br>C. Manually place a bid for additional Spot Instances at a higher price in the same AWS Region and Availability Zone.<br>D. Ensure that the Amazon Machine Image associated with the application has the latest configurations for the launch configuration.</p><p>Answer: A</p><ul><li><p>Taking Advantage of Amazon EC2 Spot Instance Interruption Notices(<a href="https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/</a>)</p><blockquote><p>In January 2018, the Spot Instance interruption notice also became available as an event in Amazon CloudWatch Events. This allows targets such as AWS Lambda functions or Amazon SNS topics to process Spot Instance interruption notices by creating a CloudWatch Events rule to monitor for the notice.</p></blockquote></li><li><p>分析：网站给出的答案是B，但是这道题目考察的应该是合理利用Spot Instance的通知是这道题目的关键，”must include a two-minute warning” =&gt; Need CloudWatch</p></li></ul><h2 id="A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved"><a href="#A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved" class="headerlink" title="A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?"></a>A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?</h2><p>A. (争议)Use asynchronous replication for standby to maximize throughput during peak demand.<br>B. Offload SELECT queries that can tolerate stale data to READ replica.<br>C. Offload SELECT and UPDATE queries to READ replica.<br>D. Offload SELECT query that needs the most current data to READ replica.</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，大部分反对的理由是RDS之间的复制是同步的并不是异步的。</li></ul><h2 id="A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend"><a href="#A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend" class="headerlink" title="A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?"></a>A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?</h2><p>A. Configure active-active failover using Route 53 latency DNS records.<br>B. Configure active-passive failover using Route 53 latency DNS records.<br>C. Configure active-active failover using Route 53 failover DNS records.<br>D. Configure active-passive failover using Route 53 failover DNS records.</p><p>Answer: A</p><ul><li>分析：这道题很明显是需要AA模式的，with least latency，所以需要latency。</li></ul><blockquote><p>Active-Active Failover<br>Use this failover configuration when you want all of your resources to be available the majority of the time. When a resource becomes unavailable, Route 53 can detect that it’s unhealthy and stop including it when responding to queries.</p><p>In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.</p></blockquote><h2 id="A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS"><a href="#A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS" class="headerlink" title="A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?"></a>A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?</h2><p>A. Use Amazon ECS and Service Auto Scaling.<br>B. Use Spot Instances for orchestration and for scaling containers on existing Amazon EC2 instances.<br>C. Use AWS OpsWorks to launch containers in new Amazon EC2 instances.<br>D. Use Auto Scaling groups to launch containers on existing Amazon EC2 instances.</p><p>Answer: A</p><blockquote><p>Amazon Elastic Container Service (Amazon ECS) 是用于在可扩展群集上运行 Docker 应用程序的 Amazon Web Service。在本教程中，您将了解如何在负载均衡器后面的 Amazon ECS 集群上运行支持 Docker 的示例应用程序，对该示例应用程序进行测试，然后删除您的资源以免产生费用。</p></blockquote><h2 id="争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate"><a href="#争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate" class="headerlink" title="(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?"></a>(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?</h2><p>A. Amazon API Gateway and AWS Lambda<br>B. Elastic Load Balancing with Auto Scaling groups and Amazon EC2<br>C. Amazon API Gateway and Amazon EC2<br>D. Amazon CloudFront and AWS Lambda</p><p>Answer: A</p><ul><li>分析：题目的需求是：1、不想运维；2、集中精力去做开发。这是非常典型的Serverless的需求，将底层交给云原生服务，专注于业务开发。首先应该选择AWS Lambda服务，则AD作为选项。原答案给出的是D，但是CloudFront作为CDN服务，好像并没有向外提供接口的能力，AWS Lambda服务并不像阿里的函数计算提供了Http trigger，所以无法对外提供API接口的能力，从这个角度看A更合理一些。</li></ul><h2 id="A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS"><a href="#A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS" class="headerlink" title="A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?"></a>A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?</h2><p>A. Create a read replica of the primary database and deploy it in a different AWS Region.<br>B. Enable multi-AZ to create a standby database in a different Availability Zone.<br>C. Enable multi-AZ to create a standby database in a different AWS Region.<br>D. Create a read replica of the primary database and deploy it in a different Availability Zone.</p><p>Answer: B</p><ul><li>分析：原题目给出的是A，但是B明显是正确答案。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/rds/ha/?nc1=h_ls" target="_blank" rel="noopener">https://aws.amazon.com/cn/rds/ha/?nc1=h_ls</a><br>Amazon Relational Database Service (Amazon RDS) supports two easy-to-use options for ensuring High Availability of your relational database.<br>For your MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database (DB) instances, you can use Amazon RDS Multi-AZ deployments. When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby DB instance. Since the endpoint for your DB instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. Learn more &gt;&gt;</p></blockquote><h2 id="An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements"><a href="#An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements" class="headerlink" title="An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?"></a>An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?</h2><p>A. Use Amazon Cognito Identity with SMS-based MFA.<br>B. Edit AWS IAM policies to require MFA for all users.<br>C. Federate IAM against corporate AD that requires MFA.<br>D. Use Amazon API Gateway and require SSE for photos.</p><p>Answer: A</p><h2 id="争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements"><a href="#争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements" class="headerlink" title="(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?"></a>(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon RDS<br>D. Amazon EBS</p><p>Answer: A</p><ul><li>分析：这道题很多人都认为B是正确的，但是从篇气象公司最佳实践看，确实采用的是S3方案。</li></ul><blockquote><p>AWS Case Study: The Weather Company(<a href="https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/" target="_blank" rel="noopener">https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/</a>)<br>The platform ingests information from more than 100 different sources and generates close to one-half terabyte (TB) of data each time it updates. The information is mapped and processed into forecast points that can be retrieved in real time, based on queries coming into the system. All data is stored in Amazon Simple Storage Service (Amazon S3), leveraging the efficiency of cloud storage as opposed to an on-premises storage solution and eliminating the hassle of managing a storage platform.</p></blockquote><h2 id="A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency"><a href="#A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency" class="headerlink" title="A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?"></a>A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?</h2><p>A. Amazon CloudFront with on-premises servers as the origin<br>B. ELB Application Load Balancer<br>C. Amazon Route 53 latency-based routing<br>D. Amazon EFS to store and server static files</p><p>Answer: A</p><h2 id="A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used"><a href="#A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used" class="headerlink" title="A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?"></a>A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?</h2><p>A. Amazon DynamoDB<br>B. Amazon Aurora MySQL<br>C. Amazon RDS MySQL<br>D. Amazon Redshift</p><p>Answer: D</p><h2 id="A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation"><a href="#A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation" class="headerlink" title="A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?"></a>A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?</h2><p>A. The instance with the oldest launch configuration.<br>B. The instance in the Availability Zone that has most instances.<br>C. The instance closest to the next billing hour.<br>D. The oldest instance in the group.</p><p>Answer: B</p><ul><li>控制在缩小过程中终止哪些 Auto Scaling 实例(<a href="https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a>)</li></ul><blockquote><p>当达到缩减策略的阈值时，策略生效，Auto Scaling 组终止其中一个实例。如果您没有为该组分配特定的终止策略，则使用默认终止策略。它选择有两个实例的可用区，并终止从最旧启动配置启动的实例。如果这些实例是从同一启动配置启动的，则 Auto Scaling 组选择最接近下一个计费小时的实例并终止该实例。</p></blockquote><h2 id="A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements"><a href="#A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements" class="headerlink" title="A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?"></a>A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?</h2><p>A. Implement an Amazon API Gateway to server as the HTTP endpoint. Have the API Gateway trigger an AWS Lambda function to process the messages, and save the results to an Amazon DynamoDB table.<br>B. Create an Amazon EC2 instance to server as the HTTP endpoint and to process the messages. Save the results to Amazon S3 for the Data Analysis team to download.<br>C. Use Amazon Route 53 to direct incoming sensor messages to a Lambda function to process the message and save the results to a Amazon DynamoDB table.<br>D. Use AWS Direct Connect to connect sensors to DynamoDB so that data can be written directly to a DynamoDB table where it can be accessed by the Data Analysis team.</p><p>Answer: A</p><ul><li>分析：原来AWS Lambda的HTTP trigger是这么实现的</li></ul><h2 id="A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements"><a href="#A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements" class="headerlink" title="A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?"></a>A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?</h2><p>A. Amazon RDS<br>B. Amazon RedShift<br>C. Amazon DynamoDB Accelerator<br>D. Amazon ElastiCache</p><p>Answer: A</p><h2 id="争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met"><a href="#争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met" class="headerlink" title="(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?"></a>(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?</h2><p>A. Use an Amazon Redshift database. Copy the product database into Redshift and allow the team to query it.<br>B. Use an Amazon RDS read replica of the production database and allow the team to query against it.<br>C. Use multiple Amazon EC2 instances running replicas of the production database, placed behind a load balancer.<br>D. Use an Amazon DynamoDB table to store a copy of the data.</p><p>Answer: B</p><ul><li>争议：原有答案给出的是A，根据题目描述看起来是一个数据仓库的需求。从easy to maintain的角度说B，更简单</li></ul><h2 id="A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two"><a href="#A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two" class="headerlink" title="A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)"></a>A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)</h2><p>A. Amazon Kinesis Data Firehouse<br>B. Amazon SQS<br>C. Amazon Redshift<br>D. Amazon SNS<br>E. Amazon DynamoDB</p><p>Answer: AC</p><ul><li>分析：A负责接收数据并处理，C作为数据仓库存储下来</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q1-Q100)</title>
    <link href="http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/"/>
    <id>http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/</id>
    <published>2019-12-31T01:11:55.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接：<a href="https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/" target="_blank" rel="noopener">https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/</a></p><p>一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。</p><p>由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。</p><a id="more"></a><h2 id="A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest"><a href="#A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest" class="headerlink" title="A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?"></a>A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?</h2><p>A. Place the Redshift cluster in a private subnet.<br>B. Use the AWS KMS Default Customer master key.<br>C. Encrypt the Amazon EBS volumes.<br>D. Encrypt the data using SSL/TLS.</p><p>Answer: B</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html</a></li><li>分析：Amazon Redshift 使用加密密钥层次结构来加密数据库。您可以使用 AWS Key Management Service (AWS KMS) 或硬件安全模块 (HSM) 来管理该层次结构中的顶级加密密钥。Amazon Redshift 用于加密的流程因您管理密钥的方式而异。Amazon Redshift 自动与 AWS KMS 集成，而不与 HSM 集成。当您使用 HSM 时，必须使用客户端和服务器证书在 Amazon Redshift 和 HSM 之间配置受信任的连接。</li></ul><h2 id="A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database"><a href="#A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database" class="headerlink" title="A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?"></a>A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?</h2><p>A. Amazon SQS<br>B. Amazon EFS<br>C. Amazon S3<br>D. AWS Lambda</p><p>Answer: A</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/sqs/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/cn/sqs/faqs/</a></li><li>分析：关键词是unpredictable traffic, keep up with write request, decouple the web application, 所以通过消息队列服务可以让写入请求排队，从而实现前端应用和后端数据库的解耦。</li></ul><h2 id="A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements"><a href="#A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements" class="headerlink" title="A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?"></a>A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?</h2><p>A. AWS Snowball storage for the legacy application until the application can be re-architected.<br>B. AWS Storage Gateway in cached mode for the legacy application storage to write data to Amazon S3.<br>C. AWS Storage Gateway in stored mode for the legacy application storage to write data to Amazon S3.<br>D. An Amazon S3 volume mounted on the legacy application server locally using the File Gateway service.</p><p>Answer: C</p><ul><li>分析：关键词是local stroage with iSCSI, 并且需要将所有新的存储用AWS提供，所以排除A选项；因为用到了iSCSI协议，所以S3使用文件网关方式也不适用，排除D；剩下的B和C区别在于存储模式，因为需要本地应用请求，所以需要使用存储模式，而不能用缓存模式，所以最终选择C。</li></ul><h2 id="A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck"><a href="#A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck" class="headerlink" title="A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?"></a>A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?</h2><p>A. Multi-AZ feature on the RDS MySQL Database<br>B. ELB Classic Load Balancer in front of the web application tier<br>C. Amazon SQS in front of RDS MySQL Database<br>D. Amazon ElastiCache in front of the RDS MySQL Database</p><p>Answer: D</p><ul><li>分析：该问题的关键在于bottleneck due to frequently accessed queries，查询变成瓶颈，可以使用ElastiCache服务作为缓存，降低读取频率解决问题。</li></ul><h2 id="A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives"><a href="#A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives" class="headerlink" title="A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?"></a>A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?</h2><p>A. Amazon Redshift<br>B. Amazon DynamoDB<br>C. Amazon RDS MySQL<br>D. Amazon Aurora</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，但是经过分析觉得有些问题，这道题的几个关键词：scale with the rest of the application, schema will change frequently, cannot afford any downtime for database changes. 首先，schema总是变更，所以这里需要的非关系型数据库，排除C和D。Redshift是数据仓库，其实也是数据仓库，从第一点上就可以排除。另外从这个链接（<a href="http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero" target="_blank" rel="noopener">http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero</a> downtime，其他的都不行。所以原网站给出的答案是错误的。</li></ul><h2 id="A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements"><a href="#A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements" class="headerlink" title="A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?"></a>A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?</h2><p>A. Use AWS CloudFormation to deploy the cluster in a second region.<br>B. Take a snapshot of the cluster and copy it to another Availability Zone.<br>C. Modify the Redshift cluster to span two regions.<br>D. Enable cross-region snapshots to a different region.</p><p>Answer: D</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/</a></li></ul><h2 id="A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct"><a href="#A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct" class="headerlink" title="A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?"></a>A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?</h2><p>A. The application is reading parts of objects from Amazon S3 using a range header.<br>B. The application is reading objects from Amazon S3 using parallel object requests.<br>C. The application is updating records by writing new objects with unique keys.<br>D. The application is updating records by overwriting existing objects with the same keys.</p><p>Answer: D</p><ul><li>分析：这道题也是争论很大的一道题，原网站答案为A。问题简单描述为客户端访问不到最新的数据，发生的时间点在于应用上传信息时候速率提高导致的，所以问题应该出现在写入的时候，这样排除A和B读取的问题。因为S3同一object永远是覆盖，所以最有可能的问题是在same key的情况下，所以选择D。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements" class="headerlink" title="A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?"></a>A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?</h2><p>A. Use Amazon Kinesis with AWS CloudTrail for auditing the specific times when profile photos are uploaded.<br>B. Use Amazon EBS volumes with IAM policies restricting user access to specific time periods.<br>C. Use Amazon S3 with the default private access policy and generate pre-signed URLs each time a new site profile is created.<br>D. Use Amazon CloudFront with AWS CloudTrail for auditing the specific times when profile photos are uploaded.</p><p>Answer: C</p><h2 id="An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application"><a href="#An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application" class="headerlink" title="An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?"></a>An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Amazon Glacier</p><p>Answer: C</p><ul><li>分析：没想到这道题原网站给出的答案是B，争议比较大，但是从题目描述需要Block Storage角度来看，选择C才是最合理的。这道题还需要进一步确认一下。</li></ul><h2 id="A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements"><a href="#A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements" class="headerlink" title="A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?"></a>A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?</h2><p>A. Create a Lambda function to move files older than 30 days to Amazon EBS and move files older than 60 days to Amazon Glacier.<br>B. Create a Lambda function to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon EBS.<br>C. Create lifecycle rules to move files older than 30 days to Amazon S3 Standard Infrequent Access and move files older than 60 days to Amazon Glacier.<br>D. Create lifecycle rules to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon S3 Standard Infrequent Access.</p><p>Answer: C</p><ul><li>分析：很明显的排除A和B，S3可以自定义规则，那么问题就是30天后和60天后需要哪种存储的问题了，根据题目C是明显正确的。</li></ul><h2 id="An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs"><a href="#An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs" class="headerlink" title="An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?"></a>An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?</h2><p>A. Amazon Redshift<br>B. Amazon RDS<br>C. Amazon DynamoDB<br>D. Amazon Aurora</p><p>Answer: C</p><h2 id="A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two"><a href="#A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two" class="headerlink" title="A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)"></a>A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)</h2><p>A. Create a VPC Endpoint for Amazon RDS.<br>B. Create the Lambda function within the Amazon RDS VPC.<br>C. Change the ingress rules of Lambda security group, allowing the Amazon RDS security group.<br>D. Change the ingress rules of the Amazon RDS security group, allowing the Lambda security group.<br>E. Add an Internet Gateway (IGW) to the VPC, route the private subnet to the IGW.</p><p>Answer: BD</p><ul><li>分析：又是原网站一道错题，原网站答案为AD。D选项是允许Lambda服务访问RDS，所以在进方向允许。</li><li>目前VPC支持Endpoint的服务：<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html</a></li></ul><blockquote><p>Amazon API Gateway<br>Amazon AppStream 2.0<br>AWS App Mesh<br>Amazon Athena<br>AWS CloudFormation<br>AWS CloudTrail<br>Amazon CloudWatch<br>Amazon CloudWatch Events<br>Amazon CloudWatch Logs<br>AWS CodeBuild<br>AWS CodeCommit<br>AWS CodePipeline<br>AWS Config<br>AWS DataSync<br>Amazon EC2 API<br>Elastic Load Balancing<br>Amazon Elastic Container Registry<br>Amazon Elastic Container Service<br>AWS Glue<br>AWS Key Management Service<br>Amazon Kinesis Data Firehose<br>Amazon Kinesis Data Streams<br>Amazon Rekognition<br>Amazon SageMaker 和 Amazon SageMaker 运行时<br>Amazon SageMaker 笔记本<br>AWS Secrets Manager<br>AWS Security Token Service<br>AWS Service Catalog<br>Amazon SNS<br>Amazon SQS<br>AWS Systems Manager<br>AWS Storage Gateway<br>AWS Transfer for SFTP<br>其他 AWS 账户托管的终端节点服务</p><p>网关终端节点是一个网关，作为您在路由表中指定的路由的目标，用于发往受支持的 AWS 服务的流量。支持以下 AWS 服务：<br>Amazon S3<br>DynamoDB</p></blockquote><h2 id="待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement"><a href="#待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement" class="headerlink" title="(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?"></a>(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?</h2><p>A. Modify the Redshift cluster and configure cross-region snapshots to the other region.<br>B. Modify the Redshift cluster to take snapshots of the Amazon EBS volumes each day, sharing those snapshots with the other region.<br>C. Modify the Redshift cluster and configure the backup and specify the Amazon S3 bucket in the other region.<br>D. Modify the Redshift cluster to use AWS Snowball in export mode with data delivered to the other region.</p><p>Answer: A</p><ul><li>分析：又是一道错题，Redhift备份是通过S3实现的, 所以不存在B的情况，我个人有点倾向于C，但是A确实是Redshift在快照时默认的格式，可能是更容易恢复吧，这道题需要在实际环境进行一下验证。另外国际版本的Redshift和国内的应该比国内的高很多。</li></ul><blockquote><p>问：Amazon Redshift 如何备份数据？ 如何从备份中还原我的集群？</p><p>在加载数据时，Amazon Redshift 会复制数据仓库集群内的所有数据并将其连续备份至 S3。Amazon Redshift 始终尝试维持至少三份数据（计算节点上的正本数据、副本数据和 Amazon S3 上的备份数据）。Redshift 还能将您的快照异步复制到另一个区域的 S3 中进行灾难恢复。</p><p>默认情况下，Amazon Redshift 以一天的保留期启用数据仓库群集的自动化备份。您可将其配置为 35 天之久。</p><p>免费备份存储受限于数据仓库群集中节点上的总存储大小，并仅适用于已激活的数据仓库群集。例如，如果您有 8TB 的数据仓库总存储大小，那么我们将提供最多 8TB 的备份存储而不另外收费。如果您想将备份保留期延长为超过一天，那么您可以使用 AWS 管理控制台或 Amazon Redshift API 来实现这一目的。有关自动快照的更多信息，请参阅《Amazon Redshift 管理指南》。Amazon Redshift 仅备份已更改的数据，因此大多数快照仅占用少量的免费备份存储。</p><p>如果您需要还原备份，则可以在备份保留期内访问所有自动备份。在您选择某个要还原的备份后，我们将预置一个新的数据仓库集群并将数据还原至此集群中。</p></blockquote><h2 id="A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance"><a href="#A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance" class="headerlink" title="A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?"></a>A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?</h2><p>A. Convert the database to Amazon Redshift.<br>B. Create a CloudFront distribution.<br>C. Convert the database to use EBS Provisioned IOPS.<br>D. Create one or more read replicas.</p><p>Answer: C</p><ul><li>分析：这道题我最开始选择的是D，但是评论区的一种解释有一定的道理：这个网站应用类型为电商，原题中没有很清楚说明queris and load的压力有多大，很可能我们建立了read replicas只能临时性解决问题，并不是一劳永逸的方式。并且根据<a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></li></ul><blockquote><p>因此，所有 Aurora 副本均返回相同的查询结果数据，且副本滞后时间非常短 - 通常远远少于主实例写入更新后的 100 毫秒。副本滞后因数据库更改速率而异。也就是说，在对数据库执行大量写入操作期间，您可能发现副本滞后时间变长。</p></blockquote><p>如果读副本在这个延时上，很可能对业务系统造成很大的影响。</p><h2 id="A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two"><a href="#A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two" class="headerlink" title="A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)"></a>A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)</h2><p>A. Launch Amazon EC2 instances in an Auto Scaling group behind an ELB.<br>B. Store all static files in a multi-AZ Amazon Aurora database.<br>C. Create an CloudFront distribution pointing to static content in Amazon S3.<br>D. Use Amazon Route 53 to route traffic to the correct region.<br>E. Use Amazon S3 multi-part uploads to improve upload times.</p><p>Answer: AC</p><ul><li>分析：A是很明显的，弹性伸缩，节约成本，其他几项和题干没有太多关系，所以选的C。</li></ul><h2 id="A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements"><a href="#A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements" class="headerlink" title="A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?"></a>A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?</h2><p>A. Configure the database security group to allow database traffic from the application server IP addresses.<br>B. Configure the database security group to allow database traffic from the application server security group.<br>C. Configure the database subnet network ACL to deny all inbound non-database traffic from the application-tier subnet.<br>D. Configure the database subnet network ACL to allow inbound database traffic from the application-tier subnet.</p><p>Answer: B</p><ul><li>分析：这好像又是一道错题，原给出的答案是C。首先要明确的一点是SG是工作在instance级别，NACL是在子网级别，SG默认全部Deny，NACL默认全部Allow。A不对的原因是insance在Auto Scaling里，IP地址是不固定的。D不对的原因是NACL默认全都是Allow的。其实本质上考察的是如何选择安全组还是网络防火墙的问题。不选择C的原因是因为配置NACL规则至少需要阻止和允许，而通过安全组只需要配置一条即可。但是也有一种声音认为题目中关键词restrict意味着需要deny流量。</li></ul><blockquote><p>问：VPC 中的安全组和 VPC 中的网络 ACL 有什么区别？</p><p>VPC 中的安全组指定允许传入或传出 Amazon EC2 实例的流量。网络 ACL 则在子网级别上运作，评估进出某个子网的流量。网络 ACL 可通过设置允许和拒绝规则来进行使用。Network ACL 不能筛选同一子网中实例之间的流量。此外，网络 ACL 执行无状态筛选，而安全组则执行有状态筛选。</p></blockquote><h2 id="An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture"><a href="#An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture" class="headerlink" title="An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?"></a>An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?</h2><p>A. Amazon EC2<br>B. NAT instance<br>C. ELB Classic Load Balancer<br>D. Amazon RDS</p><p>Answer: B</p><ul><li>分析：这道题竟然给出了C的答案，很意外。</li></ul><blockquote><p><a href="https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="noopener">https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></p><p>Instances in a private subnet can access the Internet without exposing their private IP address by routing their traffic through a Network Address Translation (NAT) instance in a public subnet. A NAT instance, however, can introduce a single point of failure to your VPC’s outbound traffic. This situation is depicted in the diagram below.</p></blockquote><h2 id="A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem"><a href="#A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem" class="headerlink" title="A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?"></a>A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?</h2><p>A. Change the Auto Scaling group’s scale out event to scale based on network utilization.<br>B. Create an Auto Scaling scheduled action to scale out the necessary resources at 8:30 AM every morning.<br>C. Use Reserved Instances to ensure the system has reserved the right amount of capacity for the scale-up events.<br>D. Permanently keep a steady state of instances that is needed at 9:00 AM to guarantee available resources, but leverage Spot Instances.</p><p>Answer: B</p><ul><li>分析：竟然又是一道错题，记得在AWS听过这道题的分析。原答案是A，但是可能并不是由于网络引起的访问缓慢。</li></ul><h2 id="An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met"><a href="#An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met" class="headerlink" title="An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?"></a>An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?</h2><p>A. Add a Step Scaling policy.<br>B. Add a Dynamic Scaling policy.<br>C. Add a Scheduled Scaling action.<br>D. Add Amazon EC2 Spot Instances.</p><p>Answer: B</p><ul><li>分析：又是一道争议比较大的题目，争议最大的是C选项，因为题目中有几个词在暗示时间，但是又不明确。既然现有性能上无法应对高峰访问，那么从这个角度还是通过Dynamic配置一个规则进行动态规则最为有效。所以还是选择B。</li></ul><h2 id="A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost"><a href="#A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost" class="headerlink" title="A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?"></a>A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?</h2><p>A. Create an Auto Scaling group with a minimum of one instance and a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>B. Recreate the API using Amazon API Gateway and use AWS Lambda as the service backend.<br>C. Create an Auto Scaling group with a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>D. Recreate the API using Amazon API Gateway and integrate the new API with the existing backend service.</p><p>Answer: A</p><ul><li>分析：这道题有个陷阱，Simple API，确实如果在不考虑开发的前提下B确实是最佳选项，但是重构也是要花成本的。所以我坚持选A。</li></ul><h2 id="A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement"><a href="#A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement" class="headerlink" title="A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?"></a>A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?</h2><p>A. Create EBS snapshots directly from one region to another.<br>B. Move the data to an Amazon S3 bucket and enable cross-region replication.<br>C. Create EBS snapshots and then copy them to the desired region.<br>D. Use a script to copy data from the current Amazon EBS volume to the destination Amazon EBS volume.</p><p>Answer: C</p><h2 id="A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues"><a href="#A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues" class="headerlink" title="A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?"></a>A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?</h2><p>A. Place an elastic load balancer in front of the Amazon S3 bucket to distribute the load during peak hours.<br>B. Cache the web content with Amazon CloudFront and use all Edge locations for content delivery.<br>C. Replicate the bucket in eu-west-1 and use an Amazon Route 53 failover routing policy to determine which bucket it should serve the request to.<br>D. Use an Amazon Route 53 weighted routing policy for the CloudFront domain name to distribute the GET request between CloudFront and the Amazon S3 bucket directly.</p><p>Answer: B</p><ul><li>分析：网站给出的答案竟然是D，但是B很明显是正确的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use"><a href="#A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use" class="headerlink" title="A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:"></a>A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:</h2><p>A. an external service to ping the VPN endpoint from outside the VPC.<br>B. AWS CloudTrail to monitor the endpoint.<br>C. the CloudWatch TunnelState Metric.<br>D. an AWS Lambda function that parses the VPN connection logs.</p><p>Answer: C</p><blockquote><p>Monitoring VPN Tunnels Using Amazon CloudWatch(<a href="https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html</a>)</p></blockquote><h2 id="A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance"><a href="#A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance" class="headerlink" title="A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?"></a>A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?</h2><p>A. Auto Scaling<br>B. Amazon SQS<br>C. Amazon ElastiCache<br>D. ELB Application Load Balancer</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement"><a href="#A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement" class="headerlink" title="A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?"></a>A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?</h2><p>A. Use a NAT Instance.<br>B. Use a NAT Gateway.<br>C. Use a VPC endpoint.<br>D. Use a Virtual Private Gateway.</p><p>Answer: C</p><blockquote><p>New – VPC Endpoint for Amazon S3(<a href="https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a>)</p></blockquote><h2 id="Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance"><a href="#Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance" class="headerlink" title="Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?"></a>Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?</h2><p>A. Provisioned IOPS SSD<br>B. General Purpose SSD<br>C. Cold HDD<br>D. Throughput Optimized HDD</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements" class="headerlink" title="A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?"></a>A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?</h2><p>A. EBS Provisioned IOPS SSD (io1)<br>B. EBS General Purpose SSD (gp2)<br>C. EBS Throughput Optimized HDD (st1)<br>D. EBS Cold HDD (sc1)</p><p>Answer: C</p><h2 id="A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of"><a href="#A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of" class="headerlink" title="A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of"></a>A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of</h2><p>Amazon S3. Which naming scheme should the company use?</p><p>A. Add a date as the prefix.<br>B. Add a sequential id as the suffix.<br>C. Add a hexadecimal hash as the suffix.<br>D. Add a hexadecimal hash as the prefix.</p><p>Answer: A</p><ul><li>分析：这道题的旧答案是D，不过根据最新文档档案为A。</li></ul><blockquote><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>For example, previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes.</p></blockquote><ul><li>什么是Prefix?</li></ul><blockquote><p>For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.</p></blockquote><blockquote><p>However, if the new limits are not sufficient, prefixes would need to be used. A prefix has no fixed number of characters. It is any string between a bucket name and an object name, for example:<br>bucket/folder1/sub1/file<br>bucket/folder1/sub2/file<br>bucket/1/file<br>bucket/2/file<br>Prefixes of the object ‘file’ would be: /folder1/sub1/ , /folder1/sub2/, /1/, /2/.</p></blockquote><h2 id="A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task"><a href="#A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task" class="headerlink" title="A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?"></a>A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?</h2><p>A. Enable AWS CloudTrail logging in each individual region. Repeat this for all future regions.<br>B. Enable Amazon CloudWatch logs for all AWS services across all regions and aggregate them in a single Amazon S3 bucket.<br>C. Enable AWS Trusted Advisor security checks and report all security incidents for all regions.<br>D. Enable AWS CloudTrail by creating a new trail and apply the trail to all regions.</p><p>Answer: D</p><ul><li>分析：这道题肯定使用CloudTrail，区别在于设定范围。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/" target="_blank" rel="noopener">https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/</a><br>You can now turn on a trail across all regions for your AWS account. CloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified. Additionally, when AWS launches a new region, CloudTrail will create the same trail in the new region. As a result, you will receive log files containing API activity for the new region without taking any action. Using the CloudTrail console, you can specify that a trail applies to all regions. For more details, refer to the Applying a trail to all regions section of the CloudTrail FAQ.</p></blockquote><h2 id="A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use"><a href="#A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use" class="headerlink" title="A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?"></a>A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?</h2><p>A. Amazon DynamoDB<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon EFS</p><p>Answer: C</p><ul><li>分析：这道题有点蒙人，关键词在proprietary（专有的），EFS未必支持，只有EBS才能100%满足。</li></ul><h2 id="A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner"><a href="#A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner" class="headerlink" title="A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?"></a>A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?</h2><p>A. Purchase Reserved Instances to run all containers. Use Auto Scaling groups to schedule jobs.<br>B. Host a container management service on Spot Instances. Use Reserved Instances to run Docker containers.<br>C. Use Amazon ECS orchestration and Auto Scaling groups: one with Reserve Instances, one with Spot Instances.<br>D. Use Amazon ECS to manage container orchestration. Purchase Reserved Instances to run all batch workloads at the same time.</p><ul><li>分析：绕嘴，多读两遍。主要是应对两种不同类型的任务。</li></ul><h2 id="A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO"><a href="#A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO" class="headerlink" title="A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?"></a>A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?</h2><p>A. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Customer-Provided Keys (SSE-C).<br>B. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).<br>C. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS).<br>D. Create an Amazon S3 bucket to store the reports and use Amazon s3 versioning with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).</p><p>Answer: C</p><ul><li>分析：用S3 + KMS服务。</li></ul><h2 id="争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case"><a href="#争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case" class="headerlink" title="(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?"></a>(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?</h2><p>A. Amazon S3<br>B. Amazon RDS<br>C. Amazon RedShift<br>D. AWS Storage Gateway</p><p>Answer: A</p><ul><li>分析：这道题的争议点在答案B，因为S3提供eventual consistency for overwirte PUTS and DELETES，可能会导致无法获取最新数据的问题。不确定该问题是否会考到，暂时没有找到更合理的解释。</li></ul><blockquote><p>Amazon S3 Data Consistency Model(<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html</a>)<br>Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one caveat. The caveat is that if you make a HEAD or GET request to the key name (to find if the object exists) before creating the object, Amazon S3 provides eventual consistency for read-after-write.</p><p>Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions.</p><p>Updates to a single key are atomic. For example, if you PUT to an existing key, a subsequent read might return the old data or the updated data, but it never returns corrupted or partial data.</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a"><a href="#A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a" class="headerlink" title="A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a"></a>A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a</h2><p>DynamoDB table. Which AWS-managed service is the BEST fit to insert the item?</p><p>A. Lambda@Edge<br>B. AWS Lambda<br>C. Amazon API Gateway<br>D. Amazon EC2 instances</p><p>Answer: B</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/</a></li></ul><h2 id="An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used"><a href="#An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used" class="headerlink" title="An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?"></a>An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?</h2><p>A. Amazon SQS<br>B. Amazon SNS<br>C. Amazon ECS<br>D. AWS STS</p><p>Answer: A</p><blockquote><p>问：Amazon SNS 与 Amazon SQS 有何不同？</p><p>Amazon Simple Queue Service (SQS) 和 Amazon SNS 都是 AWS 中的消息收发服务，但为开发人员提供了不同的优势。Amazon SNS 允许应用程序通过“推送”机制向多个订阅者发送时间关键型消息，并且无需定期检查或“轮询”更新。Amazon SQS 是一种供分布式应用程序使用的消息队列服务，通过轮询模式交换消息，可用于分离收发组件。Amazon SQS 使应用程序的分布式组件可以灵活地收发消息，并且不要求每个组件同时可用。</p><p>一种常见的模式是使用 SNS 将消息发布到 Amazon SQS 队列，进而以可靠的方式将消息异步发送到一个或多个系统组件。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement"><a href="#A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement" class="headerlink" title="A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?"></a>A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?</h2><p>A. Enable SSL on Amazon EC2 instances.<br>B. Encrypt Amazon EBS volumes on Amazon EC2 instances.<br>C. Enable server-side encryption on Amazon S3.<br>D. Encrypt Amazon EC2 Instance Storage.</p><p>Answer: B</p><ul><li>New EBS Encryption for Additional Data Protection(<a href="https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/</a>)</li></ul><h2 id="争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two"><a href="#争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two" class="headerlink" title="(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)"></a>(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)</h2><p>A. Host the website on an Amazon EC2 instance with ELB and Auto Scaling, and map a Route 53 alias record to the ELB endpoint.<br>B. Host the website using AWS Elastic Beanstalk, and map a Route 53 alias record to the Beanstalk stack.<br>C. Host the website on an Amazon EC2 instance, and map a Route 53 alias record to the public IP address of the Amazon EC2 instance.<br>D. Serve the website from an Amazon S3 bucket, and map a Route 53 alias record to the website endpoint.<br>E. Create a Route 53 hosted zone, and set the NS records of the domain to use Route 53 name servers.</p><p>Answer: DE</p><ul><li>分析：又是一道争议非常大的题，原来的答案是CD，从cost-effective的角度说C确实不够经济。参考AWS如何构建静态网站的最佳实践：<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html</a></li></ul><h2 id="争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale"><a href="#争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale" class="headerlink" title="(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?"></a>(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?</h2><p>A. Create an Amazon Kinesis Firehouse delivery stream to store the data in Amazon S3.<br>B. Create an Auto Scaling group of Amazon EC2 servers behind ELBs to write the data into Amazon RDS.<br>C. Create an Amazon SQS queue, and have the machines write to the queue.<br>D. Create an Amazon EC2 server farm behind an ELB to store the data in Amazon EBS Cold HDD volumes.</p><p>Answer: A</p><ul><li>分析：很奇怪为什么原有答案给出B，这道题明显是暗指实时计算Kinesis服务。</li></ul><h2 id="A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database"><a href="#A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database" class="headerlink" title="A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?"></a>A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?</h2><p>A. Amazon S3<br>B. Amazon Aurora<br>C. Amazon DynamoDB<br>D. Amazon Redshift</p><p>Answer: B</p><ul><li>分析：很明显需要关系型数据库。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost"><a href="#A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost" class="headerlink" title="A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?"></a>A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?</h2><p>A. Add rules to the security groups in each account.<br>B. Establish a VPC Peering connection between accounts.<br>C. Configure Direct Connect in each account.<br>D. Add a NAT Gateway to the data account.</p><p>Answer: B</p><ul><li>分析：B的方案是成本最低的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3"><a href="#A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3" class="headerlink" title="A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?"></a>A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?</h2><p>A. Upload directly to S3 using a pre-signed URL.<br>B. Upload to a second bucket, and have a Lambda event copy the image to the primary bucket.<br>C. Upload to a separate Auto Scaling group of servers behind an ELB Classic Load Balancer, and have them write to the Amazon S3 bucket.<br>D. Expand the web server fleet with Spot Instances to provide the resources to handle the images.</p><p>Answer: C</p><ul><li>分析：A选项相较于题目中描述的并没有本质区别。</li></ul><blockquote><p>A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object.</p><p>All objects and buckets by default are private. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don’t require them to have AWS security credentials or permissions. When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The presigned URLs are valid only for the specified duration.</p></blockquote><h2 id="A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal"><a href="#A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal" class="headerlink" title="A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal."></a>A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal.</h2><p>A. Create VPC flow logs on the subnet.<br>B. Enable source destination check on private Amazon EC2 instances.<br>C. Enable AWS CloudTrail logging and specify an Amazon S3 bucket for storing log files.<br>D. Create an Amazon CloudWatch log to capture packet information.</p><p>Answer: A</p><ul><li>分析：启动VPC流表日志, CloudTrail没有此能力</li></ul><h2 id="A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two"><a href="#A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two" class="headerlink" title="A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)"></a>A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)</h2><p>A. Replace the Amazon EC2 reverse proxy with an ELB internal Classic Load Balancer.<br>B. Add Auto Scaling to the Amazon EC2 backend fleet.<br>C. Add Auto Scaling to the Amazon EC2 reverse proxy layer.<br>D. Use t2 burstable instance types for the backend fleet.<br>E. Replace both the frontend and reverse proxy layers with an ELB Application Load Balancer.</p><p>Answer: BE</p><ul><li>分析：又是一道错题，原答案是AB。根据题目分析，出现瓶颈的地方来自于两处：反向代理和后端服务。后端服务的扩展没有什么争议，所以B很明显是正确的。最大的争议来自于是使用什么方式替代目前成为瓶颈的反向代理EC2。原题里反向代理EC2作为content-based routing，那么问题的关键就是CLB、ELB谁能做content-based routing了。根据目前最新的内容，所以需要使用Application Load Balancer来提供content-based routing了。</li></ul><blockquote><p>There are three types of Elastic Load Balancer (ELB) on AWS:</p><p>Classic Load Balancer (CLB) – this is the oldest of the three and provides basic load balancing at both layer 4 and layer 7.</p><p>Application Load Balancer (ALB) – layer 7 load balancer that routes connections based on the content of the request.</p><p>Network Load Balancer (NLB) – layer 4 load balancer that routes connections based on IP protocol data.</p><p>Note: The Classic Load Balancer may be phased out over time and Amazon are promoting the ALB and NLB for most use cases within VPC.</p></blockquote><blockquote><p>Introducing Amazon EC2 Fleet<br>Posted On: May 2, 2018</p><p>Amazon EC2 Fleet is a new feature that simplifies the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, Availability Zones and across On-Demand, Amazon EC2 Reserved Instances (RI) and Amazon EC2 Spot purchase models. With a single API call, now you can provision capacity across EC2 instance types and across purchase models to achieve desired scale, performance and cost.</p><p>You can create an EC2 Fleet specification defining target capacity, which EC2 instance types work for you, and how much of your fleet should be filled using On-Demand, RI and Spot purchase models. You can also indicate whether EC2 Fleet should take into account the number of cores and amount of memory on each instance or consider all instances equal when scaling. EC2 Fleet then launches the lowest price combination of instances to meet the target capacity based on these preferences. EC2 fleet enables you to use multiple instance types and purchase models to provision capacity cost effectively, with just a few clicks in the AWS Management Console.</p><p>Amazon EC2 Fleet is now available in all public Regions. To learn more about simplifying the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, AWS Availability Zones and across On-Demand, RI and Spot purchase models using Amazon EC2 Fleet, visit this blog. To learn more about Amazon EC2 pricing models, visit this page.</p></blockquote><ul><li>EC2 Fleet – Manage Thousands of On-Demand and Spot Instances with One Request(<a href="https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/</a>)</li><li>New – Advanced Request Routing for AWS Application Load Balancers(<a href="https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/</a>)</li><li>ELASTIC LOAD BALANCING(<a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a>)</li></ul><h2 id="A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers"><a href="#A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers" class="headerlink" title="A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?"></a>A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?</h2><p>A. Migrate the database to Amazon RDS.<br>B. Set up DNS failover to a statistic website.<br>C. Use Auto Scaling to keep up with the demand.<br>D. Use both a SQL and a NoSQL database in the design.</p><p>Answer: C</p><ul><li>分析：明天就上线了，改啥都来不及了，还是价格Auto scaling策略吧。</li></ul><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway"><a href="#A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway" class="headerlink" title="A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?"></a>A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?</h2><p>A. Remove source/destination check on NAT instances.<br>B. Launch a NAT gateway in each Availability Zone.<br>C. Use a mix of NAT instances and NAT gateway.<br>D. Add an ELB Application Load Balancer in front of NAT gateway.</p><p>Answer: B</p><ul><li>NAT 实例与 NAT 网关的比较(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html</a>)</li></ul><blockquote><p>每个 NAT 网关都在特定可用区中创建，并在该可用区进行冗余实施。您可以在一个可用区中创建的 NAT 网关存在数量限制。有关更多信息，请参阅 Amazon VPC 限制。</p><p>注意:<br>如果您在多个可用区中拥有资源，并且它们共享一个 NAT 网关，则在该 NAT 网关的可用区不可用时，其他可用区中的资源将无法访问 Internet。要创建不依赖于可用区的架构，请在每个可用区中都创建一个 NAT 网关，并配置路由以确保这些资源使用自身可用区中的 NAT 网关。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance"><a href="#A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance" class="headerlink" title="A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?"></a>A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?</h2><p>A. Randomize a key name prefix.<br>B. Store the event data in separate buckets.<br>C. Randomize the key name suffix.<br>D. Use Amazon S3 Transfer Acceleration.</p><p>Answer: A</p><ul><li>分析：这道题和上面有道题类似，目前S3建议使用时间作为prefix，原来是建议使用hash方式。</li><li>最佳实践设计模式：优化 Amazon S3 性能(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a>)</li></ul><blockquote><p>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><h2 id="A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload"><a href="#A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload" class="headerlink" title="A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?"></a>A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?</h2><p>A. Amazon EC2<br>B. Amazon Kinesis Firehose<br>C. Amazon EBS<br>D. Amazon API Gateway</p><p>Answer: B</p><ul><li>分析：又是一道争议题，大部分给出的答案是B，就应用场景上看Kinesis更适合实时计算场景。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required"><a href="#A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required" class="headerlink" title="A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?"></a>A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?</h2><p>A. Attach an Elastic IP address to each Amazon EC2 instance and add a route from the private subnet to the public subnet.<br>B. Launch a NAT gateway in the public subnet and add a route to it from the private subnet.<br>C. Launch Amazon EC2 instances in the public subnet and change the security group to allow outbound traffic on port 80.<br>D. Launch a NAT gateway in the private subnet and deploy a NAT instance in the private subnet.</p><p>Answer: B</p><h2 id="An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task"><a href="#An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task" class="headerlink" title="An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?"></a>An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?</h2><p>A. Create a private subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.<br>B. Create a private subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>C. Create a public subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>D. Create a public subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.</p><p>Answer: B</p><ul><li>分析：答案给出的是C，有多种方式证明这个答案是错误的。</li><li>How do I connect a public-facing load balancer to EC2 instances that have private IP addresses?(<a href="https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/" target="_blank" rel="noopener">https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a>)</li><li>AWS Best Practices: 3-Tier Infrastructure(<a href="https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure" target="_blank" rel="noopener">https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure</a>)</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application"><a href="#A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application" class="headerlink" title="A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?"></a>A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?</h2><p>A. EBS Provisioned IOPS SSD<br>B. EBS General Purpose SSD<br>C. EBS Cold HDD<br>D. EBS Throughput Optimized HDD</p><p>Answer: D</p><h2 id="争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement"><a href="#争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement" class="headerlink" title="(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?"></a>(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?</h2><p>A. Create an IAM role that allows access from the corporate network to Amazon S3.<br>B. Configure a proxy on Amazon EC2 and use an Amazon S3 VPC endpoint.<br>C. Use Amazon API Gateway to do IP whitelisting.<br>D. Configure IP whitelisting on the customer’s gateway.</p><p>Answer: B</p><ul><li>分析：争议较大的一道题，这里采用了这个解释：<a href="https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf" target="_blank" rel="noopener">https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf</a></li></ul><h2 id="A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements" class="headerlink" title="A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?"></a>A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?</h2><p>A. An Amazon EBS Provisioned IOPS volume<br>B. An Amazon EBS General Purpose volume<br>C. An Amazon EBS Magnetic volume<br>D. An Amazon EC2 Instance Store</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way"><a href="#A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way" class="headerlink" title="A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?"></a>A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?</h2><p>A. Create a Lambda function for each individual environment.<br>B. Use Amazon DynamoDB to store environmental variables.<br>C. Use encrypted AWS Lambda environmental variables.<br>D. Implement a dedicated Lambda function for distributing variables.</p><p>Answer: C</p><h2 id="A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage"><a href="#A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage" class="headerlink" title="A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?"></a>A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?</h2><p>A. Store the archive in Amazon S3 Reduced Redundancy Storage.<br>B. Store the archive in Amazon Glacier and use standard retrieval for all content.<br>C. Store the archive in Amazon Glacier and pay the additional charge for expedited retrieval when needed.<br>D. Store the archive in Amazon S3 with a lifecycle policy to move this to S3 Infrequent Access after 30 days.</p><p>Answer: C</p><blockquote><p>问：从 Amazon S3 Glacier 检索数据如何收费？</p><p>从 Amazon S3 Glacier 检索数据的方式有三种：加急、标准和批量检索。每种方式具有不同的每 GB 检索费和每存档请求费（即请求一个存档计为一个请求）。有关不同 AWS 区域的 S3 Glacier 定价的详细信息，请访问 Amazon S3 Glacier 定价页面。</p><p>定价标准： <a href="https://amazonaws-china.com/cn/glacier/pricing/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/glacier/pricing/</a></p></blockquote><h2 id="A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements"><a href="#A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements" class="headerlink" title="A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?"></a>A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?</h2><p>A. Assign Elastic IP addresses to the database instances.<br>B. Allow Internet traffic on the private subnet through the network ACL.<br>C. Use a NAT Gateway.<br>D. Use an egress-only Internet Gateway.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized"><a href="#A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized" class="headerlink" title="A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?"></a>A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?</h2><p>A. Create an IAM access and secret key, and store it in the Lambda function.<br>B. Create an IAM role to the Lambda function with permissions to list all Amazon RDS instances.<br>C. Create an IAM role to Amazon RDS with permissions to list all Amazon RDS instances.<br>D. Create an IAM access and secret key, and store it in an encrypted RDS database.</p><p>Answer: B</p><blockquote><p>教程：配置 Lambda 函数以访问 Amazon VPC 中的 Amazon RDS</p><p>打开 IAM 控制台中的“角色”页面。<br>选择 Create role (创建角色)。<br>创建具有以下属性的角色。<br>Trusted entity (可信任的实体) – Lambda.<br>权限 – AWSLambdaVPCAccessExecutionRole。<br>角色名称 (角色名称) – lambda-vpc-role。<br>AWSLambdaVPCAccessExecutionRole 具有函数管理与 VPC 的网络连接所需的权限。</p></blockquote><h2 id="A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements" class="headerlink" title="A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?"></a>A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?</h2><p>A. Change the Amazon EC2 instant types.<br>B. Change the EBS volume type to Provisioned IOPS.<br>C. Stop the Amazon EC2 instance and provision IOPS for the EBS volume.<br>D. Enable an API Gateway to change the endpoints for the Amazon EC2 instances.</p><p>Answer: B</p><blockquote><p>打开 Amazon EC2 控制台 <a href="https://console.aws.amazon.com/ec2/。" target="_blank" rel="noopener">https://console.aws.amazon.com/ec2/。</a></p><p>选择 Volumes，选择要修改的卷，然后依次选择 Actions、Modify Volume。</p><p>Modify Volume 窗口显示卷 ID 和卷的当前配置，包括类型、大小和 IOPS。您可以在单个操作中更改任何或所有这些设置。设置新的配置值，如下所述：</p><p>要修改类型，请为 Volume Type 选择一个值。</p><p>要修改大小，请为 Size 输入一个允许的整数值。</p><p>如果选择预配置 IOPS SSD (io1) 作为卷类型，请为 IOPS 输入一个允许的整数值。</p><p>完成更改卷设置后，请选择 Modify (修改)。当系统提示您确认时，请选择 Yes。</p><p>在扩展卷的文件系统以使用新的存储容量之前，修改卷大小没有实际效果。有关更多信息，请参阅调整卷大小后扩展 Linux 文件系统。</p></blockquote><h2 id="A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata"><a href="#A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata" class="headerlink" title="A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?"></a>A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon Kinesis<br>D. Amazon EFC</p><p>Answer: B</p><ul><li>Building and Maintaining an Amazon S3 Metadata Index without Servers(<a href="https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers" target="_blank" rel="noopener">https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers</a>)</li></ul><blockquote><p>In this post, I walk through an approach for building such an index using Amazon DynamoDB and AWS Lambda. With these technologies, you can create a high performance, low-cost index that scales and remains highly available without the need to maintain traditional servers.</p></blockquote><h2 id="An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available"><a href="#An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available" class="headerlink" title="An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?"></a>An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?</h2><p>A. Move some Amazon EC2 instances to a subnet in a different way(different AZ).<br>B. Move the website to Amazon S3.<br>C. Change the ELB to an Application Load Balancer.<br>D. Move some Amazon EC2 instances to a subnet in the same Availability Zone.</p><p>Answer: A</p><ul><li>分析：这道题的选项A可能是写错了，根据评论区是different AZ，那么选择A就比较容易理解了。评论区有一种声音是选择C，但是从高可用性上讲，C选项并没有实质的价值。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two"><a href="#A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two" class="headerlink" title="A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)"></a>A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)</h2><p>A. Store an access key on the Amazon EC2 instance with rights to the Dynamo DB table.<br>B. Attach an IAM user to the Amazon EC2 instance.<br>C. Create an IAM role with permissions to write to the DynamoDB table.<br>D. Attach an IAM role to the Amazon EC2 instance.<br>E. Attach an IAM policy to the Amazon EC2 instance.</p><p>Answer: CD</p><ul><li>分析：AWS一向重视安全性，所以更推荐使用STS方式进行接口调用</li></ul><h2 id="争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements"><a href="#争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements" class="headerlink" title="(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:"></a>(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:</h2><p>1 They get an alert when the requests per second go over 50,000<br>2 They get an alert when latency goes over 5 seconds<br>3 They can validate how many times a day users call the API requesting highly-sensitive data<br>Which combination of steps does the Architect need to take to satisfy these operational requirements? (Select two.)</p><p>A. Ensure that CloudTrail is enabled.<br>B. Create a custom CloudWatch metric to monitor the API for data access.<br>C. Configure CloudWatch alarms for any metrics the support team requires.<br>D. Ensure that detailed monitoring for the EC2 instances is enabled.<br>E. Create an application to export and save CloudWatch metrics for longer term trending analysis.</p><p>Answer: BC</p><ul><li>分析：原题给出的答案是BD，但是EC2的详细监控其实并没有包含API级别的监控，ELB的监控才包含了API访问的监控。</li></ul><h2 id="争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement"><a href="#争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement" class="headerlink" title="(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?"></a>(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?</h2><p>A. Use Amazon CloudWatch to monitor utilization.<br>B. Use Amazon API Gateway to monitor availability.<br>C. Use an Amazon Elastic Load Balancer.<br>D. Use Amazon Route 53 health checks.</p><p>Answer: A</p><ul><li>分析：不同网站给出不同答案，原网站给出的答案是C，但是从题目分析关键词是the Architect needs to remove it，所以看起来A更合理一些。但是ELB增加health check之后应该可以自动的将不可用节点移除掉。</li></ul><h2 id="A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications"><a href="#A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications" class="headerlink" title="A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?"></a>A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?</h2><p>A. Create a network ACL on the web server’s subnet, and allow HTTPS inbound and MySQL outbound. Place both database and web servers on the same subnet.<br>B. Open an HTTPS port on the security group for web servers and set the source to 0.0.0.0/0. Open the MySQL port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.<br>C. Create a network ACL on the web server’s subnet, and allow HTTPS inbound, and specify the source as 0.0.0.0/0. Create a network ACL on a database subnet, allow MySQL port inbound for web servers, and deny all outbound traffic.<br>D. Open the MySQL port on the security group for web servers and set the source to 0.0.0.0/0. Open the HTTPS port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.</p><p>Answer: B</p><h2 id="Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx"><a href="#Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx" class="headerlink" title="Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?"></a>Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?</h2><p>A. AWS Lambda<br>B. Auto Scaling<br>C. AWS Elastic Beanstalk<br>D. Elastic Load Balancing</p><p>Answer: C</p><blockquote><p>AWS Elastic Beanstalk 是一项易于使用的服务，用于在熟悉的服务器（例如 Apache 、Nginx、Passenger 和 IIS ）上部署和扩展使用 Java、.NET、PHP、Node.js、Python、Ruby、GO 和 Docker 开发的 Web 应用程序和服务。<br>您只需上传代码，Elastic Beanstalk 即可自动处理包括容量预配置、负载均衡、自动扩展和应用程序运行状况监控在内的部署工作。同时，您能够完全控制为应用程序提供支持的 AWS 资源，并可以随时访问底层资源。<br>Elastic Beanstalk 不额外收费 – 您只需为存储和运行应用程序所需的 AWS 资源付费。</p></blockquote><h2 id="An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances"><a href="#An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances" class="headerlink" title="An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?"></a>An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?</h2><p>A. Amazon SQS<br>B. Auto Scaling group<br>C. Amazon EC2 security group<br>D. Amazon ELB</p><p>Answer: D</p><ul><li>分析：这道题原网站给出答案是B，但是明显应该是D</li></ul><h2 id="A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload"><a href="#A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload" class="headerlink" title="A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?"></a>A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?</h2><p>A. ELB Classic Load Balancer<br>B. Amazon Route 53 DNS<br>C. ELB Application Load Balancer<br>D. Amazon CloudFront</p><p>Answer: C</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page</a></li></ul><h2 id="A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements"><a href="#A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements" class="headerlink" title="A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?"></a>A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?</h2><p>A. Store data in a filesystem backed by Amazon Elastic File System (EFS).<br>B. Store data in Amazon S3 and use a third-party solution to expose Amazon S3 as a filesystem to the database server.<br>C. Store data in Amazon Dynamo DB and emulate relational database semantics.<br>D. Stripe data across multiple Amazon EBS volumes using RAID 0.</p><p>Answer: D</p><h2 id="A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with"><a href="#A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with" class="headerlink" title="A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:"></a>A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:</h2><p>A. an egress-only internet gateway<br>B. a NAT gateway<br>C. a custom NAT instance<br>D. a VPC endpoint</p><p>Answer: A</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a><blockquote><p>An egress-only Internet gateway. This enables instances in the private subnet to send requests to the Internet over IPv6 (for example, for software updates). An egress-only Internet gateway is necessary if you want instances in the private subnet to be able to initiate communication with the Internet over IPv6. For more information, see Egress-Only Internet Gateways.</p></blockquote></li></ul><h2 id="A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application"><a href="#A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application" class="headerlink" title="A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?"></a>A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?</h2><p>A. Create a read replica of the database.<br>B. Provision a new RDS instance as a secondary master.<br>C. Configure the database to be in multiple regions.<br>D. Increase the number of provisioned storage IOPS.</p><p>Answer: A</p><ul><li>分析：原有网站给出的答案是B，明显是A，搞这么复杂干啥</li></ul><h2 id="A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement"><a href="#A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement" class="headerlink" title="A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?"></a>A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?</h2><p>A. Amazon EFS<br>B. Amazon RDS<br>C. AWS Storage Gateway<br>D. Amazon S3</p><p>Answer: D</p><h2 id="A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution"><a href="#A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution" class="headerlink" title="A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?"></a>A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?</h2><p>A. Associate AWS KMS metrics with Amazon CloudWatch.<br>B. Use AWS CloudTrail to log AWS KMS key usage.<br>C. Deploy a monitoring agent on the RDS instances.<br>D. Poll AWS KMS periodically with a scheduled job.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform"><a href="#A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform" class="headerlink" title="A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?"></a>A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?</h2><p>A. Scheduled Reserved Instances<br>B. Convertible Reserved Instances<br>C. Standard Reserved Instances<br>D. Spot Instances</p><p>Answer: C</p><ul><li>分析：根据题目要求7*24小时不停机，所以需要排除A和D两个选项, B选项在这个场景下并不需要，所以选C</li></ul><blockquote><p>Exchanging Convertible Reserved Instances: You can exchange one or more Convertible Reserved Instances for another Convertible Reserved Instance with a different configuration, including instance family, operating system, and tenancy. There are no limits to how many times you perform an exchange, as long as the target Convertible Reserved Instance is of an equal or higher value than the Convertible Reserved Instances that you are exchanging.</p></blockquote><h2 id="A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements"><a href="#A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements" class="headerlink" title="A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?"></a>A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?</h2><p>A. Create an Amazon S3 bucket and store all of the documents in this bucket.<br>B. Create an Amazon EBS volume and allow multiple users to mount that volume to their EC2 instance(s).<br>C. Use Amazon Glacier to store all of the documents.<br>D. Create an Amazon Elastic File System (Amazon EFS) to store and share the documents.</p><p>Answer: D</p><ul><li>分析：需要文件接口，并且同时访问，那么只有EFS能够满足</li></ul><h2 id="A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two"><a href="#A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two" class="headerlink" title="A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)"></a>A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)</h2><p>A. Auto Scaling group<br>B. AWS CloudTrail<br>C. ELB Classic Load Balancer<br>D. Amazon DynamoDB<br>E. Amazon ElastiCache</p><p>Answer: AC</p><ul><li>分析：原网站给出的答案是AE，E显然没什么用对于目标</li></ul><h2 id="A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application"><a href="#A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application" class="headerlink" title="A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?"></a>A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?</h2><p>A. Use AWS Key Management Service and move the encrypted data to Amazon S3.<br>B. Use an application-specific encryption API with AWS server-side encryption.<br>C. Use encrypted EBS storage volumes with AWS-managed keys.<br>D. Use third-party tools to encrypt the EBS data volumes with Key Management Service Bring Your Own Keys.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements"><a href="#A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements" class="headerlink" title="A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?"></a>A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?</h2><p>A. Store the AWS Access Key ID/Secret Access Key combination in software comments.<br>B. Assign an IAM user to the Amazon EC2 instance.<br>C. Assign an IAM role to the Amazon EC2 instance.<br>D. Enable multi-factor authentication for the AWS root account.</p><p>Answer: C</p><h2 id="An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage"><a href="#An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage" class="headerlink" title="An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?"></a>An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?</h2><p>A. Migrate the data on the Amazon EBS volume to an SSD-backed volume.<br>B. Change the EC2 instance type to one with EC2 instance store volumes.<br>C. Migrate the data on the EBS volume to provisioned IOPS SSD (io1).<br>D. Change the EC2 instance type to one with burstable performance.</p><p>Answer: C</p><ul><li>分析：原有答案给出的是D，但是从性能角度看C明显是正确的</li></ul><h2 id="A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user"><a href="#A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user" class="headerlink" title="A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?"></a>A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?</h2><p>A. Amazon Kinesis Stream<br>B. Amazon SQS standard queue<br>C. Amazon SQS FIFO queue<br>D. AWS CloudTrail trail</p><p>Answer: A</p><ul><li>Create real-time clickstream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena(<a href="https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/</a>)</li><li>Amazon Kinesis – Real-Time Processing of Streaming Big Data(<a href="https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/</a>)</li></ul><h2 id="A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend"><a href="#A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend" class="headerlink" title="A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?"></a>A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?</h2><p>A. Amazon Aurora<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon RDS MySQL</p><p>Answer: A</p><ul><li>分析：A和D的区别没有找到合适的解释，Aurora的扩展性更好，而且是AWS云原生的。</li></ul><h2 id="争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two"><a href="#争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two" class="headerlink" title="(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)"></a>(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)</h2><p>A. Security group rule that allows inbound Internet traffic for port 443.<br>B. Security group rule that denies all inbound Internet traffic except port 443.<br>C. Network ACL rule that allows port 443 inbound and all ports outbound for Internet traffic.<br>D. Security group rule that allows Internet traffic for port 443 in both inbound and outbound.<br>E. Network ACL rule that allows port 443 for both inbound and outbound for all Internet traffic.</p><p>Answer: AC</p><ul><li>分析：答案给出的是AE，根据Network ACL的描述，默认情况为白名单，是无状态性的，返回的端口不会自动允许，所以需要C选项打开所有返回的端口。</li><li>临时端口(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a>)</li></ul><blockquote><p>临时端口<br>上一个部分中的网络 ACL 实例使用了临时端口范围 32768-65535。但是，您可能需要根据自己使用的或作为通信目标的客户端的类型为网络 ACL 使用不同的范围。<br>发起请求的客户端会选择临时端口范围。根据客户端的操作系统不同，范围也随之更改。<br>许多 Linux 内核（包括 Amazon Linux 内核）使用端口 32768-61000。<br>生成自 Elastic Load Balancing 的请求使用端口 1024-65535。<br>Windows 操作系统通过 Windows Server 2003 使用端口 1025-5000。<br>Windows Server 2008 及更高版本使用端口 49152-65535。<br>NAT 网关使用端口 1024 - 65535。<br>AWS Lambda 函数使用端口 1024-65535。<br>例如，如果一个来自 Internet 上的 Windows XP 客户端的请求到达您的 VPC 中的 Web 服务器，则您的网络 ACL 必须有相应的出站规则，以支持目标为端口 1025-5000 的数据流。<br>如果您的 VPC 中的一个实例是发起请求的客户端，则您的网络 ACL 必须有入站规则来支持发送到实例类型（Amazon Linux、Windows Server 2008 等）特有的临时端口的数据流。<br>在实际中，为使不同客户端类型可以启动流量进入您 VPC 中的公有实例，您可以开放临时端口 1024-65535。但是，您也可以在 ACL 中添加规则以拒绝任何在此范围内的来自恶意端口的数据流。请务必将拒绝 规则放在表的较前端，先于开放一系列临时端口的允许 规则。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through"><a href="#A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through" class="headerlink" title="A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:"></a>A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:</h2><p>A. VPC peering connection.<br>B. NAT gateway<br>C. VPC endpoint<br>D. AWS Direct Connect</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement"><a href="#A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement" class="headerlink" title="A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?"></a>A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?</h2><p>A. Amazon EFS<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon ElastiCache</p><p>Answer: B</p><ul><li>分析：这道题给出的答案竟然是A，不明白这个网站是不是专门负责坑人的。object-based storage system，很明显是S3.</li></ul><h2 id="A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case"><a href="#A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case" class="headerlink" title="A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?"></a>A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Cached Volumes</p><p>Answer: B</p><h2 id="A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload"><a href="#A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload" class="headerlink" title="A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?"></a>A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?</h2><p>A. Amazon EC2 instance storage<br>B. Amazon EBS General Purpose SSD (gp2) storage<br>C. Amazon S3<br>D. Amazon EBS Provision IOPS SSD (io1) storage</p><p>Answer: B</p><ul><li>分析：原始答案给出的是C，一个legcy application为什么会用S3呢？这可是需要应用改造的。</li></ul><h2 id="争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements"><a href="#争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements" class="headerlink" title="(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?"></a>(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?</h2><p>A. Use AWS IAM authorization and add least-privileged permissions to each respective IAM role.<br>B. Use an API Gateway custom authorizer to invoke an AWS Lambda function to validate each user’s identity.<br>C. Use Amazon Cognito user pools to provide built-in user management.<br>D. Use Amazon Cognito user pools to integrate with external identity providers.</p><p>Answer: B</p><ul><li>分析：正义点在答案D，参考链接：<a href="https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html" target="_blank" rel="noopener">https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html</a></li></ul><h2 id="An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution"><a href="#An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution" class="headerlink" title="An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?"></a>An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?</h2><p>A. Redesign the website to use Amazon API Gateway, and use AWS Lambda to deliver content.<br>B. Add server instances using Amazon EC2 and use Amazon Route 53 with a failover routing policy.<br>C. Serve the images and videos via an Amazon CloudFront distribution created using the news site as the origin.<br>D. Use Amazon ElasticCache for Redis for caching and reducing the load requests from the origin.</p><p>Answer: C</p><h2 id="A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process"><a href="#A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process" class="headerlink" title="A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?"></a>A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?</h2><p>A. Use the AWS CLI to create queues using AWS IAM Access Keys.<br>B. Write a script to create the Amazon SQS queue using AWS Lambda.<br>C. Use AWS Elastic Beanstalk to automatically create the Amazon SQS queues.<br>D. Use AWS CloudFormation Templates to manage the Amazon SQS queue creation.</p><p>Answer: D</p><ul><li>教程：创建 Amazon SQS 队列(<a href="https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation</a>)</li></ul><h2 id="争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone"><a href="#争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone" class="headerlink" title="(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?"></a>(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?</h2><p>A. One public subnet for the load balancer tier, one public subnet for the front-end tier, and one private subnet for the backend tier.<br>B. One shared public subnet for all tiers of the application.<br>C. One public subnet for the load balancer tier and one shared private subnet for the application tiers.<br>D. One shared private subnet for all tiers of the application.</p><p>Answer: C</p><ul><li>分析：答案给出的是A，但是题目中说道only the front-end load balancer should be exposed to the internet，所以A答案中为front-end tier一个公网subnet有点多余了</li></ul><h2 id="A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application"><a href="#A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application" class="headerlink" title="A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?"></a>A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?</h2><p>A. An Amazon EC2 instance store local SSD volume.<br>B. An Amazon EBS provisioned IOPS SSD volume.<br>C. An Amazon EBS throughput optimized HDD volume.<br>D. An Amazon EBS general purpose SSD volume.</p><p>Answer: C</p><ul><li>分析：这道题需要高顺序I/O和低成本，显然C正确</li></ul><h2 id="Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy"><a href="#Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy" class="headerlink" title="Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?"></a>Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?</h2><p>A. Using security groups that reference the security groups of the other application<br>B. Using security groups that reference the application server’s IP addresses<br>C. Using Network Access Control Lists to allow/deny traffic based on application IP addresses<br>D. Migrating the applications to separate subnets from each other</p><p>Answer: A</p><h2 id="Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two"><a href="#Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two" class="headerlink" title="Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)"></a>Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)</h2><p>A. Amazon SNS<br>B. AWS STS<br>C. Amazon SQS<br>D. Amazon Route 53<br>E. AWS Glue</p><p>Answer: AC</p><ul><li>分析：根据题目是要解决消息的问题，消息服务有两种SNS和SQS，因为题目里并没有说消息模式，所以这两种也许能解决需求。</li></ul><h2 id="A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model"><a href="#A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model" class="headerlink" title="A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?"></a>A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?</h2><p>A. Amazon EC2 Spot Instances<br>B. On-Demand Amazon EC2 Instances<br>C. Scheduled Reserved Instances<br>D. Dedicated Amazon EC2 Instances</p><p>Answer: C</p><h2 id="A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available"><a href="#A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available" class="headerlink" title="A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?"></a>A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?</h2><p>A. Change the Amazon EC2 instance to compute optimized.<br>B. Launch a second Amazon EC2 instance to monitor the health of the first.<br>C. Trigger a Lambda function when a new object is uploaded.<br>D. Initially copy the images to an attached Amazon EBS volume.</p><p>Answer: C</p><ul><li>分析：Lambda的触发器很适合做这个</li></ul><h2 id="An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met"><a href="#An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met" class="headerlink" title="An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?"></a>An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?</h2><p>A. Configure a NAT gateway in a public subnet and route all traffic to Amazon Kinesis through the NAT gateway.<br>B. Configure a gateway VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>C. Configure an interface VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>D. Configure an AWS Direct Connect private virtual interface for Kinesis and route all traffic to Kinesis through the virtual interface.</p><p>Answer: C</p><ul><li>分析：误选了B，从题目说是Kinesis需要读取EC2的数据，所以应该是在VPC interface上建立一个endpoint</li></ul><h2 id="A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements" class="headerlink" title="A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?"></a>A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?</h2><p>A. Amazon S3<br>B. Amazon Glacier<br>C. Amazon EFS<br>D. AWS Storage Gateway</p><p>Answer: B</p><blockquote><p>Data stored in Amazon Glacier is immutable, meaning that after an archive is created it cannot be updated. This ensures that data such as compliance and regulatory records cannot be altered after they have been archived.</p></blockquote><ul><li>分析：从这道题我们可以看出，考试的时候选中文的重要性，不会因为一个单词意思不明确导致整个题目判断失误，关键词是immutalbe，不可改变的</li></ul><h2 id="争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted"><a href="#争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted" class="headerlink" title="(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?"></a>(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?</h2><p>A. Set a CORS configuration.<br>B. Set a bucket policy to encrypt all Amazon S3 objects.<br>C. Enable default encryption on the bucket.<br>D. Set permission for users.</p><p>Answer: B</p><ul><li>分析：争议点在于答案C，从界面操作上看BC好像是在做同一件事情</li><li>如何为 Amazon S3 存储桶启用默认加密？(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html</a>)</li><li>How to Prevent Uploads of Unencrypted Objects to Amazon S3(<a href="https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a>)</li></ul><h2 id="An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path"><a href="#An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path" class="headerlink" title="An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?"></a>An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?</h2><p>A. AWS Application Load Balancer<br>B. Amazon CloudFront<br>C. Amazon Classic Load Balancer<br>D. Amazon Route 53</p><p>Answer: A</p><h2 id="A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend"><a href="#A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend" class="headerlink" title="A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?"></a>A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?</h2><p>A. Amazon RDS<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon S3</p><p>Answer: C</p><h2 id="争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows"><a href="#争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows" class="headerlink" title="(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:"></a>(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:</h2><p>✑ Limit access to users origination from the corporate network.<br>✑ Web servers cannot have SSH access directly from the Internet.<br>✑ Web servers reside in a private subnet.<br>Which combination of steps must the Architect complete to meet these requirements? (Choose two.)</p><p>A. Create a bastion host that authenticates users against the corporate directory.<br>B. Create a bastion host with security group rules that only allow traffic from the corporate network.<br>C. Attach an IAM role to the bastion host with relevant permissions.<br>D. Configure the web servers’ security group to allow SSH traffic from a bastion host.<br>E. Deny all SSH traffic from the corporate network in the inbound network ACL.</p><p>Answer: BD</p><ul><li>分析：原有答案给出的是AC，感觉并不能完全解决该问题</li><li>How to Record SSH Sessions Established Through a Bastion Host(<a href="https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/</a>)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考链接：&lt;a href=&quot;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。&lt;/p&gt;
&lt;p&gt;由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>使用阿里云函数计算构建小程序</title>
    <link href="http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/"/>
    <id>http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/</id>
    <published>2019-12-19T15:19:16.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a>1、需求</h1><p>在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。</p><a id="more"></a><img src="/images/blogs/2019-12-19/architecture.png" class=""><p>在原有设计中，使用了Python Flask提供WEB界面，后台使用Celery异步的将用户请求发送至钉钉中，之后采用轮询方式监控审批工单状态，当工单完成审批后，将生成好的License发送至客户提供的邮箱中。</p><p>实现的效果：</p><img src="/images/blogs/2019-12-19/UI.jpeg" class=""><p>这种方式虽然可以满足需求，但是在使用过程中也发现有如下痛点：<br>1、由于对于可用性要求比较高，所以将整套应用以容器化方式部署在云主机上，程序高可用性依赖于底层的平台，基于成本考虑并没有在多可用区进行部署。<br>2、当业务变化时，需要专人将容器从本地容器库上传后进行更新，更新速度慢，敏捷性低。<br>3、需要专人对操作系统层进行维护，并且由于该云主机还运行了其他程序，所以管控上也存在安全风险。</p><p>基于以上出现的问题，决定对原有二维码程序进行重构，并重新部署在阿里云函数计算服务上。<br>1、第一阶段的改造主要是将二维码扫描程序移植到函数计算服务中。<br>2、第二阶段的改造主要是将发送二维码程序改造为函数计算服务，使用钉钉流程接口中的Callback方法调用该接口，在审批结束后触发发送License流程。</p><h1 id="2、函数计算服务——无服务，零运维"><a href="#2、函数计算服务——无服务，零运维" class="headerlink" title="2、函数计算服务——无服务，零运维"></a>2、函数计算服务——无服务，零运维</h1><p>最早接触Serverless的雏形是在2011年开发Cloud Foundry项目时，当时留下一个非常深的印象就是把写好的应用直接上传就完成了部署、扩展等。但是当时Cloud Foundry有一个非常大的局限性，受限于几种开发语言和框架。记得当时的Cloud Foundry只支持Node.js、Python、Java、PHP、Ruby on Rails等，脱离了这个范围则就无法支持，所以当时我其实对这种形态的应用场景存在很大的疑问。<br>这种困惑直到2013年Docker的出现而逐步解开，Docker的出现让开发语言、框架不再是问题，巧妙的解决了Cloud Foundry上述局限性。但是Docker毕竟只是一种工具形态，还不能称得上是平台，紧接着k8s的出现弥补了这一空白，使得Docker从游击队变成了正规军。<br>在这个发展过程中我们不难看出，软件领域发展出现了重大变革，从服务器为王逐渐演进到应用为王的阶段。如果说虚拟化改变了整个物理机的格局，那么无服务化的出现则改变了整个软件开发行业。<br>由于网上各种文档太多了，这里就不对Serverless基本概念进行介绍了，借用一张图说明下。另外还有一点，我们从这里面看到IT行业里的某些岗位，注定要消失的，比如传统运维。</p><img src="/images/blogs/2019-12-19/compare.png" class=""><h1 id="3、应用架构"><a href="#3、应用架构" class="headerlink" title="3、应用架构"></a>3、应用架构</h1><p>整个架构上，分为两个函数计算服务完成：</p><ul><li>二维码前端：主要用于显示页面，并承担HTTP请求转发代理的角色，将请求转发至二维码后端，发给钉钉，采用HTTP触发器，允许公网访问。</li><li>二维码后端：用于将用户请求发送给钉钉，该部分服务仍然采用HTTP触发器，不同于前端，该服务是不允许公网直接访问的，但是需要配置NAT网关，通过网关访问钉钉，实现固定IP访问钉钉的效果。</li></ul><img src="/images/blogs/2019-12-19/new_architecture.png" class=""><p>从逻辑上讲，整个应用并不复杂，但是在实际使用时遇到最大的问题来自钉钉白名单。由于函数服务对外连接的IP并不固定，所以无法在钉钉中添加，那么就要求函数服务对外连接的IP地址一定要固定。社区中提供的方法主要分为：</p><ul><li>ECI（运行Nginx充当Proxy），优势是便宜，劣势是高可用性需要自己维护</li><li>NAT网关，优势是高可用性，劣势是比ECI贵</li></ul><h1 id="4、构建过程"><a href="#4、构建过程" class="headerlink" title="4、构建过程"></a>4、构建过程</h1><p>由于篇幅原因，这里只介绍关键步骤。</p><h2 id="4-1-构建模板"><a href="#4-1-构建模板" class="headerlink" title="4.1 构建模板"></a>4.1 构建模板</h2><p>为了后续管理和扩展方便，选用了阿里云函数计算中使用flask-web模板进行构建，同时可以将前端静态文件模板存放于项目下（出于统一管理的需要，也可以存放于阿里云的OSS中，作为静态网站发布）。</p><p>前端我们使用flask-web作为模板创建函数，后端我们直接采用最简单的HTTP函数。</p><img src="/images/blogs/2019-12-19/create_function_template.png" class=""><p>函数入口配置，及触发器配置：</p><img src="/images/blogs/2019-12-19/create_function.png" class=""><p>服务配置，包含公网访问权限，专有网络配置，日志配置，权限配置。</p><ul><li>前端服务需要公网访问权限，不需要专有网络配置，需要的权限为：AliyunLogFullAccess。</li><li>后端服务不需要公网访问权限，但是需要配置好的NAT映射的专有网络，由于函数服务在北京2区中在cn-beijing-c和cn-beijing-f，所以在新建交换机时需要使用这两个区。还需要选择安全组，由于出方向并没有明确禁止，所以不需要特别的安全组规则设定。需要的权限为：AliyunLogFullAccess/AliyunECSNetworkInterfaceManagementAccess。</li></ul><p>配置好后，通过导出功能，分别下载前端和后端代码和配置，在本地进行开发调试。</p><img src="/images/blogs/2019-12-19/export_function.png" class=""><h2 id="4-2-前端开发"><a href="#4-2-前端开发" class="headerlink" title="4.2 前端开发"></a>4.2 前端开发</h2><p>我们的前端采用Vue.js进行开发，在main.py同级新建templates目录。Vue编译好的静态文件可以放入该目录中，后续Flask会加载该文件作为入口文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── templates</span><br><span class="line">│   ├── index.html</span><br><span class="line">│   ├── static</span><br><span class="line">├── main.py</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># main.py sample</span><br><span class="line">from flask import render_template</span><br><span class="line"></span><br><span class="line">LICENSE_URL &#x3D; &quot;https:&#x2F;&#x2F;[x](https:&#x2F;&#x2F;.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license).cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license&quot;</span><br><span class="line"></span><br><span class="line">@app.route(&#39;&#x2F;qr_code&#39;, methods&#x3D;[&#39;GET&#39;])</span><br><span class="line">def index():</span><br><span class="line">      return render_template(&#39;index.html&#39;)</span><br><span class="line"></span><br><span class="line">      @app.route(&#39;&#x2F;qr_code&#x2F;license&#39;, methods&#x3D;[&#39;POST&#39;])</span><br><span class="line">      def create():</span><br><span class="line">            payload &#x3D; request.json</span><br><span class="line">                resp &#x3D; requests.post(LICENSE_URL,</span><br><span class="line">                                                 json&#x3D;payload,</span><br><span class="line">                                                                              headers&#x3D;DEFAULT_HEADERS)</span><br><span class="line">                return make_response(resp.text, resp.status_code)</span><br></pre></td></tr></table></figure><h2 id="4-3-后端开发"><a href="#4-3-后端开发" class="headerlink" title="4.3 后端开发"></a>4.3 后端开发</h2><p>后端的开发较为简单，实现一个函数支持POST请求，将转发的结果发送至钉钉即可。</p><h2 id="4-4-本地调试"><a href="#4-4-本地调试" class="headerlink" title="4.4 本地调试"></a>4.4 本地调试</h2><p>阿里云在本地开发时提供了fun应用部署和开发工具，详细使用方法见：<a href="https://help.aliyun.com/document_detail/64204.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/64204.html</a>。</p><h3 id="安装fun"><a href="#安装fun" class="headerlink" title="安装fun"></a>安装fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry [https:&#x2F;&#x2F;registry.npm.taobao.org](https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;) --global</span><br><span class="line">npm config set disturl [https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist](https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist) --global</span><br><span class="line"></span><br><span class="line">npm install @alicloud&#x2F;fun -g</span><br></pre></td></tr></table></figure><h3 id="配置fun"><a href="#配置fun" class="headerlink" title="配置fun"></a>配置fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fun config</span><br><span class="line"></span><br><span class="line">(venv) [root@ray-dev test_func]# fun config</span><br><span class="line">? Aliyun Account ID xxxxxxxx</span><br><span class="line">? Aliyun Access Key ID ***********r5Qd</span><br><span class="line">? Aliyun Access Key Secret ***********kCCi</span><br><span class="line">? Default region name cn-beijing</span><br><span class="line">? The timeout in seconds for each SDK client invoking 10</span><br><span class="line">? The maximum number of retries for each SDK client 3</span><br><span class="line">? Allow to anonymously report usage statistics to improve the tool over time? Yes</span><br></pre></td></tr></table></figure><h3 id="Http-Trigger本地运行"><a href="#Http-Trigger本地运行" class="headerlink" title="Http Trigger本地运行"></a>Http Trigger本地运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun local start</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun deploy</span><br></pre></td></tr></table></figure><h2 id="4-5-配置域名解析"><a href="#4-5-配置域名解析" class="headerlink" title="4.5 配置域名解析"></a>4.5 配置域名解析</h2><p>部署完成后有一点需要特别注意，必须要绑定域名，并且设定必要的路由。如果在没有绑定域名的情况下，服务端会为 response header中强制添加 content-disposition: attachment字段，此字段会使得返回结果在浏览器中以附件的方式打开。（<a href="https://www.alibabacloud.com/help/zh/doc-detail/56103.htm" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/56103.htm</a>）</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><ul><li>灵活使用函数计算对开发成本和运行成本具有“双降”的效果</li><li>函数计算除了Http Trigger外，还包含了Event Trigger。Event Trigger中包含了连接各个服务之间的作用，在一些服务衔接上的作用越来越明显</li><li>函数计算在线开发时比较麻烦，并且查看日志不方便，所以尽量在本地开发好在上传的方式</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1、需求&quot;&gt;&lt;a href=&quot;#1、需求&quot; class=&quot;headerlink&quot; title=&quot;1、需求&quot;&gt;&lt;/a&gt;1、需求&lt;/h1&gt;&lt;p&gt;在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="阿里云" scheme="http://sunqi.me/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/tags/Cloud-Computing/"/>
    
      <category term="Serverless" scheme="http://sunqi.me/tags/Serverless/"/>
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Newton国内代码贡献</title>
    <link href="http://sunqi.me/2016/09/30/contricution-in-newton/"/>
    <id>http://sunqi.me/2016/09/30/contricution-in-newton/</id>
    <published>2016-09-30T17:00:49.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。</p><p>这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。</p><p>最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。</p><p>还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><p>OpenStack Mitaka深度解读请见：<a href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>下面是最近三个版本Release的详细对比：</p><img src="/images/blogs/contribution-in-newton-projects.png" class="center"><p>让我们来关注这次Release中的三个新项目：</p><h3 id="Panko-计量服务事件消息存储"><a href="#Panko-计量服务事件消息存储" class="headerlink" title="Panko(计量服务事件消息存储)"></a>Panko(计量服务事件消息存储)</h3><p>Panko是计量模块中的一部分，主要是为了计量模块提供事件消息存储，众所周知，在上一个OpenStack Release中，Ceilometer被一分为三，分别为aodh(告警服务)/Gnocchi(基于时间的数据库服务)/Ceilometer，为了解决当前Ceilometer中存在的性能问题，提高更好的扩展性。</p><p>现在Panko的文档并不是很丰富，如果有需要了解更多详细内容的，可以关注Developer的文档：<a href="http://docs.openstack.org/developer/panko/" target="_blank" rel="noopener">http://docs.openstack.org/developer/panko/</a></p><h3 id="Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务"><a href="#Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务" class="headerlink" title="Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)"></a>Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)</h3><p>Vitrage是一个OpenStack RCA(Root Cause Analysis)服务，用于组织、分析和扩展OpenStack的告警和事件，在真正的问题发生前找到根本原因。</p><p>众所周知，OpenStack平台最大的优势来自于架构的可扩展性，这也是OpenStack能够在基础架构曾一枝独秀的重要原因。分布式架构最大的优势在于扩展，但是过于灵活的扩展性为运维带来的极大的困难，所以Vitrage的出现在一定程度上缓解了OpenStack运维上的痛点。</p><p>我们来简单看一下他的架构，更多详细的介绍请查看WIKI：<a href="https://wiki.openstack.org/wiki/Vitrage" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Vitrage</a></p><img src="/images/blogs/contribution-in-newton-vitrage-architecture.png" class="center"><h3 id="Watcher-OpenStack平台优化服务"><a href="#Watcher-OpenStack平台优化服务" class="headerlink" title="Watcher(OpenStack平台优化服务)"></a>Watcher(OpenStack平台优化服务)</h3><p>从名字上看，我们并不能理解这个模块的具体左右，我们通过文档中用户应用场景来了解一下Watcher的作用：</p><p>作为一名云平台的管理员在云平台使用一段时间后，想根据一些物理特性对云平台虚拟机的分布进行重新平衡，例如服务器的温度、电源的状态等信息，那么这时候就可以通过watcher，利用Nova虚拟机的在线迁移对整个数据中心云平台的虚拟机进行一些优化处理，从而达到某种平衡。我认为这其实类似于VMWare的DRS功能。</p><p>当然Watcher还有更多的应用场景，更多详细的介绍请查看：<a href="https://wiki.openstack.org/wiki/Watcher" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Watcher</a></p><p>我们来简单看一下他的架构，更多架构方面的详细的介绍请查看：<a href="http://docs.openstack.org/developer/watcher/architecture.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/watcher/architecture.html</a></p><img src="/images/blogs/contribution-in-newton-watcher-architecture.svg" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits和blueprints的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司和贡献者来说，都有所上升，这也不难理解，随着OpenStack模块增加，势必涉及更多的领域，所以更多的公司加入了这个生态圈。</p><img src="/images/blogs/contribution-in-newton-companies-contributors.png" class="center"><p>从commits角度进行分析，传统几大好强几乎没有变化，日本的Fujitsu在commits上挤掉了华为，进入了前十名的位置。模块方面，核心模块的贡献仍然位于前十名，也说明是应用最多的模块，所以才会不断的发现问题。本次统计的总项目数量为629个，可能stackalytics在统计策略上有所调整。</p><img src="/images/blogs/contribution-in-newton-companies-modules-commits.png" class="center"><p>单从commits角度统计其实有失偏颇，真正能够体现公司在OpenStack实力的指标应该是Blueprints。我认为完成Blueprints至少具备三个必要条件：英语要好、在社区有一定的影响力、架构设计能力。这些都是需要不断在社区进行积累和沉淀的。</p><p>本次release周期内，能够完成Blueprints的公司为64个，国内的华为和九州云均进入前10名，排名比较靠前的国内企业还包括：Easystack、中兴。</p><p>完成Blueprints最多的仍然是核心模块，排在第二名的是kolla，看来在上一个周期中，kolla项目的活跃程度是较高的。</p><img src="/images/blogs/contribution-in-newton-companies-modules-blueprints.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完总体的状况，再来关注一些国内的贡献情况，与去年相比，今年上榜的国内企业达到了21家，创历年之最，比去年的15家企业整整多了7家，并且我们发现在这些新增企业中大部分都是提供企业服务的公司，说明OpenStack在国内的企业级市场开始站稳脚跟。下面我们来做一个详细的分析：</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在最近的三个版本连续对社区有贡献的企业包括：华为，Easystack，九州云，海云捷迅，华三，Unitedstack，乐视，中国移动和北京休伦科技(Huron)。</p><p>本次爬升最快的企业：中兴，从108位攀升至13位。</p><p>本次统计新增的7家企业：云途腾(t2cloud)，大唐高鸿数据(GohighSec)，华云数据，烽火通信，爱数，北京国电通，云英，中国银联，赛特斯信息。</p><p>本次排名中OpenStack的直接用户：中国移动和中国银联。中国移动更是参选了OpenStack SuperAward的评比，预祝他们能顺利当选。</p><img src="/images/blogs/contribution-in-newton-china-companies.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>我们再来从人员投入来分析贡献情况一下：</p><ul><li>投入人数最多的仍然是华为，有65名工程师贡献了本次的commits</li><li>中兴无疑是本次人员投入增长最快的，从6名工程师一下子扩张到61名，也是唯一能和华为抗衡的</li><li>超过2位数人员投入的包括，Easystack，九州云和Unitedstack，另外海云捷迅有9人，华三有8人，中国移动有7人参与社区贡献</li></ul><img src="/images/blogs/contribution-in-newton-companies-effort.png" class="center"><h3 id="模块贡献分析"><a href="#模块贡献分析" class="headerlink" title="模块贡献分析"></a>模块贡献分析</h3><p>从模块贡献角度来分析，国内企业的贡献仍然没有出现一个统一的趋势，与Mitaka Release相比，贡献涉及模块的总量从192个增加至Newton Release的246个，一方面说明OpenStack本身模块的增加，也说明国内企业使用或开发OpenStack在方向上的多元化。</p><p>从贡献的模块来看，华为主导的dargonflow高居榜首，紧随其后的是手册和clients两个项目，随后的贡献集中在OpenStack的核心模块，与Docker相关的几个模块中。Kolla项目无疑是最近关注的热点，随着Docker的快速发展，OpenStack和Docker不断碰撞出新的火花。</p><img src="/images/blogs/contribution-in-newton-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这个问题仍然是比较敏感的问题，只有每个公司的CEO能够回答这个问题，这里面我从融资的角度来回顾一下2015至2016年之间在OpenStack领域发生过什么。</p><ul><li>2015年9月17日，英特尔投资部门披露了此前投资的中国8家公司名单。投资总额达6700万美元，领域覆盖了新材料、智能设备、物联网、云服务等领域。其中包含九州云和海云捷迅两家OpenStack企业。(<a href="http://tech.qq.com/a/20150917/038604.htm" target="_blank" rel="noopener">http://tech.qq.com/a/20150917/038604.htm</a>)</li><li>2015年10月17日，中国最大的独立公有云提供商UCloud和全球领先的OpenStack厂商Mirantis在东京的OpenStack峰会上正式宣布成立合资公司UMCloud，以求更好的在中国做OpenStack。(<a href="http://www.doit.com.cn/article/1027290510.html" target="_blank" rel="noopener">http://www.doit.com.cn/article/1027290510.html</a>)</li><li>2015年12月16日，UnitedStack有云宣布完成C轮融资，该轮融资由思科和红杉资本投资，具体数额未公布(<a href="http://www.infoq.com/cn/news/2015/12/unitedstack-financing" target="_blank" rel="noopener">http://www.infoq.com/cn/news/2015/12/unitedstack-financing</a>)</li><li>2016年5月20日，云途腾(T2Cloud)完成A轮3650万融资(<a href="http://iimedia.cn/42262.html" target="_blank" rel="noopener">http://iimedia.cn/42262.html</a>)</li><li>2016年9月21日，腾讯与海云捷迅昨日下午在京共同宣布达成战略投资合作关系，海云捷迅接受腾讯的战略投资(<a href="http://www.36dsj.com/archives/62353" target="_blank" rel="noopener">http://www.36dsj.com/archives/62353</a>)</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回到开篇的那句话，OpenStack贡献量只能反应中国企业对于开源项目的重视程度，无法反应真实的用户需求。VMWare花了将近10年的时间教育用户，说服用户把应用从物理机迁移至虚拟机。OpenStack从2011年出生到现在也仅仅短短的5年，可见OpenStack还有很长的路要走。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。&lt;/p&gt;
&lt;p&gt;这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。&lt;/p&gt;
&lt;p&gt;最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。&lt;/p&gt;
&lt;p&gt;还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Mitaka深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>使用国内源部署Ceph</title>
    <link href="http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/"/>
    <id>http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/</id>
    <published>2016-06-19T01:25:37.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。</p><a id="more"></a><h2 id="关于国内源"><a href="#关于国内源" class="headerlink" title="关于国内源"></a>关于国内源</h2><p>根据Ceph中国社区的<a href="http://bbs.ceph.org.cn/?/page/image" target="_blank" rel="noopener">统计</a>，国内已经有四家网站开始同步Ceph源，分别是：</p><ul><li>网易镜像源<a href="http://mirrors.163.com/ceph" target="_blank" rel="noopener">http://mirrors.163.com/ceph</a></li><li>阿里镜像源<a href="http://mirrors.aliyun.com/ceph" target="_blank" rel="noopener">http://mirrors.aliyun.com/ceph</a></li><li>中科大镜像源<a href="http://mirrors.ustc.edu.cn/ceph" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ceph</a></li><li>宝德镜像源 <a href="http://mirrors.plcloud.com/ceph" target="_blank" rel="noopener">http://mirrors.plcloud.com/ceph</a></li></ul><h2 id="国内源分析"><a href="#国内源分析" class="headerlink" title="国内源分析"></a>国内源分析</h2><p>以163为例，是以天为单位向回同步Ceph源，完全可以满足大多数场景的需求，同步的源也非常全，包含了calamari，debian和rpm的全部源，最近几个版本的源也能从中找到。</p><h2 id="安装指定版本的Ceph"><a href="#安装指定版本的Ceph" class="headerlink" title="安装指定版本的Ceph"></a>安装指定版本的Ceph</h2><p>这里以安装最新版本的Jewel为例，由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-jewel&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br></pre></td></tr></table></figure><p>之后的过程就没有任何区别了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create monitor node</span><br><span class="line">ceph-deploy new node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Software Installation</span><br><span class="line">ceph-deploy install deploy node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Gather keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"># Ceph deploy parepare and activate</span><br><span class="line">ceph-deploy osd prepare node1:&#x2F;dev&#x2F;sdb node2:&#x2F;dev&#x2F;sdb node3:&#x2F;dev&#x2F;sdb</span><br><span class="line">ceph-deploy osd activate node1:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 node2:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1 node3:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-2</span><br><span class="line"></span><br><span class="line"># Make 3 copies by default</span><br><span class="line">echo &quot;osd pool default size &#x3D; 3&quot; | tee -a $HOME&#x2F;ceph.conf</span><br><span class="line"></span><br><span class="line"># Copy admin keys and configuration files</span><br><span class="line">ceph-deploy --overwrite-conf admin deploy node1 node2 node3</span><br></pre></td></tr></table></figure><p>这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Docker部署Ceph</title>
    <link href="http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/"/>
    <id>http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/</id>
    <published>2016-06-12T23:20:50.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是根据Sébastien Han的<a href="https://www.youtube.com/watch?v=FUSTjTBA8f8&feature=youtu.be" target="_blank" rel="noopener">演示视频</a>进行整理的，对过程中有问题的部分进行了修复。</p><p>Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。</p><a id="more"></a><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><ul><li>至少需要三台虚拟机或者物理机，每台虚拟机或者物理机至少有两块硬盘，这里我是在一台物理机上用vagrant模拟出三台CentOS 6.6虚拟机进行的实验</li><li>三台虚拟机需要安装docker，本文附带Docker加速方案</li><li>获取ceph/daemon镜像</li></ul><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><img src="/images/blogs/bootstrap-ceph-docker-flow.png" class="center"><h2 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h2><p>主机名和集群的对应关系如下：</p><ul><li>node1 -&gt; 192.168.33.11</li><li>node2 -&gt; 192.168.33.12</li><li>node3 -&gt; 192.168.33.13</li></ul><img src="/images/blogs/bootstrap-ceph-docker-architecture.png" class="center"><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="安装Docker，下载镜像"><a href="#安装Docker，下载镜像" class="headerlink" title="安装Docker，下载镜像"></a>安装Docker，下载镜像</h3><p>国内安装Dcoker还是速度很慢的，这里推荐使用daocloud的加速方案。不但docker安装速度提高了，pull镜像的速度也大幅度提高。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh</span><br></pre></td></tr></table></figure><p>我是在CentOS系统上进行的测试，将docker加入自动启动，并启动docker，接下来pull ceph daemon镜像，该镜像包含了所有的ceph服务和entrypoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chkconfig docker</span><br><span class="line">service docker start</span><br><span class="line">docker pull ceph&#x2F;daemon</span><br></pre></td></tr></table></figure><h3 id="启动第一个Monitor"><a href="#启动第一个Monitor" class="headerlink" title="启动第一个Monitor"></a>启动第一个Monitor</h3><p>在node1上启动第一个Monitor，注意，如果你的环境中IP和我不同，请修改MON_IP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.11 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>验证一下效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES</span><br><span class="line">7babea544ef1        ceph&#x2F;daemon         &quot;&#x2F;entrypoint.sh mon&quot;   3 seconds ago       Up 2 seconds                            backstabbing_brattain</span><br></pre></td></tr></table></figure><p>查看一下集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec 7babea544ef1 ceph -s</span><br></pre></td></tr></table></figure><p>当前集群状态，能看到当前已经有一个mon启动起来了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e1: 1 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 2, quorum 0 node1.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="复制配置文件"><a href="#复制配置文件" class="headerlink" title="复制配置文件"></a>复制配置文件</h3><p>接下来需要将node1的配置文件复制到node2和node3上，复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。这些配置文件非常重要，如果没有这些配置文件的存在，我们在其他节点启动新的docker ceph daemon的时候会被认为是一个新的集群。<br>我们在node1执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssh root@node2 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node2:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node2:&#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line"></span><br><span class="line">ssh root@node3 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node3:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node3:&#x2F;var&#x2F;lib&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="启动第二个和第三个Monitor"><a href="#启动第二个和第三个Monitor" class="headerlink" title="启动第二个和第三个Monitor"></a>启动第二个和第三个Monitor</h3><p>在node2上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.12 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node3上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.13 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node1上查看集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="启动OSD的遇到的问题"><a href="#启动OSD的遇到的问题" class="headerlink" title="启动OSD的遇到的问题"></a>启动OSD的遇到的问题</h3><p>按照原视频的介绍的方法，启动OSD可以直接指定某个分区，然后用osd_ceph_disk作为启动ceph/daemon的参数，之后docker镜像会自动的进行分区等动作。但是经过实际验证却发现在mkjournal创建错误，OSD无法启动。</p><p>经过和社区确认，发现这个Bug在之前版本中得到过修复，但是之后的版本又出现了。根据社区的建议使用jewel版本的ceph daemon进行了再次验证，发现问题依旧，所以这里介绍的方法只能退而求其次，采用手动方式分区、格式化，之后用osd_directory启动ceph/daemon。</p><p>这是github上的相关讨论：<a href="https://github.com/ceph/ceph-docker/issues/171" target="_blank" rel="noopener">https://github.com/ceph/ceph-docker/issues/171</a></p><p>这是用osd_ceph_disk方式启动后的错误日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd --cluster ceph --mkfs --mkkey -i 4 --monmap &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;activate.monmap --osd-data &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG --osd-journal &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal --osd-uuid 89e240e1-17e9-4d6c-8d4f-f1a3e0278b91 --keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;keyring --setuser ceph --setgroup disk</span><br><span class="line">2016-06-12 23:37:26.180610 7f8889654800 -1 filestore(&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG) mkjournal error creating journal on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal: (2) No such file or directory</span><br><span class="line">2016-06-12 23:37:26.180752 7f8889654800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -2</span><br><span class="line">2016-06-12 23:37:26.180918 7f8889654800 -1 ** ERROR: error creating empty object store in &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG: (2) No such file or directory</span><br><span class="line">mount_activate: Failed to activate</span><br><span class="line">unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br><span class="line">command_check_call: Running command: &#x2F;bin&#x2F;umount -- &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br></pre></td></tr></table></figure><h3 id="启动OSD"><a href="#启动OSD" class="headerlink" title="启动OSD"></a>启动OSD</h3><p>第一步先进行分区和格式化，这里只给出node1的操作方式，其他两个节点的方式类似。</p><p>先来安装必要的工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y parted xfsprogs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# parted &#x2F;dev&#x2F;sdb</span><br><span class="line">GNU Parted 2.1</span><br><span class="line">Using &#x2F;dev&#x2F;sdb</span><br><span class="line">(parted) mklabel</span><br><span class="line">New disk label type? gpt</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition name?  []? &quot;Linux filesystem&quot;</span><br><span class="line">File system type?  [ext2]? xfs</span><br><span class="line">Start? 0G</span><br><span class="line">End? 107GB</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name              Flags</span><br><span class="line"> 1      1049kB  107GB  107GB               Linux filesystem</span><br><span class="line"></span><br><span class="line">(parted) quit</span><br></pre></td></tr></table></figure><p>格式化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# mkfs.xfs &#x2F;dev&#x2F;sdb1</span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdb1              isize&#x3D;256    agcount&#x3D;4, agsize&#x3D;6553472 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;0        finobt&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;0</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br></pre></td></tr></table></figure><p>我们把目录在node1上进行挂载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;ceph&#x2F;sdb</span><br><span class="line">mount &#x2F;dev&#x2F;sdb1 &#x2F;ceph&#x2F;sdb</span><br></pre></td></tr></table></figure><p>最后启动OSD，这里最重要的就是把我们刚刚挂载好的OSD的实际路径透传给Docker内部的/var/lib/ceph/osd，如果每个节点有多个OSD的情况下，只需要在Host上映射到不同的目录，启动Docker的时候变更和/var/lib/ceph/osd的映射关系即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -v &#x2F;dev&#x2F;:&#x2F;dev&#x2F; \</span><br><span class="line">    -v &#x2F;ceph&#x2F;sdb:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd \</span><br><span class="line">    --privileged&#x3D;true \</span><br><span class="line">    ceph&#x2F;daemon osd_directory</span><br></pre></td></tr></table></figure><p>按照同样的方法，将node2和node3的OSD也加入到集群，最终的效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        64 pgs degraded</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        64 pgs undersized</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e13: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        4551 MB used, 11306 MB &#x2F; 16720 MB avail</span><br><span class="line">              64 active+undersized+degraded</span><br></pre></td></tr></table></figure><h3 id="创建MDS"><a href="#创建MDS" class="headerlink" title="创建MDS"></a>创建MDS</h3><p>创建好基本的环境，其他的就容易了很多，下面来启动MDS。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -e CEPHFS_CREATE&#x3D;1 \</span><br><span class="line">    ceph&#x2F;daemon mds</span><br></pre></td></tr></table></figure><h3 id="启动RGW，并且映射80端口"><a href="#启动RGW，并且映射80端口" class="headerlink" title="启动RGW，并且映射80端口"></a>启动RGW，并且映射80端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    -p 80:80 \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    ceph&#x2F;daemon rgw</span><br></pre></td></tr></table></figure><h3 id="最终的集群状态"><a href="#最终的集群状态" class="headerlink" title="最终的集群状态"></a>最终的集群状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        48 pgs stuck inactive</span><br><span class="line">        48 pgs stuck unclean</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> mdsmap e5: 1&#x2F;1&#x2F;1 up &#123;0&#x3D;mds-node1.docker.com&#x3D;up:active&#125;</span><br><span class="line"> osdmap e25: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v38: 128 pgs, 9 pools, 588 bytes data, 11 objects</span><br><span class="line">        6791 MB used, 16996 MB &#x2F; 25081 MB avail</span><br><span class="line">              80 active+clean</span><br><span class="line">              45 creating</span><br><span class="line">               3 creating+activating</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Docker中部署Ceph并没有想象中的那么顺利，社区的版本中仍然有Bug需要解决。</p><p>Docker作为一种快捷的部署方式，的确可以大幅度提高Ceph的部署效率，提高扩展的速度。但是从另一个角度我们应该注意到，随着Docker的引入也改变了Ceph的运维方式，比如在OSD增减的时候，需要到容器中对Ceph集群进行维护。再比如配置文件变更后的重启问题等。</p><p>但是无论如何，我相信这些问题都会得到完美的解决，用Docker部署Ceph作为一种新的尝试，值得推广。<br>之后还会为大家带来，如何使用Ansible结合Docker更快速的部署Ceph集群，敬请期待。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是根据Sébastien Han的&lt;a href=&quot;https://www.youtube.com/watch?v=FUSTjTBA8f8&amp;feature=youtu.be&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;演示视频&lt;/a&gt;进行整理的，对过程中有问题的部分进行了修复。&lt;/p&gt;
&lt;p&gt;Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Ceph/Docker/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Mitaka国内代码贡献</title>
    <link href="http://sunqi.me/2016/04/07/contribution-in-mitaka/"/>
    <id>http://sunqi.me/2016/04/07/contribution-in-mitaka/</id>
    <published>2016-04-07T07:19:39.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。</p><p>去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。</p><p>这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。</p><p>对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>在Big Tent公布之后，OpenStack的项目被分为Core Projects和Big Tent Projects。</p><img src="/images/blogs/contribution-in-mitaka-big-tent.jpg" class="center"><p>让我们来看一下在Mitaka版本中，多了哪些新项目。</p><ul><li>几个与Docker相关的项目被发布出来，magnum, senlin, solum</li><li>数据备份容灾的项目：freezer</li><li>计费的项目：cloudkitty</li><li>NFV相关的项目：tracker</li><li>监控相关的项目：monasca</li></ul><p>关于这些新项目的一些介绍，我将放在另外一篇博客里，敬请关注。</p><img src="/images/blogs/contribution-in-mitaka-projects.png" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司数量来看，Mitaka版本略有下降，但是参与的人数多了100多人。</p><img src="/images/blogs/contribution-in-mitaka-companies-contributors.png" class="center"><p>整个社区的公司贡献排名上没有明显的变化，传统的几大豪强仍然霸占公司排名的前十位，华为表现依然强劲，是中国区唯一能进入前十名的公司。</p><p>在模块方面，整体统计的绝大部分比例已经被others所占据，说明在Big Tent计划下，OpenStack正在朝更多元化的方向演进。在Mitaka排名前十位的项目中，fuel相关的两个项目都进入了前十，说明fuel在OpenStack部署的地位已经越来越重要了。同时，核心项目中的nova，neutron，cinder项目仍然在前十名的范围内，贡献量基本保持不变。值得一提的是，在Mitaka统计的项目数量已经从Liberty的708个增长到了829个，可见在短短的6个月内，OpenStack社区的蓬勃发展。</p><img src="/images/blogs/contribution-in-mitaka-companies-modules.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完了整体统计，我们再回到国内，因为已经有文章做了我在Liberty时候的分析，所以这里我换个角度来看国内的社区贡献，首先是统计排名的变化。</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在Liberty中，有13家国内企业为社区做了贡献，在Mitaka中这个数量增加到了15家企业，这里简单的将这些企业做了一下分类：</p><ul><li>互联网用户：乐视、新浪、网易</li><li>电信用户：中国移动</li><li>传统IT服务商：华为、中兴、华三</li><li>私有云服务商：Easystack、九州云、海云捷迅、北京有云、麒麟云、UMCloud、象云、Huron(休伦科技)</li></ul><img src="/images/blogs/contribution-in-mitaka-china-companies.png" class="center"><h3 id="行业分析"><a href="#行业分析" class="headerlink" title="行业分析"></a>行业分析</h3><p>通过行业的分析我们可以看出，国内的主要贡献仍然来自私有云服务商和传统IT服务商，换言之来自于以OpenStack提供产品或者服务的公司。厂商们贡献的目的很明确，主要为了展示自身在开源项目中的积累和专家形象。而用户的贡献主要来自平时在使用OpenStack时候遇到Bug，就是在实际应用过程中出现的问题。</p><img src="/images/blogs/contribution-in-mitaka-china-by-industry.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>单纯的社区贡献排名的比较仅仅是一个维度，下面我们来看一下各个公司的人员投入情况：</p><ul><li>排名前几位的公司对社区投入的人力基本都是两位数，相对于Liberty版本，人员均有所增加</li><li>在人均贡献投入上，99cloud是国内最高的，平均达到了59天，甚至超过了华为，这个统计不仅仅包含了代码贡献，还包含了邮件、Review、Blueprint的时间，基本可以衡量每个公司在OpenStack社区贡献方面的投入力量</li><li>人员投入来看，Easystack和中国移动无疑是最下本的两家，Easystack从Liberty的3人，增长到了23人，一下子增加了20人；中国移动也从最初的4个人，增加到了13个人，可见中国移动未来对OpenStack的野心</li></ul><img src="/images/blogs/contribution-in-mitaka-companies-effort.png" class="center"><h3 id="贡献模块分析"><a href="#贡献模块分析" class="headerlink" title="贡献模块分析"></a>贡献模块分析</h3><p>从模块的角度进行统计，国内企业的贡献情况并未出现一个统一的趋势，总体的贡献项目为193个，项目几乎涉及OpenStack所有最活跃的项目，从排名前十的项目来看：</p><ul><li>得益于华为的主导，dargonflow项目的贡献量超高</li><li>紧随其后的，也是当下的热点，容器相关的两个项目</li><li>几大OpenStack老模块贡献量也高居前十位，说明这些模块是在解决方案中使用频率较高的</li></ul><img src="/images/blogs/contribution-in-mitaka-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这是一个很敏感的话题，每个公司对社区的投入到底换来多少项目上的回报呢？可能这只有每个公司的CEO能够回答的问题了。我在这里就不多做过多的分析，留给大家充分讨论的空间吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>刚刚结束在南京的OpenStack开发培训，也了解到5G的通信网络上已经确定引入了OpenStack，虽然我说不清楚他的具体用途，但是我相信这对OpenStack这个项目、社区是一个重大的利好消息。我也相信，通过国内企业的集体努力，一定能让OpenStack在中国遍地开花结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。&lt;/p&gt;
&lt;p&gt;去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。&lt;/p&gt;
&lt;p&gt;这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。&lt;/p&gt;
&lt;p&gt;对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenStack培训的用户体验</title>
    <link href="http://sunqi.me/2016/03/27/openstack-training-user-experience/"/>
    <id>http://sunqi.me/2016/03/27/openstack-training-user-experience/</id>
    <published>2016-03-27T02:42:38.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？</p><p>这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。</p><a id="more"></a><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>简单来说，我带过OpenStack产品的研发团队，谈过OpenStack的合作，做过OpenStack培训讲师，也卖过OpenStack的私有云产品，也和大量的用户聊过OpenStack，所以还算是对OpenStack这个行业整体上有个清晰认识。</p><h2 id="OpenStack培训的目标群体"><a href="#OpenStack培训的目标群体" class="headerlink" title="OpenStack培训的目标群体"></a>OpenStack培训的目标群体</h2><p>我做过的OpenStack培训大体上分为两类，内训和外训。</p><p>内训是面向公司内部，因为我曾经带过的两个团队都是以开发OpenStack私有云产品为主的，所以我的培训对象主要是研发、运维、售前和销售人员。</p><p>外训的对象很多，包括知名的国企、外企和民营企业以及学校，行业大部分以传统行业为主，涉及通讯、金融、系统集成等，面向的群体主要是研发、IT和售前，培训的内容以OpenStack的基础和研发为主。</p><p>所以我把OpenStack培训的目标群体定义为：研发人员、系统工程师和运维人员、售前、销售人员、学生。</p><h2 id="针对不同的群体，到底需要哪些培训？"><a href="#针对不同的群体，到底需要哪些培训？" class="headerlink" title="针对不同的群体，到底需要哪些培训？"></a>针对不同的群体，到底需要哪些培训？</h2><h3 id="销售人员"><a href="#销售人员" class="headerlink" title="销售人员"></a>销售人员</h3><p>现在做OpenStack生意的无外乎两种：产品和服务。无论是哪一种，对传统的销售人员都是一种极大的挑战。云平台并不像传统软件一样，能够一眼看明白他到底是做什么的，解决了用户的哪些痛点。并且在企业中，能够做决策的人往往并不全是技术出身，所以想和他们解释清楚OpenStack到底能做什么，又是难上艰难。</p><p>所以对于销售人员来讲，培训的重点应该有以下几点：</p><ul><li>使用培训：我觉得无论为哪一类群体培训，演示如何使用OpenStack，都是最有效的帮助人理解的方式。但是这里的演示，必须要设定场景，即传统的业务形态下我们的业务系统是什么样子的，迁移到云平台后该如何部署，从这种比较中，加深对OpenStack的理解。销售人员通过对OpenStack操作，加深对OpenStack或OpenStack产品的理解。毕竟图形是最高效的一种记忆方式。</li><li>理解什么是开源软件：开源软件一定是未来的发展趋势，如果无法对开源软件有一个清晰的认识，也就无法理解清楚OpenStack这个项目出现的价值和意义。</li><li>了解OpenStack的发展历史、OpenStack基金会以及OpenStack社区的运营方式：学习这些的目的是为了给用户讲故事，让用户了解为什么要选择OpenStack，为什么OpenStack项目有持续的生命力，让用户相信使用了OpenStack能够保证未来的基础架构灵活面对业务层面敏捷性的需求。</li><li>案例学习：案例最大的价值就是教育用户，VMWare花了十几年的时间教育了用户，OpenStack不可能在短短的几年时间内就改变这样的局面，所以“学会用别人的案例来教育自己的用户”，是在销售人员OpenStack培训中非常重要的一课。</li></ul><h3 id="售前人员"><a href="#售前人员" class="headerlink" title="售前人员"></a>售前人员</h3><p>售前人员不但要从技术层面让用户信服产品，而且还要结合用户的业务系统需求提供建设方案，外企中的很多售前工程师还要承担搭建POC环境的职责。售前人员沟通的主要对象是企业中有实际需求的业务部门，也是最有可能落地的部门，沟通的成败决定了是否能签单，所以需要更多的专业知识来满足和用户的沟通需要。 培训的重点应该是：</p><ul><li>使用培训：理由同上，但是我觉得售前人员还需要站在用户的角度来思考一下，我的用户到底会如何使用云平台？业务系统迁移到平台后，会有哪些问题？</li><li>如何部署：部署培训向来是各大OpenStack培训必讲的内容，而且90%的内容都是围绕部署展开的，例如某知名企业的OpenStack授证培训。对于售前人员，我认为OpenStack部署训练还是很有必要的。一方面，能够帮助培训对象快速理解OpenStack的架构；另一方面，也能在未来的方案设计上提供参考和依据。由于云平台在使用上与企业传统的IT环境有较大的区别，所以售前人员在学习过程中，应该更多的了解OpenStack部署的特点，服务和服务之间的关系，云平台高可靠等和生产环境部署息息相关的问题。另外还要关注，用户的业务系统迁移到云平台后，可能带来的变化以及应对方式。例如：OpenStack里的网络分为fixed ip和floating ip，但是用户原有的业务系统只会有一个IP，这时候就需要考虑如何为用户选择适当的部署方案。</li><li>OpenStack架构：掌握OpenStack模块的基本工作原理和模块的详细作用。学习这些内容，是为了帮助售前人员在和用户后续交流中，帮助用户选择适当的模块解决用户的需求。</li><li>OpenStack的发展趋势：这部分内容就是能够引导客户未来的项目需求。例如在分布式存储，NFV和SDN方面。</li></ul><h3 id="系统工程师和运维人员"><a href="#系统工程师和运维人员" class="headerlink" title="系统工程师和运维人员"></a>系统工程师和运维人员</h3><p>Iaas云平台不但是对传统的企业IT架构进行了变革，也从管理上对企业原有的流程形成了冲击。需要培训的用户往往集中在自用OpenStack云平台的企业。</p><ul><li>使用培训，不同于上面两种简单的使用，运维人员要求对OpenStack管理部分的使用也要有很深的理解，而且还需要掌握命令行方式的相关操作。</li><li>OpenStack架构，了解OpenStack内部的工作原理，有助于快速定位问题，对系统进行维护。这部分包含的内容比较多，从OpenStack自身的原理到虚拟机，存储，再到虚拟网络的实现都需要有一个系统的了解才可以。</li><li>部署培训，要求详细掌握安装的过程，了解全部配置文件的功能及常用选项和参数。</li><li>自动化部署培训，手动部署即耗费时间又不能保证准确，所以作为运维人员，必须要掌握至少一种自动化部署的方法。这方面的方案有很多，从TripleO、Fuel到Puppet，Salt，Ansible。个人还是推崇应该选择Salt或者Ansible的一种进行学习和掌握。</li><li>运维培训，要求就是在云平台出现问题之后快速定位问题。</li><li>自动化运维培训，DevOps作为未来运维的趋势，反复被提到。云平台自动化运维的内容很多，部署、监控、告警、自动巡检、健康检查等等，使用的工具无外乎上面提到的Salt或者Ansible这样的工具。自动化运维不仅仅是云平台未来培训的一大趋势，也是企业有需求的培训内容。</li></ul><h3 id="开发人员"><a href="#开发人员" class="headerlink" title="开发人员"></a>开发人员</h3><p>开发人员对OpenStack培训的需求主要和未来的工作有关（除了是公司强制或者兴趣之外），从我的经验来看：一种是基于OpenStack API开发，一种是开发OpenStack。所以针对两种不同的需求，培训内容需要单独进行设计，总体来说后一种包含前一种培训。</p><p>与之前几种培训不同，我认为部署培训对开发人员并不是必须的，因为在实际工作中，开发人员很难有机会真正接触到安装过程，这部分工作往往由公司的IT人员去完成，并且其中涉及到大量的Linux基础命令，很多研发人员其实对这部分并不是十分熟悉，所以即使学习了安装内容，也还是一知半解。与其在安装上浪费时间，不如多了解一些架构方面的细节。</p><ul><li>使用培训，帮助开发人员快速了解OpenStack。</li><li>了解社区的开发流程，OpenStack之所以发展到今天的程度，和社区的代码的管理流程密不可分，所以这部分是值得每一名开发人员学习的。</li><li>搭建研发环境，既然要开发OpenStack就应该按照开发的方式搭建研发环境，这样屏蔽了很多安装上的细节，并且让开发人员有个快速能使用和开发的环境。</li><li>基于OpenStack API开发，这部分应该是个重点，我通常会设定一个具体的用户需求，通过解决用户需求来了解API的使用。例如：作为一名用户，我想给我的虚拟机挂卷并自动分区，挂载到/mnt目录。这里的内容包含API文档的使用，通过浏览器REST Client插件详细了解OpenStack API的调用过程，学习使用OpenStack SDK。</li><li>OpenStack编排服务，将API开发中的场景，用编排服务加以实现，还可以包含Scaling和Auto Scaling的场景。这部分很可能是开发人员在未来开发中非常需要的一部分内容。</li><li>OpenStack发展方向，OpenStack的大帐篷展现了对未来的野心，所以了解OpenStack未来的发展方向是很有必要的。</li></ul><p>针对于以后开发OpenStack的研发人员，还需要根据实际的开发内容增加以下的培训内容：</p><ul><li>OpenStack通用技术，学习OpenStack的通用技术有助于理解OpenStack的所有模块，这部分内容主要包括：Eventlet，REST和WSGI，Taskflow，OSLO项目等诸多重要的类库。</li><li>典型模块的架构及开发入门，这里面推荐的模块包含：Nova/Neutron/Horizon/Ceilometer，这几种模块几乎涵盖了OpenStack大部分模块的架构，所以重点理解这些模块的架构和工作原理，对于理解整个OpenStack项目都非常有帮助。直接将代码其实真的很困难，我习惯于使用场景的方式追踪代码的运行轨迹，从而整理出时序图的方式讲解。</li></ul><h3 id="学生"><a href="#学生" class="headerlink" title="学生"></a>学生</h3><p>学生群体事实上是相当有潜力的市场，现在国内OpenStack人才紧缺，所以OpenStack一定要从大学抓起。学生对OpenStack的学习不能仅仅停留在OpenStack本身，与之相关的内容都要学习，但是又不建议完全理论化的学习，强调动手的能力是关键。例如：对Python的学习，虚拟化软件的学习，OpenStack的安装，OpenStack的开发进行循序渐进的学习。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我认为培训中很重要的一环就是让学员动手，否则培训的效果不会很好。以上就是我对OpenStack培训的粗浅认识，还请各位多多指教。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？&lt;/p&gt;
&lt;p&gt;这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul主要使用场景</title>
    <link href="http://sunqi.me/2015/12/24/use-consul/"/>
    <id>http://sunqi.me/2015/12/24/use-consul/</id>
    <published>2015-12-24T02:07:24.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：<a href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/</a></p><p>这篇文章里主要介绍Consul的使用场景，服务和健康检查。</p><a id="more"></a><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>服务注册有点像OpenStack Keystone的Endpoints，可以通过API方式查询到所有服务的端点信息。</p><p>在Agent的节点上添加一个service，之后重启服务。</p><ul><li>添加一个服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80&#125;&#125;&#39; \</span><br><span class="line">    &gt;&#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><p>重新加载新的服务并不需要杀死进程重启服务，只需要给进程直接发送一个SIGHUP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:01:11 [INFO] agent: Synced service &#39;web&#39;</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>我们在任意节点上利用REST API查看服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;catalog&#x2F;service&#x2F;web</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;Address&quot;:&quot;200.21.1.101&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;,&quot;ServiceTags&quot;:[&quot;rails&quot;],&quot;ServiceAddress&quot;:&quot;&quot;,&quot;ServicePort&quot;:80&#125;]</span><br></pre></td></tr></table></figure><h2 id="Health-Check"><a href="#Health-Check" class="headerlink" title="Health Check"></a>Health Check</h2><p>健康检查的方法主要是通过运行一小段脚本的方式，根据运行的结果判断检查对象的健康状况。所以可以通过任意语言定义这个脚本，脚本运行将通过和consul执行的相同用户执行。</p><ul><li>添加一个健康检查</li></ul><p>每30秒ping google.com</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;check&quot;: &#123;&quot;name&quot;: &quot;ping&quot;,</span><br><span class="line">  &quot;script&quot;: &quot;ping -c1 google.com &gt;&#x2F;dev&#x2F;null&quot;, &quot;interval&quot;: &quot;30s&quot;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;ping.json</span><br></pre></td></tr></table></figure><p>为刚才的服务添加健康检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80,</span><br><span class="line">  &quot;check&quot;: &#123;&quot;script&quot;: &quot;curl localhost &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1&quot;, &quot;interval&quot;: &quot;10s&quot;&#125;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced service &#39;web&#39;</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced check &#39;ping&#39;</span><br></pre></td></tr></table></figure><p>经过一段时间后出现了critical和warning日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 12:43:58 [WARN] agent: Check &#39;service:web&#39; is now critical</span><br><span class="line">2015&#x2F;12&#x2F;24 12:44:08 [WARN] agent: Check &#39;ping&#39; is now warning</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>Health check的状态包含了很多种，有any, unkown, passing, warning, critical。any包含了所有状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;health&#x2F;state&#x2F;critical</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;CheckID&quot;:&quot;service:web&quot;,&quot;Name&quot;:&quot;Service &#39;web&#39; check&quot;,&quot;Status&quot;:&quot;critical&quot;,&quot;Notes&quot;:&quot;&quot;,&quot;Output&quot;:&quot;&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;&#125;]</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.consul.io/docs/agent/http/catalog.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/catalog.html</a></li><li><a href="http://www.consul.io/docs/agent/http/health.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/health.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章里主要介绍Consul的使用场景，服务和健康检查。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul的安装方法</title>
    <link href="http://sunqi.me/2015/12/06/consul-installation/"/>
    <id>http://sunqi.me/2015/12/06/consul-installation/</id>
    <published>2015-12-06T18:00:13.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Consul"><a href="#什么是Consul" class="headerlink" title="什么是Consul?"></a>什么是Consul?</h2><p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p><a id="more"></a><h2 id="安装Consul"><a href="#安装Consul" class="headerlink" title="安装Consul"></a>安装Consul</h2><p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html" target="_blank" rel="noopener">https://www.consul.io/downloads.html</a></p><p>我们把consul直接放在/usr/local/bin目录中。</p><h2 id="Consul-Server"><a href="#Consul-Server" class="headerlink" title="Consul Server"></a>Consul Server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -server -bootstrap-expect 3 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;server1 -bind&#x3D;10.10.10.10</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>-server - Serve模式</li><li>-bootstrap-expect - Server数量</li><li>-data-dir - 数据目录</li><li>-node - Node名称</li><li>-bind - 集群通讯地址</li></ul><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;server1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: true (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader</span><br></pre></td></tr></table></figure><h3 id="查看成员"><a href="#查看成员" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="Consul-Agent"><a href="#Consul-Agent" class="headerlink" title="Consul Agent"></a>Consul Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;agent1 -bind&#x3D;10.10.10.100 -config-dir &#x2F;etc&#x2F;consul.d</span><br></pre></td></tr></table></figure><ul><li>输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;agent1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: false (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:41 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br></pre></td></tr></table></figure><ul><li>server日志输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br></pre></td></tr></table></figure><h3 id="查看成员-1"><a href="#查看成员-1" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.consul.io/intro/getting-started/install.html" target="_blank" rel="noopener">https://www.consul.io/intro/getting-started/install.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是Consul&quot;&gt;&lt;a href=&quot;#什么是Consul&quot; class=&quot;headerlink&quot; title=&quot;什么是Consul?&quot;&gt;&lt;/a&gt;什么是Consul?&lt;/h2&gt;&lt;p&gt;Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Grafana+Diamond+Graphite构造完美监控面板</title>
    <link href="http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/"/>
    <id>http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/</id>
    <published>2015-11-30T15:59:46.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。</p><p>本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：</p><img src="/images/blogs/grafana-screenshot.png" class="center" title="800x600"><a id="more"></a><h2 id="安装及配置Dimaond"><a href="#安装及配置Dimaond" class="headerlink" title="安装及配置Dimaond"></a>安装及配置Dimaond</h2><p>安装Diamond最直接和简单的方法就是自己编译RPM或者DEB的安装包, Diamond在这方面提供了比较好的支持。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /root</span></span><br><span class="line"><span class="comment"># yum install -y git rpm-build python-configobj python-setuptools</span></span><br><span class="line"><span class="comment"># git clone https://github.com/python-diamond/Diamond</span></span><br><span class="line"><span class="comment"># cd Diamond</span></span><br><span class="line"><span class="comment"># make rpm</span></span><br><span class="line"><span class="comment"># cd dist</span></span><br><span class="line"><span class="comment"># rpm -ivh diamond-*.noarch.rpm</span></span><br></pre></td></tr></table></figure><p>默认情况下，Diamond开启了基本的监控信息，包括CPU、内存、磁盘的性能数据。当然，我们可以通过配置启动相应的监控项，也能通过自定义的方式进行相应的扩展。这里，我们在/etc/diamond/collectors加载额外的插件，下面的例子中开启了网络的监控。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp -f /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/diamond/diamond.conf</span></span><br><span class="line">[configs]</span><br><span class="line">path = <span class="string">"/etc/diamond/collectors/"</span></span><br><span class="line">extension = <span class="string">".conf"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/net.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[NetworkCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>那么到目前为止，Diamond的基本安装和配置已经完成，但是现在只是简单的采集数据，并没有指明数据要发送给谁，所以下一步我们来开始配置Graphite。</p><h2 id="安装及配置Graphite"><a href="#安装及配置Graphite" class="headerlink" title="安装及配置Graphite"></a>安装及配置Graphite</h2><p>Graphite主要做两件事情：按照时间存储数据、生成图表，在我们的场景里面，实质上就是把Graphite作为数据源给Grafana提供数据。另外还需要安装的是carbon，负责通过网络接受数据并保存到后端存储中；另外还需要whisper，负责生成Graphite样式的基于文件的时间序列的数据库。</p><h3 id="安装软件包"><a href="#安装软件包" class="headerlink" title="安装软件包"></a>安装软件包</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y graphite-web graphite-web-selinux</span></span><br><span class="line"><span class="comment"># yum install -y mysql mysql-server MySQL-python</span></span><br><span class="line"><span class="comment"># yum install -y python-carbon python-whisper</span></span><br></pre></td></tr></table></figure><h3 id="配置MySQL"><a href="#配置MySQL" class="headerlink" title="配置MySQL"></a>配置MySQL</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/init.d/mysqld start</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mysql -e "CREATE DATABASE graphite;" -u root</span></span><br><span class="line"><span class="comment"># mysql -e "GRANT ALL PRIVILEGES ON graphite.* TO 'graphite'@'localhost' IDENTIFIED BY 'sysadmin';" -u root</span></span><br><span class="line"><span class="comment"># mysql -e 'FLUSH PRIVILEGES;' -u root</span></span><br></pre></td></tr></table></figure><h3 id="配置Graphite"><a href="#配置Graphite" class="headerlink" title="配置Graphite"></a>配置Graphite</h3><ul><li>local setting</li></ul><figure class="highlight bash"><figcaption><span>/etc/graphite-web/local_settings.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SECRET_KEY=$(md5sum /etc/passwd | awk &#123;'print $1'&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "SECRET_KEY = '$SECRET_KEY'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"><span class="comment"># echo "TIME_ZONE = 'Asia/Shanghai'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    <span class="string">'default'</span>: &#123;</span><br><span class="line">        <span class="string">'NAME'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'ENGINE'</span>: <span class="string">'django.db.backends.mysql'</span>,</span><br><span class="line">        <span class="string">'USER'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'PASSWORD'</span>: <span class="string">'sysadmin'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cd /usr/lib/python2.6/site-packages/graphite</span></span><br><span class="line"><span class="comment"># ./manage.py syncdb --noinput</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@hihuron.com', 'sysadmin')" | ./manage.py shell</span></span><br></pre></td></tr></table></figure><ul><li>Apache配置</li></ul><figure class="highlight bash"><figcaption><span>/etc/httpd/conf.d/graphite-web.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Listen 0.0.0.0:10000</span><br><span class="line">&lt;VirtualHost *:10000&gt;</span><br><span class="line">    ServerName graphite-web</span><br><span class="line">    DocumentRoot <span class="string">"/usr/share/graphite/webapp"</span></span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/httpd/graphite-web-error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/httpd/graphite-web-access.log common</span><br><span class="line">    Alias /media/ <span class="string">"/usr/lib/python2.6/site-packages/django/contrib/admin/media/"</span></span><br><span class="line"></span><br><span class="line">    WSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi</span><br><span class="line">    WSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%&#123;GLOBAL&#125; application-group=%&#123;GLOBAL&#125;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/content/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/media/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line">&lt;/VirtualHost&gt;</span><br></pre></td></tr></table></figure><ul><li>Diamond配置</li></ul><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HOST_IP=$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.)&#123;3&#125;[0-9]*).*/\2/p' | head -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphiteHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphitePickleHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service carbon-cache restart</span></span><br><span class="line"><span class="comment"># service httpd restart</span></span><br><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure><h2 id="安装和配置Grafana"><a href="#安装和配置Grafana" class="headerlink" title="安装和配置Grafana"></a>安装和配置Grafana</h2><p>Grafana最主要的功能就是对数据的呈现，基于一切可提供time series的后台服务。这里面我们使用Graphite为Grafana提供数据。</p><h3 id="安装及配置"><a href="#安装及配置" class="headerlink" title="安装及配置"></a>安装及配置</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y nodejs</span></span><br><span class="line"><span class="comment"># rpm -ivh https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0-1.x86_64.rpm</span></span><br><span class="line"><span class="comment"># sudo /sbin/chkconfig --add grafana-server</span></span><br><span class="line"><span class="comment"># sed -i 's/^;http_port = 3000$/http_port = 10001/g' /etc/grafana/grafana.ini</span></span><br><span class="line"><span class="comment"># sudo service grafana-server start</span></span><br></pre></td></tr></table></figure><h3 id="添加datasource"><a href="#添加datasource" class="headerlink" title="添加datasource"></a>添加datasource</h3><p>Grafana提供了非常丰富的REST API，我们不仅可以直接利用Grafana作为数据呈现层，还可以利用REST API直接将Grafana的Graph集成在我们的应用中。下面我们利用REST API为Grafana添加datasource。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -i 'http://admin:admin@localhost:10001/api/datasources' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -d '&#123;"name": "graphite", "type": "graphite", "url": "http://localhost:10000", "access": "proxy", "basicAuth": false&#125;'</span></span><br></pre></td></tr></table></figure><h2 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h2><h3 id="修改ceph脚本兼容性"><a href="#修改ceph脚本兼容性" class="headerlink" title="修改ceph脚本兼容性"></a>修改ceph脚本兼容性</h3><p>Diamond是基于Python开发的，但是由于CentOS 6.5的Python版本较低(2.6)，所以直接使用社区版本的Ceph监控时，会导致错误。可以通过简单的修改进行修复。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_stats_from_socket</span><span class="params">(self, name)</span>:</span></span><br><span class="line">    <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">    dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">    an empty result set is returned.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">        <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">        <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">        <span class="comment">#     name,</span></span><br><span class="line">        <span class="comment">#     'perf',</span></span><br><span class="line">        <span class="comment">#     'dump',</span></span><br><span class="line">        <span class="comment">#     ])</span></span><br><span class="line">        cmd = [</span><br><span class="line">             self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">             <span class="string">'--admin-daemon'</span>,</span><br><span class="line">             name,</span><br><span class="line">             <span class="string">'perf'</span>,</span><br><span class="line">             <span class="string">'dump'</span>,</span><br><span class="line">        ]</span><br><span class="line">        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">        json_blob = process.communicate()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="增加对ceph-osd-perf监控"><a href="#增加对ceph-osd-perf监控" class="headerlink" title="增加对ceph osd perf监控"></a>增加对ceph osd perf监控</h3><p>在实际运维Ceph过程中，ceph osd perf是一个非常重要的指令，能够观察出集群中磁盘的latency的信息，通过观察变化，可以辅助判断磁盘出现性能问题。Diamond的设计中，每个Diamond Agent只会采集自己本机的指标，所以我们在添加的时候，只需要在一个节点上增加这个监控就可以了。在ceph.py中结尾处新增加一个类。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CephOsdCollector</span><span class="params">(CephCollector)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_stats</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">        dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">        an empty result set is returned.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">            <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">            <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">            <span class="comment">#     name,</span></span><br><span class="line">            <span class="comment">#     'perf',</span></span><br><span class="line">            <span class="comment">#     'dump',</span></span><br><span class="line">            <span class="comment">#     ])</span></span><br><span class="line">            cmd = [</span><br><span class="line">                 self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">                 <span class="string">'osd'</span>,</span><br><span class="line">                 <span class="string">'perf'</span>,</span><br><span class="line">                 <span class="string">'--format=json'</span>,</span><br><span class="line">            ]</span><br><span class="line">            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">            json_blob = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">except</span> subprocess.CalledProcessError, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not get stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not get stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            json_data = json.loads(json_blob)</span><br><span class="line">        <span class="keyword">except</span> Exception, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not parse stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not parse stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> json_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_publish_stats</span><span class="params">(self, stats)</span>:</span></span><br><span class="line">        <span class="string">"""Given a stats dictionary from _get_stats_from_socket,</span></span><br><span class="line"><span class="string">        publish the individual values.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> perf <span class="keyword">in</span> stats[<span class="string">'osd_perf_infos'</span>]:</span><br><span class="line">            counter_prefix = <span class="string">'osd.'</span> + str(perf[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">for</span> stat_name, stat_value <span class="keyword">in</span> flatten_dictionary(</span><br><span class="line">                perf[<span class="string">'perf_stats'</span>],</span><br><span class="line">                prefix=counter_prefix,</span><br><span class="line">            ):</span><br><span class="line">              self.log.info(<span class="string">'stat_name is %s'</span>, stat_name)</span><br><span class="line">              self.log.info(<span class="string">'stat_value is %s'</span>, stat_value)</span><br><span class="line">              self.publish_gauge(stat_name, stat_value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Collect stats</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.log.info(<span class="string">'in ceph osd collector'</span>)</span><br><span class="line">        stats = self._get_stats()</span><br><span class="line">        self._publish_stats(stats)</span><br></pre></td></tr></table></figure><h3 id="修改Diamond监控配置"><a href="#修改Diamond监控配置" class="headerlink" title="修改Diamond监控配置"></a>修改Diamond监控配置</h3><figure class="highlight bash"><figcaption><span>/etc/diamond/collectors/ceph.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/ceph.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[CephCollector]]</span><br><span class="line">enabled = True</span><br><span class="line"></span><br><span class="line">[[CephOsdCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。&lt;/p&gt;
&lt;p&gt;本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：&lt;/p&gt;
&lt;img src=&quot;/images/blogs/grafana-screenshot.png&quot; class=&quot;center&quot; title=&quot;800x600&quot;&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Liberty国内代码贡献</title>
    <link href="http://sunqi.me/2015/10/29/contribution-in-liberty/"/>
    <id>http://sunqi.me/2015/10/29/contribution-in-liberty/</id>
    <published>2015-10-29T02:56:06.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p><p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p><p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p><a id="more"></a><h2 id="社区贡献分析"><a href="#社区贡献分析" class="headerlink" title="社区贡献分析"></a>社区贡献分析</h2><p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p><p>我们仍然要先看一下模块的贡献情况：</p><img src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" class="left" title="400x300"><p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p><p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p><p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p><h2 id="国内社区贡献分析"><a href="#国内社区贡献分析" class="headerlink" title="国内社区贡献分析"></a>国内社区贡献分析</h2><img src="/images/blogs/contribution-in-liberty-contributor.png" class="center" title="400x300"><p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p><p><strong><em>华为社区贡献统计</em></strong></p><img src="/images/blogs/contribution-in-liberty-huawei.png" class="center" title="800x600"><p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p><p><strong>* 99cloud社区贡献统计*</strong></p><img src="/images/blogs/contribution-in-liberty-99cloud.png" class="center" title="800x600"><p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。<br>更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p><p><strong>* UnitedStack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-unitedstack.png" class="center" title="800x600"><p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p><p><strong>* Kylin Cloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-kylincloud.png" class="center" title="800x600"><p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p><p><strong>* Easystack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-easystack.png" class="center" title="800x600"><p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p><p><strong>* Awcloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-awcloud.png" class="center" title="800x600"><p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于”Neutron &amp; OpenStack漫谈”，非常值得一读。</p><p><strong>* LeTV社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-letv.png" class="center" title="800x600"><p><strong>* Netease社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-netease.png" class="center" title="800x600"><p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p><p><strong>* Huron社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-huron.png" class="center" title="800x600"><p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p><p><strong>* China Mobile社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-chinamobile.png" class="center" title="800x600"><p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p><p><strong>* Lenovo社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-lenovo.png" class="center" title="800x600"><p>排名第135位的是联想。不评论了。</p><p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p><p><strong>* H3C社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-h3c.png" class="center" title="800x600"><p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p><p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p><h2 id="社区贡献内容分析"><a href="#社区贡献内容分析" class="headerlink" title="社区贡献内容分析"></a>社区贡献内容分析</h2><img src="/images/blogs/contribution-in-liberty-complete-blueprints.png" class="center" title="800x600"><p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p><p><strong>* Huawei社区贡献——完成Blueprint *</strong></p><img src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" class="center" title="800x600"><p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p><p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p><p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p><p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty完整版本的翻译可见：&lt;a href=&quot;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 14.04 Server开发者安装指南</title>
    <link href="http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/"/>
    <id>http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/</id>
    <published>2015-09-08T21:50:24.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么会写这篇Blog"><a href="#为什么会写这篇Blog" class="headerlink" title="为什么会写这篇Blog"></a>为什么会写这篇Blog</h2><p>近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。</p><a id="more"></a><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>这篇Blog没有提及的地方：</p><ul><li>网络，需要根据实际情况进行配置，我这里面使用的是DHCP自动获取，所以没有相关步骤</li><li>分区，这里面使用的是默认配置，但是生产环境的配置一般需要手动划分</li></ul><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><ul><li>一定要选择English，否则处理中文的时候太麻烦<img src="/images/blogs/install-ubuntu/1.png" class="center"></li><li>正式开始进入安装<img src="/images/blogs/install-ubuntu/2.png" class="center"></li><li>与上面的原则一致，一定要选择English<img src="/images/blogs/install-ubuntu/3.png" class="center"></li><li>Location一定要选择中国，否则默认不会使用中文的Ubuntu源，影响安装速度，这一步很多初学者不会在意<img src="/images/blogs/install-ubuntu/4.png" class="center"><img src="/images/blogs/install-ubuntu/5.png" class="center"><img src="/images/blogs/install-ubuntu/6.png" class="center"></li><li>这里面主要是字符集的问题，选择United States<img src="/images/blogs/install-ubuntu/7.png" class="center"></li><li>不需要检查键盘布局<img src="/images/blogs/install-ubuntu/8.png" class="center"></li><li>默认使用English布局就好了<img src="/images/blogs/install-ubuntu/9.png" class="center"></li><li>主机名设置，就是hostname<img src="/images/blogs/install-ubuntu/10.png" class="center"></li><li>用户设置，建议建立一个普通用户<img src="/images/blogs/install-ubuntu/11.png" class="center"><img src="/images/blogs/install-ubuntu/12.png" class="center"><img src="/images/blogs/install-ubuntu/13.png" class="center"><img src="/images/blogs/install-ubuntu/15.png" class="center"><img src="/images/blogs/install-ubuntu/16.png" class="center"></li><li>不加密Home目录<img src="/images/blogs/install-ubuntu/17.png" class="center"></li><li>设置时区，这一步也很重要，默认情况下会自动检测到，但是如果不对，一定要修改一下，否则你的系统时间与你实际不一致，你程序里的时间跟着不对，跟调试增加难度<img src="/images/blogs/install-ubuntu/18.png" class="center"></li><li>这里面分区用默认的就好啦，当然如果你知道该如何分区，可以采用Manual方式<img src="/images/blogs/install-ubuntu/19.png" class="center"><img src="/images/blogs/install-ubuntu/20.png" class="center"><img src="/images/blogs/install-ubuntu/21.png" class="center"><img src="/images/blogs/install-ubuntu/22.png" class="center"><img src="/images/blogs/install-ubuntu/23.png" class="center"></li><li>如果访问网络需要使用代理，可以设置一下<img src="/images/blogs/install-ubuntu/24.png" class="center"></li><li>不选择自动更新<img src="/images/blogs/install-ubuntu/25.png" class="center"></li><li>默认只需要选择SSH服务，保证我们在安装后能够SSH登陆服务器即可<img src="/images/blogs/install-ubuntu/26.png" class="center"></li><li>安装grub<img src="/images/blogs/install-ubuntu/27.png" class="center"></li><li>重启完成安装<img src="/images/blogs/install-ubuntu/28.png" class="center"></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>谨记此篇Blog送给我的小徒弟周小球小朋友，希望你能利用利用最后的一年的时间努力学习，找到称心如意的工作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么会写这篇Blog&quot;&gt;&lt;a href=&quot;#为什么会写这篇Blog&quot; class=&quot;headerlink&quot; title=&quot;为什么会写这篇Blog&quot;&gt;&lt;/a&gt;为什么会写这篇Blog&lt;/h2&gt;&lt;p&gt;近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ubuntu" scheme="http://sunqi.me/categories/openstack/ubuntu/"/>
    
    
  </entry>
  
  <entry>
    <title>(Kilo)Devstack完全用户手册</title>
    <link href="http://sunqi.me/2015/09/03/devstack-guide/"/>
    <id>http://sunqi.me/2015/09/03/devstack-guide/</id>
    <published>2015-09-03T02:34:20.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p><p>本篇Blog主要介绍以下几个实用场景：</p><ul><li>如何利用Devstack构建一套完美的开发环境</li><li>提高Devstack安装成功率的方法</li><li>Devstack的实用技巧</li><li>各种场景下的配置和注意事项</li></ul><p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p><a id="more"></a><h2 id="运行环境的选择"><a href="#运行环境的选择" class="headerlink" title="运行环境的选择"></a>运行环境的选择</h2><p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p><ul><li>VMWare Workstation是收费软件，Virtualbox是免费软件</li><li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li><li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li><li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li></ul><h2 id="Virtualbox网络设置"><a href="#Virtualbox网络设置" class="headerlink" title="Virtualbox网络设置"></a>Virtualbox网络设置</h2><img src="/images/blogs/devstack-guide-network-topology.jpg" class="center"><ul><li>Nova Network网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 172.16.0.101</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure><ul><li>Neutron网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth2 inet manual</span><br><span class="line">up ip link set dev $IFACE up</span><br><span class="line">down ip link set dev $IFACE down</span><br></pre></td></tr></table></figure><ul><li>MAC网卡NAT映射</li></ul><p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl net.inet.ip.forwarding&#x3D;1</span><br></pre></td></tr></table></figure><p>在nat-anchor后面添加</p><figure class="highlight plain"><figcaption><span>/etc/pf.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nat on en0 from 172.16.0.0&#x2F;24 -&gt; (en0)</span><br></pre></td></tr></table></figure><p>之后加载</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pfctl -e -f /etc/pf.conf</span><br></pre></td></tr></table></figure><ul><li>Linux网卡NAT映射</li></ul><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward</span><br><span class="line">iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><h2 id="Devstack快速开始"><a href="#Devstack快速开始" class="headerlink" title="Devstack快速开始"></a>Devstack快速开始</h2><p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p><ul><li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li><li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li><li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li></ul><p>下面给出一个最简安装步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># adduser stack</span><br><span class="line"># apt-get install sudo -y</span><br><span class="line"># echo &quot;stack ALL&#x3D;(ALL) NOPASSWD: ALL&quot; &gt;&gt; &#x2F;etc&#x2F;sudoers</span><br><span class="line"># sudo su - stack</span><br><span class="line"></span><br><span class="line">(stack)$ git clone https:&#x2F;&#x2F;git.openstack.org&#x2F;openstack-dev&#x2F;devstack --branch&#x3D;stable&#x2F;kilo</span><br><span class="line">(stack)$ cd devstack &amp;&amp; .&#x2F;stack.sh</span><br></pre></td></tr></table></figure><h2 id="提高Devstack安装成功率"><a href="#提高Devstack安装成功率" class="headerlink" title="提高Devstack安装成功率"></a>提高Devstack安装成功率</h2><p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p><img src="/images/blogs/devstack-guide-flow.png" class="center"><p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p><ul><li>安装依赖时，需要访问Ubuntu的源</li><li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">https://bootstrap.pypa.io/get-pip.py</a></li><li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li><li>安装过程中执行pip install requirements，需要访问pip repo</li><li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li></ul><hr><p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p><ul><li>使用国内源</li></ul><figure class="highlight plain"><figcaption><span>/etc/apt/sources.list</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><ul><li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li></ul><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FILES=<span class="variable">$TOP_DIR</span>/files</span><br><span class="line"></span><br><span class="line">PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py</span><br><span class="line">LOCAL_PIP=<span class="string">"<span class="variable">$FILES</span>/<span class="variable">$(basename $PIP_GET_PIP_URL)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> install_get_pip &#123;</span><br><span class="line">    <span class="comment"># The OpenStack gate and others put a cached version of get-pip.py</span></span><br><span class="line">    <span class="comment"># for this to find, explicitly to avoid download issues.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># However, if DevStack *did* download the file, we want to check</span></span><br><span class="line">    <span class="comment"># for updates; people can leave their stacks around for a long</span></span><br><span class="line">    <span class="comment"># time and in the mean-time pip might get upgraded.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Thus we use curl's "-z" feature to always check the modified</span></span><br><span class="line">    <span class="comment"># since and only download if a new version is out -- but only if</span></span><br><span class="line">    <span class="comment"># it seems we downloaded the file originally.</span></span><br><span class="line">    <span class="keyword">if</span> [[ ! -r <span class="variable">$LOCAL_PIP</span> || -r <span class="variable">$LOCAL_PIP</span>.downloaded ]]; <span class="keyword">then</span></span><br><span class="line">        curl --retry 6 --retry-delay 5 \</span><br><span class="line">            -z <span class="variable">$LOCAL_PIP</span> -o <span class="variable">$LOCAL_PIP</span> <span class="variable">$PIP_GET_PIP_URL</span> || \</span><br><span class="line">            die <span class="variable">$LINENO</span> <span class="string">"Download of get-pip.py failed"</span></span><br><span class="line">        touch <span class="variable">$LOCAL_PIP</span>.downloaded</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    sudo -H -E python <span class="variable">$LOCAL_PIP</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改为我在coding.net上缓存的get-pip脚本</p><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py</span><br></pre></td></tr></table></figure><ul><li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li><li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li><li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li></ul><h2 id="无网络状况下安装Devstack"><a href="#无网络状况下安装Devstack" class="headerlink" title="无网络状况下安装Devstack"></a>无网络状况下安装Devstack</h2><p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p><ul><li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li><li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li><li>建立本地的Gerrit，并且上传所有代码</li><li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li><li>将镜像下载到刚刚创建的Apache服务器</li></ul><p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p><h2 id="OFFLINE模式下安装Devstack"><a href="#OFFLINE模式下安装Devstack" class="headerlink" title="OFFLINE模式下安装Devstack"></a>OFFLINE模式下安装Devstack</h2><p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OFFLINE=True</span><br></pre></td></tr></table></figure><h2 id="虚拟机重启后，如何利用rejoin-stack-sh，免重新安装"><a href="#虚拟机重启后，如何利用rejoin-stack-sh，免重新安装" class="headerlink" title="虚拟机重启后，如何利用rejoin-stack.sh，免重新安装"></a>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2><p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p><figure class="highlight bash"><figcaption><span>/etc/init.d/cinder-setup-backing-file</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file</span><br><span class="line">losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file</span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as root</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /etc/init.d/cinder-setup-backing-file</span><br><span class="line">ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as normal user</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/devstack</span><br><span class="line">./rejoin-stack.sh</span><br></pre></td></tr></table></figure><h2 id="Scenario-0-公共部分"><a href="#Scenario-0-公共部分" class="headerlink" title="Scenario 0: 公共部分"></a>Scenario 0: 公共部分</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Misc</span></span><br><span class="line">ADMIN_PASSWORD=sysadmin</span><br><span class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_TOKEN=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target Path</span></span><br><span class="line">DEST=/opt/stack.kilo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable Logging</span></span><br><span class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</span><br><span class="line">VERBOSE=True</span><br><span class="line">LOG_COLOR=True</span><br><span class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs</span><br></pre></td></tr></table></figure><h2 id="Scenario-1-单节点Nova-Network的安装"><a href="#Scenario-1-单节点Nova-Network的安装" class="headerlink" title="Scenario 1: 单节点Nova-Network的安装"></a>Scenario 1: 单节点Nova-Network的安装</h2><p>这应该就是Devstack默认的模式，有以下几点需要注意：</p><ul><li>根据上面的网卡配置</li></ul><blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p><p>第二块为Host-only Adaptor，用于访问云平台</p><p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p><p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote><ul><li>打开novnc和consoleauth，否则无法访问VNC</li></ul><p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># eth1 address</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=172.24.17.0/24</span><br><span class="line">FIXED_NETWORK_SIZE=254</span><br><span class="line">FLOATING_RANGE=172.16.0.128/25</span><br></pre></td></tr></table></figure><h2 id="Scenario-2-双节点Nova-Network的安装"><a href="#Scenario-2-双节点Nova-Network的安装" class="headerlink" title="Scenario 2: 双节点Nova-Network的安装"></a>Scenario 2: 双节点Nova-Network的安装</h2><ul><li>控制节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line">MULTI_HOST=1</span><br></pre></td></tr></table></figure><ul><li>计算节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># needed by cinder-volume service</span></span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># controller ip</span></span><br><span class="line">SERVICE_HOST=192.168.56.101</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br></pre></td></tr></table></figure><h2 id="Scenario-3-单节点Neutron的安装"><a href="#Scenario-3-单节点Neutron的安装" class="headerlink" title="Scenario 3: 单节点Neutron的安装"></a>Scenario 3: 单节点Neutron的安装</h2><ul><li>基本配置</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron</span></span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line">ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas</span><br><span class="line"></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=20.0.0.0/24</span><br><span class="line">NETWORK_GATEWAY=20.0.0.1</span><br><span class="line">FLOATING_RANGE=172.16.0.0/24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY=172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span><br></pre></td></tr></table></figure><ul><li>OVS设置</li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-4-多节点Neutron的安装-控制-网络-计算节点"><a href="#Scenario-4-多节点Neutron的安装-控制-网络-计算节点" class="headerlink" title="Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)"></a>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2><ul><li><p>控制/网络节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">HOST_IP&#x3D;192.168.56.101</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"># Neutron</span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta</span><br><span class="line">FIXED_RANGE&#x3D;20.0.0.0&#x2F;24</span><br><span class="line">NETWORK_GATEWAY&#x3D;20.0.0.1</span><br><span class="line">FLOATING_RANGE&#x3D;172.16.0.0&#x2F;24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;172.16.0.101,end&#x3D;172.16.0.200</span><br></pre></td></tr></table></figure></li><li><p>计算节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">disable_all_services</span><br><span class="line">ENABLED_SERVICES&#x3D;n-cpu,rabbit,neutron,q-agt,c-vol</span><br><span class="line"></span><br><span class="line"># current host ip</span><br><span class="line">HOST_IP&#x3D;192.168.56.103</span><br><span class="line"># needed by cinder-volume service</span><br><span class="line">DATABASE_TYPE&#x3D;mysql</span><br><span class="line"></span><br><span class="line"># controller ip</span><br><span class="line">SERVICE_HOST&#x3D;192.168.56.101</span><br><span class="line">MYSQL_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">RABBIT_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">GLANCE_HOSTPORT&#x3D;$SERVICE_HOST:9292</span><br><span class="line">NOVA_VNC_ENABLED&#x3D;True</span><br><span class="line">NOVNCPROXY_URL&#x3D;&quot;http:&#x2F;&#x2F;$SERVICE_HOST:6080&#x2F;vnc_auto.html&quot;</span><br><span class="line">VNCSERVER_LISTEN&#x3D;$HOST_IP</span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS&#x3D;$VNCSERVER_LISTEN</span><br><span class="line">Q_HOST&#x3D;$SERVICE_HOST</span><br></pre></td></tr></table></figure></li><li><p>OVS设置</p></li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-5-从源代码安装客户端"><a href="#Scenario-5-从源代码安装客户端" class="headerlink" title="Scenario 5: 从源代码安装客户端"></a>Scenario 5: 从源代码安装客户端</h2><p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEYSTONECLIENT_BRANCH=1.3.1</span><br><span class="line">CINDERCLIENT_BRANCH=1.1.1</span><br><span class="line">GLANCECLIENT_BRANCH=0.17.1</span><br><span class="line">HEATCLIENT_BRANCH=0.4.0</span><br><span class="line">NEUTRONCLIENT_BRANCH=2.4.0</span><br><span class="line">NOVACLIENT_BRANCH=2.23.0</span><br><span class="line">SWIFTCLIENT_BRANCH=2.4.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># client code</span></span><br><span class="line">LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient</span><br></pre></td></tr></table></figure><h2 id="Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift"><a href="#Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift" class="headerlink" title="Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift"></a>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceilometer</span></span><br><span class="line">enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api</span><br><span class="line">enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Heat</span></span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trove</span></span><br><span class="line">enable_service trove tr-api tr-tmgr tr-cond</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sahara</span></span><br><span class="line">enable_service sahara</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swift</span></span><br><span class="line">enable_service s-proxy s-object s-container s-account</span><br><span class="line">SWIFT_REPLICAS=1</span><br><span class="line">SWIFT_HASH=011688b44136573e209e</span><br></pre></td></tr></table></figure><h2 id="Scenario-7-安装Ceph"><a href="#Scenario-7-安装Ceph" class="headerlink" title="Scenario 7: 安装Ceph"></a>Scenario 7: 安装Ceph</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceph</span></span><br><span class="line">ENABLED_SERVICES+=,ceph</span><br><span class="line">CEPH_LOOPBACK_DISK_SIZE=200G</span><br><span class="line">CEPH_CONF=/etc/ceph/ceph.conf</span><br><span class="line">CEPH_REPLICAS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Glance - Image Service</span></span><br><span class="line">GLANCE_CEPH_USER=glance</span><br><span class="line">GLANCE_CEPH_POOL=glance-pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cinder - Block Device Service</span></span><br><span class="line">CINDER_DRIVER=ceph</span><br><span class="line">CINDER_CEPH_USER=cinder</span><br><span class="line">CINDER_CEPH_POOL=cinder-pool</span><br><span class="line">CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73</span><br><span class="line">CINDER_BAK_CEPH_POOL=cinder-backups</span><br><span class="line">CINDER_BAK_CEPH_USER=cinder-backups</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nova - Compute Service</span></span><br><span class="line">NOVA_CEPH_POOL=nova-pool</span><br></pre></td></tr></table></figure><h2 id="Scenario-8-安装Murano"><a href="#Scenario-8-安装Murano" class="headerlink" title="Scenario 8: 安装Murano"></a>Scenario 8: 安装Murano</h2><p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/stack.kilo</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/murano --branch=stable/kilo</span><br><span class="line"><span class="built_in">cd</span> murano/contrib/devstack</span><br><span class="line">cp lib/murano <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp lib/murano-dashboard <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp extras.d/70-murano.sh <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/extras.d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Enable Neutron</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line"></span><br><span class="line"># Enable Heat</span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"># Enable Murano</span><br><span class="line">enable_service murano murano-api murano-engine</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。&lt;/p&gt;
&lt;p&gt;本篇Blog主要介绍以下几个实用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何利用Devstack构建一套完美的开发环境&lt;/li&gt;
&lt;li&gt;提高Devstack安装成功率的方法&lt;/li&gt;
&lt;li&gt;Devstack的实用技巧&lt;/li&gt;
&lt;li&gt;各种场景下的配置和注意事项&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Devstack" scheme="http://sunqi.me/categories/OpenStack/Devstack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Devstack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>为什么叫Monkey Patch？</title>
    <link href="http://sunqi.me/2015/08/18/about-monkey-patch/"/>
    <id>http://sunqi.me/2015/08/18/about-monkey-patch/</id>
    <published>2015-08-18T02:51:21.000Z</published>
    <updated>2020-07-21T13:52:56.872Z</updated>
    
    <content type="html"><![CDATA[<p>在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？</p><p>估计刚刚看到这个表述的开发人员可能很难理解到底这是什么意思，其实Monkey Patch本与猴子无关，这个词原来为Guerrilla Patch，这样看着好像能明白一些了，游击队嘛，神出鬼没的，好像和运行状态打补丁这个功能贴近点了，但是为什么又变成猴子了。原来老外们都是很顽皮的，他们喜欢一些玩笑式的表述，就像很多技术的文档中一样。在英文里，Guerrilla和Gorilla读音是几乎一样的，Gorilla当什么讲呢？大猩猩。但是大猩猩有点吓人，所以干脆换成了大猩猩的近亲——猴子。就这样Monkey Patch形成了。</p><p>当然这并不是这个词的唯一解释，还有一种解释是说由于这种方式将原来的代码弄乱了(messing with it)，在英文里叫monkeying about(顽皮的)，所以叫做Monkey Patch。这种描述应该是和Monkey Test有异曲同工之妙。但是无论这个词从哪里来，我们只要正确理解Monkey Patch的含义就好了。</p><p>相同的表述还有Duck Typing，描述的是动态类型的一种风格。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？&lt;/p&gt;
&lt;p
      
    
    </summary>
    
    
    
  </entry>
  
</feed>
