<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ray&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.me/"/>
  <updated>2020-01-14T08:51:56.536Z</updated>
  <id>http://sunqi.me/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>[Digitalcloud.Training]AWS CERTIFIED SOLUTIONS ARCHITECT ASSOCIATE</title>
    <link href="http://sunqi.me/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/"/>
    <id>http://sunqi.me/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/</id>
    <published>2020-01-14T08:49:16.000Z</published>
    <updated>2020-01-14T08:51:56.536Z</updated>
    
    <content type="html"><![CDATA[<p>Categories<br>AWS Analytics<br>100%<br>AWS Application Integration<br>100%<br>AWS Compute<br>33.33%<br>AWS Database<br>66.67%<br>AWS Management &amp; Governance<br>100%<br>AWS Networking &amp; Content Delivery<br>33.33%<br>AWS Security, Identity, &amp; Compliance<br>50%<br>AWS Storage<br>33.33%<br>Better luck next time!<br>Unfortunately on this occasion you did not pass the exam. The passing mark is a minimum score of 70%. The categories above show your performance in each knowledge area. Please use the “View Questions” button below to review answers, explanations, and reference links for each question.</p><ol><li>Question<br>A Solutions Architect has been asked to suggest a solution for analyzing data in S3 using standard SQL queries. The solution should use a serverless technology.</li></ol><p>Which AWS service can the Architect use?</p><p>Amazon RedShift<br>AWS Data Pipeline<br>AWS Glue<br>Amazon Athena<br>Correct<br>Explanation:</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run<br>Amazon RedShift is used for analytics but cannot analyze data in S3<br>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. It is not used for analyzing data in S3<br>AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals<br>References:</p><p><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a><br>2. Question<br>A systems integration company that helps customers migrate into AWS repeatedly build large, standardized architectures using several AWS services. The Solutions Architects have documented the architectural blueprints for these solutions and are looking for a method of automating the provisioning of the resources.</p><p>Which AWS service would satisfy this requirement?</p><p>AWS OpsWorks<br>AWS CloudFormation<br>AWS CodeDeploy<br>Elastic Beanstalk<br>Correct<br>Explanation:</p><p>CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts<br>Elastic Beanstalk is a PaaS service that helps you to build and manage web applications<br>AWS OpsWorks is a configuration management service that helps you build and operate highly dynamic applications, and propagate changes instantly<br>AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a><br>3. Question<br>A data-processing application runs on an i3.large EC2 instance with a single 100 GB EBS gp2 volume. The application stores temporary data in a small database (less than 30 GB) located on the EBS root volume. The application is struggling to process the data fast enough, and a Solutions Architect has determined that the I/O speed of the temporary database is the bottleneck.</p><p>What is the MOST cost-efficient way to improve the database response times?</p><p>Enable EBS optimization on the instance and keep the temporary files on the existing volume<br>Put the temporary database on a new 50-GB EBS gp2 volume<br>Move the temporary database onto instance storage<br>Put the temporary database on a new 50-GB EBS io1 volume with a 3000 IOPS allocation<br>Incorrect<br>Explanation:</p><p>EC2 Instance Stores are high-speed ephemeral storage that is physically attached to the EC2 instance. The i3.large instance type comes with a single 475GB NVMe SSD instance store so it would be a good way to lower cost and improve performance by using the attached instance store. As the files are temporary, it can be assumed that ephemeral storage (which means the data is lost when the instance is stopped) is sufficient.<br>Enabling EBS optimization will not lower cost. Also, EBS Optimization is a network traffic optimization, it does not change the I/O speed of the volume.<br>Moving the DB to a new 50-GB EBS gp2 volume will not result in a performance improvement as you get IOPS allocated per GB so a smaller volume will have lower performance.<br>Moving the DB to a new 50-GB EBS io1 volume with a 3000 IOPS allocation will improve performance but is more expensive so will not be the most cost-efficient solution.<br>References:</p><p><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a></p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/</a></p><ol start="4"><li>Question<br>An application you are designing receives and processes files. The files are typically around 4GB in size and the application extracts metadata from the files which typically takes a few seconds for each file. The pattern of updates is highly dynamic with times of little activity and then multiple uploads within a short period of time.</li></ol><p>What architecture will address this workload the most cost efficiently?</p><p>Upload files into an S3 bucket, and use the Amazon S3 event notification to invoke a Lambda function to extract the metadata<br>Place the files in an SQS queue, and use a fleet of EC2 instances to extract the metadata<br>Store the file in an EBS volume which can then be accessed by another EC2 instance for processing<br>Use a Kinesis data stream to store the file, and use Lambda for processing<br>Correct<br>Explanation:</p><p>Storing the file in an S3 bucket is the most cost-efficient solution, and using S3 event notifications to invoke a Lambda function works well for this unpredictable workload<br>Kinesis data streams runs on EC2 instances and you must therefore provision some capacity even when the application is not receiving files. This is not as cost-efficient as storing them in an S3 bucket prior to using Lambda for the processing<br>SQS queues have a maximum message size of 256KB. You can use the extended client library for Java to use pointers to a payload on S3 but the maximum payload size is 2GB<br>Storing the file in an EBS volume and using EC2 instances for processing is not cost efficient<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/</a><br><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a><br>5. Question<br>A Solutions Architect needs to deploy an HTTP/HTTPS service on Amazon EC2 instances that will be placed behind an Elastic Load Balancer. The ELB must support WebSockets.</p><p>How can the Architect meet these requirements?</p><p>Launch an Application Load Balancer (ALB)<br>Launch a Network Load Balancer (NLB)<br>Launch a Classic Load Balancer (CLB)<br>Launch a Layer-4 Load Balancer<br>Correct<br>Explanation:</p><p>Both the ALB and NLB support WebSockets. However, only the ALB supports HTTP/HTTPS listeners. The NLB only supports TCP, TLS, UDP, TCP_UDP.<br>The CLB does not support WebSockets.<br>A “Layer-4 Load Balancer” is not suitable, we need a layer 7 load balancer for HTTP/HTTPS.<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a></p><p><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html</a></p><ol start="6"><li>Question<br>You are building an application that will collect information about user behavior. The application will rapidly ingest large amounts of dynamic data and requires very low latency. The database must be scalable without incurring downtime. Which database would you recommend for this scenario?</li></ol><p>RDS with Microsoft SQL<br>RedShift<br>DynamoDB<br>RDS with MySQL<br>Correct<br>Explanation:</p><p>Amazon Dynamo DB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability<br>Push button scaling means that you can scale the DB at any time without incurring downtime<br>DynamoDB provides low read and write latency<br>RDS uses EC2 instances so you have to change your instance type/size in order to scale compute vertically<br>RedShift uses EC2 instances as well, so you need to choose your instance type/size for scaling compute vertically, but you can also scale horizontally by adding more nodes to the cluster<br>Rapid ingestion of dynamic data is not an ideal use case for RDS or RedShift<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a><br>7. Question<br>You are running an Auto Scaling Group (ASG) with an Elastic Load Balancer (ELB) and a fleet of EC2 instances. Health checks are configured on the ASG to use EC2 status checks. The ELB has determined that an EC2 instance is unhealthy and has removed it from service. However, you noticed that the instance is still running and has not been terminated by the ASG.</p><p>What would be an explanation for this behavior?</p><p>The health check grace period has not yet expired<br>The ELB health check type has not been selected for the ASG and so it is unaware that the instance has been determined to be unhealthy by the ELB and has been removed from service<br>Connection draining is enabled and the ASG is waiting for in-flight requests to complete<br>The ASG is waiting for the cooldown timer to expire before terminating the instance<br>Incorrect<br>Explanation:</p><p>If using an ELB it is best to enable ELB health checks as otherwise EC2 status checks may show an instance as being healthy that the ELB has determined is unhealthy. In this case the instance will be removed from service by the ELB but will not be terminated by Auto Scaling<br>Connection draining is not the correct answer as the ELB has taken the instance out of service so there are no active connections<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check<br>More information on ASG health checks:<br>By default uses EC2 status checks<br>Can also use ELB health checks and custom health checks<br>ELB health checks are in addition to the EC2 status checks<br>If any health check returns an unhealthy status the instance will be terminated<br>With ELB an instance is marked as unhealthy if ELB reports it as OutOfService<br>A healthy instance enters the InService state<br>If an instance is marked as unhealthy it will be scheduled for replacement<br>If connection draining is enabled, Auto Scaling waits for in-flight requests to complete or timeout before terminating instances<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check (300 seconds by default)<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/</a><br>8. Question<br>A solutions Architect is designing a new workload where an AWS Lambda function will access an Amazon DynamoDB table.</p><p>What is the MOST secure means of granting the Lambda function access to the DynamoDB table?</p><p>Create an identity and access management (IAM) role allowing access from AWS Lambda and assign the role to the DynamoDB table<br>Create an identity and access management (IAM) role with the necessary permissions to access the DynamoDB table, and assign the role to the Lambda function<br>Create a DynamoDB username and password and give them to the Developer to use in the Lambda function<br>Create an identity and access management (IAM) user and create access and secret keys for the user. Give the user the necessary permissions to access the DynamoDB table. Have the Developer use these keys to access the resources<br>Correct<br>Explanation:</p><p>The most secure method is to use an IAM role so you don’t need to embed any credentials in code and can tightly control the services that your Lambda function can access. You need to assign the role to the Lambda function, NOT to the DynamoDB table<br>You should not provide a username and password to the Developer to use with the function. This is insecure – always avoid using credentials in code!<br>You should not use an access key and secret ID to access DynamoDB. Again, this means embedding credentials in code which should be avoided.<br>References:</p><p><a href="https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/</a></p><ol start="9"><li>Question<br>Your company has offices in several locations around the world. Each office utilizes resources deployed in the geographically closest AWS region. You would like to implement connectivity between all of the VPCs so that you can provide full access to each other’s resources. As you are security conscious you would like to ensure the traffic is encrypted and does not traverse the public Internet. The topology should be many-to-many to enable all VPCs to access the resources in all other VPCs.</li></ol><p>How can you successfully implement this connectivity using only AWS services? (choose 2)</p><p>Use inter-region VPC peering<br>Use software VPN appliances running on EC2 instances<br>Use VPC endpoints between VPCs<br>Implement a fully meshed architecture<br>Implement a hub and spoke architecture<br>Incorrect<br>Explanation:</p><p>Peering connections can be created with VPCs in different regions (available in most regions now)<br>Data sent between VPCs in different regions is encrypted (traffic charges apply)<br>You cannot do transitive peering so a hub and spoke architecture would not allow all VPCs to communicate directly with each other. For this you need to establish a mesh topology<br>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services, it does not provide full VPC to VPC connectivity<br>Using software VPN appliances to connect VPCs together is not the best solution as it is cumbersome, expensive and would introduce bandwidth and latency constraints (amongst other problems)<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a><br>10. Question<br>A research company is developing a data lake solution in Amazon S3 to analyze huge datasets. The solution makes infrequent SQL queries only. In addition, the company wants to minimize infrastructure costs.</p><p>Which AWS service should be used to meet these requirements?</p><p>Amazon Athena<br>Amazon Redshift Spectrum<br>Amazon Aurora<br>Amazon RDS for MySQL<br>Correct<br>Explanation:</p><p>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run – this satisfies the requirement to minimize infrastructure costs for infrequent queries.<br>Amazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. However, RedShift nodes run on EC2 instances, so for infrequent queries this will not minimize infrastructure costs.<br>Amazon RDS and Aurora are not suitable solutions for analyzing datasets on S3 – these are both relational databases typically used for transactional (not analytical) workloads.<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/</a></p><p><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><ol start="11"><li>Question<br>An Architect is designing a serverless application that will accept images uploaded by users from around the world. The application will make API calls to back-end services and save the session state data of the user to a database.</li></ol><p>Which combination of services would provide a solution that is cost-effective while delivering the least latency?</p><p>Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>Amazon S3, API Gateway, AWS Lambda, Amazon RDS<br>API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, Amazon RDS<br>Incorrect<br>Explanation:</p><p>Amazon CloudFront caches content closer to users at Edge locations around the world. This is the lowest latency option for uploading content. API Gateway and AWS Lambda are present in all options. DynamoDB can be used for storing session state data<br>The option that presents API Gateway first does not offer a front-end for users to upload content to<br>Amazon RDS is not a serverless service so this option can be ruled out<br>Amazon S3 alone will not provide the least latency for users around the world unless you have many buckets in different regions and a way of directing users to the closest bucket (such as Route 3 latency based routing). However, you would then need to manage replicating the data<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/</a><br><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a><br>12. Question<br>A training provider hosts a website using Amazon API Gateway on the front end. Recently, there has been heavy traffic on the website and the company wants to control access by allowing authenticated traffic from paying students only.</p><p>How should the company limit access to authenticated users only? (choose 2)</p><p>Deploy AWS KMS to identify users<br>Allow X.509 certificates to authenticate traffic<br>Assign permissions in AWS IAM to allow users<br>Limit traffic through API Gateway<br>Allow users that are authenticated through Amazon Cognito<br>Incorrect<br>Explanation:</p><p>API Gateway supports multiple mechanisms for controlling and managing access to your API. These include resource policies, standard IAM roles and policies, Lambda authorizers, and Amazon Cognito user pools.<br>Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.<br>IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs as well as who can invoke them.<br>Limiting traffic through the API Gateway will not filter authenticated traffic, it will just limit overall invocations. This may prevent users from connecting who have a legitimate need.<br>X.509 certificates are not a method of authentication you can use with API Gateway.<br>AWS KMS is used for key management not user identification.<br>References:</p><p><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a></p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/</a></p><ol start="13"><li>Question<br>An Auto Scaling Group is unable to respond quickly enough to load changes resulting in lost messages from another application tier. The messages are typically around 128KB in size.</li></ol><p>What is the best design option to prevent the messages from being lost?</p><p>Launch an Elastic Load Balancer<br>Store the messages on Amazon S3<br>Use larger EC2 instance sizes<br>Store the messages on an SQS queue<br>Correct<br>Explanation:</p><p>In this circumstance the ASG cannot launch EC2 instances fast enough. You need to be able to store the messages somewhere so they don’t get lost whilst the EC2 instances are launched. This is a classic use case for decoupling and SQS is designed for exactly this purpose<br>Amazon Simple Queue Service (Amazon SQS) is a web service that gives you access to message queues that store messages waiting to be processed. SQS offers a reliable, highly-scalable, hosted queue for storing messages in transit between computers. An SQS queue can be used to create distributed/decoupled applications<br>Storing the messages on S3 is potentially feasible but SQS is the preferred solution as it is designed for decoupling. If the messages are over 256KB and therefore cannot be stored in SQS, you may want to consider using S3 and it can be used in combination with SQS by using the Amazon SQS Extended Client Library for Java<br>An ELB can help to distribute incoming connections to the back-end EC2 instances however if the ASG is not scaling fast enough then there aren’t enough resources for the ELB to distributed traffic to<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/</a><br>14. Question<br>Your company would like to restrict the ability of most users to change their own passwords whilst continuing to allow a select group of users within specific user groups.</p><p>What is the best way to achieve this? (choose 2)</p><p>Under the IAM Password Policy deselect the option to allow users to change their own passwords<br>Create an IAM Policy that grants users the ability to change their own password and attach it to the individual user accounts<br>Create an IAM Policy that grants users the ability to change their own password and attach it to the groups that contain the users<br>Create an IAM Role that grants users the ability to change their own password and attach it to the groups that contain the users<br>Disable the ability for all users to change their own passwords using the AWS Security Token Service<br>Incorrect<br>Explanation:</p><p>A password policy can be defined for enforcing password length, complexity etc. (applies to all users)<br>You can allow or disallow the ability to change passwords using an IAM policy and you should attach this to the group that contains the users, not to the individual users themselves<br>You cannot use an IAM role to perform this function<br>The AWS STS is not used for controlling password policies<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/</a><br>15. Question<br>A Solutions Architect has created a VPC design that meets the security requirements of their organization. Any new applications that are deployed must use this VPC design.</p><p>How can project teams deploy, manage, and delete VPCs that meet this design with the LEAST administrative effort?</p><p>Deploy an AWS CloudFormation template that defines components of the VPC<br>Use AWS Elastic Beanstalk to deploy both the VPC and the application<br>Clone the existing authorized VPC for each new project<br>Run a script that uses the AWS Command Line interface to deploy the VPC<br>Correct<br>Explanation:</p><p>CloudFormation allows you to define your infrastructure through code and securely and repeatably deploy the infrastructure with minimal administrative effort. This is a perfect use case for CloudFormation.<br>You can use a script to create the VPCs using the AWS CLI however this would be a lot more work to create and manage the scripts.<br>You cannot clone VPCs.<br>You cannot deploy the VPC through Elastic Beanstalk – you need to deploy the VPC first and then deploy your application using Beanstalk.<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a></p><p><a href="https://aws.amazon.com/cloudformation/" target="_blank" rel="noopener">https://aws.amazon.com/cloudformation/</a></p><ol start="16"><li>Question<br>Your organization has a data lake on S3 and you need to find a solution for performing in-place queries of the data assets in the data lake. The requirement is to perform both data discovery and SQL querying, and complex queries from a large number of concurrent users using BI tools.</li></ol><p>What is the BEST combination of AWS services to use in this situation? (choose 2)</p><p>AWS Glue for the ad hoc SQL querying<br>AWS Lambda for the complex queries<br>RedShift Spectrum for the complex queries<br>Amazon Athena for the ad hoc SQL querying<br>Incorrect<br>Explanation:</p><p>Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a data warehouse<br>You can use both Athena and Redshift Spectrum against the same data assets. You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads<br>AWS Lambda is a serverless technology for running functions, it is not the best solution for running analytics queries<br>AWS Glue is an ETL service<br>References:</p><p><a href="https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html</a><br><a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">https://aws.amazon.com/redshift/</a><br><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a><br>17. Question<br>You have recently enabled Access Logs on your Application Load Balancer (ALB). One of your colleagues would like to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?</p><p>Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files<br>Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files<br>Configure Access Logs to be delivered to S3 and use EMR for processing the log files<br>Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files<br>Correct<br>Explanation:</p><p>Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon S3<br>Neither Kinesis or EC2 provide a hosted Hadoop service<br>You cannot configure access logs to be delivered to DynamoDB<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a><br>18. Question<br>A company is deploying a big data and analytics workload. The analytics will be run from a fleet of thousands of EC2 instances across multiple AZs. Data needs to be stored on a shared storage layer that can be mounted and accessed concurrently by all EC2 instances. Latency is not a concern however extremely high throughput is required.</p><p>What storage layer would be most suitable for this requirement?</p><p>Amazon EFS in General Purpose mode<br>Amazon EFS in Max I/O mode<br>Amazon S3<br>Amazon EBS PIOPS<br>Correct<br>Explanation:</p><p>Amazon EFS file systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations<br>Amazon S3 is not a storage layer that can be mounted and accessed concurrently<br>Amazon EBS volumes cannot be shared between instances<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/</a><br><a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a><br>19. Question<br>An application launched on Amazon EC2 instances needs to publish personally identifiable information (PII) about customers using Amazon SNS. The application is launched in private subnets within an Amazon VPC.</p><p>Which is the MOST secure way to allow the application to access service endpoints in the same region?</p><p>Use a proxy instance<br>Use a NAT gateway<br>Use AWS PrivateLink<br>Use an Internet Gateway<br>Correct<br>Explanation:</p><p>To publish messages to Amazon SNS topics from an Amazon VPC, create an interface VPC endpoint. Then, you can publish messages to SNS topics while keeping the traffic within the network that you manage with the VPC. This is the most secure option as traffic does not need to traverse the Internet.<br>Internet Gateways are used by instances in public subnets to access the Internet and this is less secure than an VPC endpoint.<br>A NAT Gateway is used by instances in private subnets to access the Internet and this is less secure than an VPC endpoint.<br>A proxy instance will also use the public Internet and so is less secure than a VPC endpoint.<br>References:</p><p><a href="https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html</a></p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a></p><ol start="20"><li>Question<br>A retail organization is deploying a new application that will read and write data to a database. The company wants to deploy the application in three different AWS Regions in an active-active configuration. The databases need to replicate to keep information in sync.</li></ol><p>Which solution best meets these requirements?</p><p>Amazon Aurora Global Database<br>Amazon DynamoDB with global tables<br>Amazon Athena with Amazon S3 cross-region replication<br>AWS Database Migration Service with change data capture<br>Incorrect<br>Explanation:</p><p>Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution presented that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.<br>Amazon Athena with S3 cross-region replication is not suitable. This is not a solution that provides a transactional database solution (Athena is used for analytics), or active-active synchronization.<br>Amazon Aurora Global Database provides read access to a database in multiple regions – it does not provide active-active configuration with bi-directional synchronization (though you can failover to your read-only DBs and promote them to writable).<br>References:</p><p><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a></p><p><a href="https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Categories&lt;br&gt;AWS Analytics&lt;br&gt;100%&lt;br&gt;AWS Application Integration&lt;br&gt;100%&lt;br&gt;AWS Compute&lt;br&gt;33.33%&lt;br&gt;AWS Database&lt;br&gt;66.67%&lt;br&gt;AWS Mana
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q101-Q200)</title>
    <link href="http://sunqi.me/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/"/>
    <id>http://sunqi.me/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/</id>
    <published>2020-01-08T07:36:23.000Z</published>
    <updated>2020-01-14T08:51:56.536Z</updated>
    
    <content type="html"><![CDATA[<p>通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。</p><a id="more"></a><h2 id="争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment"><a href="#争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment" class="headerlink" title="(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?"></a>(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?</h2><p>A. Continuously replicate the production database server to Amazon RDS. Use AWS CloudFormation to deploy the application and any additional servers if necessary.<br>B. Continuously replicate the production database server to Amazon RDS. Create one application load balancer and register on-premises servers. Configure ELB Application Load Balancer to automatically deploy Amazon EC2 instances for application and additional servers if the on-premises application is down.<br>C. Use a scheduled Lambda function to replicate the production database to AWS. Use Amazon Route 53 health checks to deploy the application automatically to Amazon S3 if production is unhealthy.<br>D. Use a scheduled Lambda function to replicate the production database to AWS. Register on-premises servers to an Auto Scaling group and deploy the application and additional servers if production is unavailable.</p><p>Answer: B</p><ul><li>分析：有人说答案是A，因为题目中的这个词pilot light(A pilot light is a small gas flame)，准确的翻译没查到，从字面理解应该就是简单轻量级的意思。A选项的方式是当出现灾难时，使用CloudFormation进行除数据库外的重建。所以很多人认为这种方式更符合题目的要求。但是从B选项看，更符合一个容灾的场景，当发生灾难时，通过B中的配置，可以做到马上接管的效果，比A选项更像是一个容灾的解决方案。</li></ul><h2 id="A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation"><a href="#A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation" class="headerlink" title="A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?"></a>A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?</h2><p>A. Change the RDS instance type to get more RAM.<br>B. Change the storage type to Provisioned IOPS.<br>C. Scale the web server tier horizontally.<br>D. Split the DB layer into separate RDS instances.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and"><a href="#A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and" class="headerlink" title="A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and"></a>A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and</h2><p>Windows. Which solution meets this requirement?</p><p>A. Default Amazon CloudWatch metrics.<br>B. Custom Amazon CloudWatch metrics.<br>C. Amazon Inspector resource monitoring.<br>D. Default monitoring of Amazon EC2 instances.</p><p>Answer: B</p><ul><li>分析：这道题又是原网站给出的错题，原来给出的答案是A。我曾经对AWS两个行为比较纳闷：一个是为什么没有VNC，另外一个是为什么不提供内存监控，直到又一次和AWS的架构师聊才理解了AWS的良苦用心。AWS始终把用户安全放在第一位，但凡用户的东西我是坚决不能碰的，而无论是VNC还是内存监控无疑与这一原则相违背的。所以内存不可能是默认监控的范畴，必须通过custom脚本完成，同样磁盘利用率也是类似的方式。</li></ul><h2 id="A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use"><a href="#A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use" class="headerlink" title="A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?"></a>A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?</h2><p>A. Amazon Aurora<br>B. Amazon RDS MySQL with Multi-AZ enabled<br>C. Amazon DynamoDB<br>D. Amazon ElastiCache</p><p>Answer: A</p><blockquote><p>问：Amazon Aurora 如何提高我的数据库对磁盘故障的容错能力？</p><p>Amazon Aurora 会将您的数据库卷分成分散在很多个磁盘上的 10GB 的区段。每 10GB 的数据库卷组块都能在三个可用区间用六种方法进行复制。Amazon Aurora 的设计可透明应对多达两个数据副本的损失，而不会影响数据库写入可用性，还能在不影响读取可用性的情况下应对多达三个副本。Amazon Aurora 存储还具有自我修复能力。可连续扫描数据块和磁盘有无出错并自动修复之。</p></blockquote><h2 id="A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design"><a href="#A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design" class="headerlink" title="A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?"></a>A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?</h2><p>A. Add an S3 lifecycle rule to move any files from the bucket in us-east-1 to the bucket in ap-southeast-2.<br>B. Create a Lambda function to be triggered for every new file in us-east-1 that copies the file to the bucket in ap-southeast-2.<br>C. Use SNS to notify the bucket in ap-southeast-2 to create a file whenever the file is created in the bucket in us-east-1.<br>D. Enable versioning and configure cross-region replication from the bucket in us-east-1 to the bucket in ap-southeast-2.</p><p>Answer: D</p><ul><li>分析：这道题要求的是最简单的方法，B理论上是可以的，但是与D相比过于复杂。</li></ul><h2 id="An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met"><a href="#An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met" class="headerlink" title="An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?"></a>An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?</h2><p>A. Use Amazon CloudWatch Events to invoke an AWS Lambda function that can launch On-Demand Instances.<br>B. Regularly store data from the application on Amazon DynamoDB. Increase the maximum number of instances in the AWS Auto Scaling group.<br>C. Manually place a bid for additional Spot Instances at a higher price in the same AWS Region and Availability Zone.<br>D. Ensure that the Amazon Machine Image associated with the application has the latest configurations for the launch configuration.</p><p>Answer: A</p><ul><li><p>Taking Advantage of Amazon EC2 Spot Instance Interruption Notices(<a href="https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/</a>)</p><blockquote><p>In January 2018, the Spot Instance interruption notice also became available as an event in Amazon CloudWatch Events. This allows targets such as AWS Lambda functions or Amazon SNS topics to process Spot Instance interruption notices by creating a CloudWatch Events rule to monitor for the notice.</p></blockquote></li><li><p>分析：网站给出的答案是B，但是这道题目考察的应该是合理利用Spot Instance的通知是这道题目的关键，”must include a two-minute warning” =&gt; Need CloudWatch</p></li></ul><h2 id="A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved"><a href="#A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved" class="headerlink" title="A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?"></a>A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?</h2><p>A. (争议)Use asynchronous replication for standby to maximize throughput during peak demand.<br>B. Offload SELECT queries that can tolerate stale data to READ replica.<br>C. Offload SELECT and UPDATE queries to READ replica.<br>D. Offload SELECT query that needs the most current data to READ replica.</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，大部分反对的理由是RDS之间的复制是同步的并不是异步的。</li></ul><h2 id="A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend"><a href="#A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend" class="headerlink" title="A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?"></a>A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?</h2><p>A. Configure active-active failover using Route 53 latency DNS records.<br>B. Configure active-passive failover using Route 53 latency DNS records.<br>C. Configure active-active failover using Route 53 failover DNS records.<br>D. Configure active-passive failover using Route 53 failover DNS records.</p><p>Answer: A</p><ul><li>分析：这道题很明显是需要AA模式的，with least latency，所以需要latency。</li></ul><blockquote><p>Active-Active Failover<br>Use this failover configuration when you want all of your resources to be available the majority of the time. When a resource becomes unavailable, Route 53 can detect that it’s unhealthy and stop including it when responding to queries.</p><p>In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.</p></blockquote><h2 id="A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS"><a href="#A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS" class="headerlink" title="A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?"></a>A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?</h2><p>A. Use Amazon ECS and Service Auto Scaling.<br>B. Use Spot Instances for orchestration and for scaling containers on existing Amazon EC2 instances.<br>C. Use AWS OpsWorks to launch containers in new Amazon EC2 instances.<br>D. Use Auto Scaling groups to launch containers on existing Amazon EC2 instances.</p><p>Answer: A</p><blockquote><p>Amazon Elastic Container Service (Amazon ECS) 是用于在可扩展群集上运行 Docker 应用程序的 Amazon Web Service。在本教程中，您将了解如何在负载均衡器后面的 Amazon ECS 集群上运行支持 Docker 的示例应用程序，对该示例应用程序进行测试，然后删除您的资源以免产生费用。</p></blockquote><h2 id="争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate"><a href="#争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate" class="headerlink" title="(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?"></a>(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?</h2><p>A. Amazon API Gateway and AWS Lambda<br>B. Elastic Load Balancing with Auto Scaling groups and Amazon EC2<br>C. Amazon API Gateway and Amazon EC2<br>D. Amazon CloudFront and AWS Lambda</p><p>Answer: A</p><ul><li>分析：题目的需求是：1、不想运维；2、集中精力去做开发。这是非常典型的Serverless的需求，将底层交给云原生服务，专注于业务开发。首先应该选择AWS Lambda服务，则AD作为选项。原答案给出的是D，但是CloudFront作为CDN服务，好像并没有向外提供接口的能力，AWS Lambda服务并不像阿里的函数计算提供了Http trigger，所以无法对外提供API接口的能力，从这个角度看A更合理一些。</li></ul><h2 id="A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS"><a href="#A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS" class="headerlink" title="A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?"></a>A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?</h2><p>A. Create a read replica of the primary database and deploy it in a different AWS Region.<br>B. Enable multi-AZ to create a standby database in a different Availability Zone.<br>C. Enable multi-AZ to create a standby database in a different AWS Region.<br>D. Create a read replica of the primary database and deploy it in a different Availability Zone.</p><p>Answer: B</p><ul><li>分析：原题目给出的是A，但是B明显是正确答案。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/rds/ha/?nc1=h_ls" target="_blank" rel="noopener">https://aws.amazon.com/cn/rds/ha/?nc1=h_ls</a><br>Amazon Relational Database Service (Amazon RDS) supports two easy-to-use options for ensuring High Availability of your relational database.<br>For your MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database (DB) instances, you can use Amazon RDS Multi-AZ deployments. When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby DB instance. Since the endpoint for your DB instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. Learn more &gt;&gt;</p></blockquote><h2 id="An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements"><a href="#An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements" class="headerlink" title="An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?"></a>An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?</h2><p>A. Use Amazon Cognito Identity with SMS-based MFA.<br>B. Edit AWS IAM policies to require MFA for all users.<br>C. Federate IAM against corporate AD that requires MFA.<br>D. Use Amazon API Gateway and require SSE for photos.</p><p>Answer: A</p><h2 id="争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements"><a href="#争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements" class="headerlink" title="(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?"></a>(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon RDS<br>D. Amazon EBS</p><p>Answer: A</p><ul><li>分析：这道题很多人都认为B是正确的，但是从篇气象公司最佳实践看，确实采用的是S3方案。</li></ul><blockquote><p>AWS Case Study: The Weather Company(<a href="https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/" target="_blank" rel="noopener">https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/</a>)<br>The platform ingests information from more than 100 different sources and generates close to one-half terabyte (TB) of data each time it updates. The information is mapped and processed into forecast points that can be retrieved in real time, based on queries coming into the system. All data is stored in Amazon Simple Storage Service (Amazon S3), leveraging the efficiency of cloud storage as opposed to an on-premises storage solution and eliminating the hassle of managing a storage platform.</p></blockquote><h2 id="A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency"><a href="#A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency" class="headerlink" title="A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?"></a>A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?</h2><p>A. Amazon CloudFront with on-premises servers as the origin<br>B. ELB Application Load Balancer<br>C. Amazon Route 53 latency-based routing<br>D. Amazon EFS to store and server static files</p><p>Answer: A</p><h2 id="A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used"><a href="#A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used" class="headerlink" title="A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?"></a>A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?</h2><p>A. Amazon DynamoDB<br>B. Amazon Aurora MySQL<br>C. Amazon RDS MySQL<br>D. Amazon Redshift</p><p>Answer: D</p><h2 id="A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation"><a href="#A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation" class="headerlink" title="A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?"></a>A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?</h2><p>A. The instance with the oldest launch configuration.<br>B. The instance in the Availability Zone that has most instances.<br>C. The instance closest to the next billing hour.<br>D. The oldest instance in the group.</p><p>Answer: B</p><ul><li>控制在缩小过程中终止哪些 Auto Scaling 实例(<a href="https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a>)</li></ul><blockquote><p>当达到缩减策略的阈值时，策略生效，Auto Scaling 组终止其中一个实例。如果您没有为该组分配特定的终止策略，则使用默认终止策略。它选择有两个实例的可用区，并终止从最旧启动配置启动的实例。如果这些实例是从同一启动配置启动的，则 Auto Scaling 组选择最接近下一个计费小时的实例并终止该实例。</p></blockquote><h2 id="A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements"><a href="#A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements" class="headerlink" title="A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?"></a>A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?</h2><p>A. Implement an Amazon API Gateway to server as the HTTP endpoint. Have the API Gateway trigger an AWS Lambda function to process the messages, and save the results to an Amazon DynamoDB table.<br>B. Create an Amazon EC2 instance to server as the HTTP endpoint and to process the messages. Save the results to Amazon S3 for the Data Analysis team to download.<br>C. Use Amazon Route 53 to direct incoming sensor messages to a Lambda function to process the message and save the results to a Amazon DynamoDB table.<br>D. Use AWS Direct Connect to connect sensors to DynamoDB so that data can be written directly to a DynamoDB table where it can be accessed by the Data Analysis team.</p><p>Answer: A</p><ul><li>分析：原来AWS Lambda的HTTP trigger是这么实现的</li></ul><h2 id="A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements"><a href="#A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements" class="headerlink" title="A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?"></a>A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?</h2><p>A. Amazon RDS<br>B. Amazon RedShift<br>C. Amazon DynamoDB Accelerator<br>D. Amazon ElastiCache</p><p>Answer: A</p><h2 id="争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met"><a href="#争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met" class="headerlink" title="(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?"></a>(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?</h2><p>A. Use an Amazon Redshift database. Copy the product database into Redshift and allow the team to query it.<br>B. Use an Amazon RDS read replica of the production database and allow the team to query against it.<br>C. Use multiple Amazon EC2 instances running replicas of the production database, placed behind a load balancer.<br>D. Use an Amazon DynamoDB table to store a copy of the data.</p><p>Answer: B</p><ul><li>争议：原有答案给出的是A，根据题目描述看起来是一个数据仓库的需求。从easy to maintain的角度说B，更简单</li></ul><h2 id="A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two"><a href="#A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two" class="headerlink" title="A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)"></a>A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)</h2><p>A. Amazon Kinesis Data Firehouse<br>B. Amazon SQS<br>C. Amazon Redshift<br>D. Amazon SNS<br>E. Amazon DynamoDB</p><p>Answer: AC</p><ul><li>分析：A负责接收数据并处理，C作为数据仓库存储下来</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q1-Q100)</title>
    <link href="http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/"/>
    <id>http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/</id>
    <published>2019-12-31T01:11:55.000Z</published>
    <updated>2020-01-14T08:51:56.536Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接：<a href="https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/" target="_blank" rel="noopener">https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/</a></p><p>一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。</p><p>由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。</p><a id="more"></a><h2 id="A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest"><a href="#A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest" class="headerlink" title="A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?"></a>A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?</h2><p>A. Place the Redshift cluster in a private subnet.<br>B. Use the AWS KMS Default Customer master key.<br>C. Encrypt the Amazon EBS volumes.<br>D. Encrypt the data using SSL/TLS.</p><p>Answer: B</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html</a></li><li>分析：Amazon Redshift 使用加密密钥层次结构来加密数据库。您可以使用 AWS Key Management Service (AWS KMS) 或硬件安全模块 (HSM) 来管理该层次结构中的顶级加密密钥。Amazon Redshift 用于加密的流程因您管理密钥的方式而异。Amazon Redshift 自动与 AWS KMS 集成，而不与 HSM 集成。当您使用 HSM 时，必须使用客户端和服务器证书在 Amazon Redshift 和 HSM 之间配置受信任的连接。</li></ul><h2 id="A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database"><a href="#A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database" class="headerlink" title="A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?"></a>A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?</h2><p>A. Amazon SQS<br>B. Amazon EFS<br>C. Amazon S3<br>D. AWS Lambda</p><p>Answer: A</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/sqs/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/cn/sqs/faqs/</a></li><li>分析：关键词是unpredictable traffic, keep up with write request, decouple the web application, 所以通过消息队列服务可以让写入请求排队，从而实现前端应用和后端数据库的解耦。</li></ul><h2 id="A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements"><a href="#A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements" class="headerlink" title="A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?"></a>A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?</h2><p>A. AWS Snowball storage for the legacy application until the application can be re-architected.<br>B. AWS Storage Gateway in cached mode for the legacy application storage to write data to Amazon S3.<br>C. AWS Storage Gateway in stored mode for the legacy application storage to write data to Amazon S3.<br>D. An Amazon S3 volume mounted on the legacy application server locally using the File Gateway service.</p><p>Answer: C</p><ul><li>分析：关键词是local stroage with iSCSI, 并且需要将所有新的存储用AWS提供，所以排除A选项；因为用到了iSCSI协议，所以S3使用文件网关方式也不适用，排除D；剩下的B和C区别在于存储模式，因为需要本地应用请求，所以需要使用存储模式，而不能用缓存模式，所以最终选择C。</li></ul><h2 id="A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck"><a href="#A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck" class="headerlink" title="A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?"></a>A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?</h2><p>A. Multi-AZ feature on the RDS MySQL Database<br>B. ELB Classic Load Balancer in front of the web application tier<br>C. Amazon SQS in front of RDS MySQL Database<br>D. Amazon ElastiCache in front of the RDS MySQL Database</p><p>Answer: D</p><ul><li>分析：该问题的关键在于bottleneck due to frequently accessed queries，查询变成瓶颈，可以使用ElastiCache服务作为缓存，降低读取频率解决问题。</li></ul><h2 id="A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives"><a href="#A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives" class="headerlink" title="A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?"></a>A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?</h2><p>A. Amazon Redshift<br>B. Amazon DynamoDB<br>C. Amazon RDS MySQL<br>D. Amazon Aurora</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，但是经过分析觉得有些问题，这道题的几个关键词：scale with the rest of the application, schema will change frequently, cannot afford any downtime for database changes. 首先，schema总是变更，所以这里需要的非关系型数据库，排除C和D。Redshift是数据仓库，其实也是数据仓库，从第一点上就可以排除。另外从这个链接（<a href="http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero" target="_blank" rel="noopener">http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero</a> downtime，其他的都不行。所以原网站给出的答案是错误的。</li></ul><h2 id="A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements"><a href="#A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements" class="headerlink" title="A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?"></a>A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?</h2><p>A. Use AWS CloudFormation to deploy the cluster in a second region.<br>B. Take a snapshot of the cluster and copy it to another Availability Zone.<br>C. Modify the Redshift cluster to span two regions.<br>D. Enable cross-region snapshots to a different region.</p><p>Answer: D</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/</a></li></ul><h2 id="A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct"><a href="#A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct" class="headerlink" title="A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?"></a>A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?</h2><p>A. The application is reading parts of objects from Amazon S3 using a range header.<br>B. The application is reading objects from Amazon S3 using parallel object requests.<br>C. The application is updating records by writing new objects with unique keys.<br>D. The application is updating records by overwriting existing objects with the same keys.</p><p>Answer: D</p><ul><li>分析：这道题也是争论很大的一道题，原网站答案为A。问题简单描述为客户端访问不到最新的数据，发生的时间点在于应用上传信息时候速率提高导致的，所以问题应该出现在写入的时候，这样排除A和B读取的问题。因为S3同一object永远是覆盖，所以最有可能的问题是在same key的情况下，所以选择D。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements" class="headerlink" title="A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?"></a>A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?</h2><p>A. Use Amazon Kinesis with AWS CloudTrail for auditing the specific times when profile photos are uploaded.<br>B. Use Amazon EBS volumes with IAM policies restricting user access to specific time periods.<br>C. Use Amazon S3 with the default private access policy and generate pre-signed URLs each time a new site profile is created.<br>D. Use Amazon CloudFront with AWS CloudTrail for auditing the specific times when profile photos are uploaded.</p><p>Answer: C</p><h2 id="An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application"><a href="#An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application" class="headerlink" title="An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?"></a>An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Amazon Glacier</p><p>Answer: C</p><ul><li>分析：没想到这道题原网站给出的答案是B，争议比较大，但是从题目描述需要Block Storage角度来看，选择C才是最合理的。这道题还需要进一步确认一下。</li></ul><h2 id="A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements"><a href="#A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements" class="headerlink" title="A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?"></a>A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?</h2><p>A. Create a Lambda function to move files older than 30 days to Amazon EBS and move files older than 60 days to Amazon Glacier.<br>B. Create a Lambda function to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon EBS.<br>C. Create lifecycle rules to move files older than 30 days to Amazon S3 Standard Infrequent Access and move files older than 60 days to Amazon Glacier.<br>D. Create lifecycle rules to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon S3 Standard Infrequent Access.</p><p>Answer: C</p><ul><li>分析：很明显的排除A和B，S3可以自定义规则，那么问题就是30天后和60天后需要哪种存储的问题了，根据题目C是明显正确的。</li></ul><h2 id="An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs"><a href="#An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs" class="headerlink" title="An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?"></a>An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?</h2><p>A. Amazon Redshift<br>B. Amazon RDS<br>C. Amazon DynamoDB<br>D. Amazon Aurora</p><p>Answer: C</p><h2 id="A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two"><a href="#A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two" class="headerlink" title="A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)"></a>A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)</h2><p>A. Create a VPC Endpoint for Amazon RDS.<br>B. Create the Lambda function within the Amazon RDS VPC.<br>C. Change the ingress rules of Lambda security group, allowing the Amazon RDS security group.<br>D. Change the ingress rules of the Amazon RDS security group, allowing the Lambda security group.<br>E. Add an Internet Gateway (IGW) to the VPC, route the private subnet to the IGW.</p><p>Answer: BD</p><ul><li>分析：又是原网站一道错题，原网站答案为AD。D选项是允许Lambda服务访问RDS，所以在进方向允许。</li><li>目前VPC支持Endpoint的服务：<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html</a></li></ul><blockquote><p>Amazon API Gateway<br>Amazon AppStream 2.0<br>AWS App Mesh<br>Amazon Athena<br>AWS CloudFormation<br>AWS CloudTrail<br>Amazon CloudWatch<br>Amazon CloudWatch Events<br>Amazon CloudWatch Logs<br>AWS CodeBuild<br>AWS CodeCommit<br>AWS CodePipeline<br>AWS Config<br>AWS DataSync<br>Amazon EC2 API<br>Elastic Load Balancing<br>Amazon Elastic Container Registry<br>Amazon Elastic Container Service<br>AWS Glue<br>AWS Key Management Service<br>Amazon Kinesis Data Firehose<br>Amazon Kinesis Data Streams<br>Amazon Rekognition<br>Amazon SageMaker 和 Amazon SageMaker 运行时<br>Amazon SageMaker 笔记本<br>AWS Secrets Manager<br>AWS Security Token Service<br>AWS Service Catalog<br>Amazon SNS<br>Amazon SQS<br>AWS Systems Manager<br>AWS Storage Gateway<br>AWS Transfer for SFTP<br>其他 AWS 账户托管的终端节点服务</p><p>网关终端节点是一个网关，作为您在路由表中指定的路由的目标，用于发往受支持的 AWS 服务的流量。支持以下 AWS 服务：<br>Amazon S3<br>DynamoDB</p></blockquote><h2 id="待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement"><a href="#待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement" class="headerlink" title="(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?"></a>(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?</h2><p>A. Modify the Redshift cluster and configure cross-region snapshots to the other region.<br>B. Modify the Redshift cluster to take snapshots of the Amazon EBS volumes each day, sharing those snapshots with the other region.<br>C. Modify the Redshift cluster and configure the backup and specify the Amazon S3 bucket in the other region.<br>D. Modify the Redshift cluster to use AWS Snowball in export mode with data delivered to the other region.</p><p>Answer: A</p><ul><li>分析：又是一道错题，Redhift备份是通过S3实现的, 所以不存在B的情况，我个人有点倾向于C，但是A确实是Redshift在快照时默认的格式，可能是更容易恢复吧，这道题需要在实际环境进行一下验证。另外国际版本的Redshift和国内的应该比国内的高很多。</li></ul><blockquote><p>问：Amazon Redshift 如何备份数据？ 如何从备份中还原我的集群？</p><p>在加载数据时，Amazon Redshift 会复制数据仓库集群内的所有数据并将其连续备份至 S3。Amazon Redshift 始终尝试维持至少三份数据（计算节点上的正本数据、副本数据和 Amazon S3 上的备份数据）。Redshift 还能将您的快照异步复制到另一个区域的 S3 中进行灾难恢复。</p><p>默认情况下，Amazon Redshift 以一天的保留期启用数据仓库群集的自动化备份。您可将其配置为 35 天之久。</p><p>免费备份存储受限于数据仓库群集中节点上的总存储大小，并仅适用于已激活的数据仓库群集。例如，如果您有 8TB 的数据仓库总存储大小，那么我们将提供最多 8TB 的备份存储而不另外收费。如果您想将备份保留期延长为超过一天，那么您可以使用 AWS 管理控制台或 Amazon Redshift API 来实现这一目的。有关自动快照的更多信息，请参阅《Amazon Redshift 管理指南》。Amazon Redshift 仅备份已更改的数据，因此大多数快照仅占用少量的免费备份存储。</p><p>如果您需要还原备份，则可以在备份保留期内访问所有自动备份。在您选择某个要还原的备份后，我们将预置一个新的数据仓库集群并将数据还原至此集群中。</p></blockquote><h2 id="A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance"><a href="#A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance" class="headerlink" title="A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?"></a>A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?</h2><p>A. Convert the database to Amazon Redshift.<br>B. Create a CloudFront distribution.<br>C. Convert the database to use EBS Provisioned IOPS.<br>D. Create one or more read replicas.</p><p>Answer: C</p><ul><li>分析：这道题我最开始选择的是D，但是评论区的一种解释有一定的道理：这个网站应用类型为电商，原题中没有很清楚说明queris and load的压力有多大，很可能我们建立了read replicas只能临时性解决问题，并不是一劳永逸的方式。并且根据<a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></li></ul><blockquote><p>因此，所有 Aurora 副本均返回相同的查询结果数据，且副本滞后时间非常短 - 通常远远少于主实例写入更新后的 100 毫秒。副本滞后因数据库更改速率而异。也就是说，在对数据库执行大量写入操作期间，您可能发现副本滞后时间变长。</p></blockquote><p>如果读副本在这个延时上，很可能对业务系统造成很大的影响。</p><h2 id="A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two"><a href="#A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two" class="headerlink" title="A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)"></a>A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)</h2><p>A. Launch Amazon EC2 instances in an Auto Scaling group behind an ELB.<br>B. Store all static files in a multi-AZ Amazon Aurora database.<br>C. Create an CloudFront distribution pointing to static content in Amazon S3.<br>D. Use Amazon Route 53 to route traffic to the correct region.<br>E. Use Amazon S3 multi-part uploads to improve upload times.</p><p>Answer: AC</p><ul><li>分析：A是很明显的，弹性伸缩，节约成本，其他几项和题干没有太多关系，所以选的C。</li></ul><h2 id="A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements"><a href="#A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements" class="headerlink" title="A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?"></a>A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?</h2><p>A. Configure the database security group to allow database traffic from the application server IP addresses.<br>B. Configure the database security group to allow database traffic from the application server security group.<br>C. Configure the database subnet network ACL to deny all inbound non-database traffic from the application-tier subnet.<br>D. Configure the database subnet network ACL to allow inbound database traffic from the application-tier subnet.</p><p>Answer: B</p><ul><li>分析：这好像又是一道错题，原给出的答案是C。首先要明确的一点是SG是工作在instance级别，NACL是在子网级别，SG默认全部Deny，NACL默认全部Allow。A不对的原因是insance在Auto Scaling里，IP地址是不固定的。D不对的原因是NACL默认全都是Allow的。其实本质上考察的是如何选择安全组还是网络防火墙的问题。不选择C的原因是因为配置NACL规则至少需要阻止和允许，而通过安全组只需要配置一条即可。但是也有一种声音认为题目中关键词restrict意味着需要deny流量。</li></ul><blockquote><p>问：VPC 中的安全组和 VPC 中的网络 ACL 有什么区别？</p><p>VPC 中的安全组指定允许传入或传出 Amazon EC2 实例的流量。网络 ACL 则在子网级别上运作，评估进出某个子网的流量。网络 ACL 可通过设置允许和拒绝规则来进行使用。Network ACL 不能筛选同一子网中实例之间的流量。此外，网络 ACL 执行无状态筛选，而安全组则执行有状态筛选。</p></blockquote><h2 id="An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture"><a href="#An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture" class="headerlink" title="An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?"></a>An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?</h2><p>A. Amazon EC2<br>B. NAT instance<br>C. ELB Classic Load Balancer<br>D. Amazon RDS</p><p>Answer: B</p><ul><li>分析：这道题竟然给出了C的答案，很意外。</li></ul><blockquote><p><a href="https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="noopener">https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></p><p>Instances in a private subnet can access the Internet without exposing their private IP address by routing their traffic through a Network Address Translation (NAT) instance in a public subnet. A NAT instance, however, can introduce a single point of failure to your VPC’s outbound traffic. This situation is depicted in the diagram below.</p></blockquote><h2 id="A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem"><a href="#A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem" class="headerlink" title="A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?"></a>A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?</h2><p>A. Change the Auto Scaling group’s scale out event to scale based on network utilization.<br>B. Create an Auto Scaling scheduled action to scale out the necessary resources at 8:30 AM every morning.<br>C. Use Reserved Instances to ensure the system has reserved the right amount of capacity for the scale-up events.<br>D. Permanently keep a steady state of instances that is needed at 9:00 AM to guarantee available resources, but leverage Spot Instances.</p><p>Answer: B</p><ul><li>分析：竟然又是一道错题，记得在AWS听过这道题的分析。原答案是A，但是可能并不是由于网络引起的访问缓慢。</li></ul><h2 id="An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met"><a href="#An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met" class="headerlink" title="An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?"></a>An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?</h2><p>A. Add a Step Scaling policy.<br>B. Add a Dynamic Scaling policy.<br>C. Add a Scheduled Scaling action.<br>D. Add Amazon EC2 Spot Instances.</p><p>Answer: B</p><ul><li>分析：又是一道争议比较大的题目，争议最大的是C选项，因为题目中有几个词在暗示时间，但是又不明确。既然现有性能上无法应对高峰访问，那么从这个角度还是通过Dynamic配置一个规则进行动态规则最为有效。所以还是选择B。</li></ul><h2 id="A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost"><a href="#A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost" class="headerlink" title="A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?"></a>A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?</h2><p>A. Create an Auto Scaling group with a minimum of one instance and a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>B. Recreate the API using Amazon API Gateway and use AWS Lambda as the service backend.<br>C. Create an Auto Scaling group with a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>D. Recreate the API using Amazon API Gateway and integrate the new API with the existing backend service.</p><p>Answer: A</p><ul><li>分析：这道题有个陷阱，Simple API，确实如果在不考虑开发的前提下B确实是最佳选项，但是重构也是要花成本的。所以我坚持选A。</li></ul><h2 id="A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement"><a href="#A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement" class="headerlink" title="A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?"></a>A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?</h2><p>A. Create EBS snapshots directly from one region to another.<br>B. Move the data to an Amazon S3 bucket and enable cross-region replication.<br>C. Create EBS snapshots and then copy them to the desired region.<br>D. Use a script to copy data from the current Amazon EBS volume to the destination Amazon EBS volume.</p><p>Answer: C</p><h2 id="A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues"><a href="#A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues" class="headerlink" title="A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?"></a>A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?</h2><p>A. Place an elastic load balancer in front of the Amazon S3 bucket to distribute the load during peak hours.<br>B. Cache the web content with Amazon CloudFront and use all Edge locations for content delivery.<br>C. Replicate the bucket in eu-west-1 and use an Amazon Route 53 failover routing policy to determine which bucket it should serve the request to.<br>D. Use an Amazon Route 53 weighted routing policy for the CloudFront domain name to distribute the GET request between CloudFront and the Amazon S3 bucket directly.</p><p>Answer: B</p><ul><li>分析：网站给出的答案竟然是D，但是B很明显是正确的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use"><a href="#A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use" class="headerlink" title="A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:"></a>A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:</h2><p>A. an external service to ping the VPN endpoint from outside the VPC.<br>B. AWS CloudTrail to monitor the endpoint.<br>C. the CloudWatch TunnelState Metric.<br>D. an AWS Lambda function that parses the VPN connection logs.</p><p>Answer: C</p><blockquote><p>Monitoring VPN Tunnels Using Amazon CloudWatch(<a href="https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html</a>)</p></blockquote><h2 id="A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance"><a href="#A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance" class="headerlink" title="A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?"></a>A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?</h2><p>A. Auto Scaling<br>B. Amazon SQS<br>C. Amazon ElastiCache<br>D. ELB Application Load Balancer</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement"><a href="#A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement" class="headerlink" title="A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?"></a>A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?</h2><p>A. Use a NAT Instance.<br>B. Use a NAT Gateway.<br>C. Use a VPC endpoint.<br>D. Use a Virtual Private Gateway.</p><p>Answer: C</p><blockquote><p>New – VPC Endpoint for Amazon S3(<a href="https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a>)</p></blockquote><h2 id="Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance"><a href="#Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance" class="headerlink" title="Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?"></a>Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?</h2><p>A. Provisioned IOPS SSD<br>B. General Purpose SSD<br>C. Cold HDD<br>D. Throughput Optimized HDD</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements" class="headerlink" title="A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?"></a>A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?</h2><p>A. EBS Provisioned IOPS SSD (io1)<br>B. EBS General Purpose SSD (gp2)<br>C. EBS Throughput Optimized HDD (st1)<br>D. EBS Cold HDD (sc1)</p><p>Answer: C</p><h2 id="A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of"><a href="#A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of" class="headerlink" title="A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of"></a>A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of</h2><p>Amazon S3. Which naming scheme should the company use?</p><p>A. Add a date as the prefix.<br>B. Add a sequential id as the suffix.<br>C. Add a hexadecimal hash as the suffix.<br>D. Add a hexadecimal hash as the prefix.</p><p>Answer: A</p><ul><li>分析：这道题的旧答案是D，不过根据最新文档档案为A。</li></ul><blockquote><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>For example, previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes.</p></blockquote><ul><li>什么是Prefix?</li></ul><blockquote><p>For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.</p></blockquote><blockquote><p>However, if the new limits are not sufficient, prefixes would need to be used. A prefix has no fixed number of characters. It is any string between a bucket name and an object name, for example:<br>bucket/folder1/sub1/file<br>bucket/folder1/sub2/file<br>bucket/1/file<br>bucket/2/file<br>Prefixes of the object ‘file’ would be: /folder1/sub1/ , /folder1/sub2/, /1/, /2/.</p></blockquote><h2 id="A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task"><a href="#A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task" class="headerlink" title="A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?"></a>A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?</h2><p>A. Enable AWS CloudTrail logging in each individual region. Repeat this for all future regions.<br>B. Enable Amazon CloudWatch logs for all AWS services across all regions and aggregate them in a single Amazon S3 bucket.<br>C. Enable AWS Trusted Advisor security checks and report all security incidents for all regions.<br>D. Enable AWS CloudTrail by creating a new trail and apply the trail to all regions.</p><p>Answer: D</p><ul><li>分析：这道题肯定使用CloudTrail，区别在于设定范围。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/" target="_blank" rel="noopener">https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/</a><br>You can now turn on a trail across all regions for your AWS account. CloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified. Additionally, when AWS launches a new region, CloudTrail will create the same trail in the new region. As a result, you will receive log files containing API activity for the new region without taking any action. Using the CloudTrail console, you can specify that a trail applies to all regions. For more details, refer to the Applying a trail to all regions section of the CloudTrail FAQ.</p></blockquote><h2 id="A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use"><a href="#A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use" class="headerlink" title="A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?"></a>A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?</h2><p>A. Amazon DynamoDB<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon EFS</p><p>Answer: C</p><ul><li>分析：这道题有点蒙人，关键词在proprietary（专有的），EFS未必支持，只有EBS才能100%满足。</li></ul><h2 id="A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner"><a href="#A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner" class="headerlink" title="A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?"></a>A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?</h2><p>A. Purchase Reserved Instances to run all containers. Use Auto Scaling groups to schedule jobs.<br>B. Host a container management service on Spot Instances. Use Reserved Instances to run Docker containers.<br>C. Use Amazon ECS orchestration and Auto Scaling groups: one with Reserve Instances, one with Spot Instances.<br>D. Use Amazon ECS to manage container orchestration. Purchase Reserved Instances to run all batch workloads at the same time.</p><ul><li>分析：绕嘴，多读两遍。主要是应对两种不同类型的任务。</li></ul><h2 id="A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO"><a href="#A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO" class="headerlink" title="A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?"></a>A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?</h2><p>A. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Customer-Provided Keys (SSE-C).<br>B. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).<br>C. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS).<br>D. Create an Amazon S3 bucket to store the reports and use Amazon s3 versioning with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).</p><p>Answer: C</p><ul><li>分析：用S3 + KMS服务。</li></ul><h2 id="争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case"><a href="#争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case" class="headerlink" title="(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?"></a>(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?</h2><p>A. Amazon S3<br>B. Amazon RDS<br>C. Amazon RedShift<br>D. AWS Storage Gateway</p><p>Answer: A</p><ul><li>分析：这道题的争议点在答案B，因为S3提供eventual consistency for overwirte PUTS and DELETES，可能会导致无法获取最新数据的问题。不确定该问题是否会考到，暂时没有找到更合理的解释。</li></ul><blockquote><p>Amazon S3 Data Consistency Model(<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html</a>)<br>Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one caveat. The caveat is that if you make a HEAD or GET request to the key name (to find if the object exists) before creating the object, Amazon S3 provides eventual consistency for read-after-write.</p><p>Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions.</p><p>Updates to a single key are atomic. For example, if you PUT to an existing key, a subsequent read might return the old data or the updated data, but it never returns corrupted or partial data.</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a"><a href="#A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a" class="headerlink" title="A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a"></a>A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a</h2><p>DynamoDB table. Which AWS-managed service is the BEST fit to insert the item?</p><p>A. Lambda@Edge<br>B. AWS Lambda<br>C. Amazon API Gateway<br>D. Amazon EC2 instances</p><p>Answer: B</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/</a></li></ul><h2 id="An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used"><a href="#An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used" class="headerlink" title="An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?"></a>An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?</h2><p>A. Amazon SQS<br>B. Amazon SNS<br>C. Amazon ECS<br>D. AWS STS</p><p>Answer: A</p><blockquote><p>问：Amazon SNS 与 Amazon SQS 有何不同？</p><p>Amazon Simple Queue Service (SQS) 和 Amazon SNS 都是 AWS 中的消息收发服务，但为开发人员提供了不同的优势。Amazon SNS 允许应用程序通过“推送”机制向多个订阅者发送时间关键型消息，并且无需定期检查或“轮询”更新。Amazon SQS 是一种供分布式应用程序使用的消息队列服务，通过轮询模式交换消息，可用于分离收发组件。Amazon SQS 使应用程序的分布式组件可以灵活地收发消息，并且不要求每个组件同时可用。</p><p>一种常见的模式是使用 SNS 将消息发布到 Amazon SQS 队列，进而以可靠的方式将消息异步发送到一个或多个系统组件。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement"><a href="#A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement" class="headerlink" title="A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?"></a>A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?</h2><p>A. Enable SSL on Amazon EC2 instances.<br>B. Encrypt Amazon EBS volumes on Amazon EC2 instances.<br>C. Enable server-side encryption on Amazon S3.<br>D. Encrypt Amazon EC2 Instance Storage.</p><p>Answer: B</p><ul><li>New EBS Encryption for Additional Data Protection(<a href="https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/</a>)</li></ul><h2 id="争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two"><a href="#争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two" class="headerlink" title="(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)"></a>(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)</h2><p>A. Host the website on an Amazon EC2 instance with ELB and Auto Scaling, and map a Route 53 alias record to the ELB endpoint.<br>B. Host the website using AWS Elastic Beanstalk, and map a Route 53 alias record to the Beanstalk stack.<br>C. Host the website on an Amazon EC2 instance, and map a Route 53 alias record to the public IP address of the Amazon EC2 instance.<br>D. Serve the website from an Amazon S3 bucket, and map a Route 53 alias record to the website endpoint.<br>E. Create a Route 53 hosted zone, and set the NS records of the domain to use Route 53 name servers.</p><p>Answer: DE</p><ul><li>分析：又是一道争议非常大的题，原来的答案是CD，从cost-effective的角度说C确实不够经济。参考AWS如何构建静态网站的最佳实践：<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html</a></li></ul><h2 id="争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale"><a href="#争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale" class="headerlink" title="(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?"></a>(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?</h2><p>A. Create an Amazon Kinesis Firehouse delivery stream to store the data in Amazon S3.<br>B. Create an Auto Scaling group of Amazon EC2 servers behind ELBs to write the data into Amazon RDS.<br>C. Create an Amazon SQS queue, and have the machines write to the queue.<br>D. Create an Amazon EC2 server farm behind an ELB to store the data in Amazon EBS Cold HDD volumes.</p><p>Answer: A</p><ul><li>分析：很奇怪为什么原有答案给出B，这道题明显是暗指实时计算Kinesis服务。</li></ul><h2 id="A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database"><a href="#A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database" class="headerlink" title="A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?"></a>A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?</h2><p>A. Amazon S3<br>B. Amazon Aurora<br>C. Amazon DynamoDB<br>D. Amazon Redshift</p><p>Answer: B</p><ul><li>分析：很明显需要关系型数据库。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost"><a href="#A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost" class="headerlink" title="A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?"></a>A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?</h2><p>A. Add rules to the security groups in each account.<br>B. Establish a VPC Peering connection between accounts.<br>C. Configure Direct Connect in each account.<br>D. Add a NAT Gateway to the data account.</p><p>Answer: B</p><ul><li>分析：B的方案是成本最低的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3"><a href="#A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3" class="headerlink" title="A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?"></a>A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?</h2><p>A. Upload directly to S3 using a pre-signed URL.<br>B. Upload to a second bucket, and have a Lambda event copy the image to the primary bucket.<br>C. Upload to a separate Auto Scaling group of servers behind an ELB Classic Load Balancer, and have them write to the Amazon S3 bucket.<br>D. Expand the web server fleet with Spot Instances to provide the resources to handle the images.</p><p>Answer: C</p><ul><li>分析：A选项相较于题目中描述的并没有本质区别。</li></ul><blockquote><p>A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object.</p><p>All objects and buckets by default are private. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don’t require them to have AWS security credentials or permissions. When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The presigned URLs are valid only for the specified duration.</p></blockquote><h2 id="A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal"><a href="#A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal" class="headerlink" title="A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal."></a>A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal.</h2><p>A. Create VPC flow logs on the subnet.<br>B. Enable source destination check on private Amazon EC2 instances.<br>C. Enable AWS CloudTrail logging and specify an Amazon S3 bucket for storing log files.<br>D. Create an Amazon CloudWatch log to capture packet information.</p><p>Answer: A</p><ul><li>分析：启动VPC流表日志, CloudTrail没有此能力</li></ul><h2 id="A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two"><a href="#A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two" class="headerlink" title="A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)"></a>A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)</h2><p>A. Replace the Amazon EC2 reverse proxy with an ELB internal Classic Load Balancer.<br>B. Add Auto Scaling to the Amazon EC2 backend fleet.<br>C. Add Auto Scaling to the Amazon EC2 reverse proxy layer.<br>D. Use t2 burstable instance types for the backend fleet.<br>E. Replace both the frontend and reverse proxy layers with an ELB Application Load Balancer.</p><p>Answer: BE</p><ul><li>分析：又是一道错题，原答案是AB。根据题目分析，出现瓶颈的地方来自于两处：反向代理和后端服务。后端服务的扩展没有什么争议，所以B很明显是正确的。最大的争议来自于是使用什么方式替代目前成为瓶颈的反向代理EC2。原题里反向代理EC2作为content-based routing，那么问题的关键就是CLB、ELB谁能做content-based routing了。根据目前最新的内容，所以需要使用Application Load Balancer来提供content-based routing了。</li></ul><blockquote><p>There are three types of Elastic Load Balancer (ELB) on AWS:</p><p>Classic Load Balancer (CLB) – this is the oldest of the three and provides basic load balancing at both layer 4 and layer 7.</p><p>Application Load Balancer (ALB) – layer 7 load balancer that routes connections based on the content of the request.</p><p>Network Load Balancer (NLB) – layer 4 load balancer that routes connections based on IP protocol data.</p><p>Note: The Classic Load Balancer may be phased out over time and Amazon are promoting the ALB and NLB for most use cases within VPC.</p></blockquote><blockquote><p>Introducing Amazon EC2 Fleet<br>Posted On: May 2, 2018</p><p>Amazon EC2 Fleet is a new feature that simplifies the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, Availability Zones and across On-Demand, Amazon EC2 Reserved Instances (RI) and Amazon EC2 Spot purchase models. With a single API call, now you can provision capacity across EC2 instance types and across purchase models to achieve desired scale, performance and cost.</p><p>You can create an EC2 Fleet specification defining target capacity, which EC2 instance types work for you, and how much of your fleet should be filled using On-Demand, RI and Spot purchase models. You can also indicate whether EC2 Fleet should take into account the number of cores and amount of memory on each instance or consider all instances equal when scaling. EC2 Fleet then launches the lowest price combination of instances to meet the target capacity based on these preferences. EC2 fleet enables you to use multiple instance types and purchase models to provision capacity cost effectively, with just a few clicks in the AWS Management Console.</p><p>Amazon EC2 Fleet is now available in all public Regions. To learn more about simplifying the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, AWS Availability Zones and across On-Demand, RI and Spot purchase models using Amazon EC2 Fleet, visit this blog. To learn more about Amazon EC2 pricing models, visit this page.</p></blockquote><ul><li>EC2 Fleet – Manage Thousands of On-Demand and Spot Instances with One Request(<a href="https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/</a>)</li><li>New – Advanced Request Routing for AWS Application Load Balancers(<a href="https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/</a>)</li><li>ELASTIC LOAD BALANCING(<a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a>)</li></ul><h2 id="A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers"><a href="#A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers" class="headerlink" title="A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?"></a>A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?</h2><p>A. Migrate the database to Amazon RDS.<br>B. Set up DNS failover to a statistic website.<br>C. Use Auto Scaling to keep up with the demand.<br>D. Use both a SQL and a NoSQL database in the design.</p><p>Answer: C</p><ul><li>分析：明天就上线了，改啥都来不及了，还是价格Auto scaling策略吧。</li></ul><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway"><a href="#A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway" class="headerlink" title="A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?"></a>A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?</h2><p>A. Remove source/destination check on NAT instances.<br>B. Launch a NAT gateway in each Availability Zone.<br>C. Use a mix of NAT instances and NAT gateway.<br>D. Add an ELB Application Load Balancer in front of NAT gateway.</p><p>Answer: B</p><ul><li>NAT 实例与 NAT 网关的比较(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html</a>)</li></ul><blockquote><p>每个 NAT 网关都在特定可用区中创建，并在该可用区进行冗余实施。您可以在一个可用区中创建的 NAT 网关存在数量限制。有关更多信息，请参阅 Amazon VPC 限制。</p><p>注意:<br>如果您在多个可用区中拥有资源，并且它们共享一个 NAT 网关，则在该 NAT 网关的可用区不可用时，其他可用区中的资源将无法访问 Internet。要创建不依赖于可用区的架构，请在每个可用区中都创建一个 NAT 网关，并配置路由以确保这些资源使用自身可用区中的 NAT 网关。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance"><a href="#A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance" class="headerlink" title="A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?"></a>A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?</h2><p>A. Randomize a key name prefix.<br>B. Store the event data in separate buckets.<br>C. Randomize the key name suffix.<br>D. Use Amazon S3 Transfer Acceleration.</p><p>Answer: A</p><ul><li>分析：这道题和上面有道题类似，目前S3建议使用时间作为prefix，原来是建议使用hash方式。</li><li>最佳实践设计模式：优化 Amazon S3 性能(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a>)</li></ul><blockquote><p>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><h2 id="A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload"><a href="#A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload" class="headerlink" title="A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?"></a>A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?</h2><p>A. Amazon EC2<br>B. Amazon Kinesis Firehose<br>C. Amazon EBS<br>D. Amazon API Gateway</p><p>Answer: B</p><ul><li>分析：又是一道争议题，大部分给出的答案是B，就应用场景上看Kinesis更适合实时计算场景。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required"><a href="#A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required" class="headerlink" title="A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?"></a>A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?</h2><p>A. Attach an Elastic IP address to each Amazon EC2 instance and add a route from the private subnet to the public subnet.<br>B. Launch a NAT gateway in the public subnet and add a route to it from the private subnet.<br>C. Launch Amazon EC2 instances in the public subnet and change the security group to allow outbound traffic on port 80.<br>D. Launch a NAT gateway in the private subnet and deploy a NAT instance in the private subnet.</p><p>Answer: B</p><h2 id="An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task"><a href="#An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task" class="headerlink" title="An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?"></a>An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?</h2><p>A. Create a private subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.<br>B. Create a private subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>C. Create a public subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>D. Create a public subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.</p><p>Answer: B</p><ul><li>分析：答案给出的是C，有多种方式证明这个答案是错误的。</li><li>How do I connect a public-facing load balancer to EC2 instances that have private IP addresses?(<a href="https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/" target="_blank" rel="noopener">https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a>)</li><li>AWS Best Practices: 3-Tier Infrastructure(<a href="https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure" target="_blank" rel="noopener">https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure</a>)</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application"><a href="#A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application" class="headerlink" title="A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?"></a>A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?</h2><p>A. EBS Provisioned IOPS SSD<br>B. EBS General Purpose SSD<br>C. EBS Cold HDD<br>D. EBS Throughput Optimized HDD</p><p>Answer: D</p><h2 id="争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement"><a href="#争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement" class="headerlink" title="(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?"></a>(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?</h2><p>A. Create an IAM role that allows access from the corporate network to Amazon S3.<br>B. Configure a proxy on Amazon EC2 and use an Amazon S3 VPC endpoint.<br>C. Use Amazon API Gateway to do IP whitelisting.<br>D. Configure IP whitelisting on the customer’s gateway.</p><p>Answer: B</p><ul><li>分析：争议较大的一道题，这里采用了这个解释：<a href="https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf" target="_blank" rel="noopener">https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf</a></li></ul><h2 id="A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements" class="headerlink" title="A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?"></a>A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?</h2><p>A. An Amazon EBS Provisioned IOPS volume<br>B. An Amazon EBS General Purpose volume<br>C. An Amazon EBS Magnetic volume<br>D. An Amazon EC2 Instance Store</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way"><a href="#A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way" class="headerlink" title="A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?"></a>A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?</h2><p>A. Create a Lambda function for each individual environment.<br>B. Use Amazon DynamoDB to store environmental variables.<br>C. Use encrypted AWS Lambda environmental variables.<br>D. Implement a dedicated Lambda function for distributing variables.</p><p>Answer: C</p><h2 id="A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage"><a href="#A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage" class="headerlink" title="A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?"></a>A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?</h2><p>A. Store the archive in Amazon S3 Reduced Redundancy Storage.<br>B. Store the archive in Amazon Glacier and use standard retrieval for all content.<br>C. Store the archive in Amazon Glacier and pay the additional charge for expedited retrieval when needed.<br>D. Store the archive in Amazon S3 with a lifecycle policy to move this to S3 Infrequent Access after 30 days.</p><p>Answer: C</p><blockquote><p>问：从 Amazon S3 Glacier 检索数据如何收费？</p><p>从 Amazon S3 Glacier 检索数据的方式有三种：加急、标准和批量检索。每种方式具有不同的每 GB 检索费和每存档请求费（即请求一个存档计为一个请求）。有关不同 AWS 区域的 S3 Glacier 定价的详细信息，请访问 Amazon S3 Glacier 定价页面。</p><p>定价标准： <a href="https://amazonaws-china.com/cn/glacier/pricing/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/glacier/pricing/</a></p></blockquote><h2 id="A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements"><a href="#A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements" class="headerlink" title="A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?"></a>A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?</h2><p>A. Assign Elastic IP addresses to the database instances.<br>B. Allow Internet traffic on the private subnet through the network ACL.<br>C. Use a NAT Gateway.<br>D. Use an egress-only Internet Gateway.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized"><a href="#A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized" class="headerlink" title="A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?"></a>A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?</h2><p>A. Create an IAM access and secret key, and store it in the Lambda function.<br>B. Create an IAM role to the Lambda function with permissions to list all Amazon RDS instances.<br>C. Create an IAM role to Amazon RDS with permissions to list all Amazon RDS instances.<br>D. Create an IAM access and secret key, and store it in an encrypted RDS database.</p><p>Answer: B</p><blockquote><p>教程：配置 Lambda 函数以访问 Amazon VPC 中的 Amazon RDS</p><p>打开 IAM 控制台中的“角色”页面。<br>选择 Create role (创建角色)。<br>创建具有以下属性的角色。<br>Trusted entity (可信任的实体) – Lambda.<br>权限 – AWSLambdaVPCAccessExecutionRole。<br>角色名称 (角色名称) – lambda-vpc-role。<br>AWSLambdaVPCAccessExecutionRole 具有函数管理与 VPC 的网络连接所需的权限。</p></blockquote><h2 id="A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements" class="headerlink" title="A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?"></a>A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?</h2><p>A. Change the Amazon EC2 instant types.<br>B. Change the EBS volume type to Provisioned IOPS.<br>C. Stop the Amazon EC2 instance and provision IOPS for the EBS volume.<br>D. Enable an API Gateway to change the endpoints for the Amazon EC2 instances.</p><p>Answer: B</p><blockquote><p>打开 Amazon EC2 控制台 <a href="https://console.aws.amazon.com/ec2/。" target="_blank" rel="noopener">https://console.aws.amazon.com/ec2/。</a></p><p>选择 Volumes，选择要修改的卷，然后依次选择 Actions、Modify Volume。</p><p>Modify Volume 窗口显示卷 ID 和卷的当前配置，包括类型、大小和 IOPS。您可以在单个操作中更改任何或所有这些设置。设置新的配置值，如下所述：</p><p>要修改类型，请为 Volume Type 选择一个值。</p><p>要修改大小，请为 Size 输入一个允许的整数值。</p><p>如果选择预配置 IOPS SSD (io1) 作为卷类型，请为 IOPS 输入一个允许的整数值。</p><p>完成更改卷设置后，请选择 Modify (修改)。当系统提示您确认时，请选择 Yes。</p><p>在扩展卷的文件系统以使用新的存储容量之前，修改卷大小没有实际效果。有关更多信息，请参阅调整卷大小后扩展 Linux 文件系统。</p></blockquote><h2 id="A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata"><a href="#A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata" class="headerlink" title="A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?"></a>A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon Kinesis<br>D. Amazon EFC</p><p>Answer: B</p><ul><li>Building and Maintaining an Amazon S3 Metadata Index without Servers(<a href="https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers" target="_blank" rel="noopener">https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers</a>)</li></ul><blockquote><p>In this post, I walk through an approach for building such an index using Amazon DynamoDB and AWS Lambda. With these technologies, you can create a high performance, low-cost index that scales and remains highly available without the need to maintain traditional servers.</p></blockquote><h2 id="An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available"><a href="#An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available" class="headerlink" title="An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?"></a>An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?</h2><p>A. Move some Amazon EC2 instances to a subnet in a different way(different AZ).<br>B. Move the website to Amazon S3.<br>C. Change the ELB to an Application Load Balancer.<br>D. Move some Amazon EC2 instances to a subnet in the same Availability Zone.</p><p>Answer: A</p><ul><li>分析：这道题的选项A可能是写错了，根据评论区是different AZ，那么选择A就比较容易理解了。评论区有一种声音是选择C，但是从高可用性上讲，C选项并没有实质的价值。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two"><a href="#A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two" class="headerlink" title="A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)"></a>A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)</h2><p>A. Store an access key on the Amazon EC2 instance with rights to the Dynamo DB table.<br>B. Attach an IAM user to the Amazon EC2 instance.<br>C. Create an IAM role with permissions to write to the DynamoDB table.<br>D. Attach an IAM role to the Amazon EC2 instance.<br>E. Attach an IAM policy to the Amazon EC2 instance.</p><p>Answer: CD</p><ul><li>分析：AWS一向重视安全性，所以更推荐使用STS方式进行接口调用</li></ul><h2 id="争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements"><a href="#争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements" class="headerlink" title="(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:"></a>(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:</h2><p>1 They get an alert when the requests per second go over 50,000<br>2 They get an alert when latency goes over 5 seconds<br>3 They can validate how many times a day users call the API requesting highly-sensitive data<br>Which combination of steps does the Architect need to take to satisfy these operational requirements? (Select two.)</p><p>A. Ensure that CloudTrail is enabled.<br>B. Create a custom CloudWatch metric to monitor the API for data access.<br>C. Configure CloudWatch alarms for any metrics the support team requires.<br>D. Ensure that detailed monitoring for the EC2 instances is enabled.<br>E. Create an application to export and save CloudWatch metrics for longer term trending analysis.</p><p>Answer: BC</p><ul><li>分析：原题给出的答案是BD，但是EC2的详细监控其实并没有包含API级别的监控，ELB的监控才包含了API访问的监控。</li></ul><h2 id="争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement"><a href="#争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement" class="headerlink" title="(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?"></a>(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?</h2><p>A. Use Amazon CloudWatch to monitor utilization.<br>B. Use Amazon API Gateway to monitor availability.<br>C. Use an Amazon Elastic Load Balancer.<br>D. Use Amazon Route 53 health checks.</p><p>Answer: A</p><ul><li>分析：不同网站给出不同答案，原网站给出的答案是C，但是从题目分析关键词是the Architect needs to remove it，所以看起来A更合理一些。但是ELB增加health check之后应该可以自动的将不可用节点移除掉。</li></ul><h2 id="A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications"><a href="#A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications" class="headerlink" title="A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?"></a>A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?</h2><p>A. Create a network ACL on the web server’s subnet, and allow HTTPS inbound and MySQL outbound. Place both database and web servers on the same subnet.<br>B. Open an HTTPS port on the security group for web servers and set the source to 0.0.0.0/0. Open the MySQL port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.<br>C. Create a network ACL on the web server’s subnet, and allow HTTPS inbound, and specify the source as 0.0.0.0/0. Create a network ACL on a database subnet, allow MySQL port inbound for web servers, and deny all outbound traffic.<br>D. Open the MySQL port on the security group for web servers and set the source to 0.0.0.0/0. Open the HTTPS port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.</p><p>Answer: B</p><h2 id="Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx"><a href="#Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx" class="headerlink" title="Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?"></a>Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?</h2><p>A. AWS Lambda<br>B. Auto Scaling<br>C. AWS Elastic Beanstalk<br>D. Elastic Load Balancing</p><p>Answer: C</p><blockquote><p>AWS Elastic Beanstalk 是一项易于使用的服务，用于在熟悉的服务器（例如 Apache 、Nginx、Passenger 和 IIS ）上部署和扩展使用 Java、.NET、PHP、Node.js、Python、Ruby、GO 和 Docker 开发的 Web 应用程序和服务。<br>您只需上传代码，Elastic Beanstalk 即可自动处理包括容量预配置、负载均衡、自动扩展和应用程序运行状况监控在内的部署工作。同时，您能够完全控制为应用程序提供支持的 AWS 资源，并可以随时访问底层资源。<br>Elastic Beanstalk 不额外收费 – 您只需为存储和运行应用程序所需的 AWS 资源付费。</p></blockquote><h2 id="An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances"><a href="#An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances" class="headerlink" title="An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?"></a>An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?</h2><p>A. Amazon SQS<br>B. Auto Scaling group<br>C. Amazon EC2 security group<br>D. Amazon ELB</p><p>Answer: D</p><ul><li>分析：这道题原网站给出答案是B，但是明显应该是D</li></ul><h2 id="A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload"><a href="#A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload" class="headerlink" title="A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?"></a>A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?</h2><p>A. ELB Classic Load Balancer<br>B. Amazon Route 53 DNS<br>C. ELB Application Load Balancer<br>D. Amazon CloudFront</p><p>Answer: C</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page</a></li></ul><h2 id="A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements"><a href="#A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements" class="headerlink" title="A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?"></a>A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?</h2><p>A. Store data in a filesystem backed by Amazon Elastic File System (EFS).<br>B. Store data in Amazon S3 and use a third-party solution to expose Amazon S3 as a filesystem to the database server.<br>C. Store data in Amazon Dynamo DB and emulate relational database semantics.<br>D. Stripe data across multiple Amazon EBS volumes using RAID 0.</p><p>Answer: D</p><h2 id="A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with"><a href="#A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with" class="headerlink" title="A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:"></a>A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:</h2><p>A. an egress-only internet gateway<br>B. a NAT gateway<br>C. a custom NAT instance<br>D. a VPC endpoint</p><p>Answer: A</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a><blockquote><p>An egress-only Internet gateway. This enables instances in the private subnet to send requests to the Internet over IPv6 (for example, for software updates). An egress-only Internet gateway is necessary if you want instances in the private subnet to be able to initiate communication with the Internet over IPv6. For more information, see Egress-Only Internet Gateways.</p></blockquote></li></ul><h2 id="A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application"><a href="#A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application" class="headerlink" title="A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?"></a>A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?</h2><p>A. Create a read replica of the database.<br>B. Provision a new RDS instance as a secondary master.<br>C. Configure the database to be in multiple regions.<br>D. Increase the number of provisioned storage IOPS.</p><p>Answer: A</p><ul><li>分析：原有网站给出的答案是B，明显是A，搞这么复杂干啥</li></ul><h2 id="A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement"><a href="#A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement" class="headerlink" title="A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?"></a>A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?</h2><p>A. Amazon EFS<br>B. Amazon RDS<br>C. AWS Storage Gateway<br>D. Amazon S3</p><p>Answer: D</p><h2 id="A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution"><a href="#A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution" class="headerlink" title="A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?"></a>A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?</h2><p>A. Associate AWS KMS metrics with Amazon CloudWatch.<br>B. Use AWS CloudTrail to log AWS KMS key usage.<br>C. Deploy a monitoring agent on the RDS instances.<br>D. Poll AWS KMS periodically with a scheduled job.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform"><a href="#A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform" class="headerlink" title="A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?"></a>A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?</h2><p>A. Scheduled Reserved Instances<br>B. Convertible Reserved Instances<br>C. Standard Reserved Instances<br>D. Spot Instances</p><p>Answer: C</p><ul><li>分析：根据题目要求7*24小时不停机，所以需要排除A和D两个选项, B选项在这个场景下并不需要，所以选C</li></ul><blockquote><p>Exchanging Convertible Reserved Instances: You can exchange one or more Convertible Reserved Instances for another Convertible Reserved Instance with a different configuration, including instance family, operating system, and tenancy. There are no limits to how many times you perform an exchange, as long as the target Convertible Reserved Instance is of an equal or higher value than the Convertible Reserved Instances that you are exchanging.</p></blockquote><h2 id="A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements"><a href="#A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements" class="headerlink" title="A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?"></a>A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?</h2><p>A. Create an Amazon S3 bucket and store all of the documents in this bucket.<br>B. Create an Amazon EBS volume and allow multiple users to mount that volume to their EC2 instance(s).<br>C. Use Amazon Glacier to store all of the documents.<br>D. Create an Amazon Elastic File System (Amazon EFS) to store and share the documents.</p><p>Answer: D</p><ul><li>分析：需要文件接口，并且同时访问，那么只有EFS能够满足</li></ul><h2 id="A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two"><a href="#A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two" class="headerlink" title="A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)"></a>A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)</h2><p>A. Auto Scaling group<br>B. AWS CloudTrail<br>C. ELB Classic Load Balancer<br>D. Amazon DynamoDB<br>E. Amazon ElastiCache</p><p>Answer: AC</p><ul><li>分析：原网站给出的答案是AE，E显然没什么用对于目标</li></ul><h2 id="A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application"><a href="#A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application" class="headerlink" title="A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?"></a>A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?</h2><p>A. Use AWS Key Management Service and move the encrypted data to Amazon S3.<br>B. Use an application-specific encryption API with AWS server-side encryption.<br>C. Use encrypted EBS storage volumes with AWS-managed keys.<br>D. Use third-party tools to encrypt the EBS data volumes with Key Management Service Bring Your Own Keys.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements"><a href="#A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements" class="headerlink" title="A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?"></a>A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?</h2><p>A. Store the AWS Access Key ID/Secret Access Key combination in software comments.<br>B. Assign an IAM user to the Amazon EC2 instance.<br>C. Assign an IAM role to the Amazon EC2 instance.<br>D. Enable multi-factor authentication for the AWS root account.</p><p>Answer: C</p><h2 id="An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage"><a href="#An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage" class="headerlink" title="An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?"></a>An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?</h2><p>A. Migrate the data on the Amazon EBS volume to an SSD-backed volume.<br>B. Change the EC2 instance type to one with EC2 instance store volumes.<br>C. Migrate the data on the EBS volume to provisioned IOPS SSD (io1).<br>D. Change the EC2 instance type to one with burstable performance.</p><p>Answer: C</p><ul><li>分析：原有答案给出的是D，但是从性能角度看C明显是正确的</li></ul><h2 id="A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user"><a href="#A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user" class="headerlink" title="A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?"></a>A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?</h2><p>A. Amazon Kinesis Stream<br>B. Amazon SQS standard queue<br>C. Amazon SQS FIFO queue<br>D. AWS CloudTrail trail</p><p>Answer: A</p><ul><li>Create real-time clickstream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena(<a href="https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/</a>)</li><li>Amazon Kinesis – Real-Time Processing of Streaming Big Data(<a href="https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/</a>)</li></ul><h2 id="A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend"><a href="#A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend" class="headerlink" title="A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?"></a>A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?</h2><p>A. Amazon Aurora<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon RDS MySQL</p><p>Answer: A</p><ul><li>分析：A和D的区别没有找到合适的解释，Aurora的扩展性更好，而且是AWS云原生的。</li></ul><h2 id="争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two"><a href="#争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two" class="headerlink" title="(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)"></a>(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)</h2><p>A. Security group rule that allows inbound Internet traffic for port 443.<br>B. Security group rule that denies all inbound Internet traffic except port 443.<br>C. Network ACL rule that allows port 443 inbound and all ports outbound for Internet traffic.<br>D. Security group rule that allows Internet traffic for port 443 in both inbound and outbound.<br>E. Network ACL rule that allows port 443 for both inbound and outbound for all Internet traffic.</p><p>Answer: AC</p><ul><li>分析：答案给出的是AE，根据Network ACL的描述，默认情况为白名单，是无状态性的，返回的端口不会自动允许，所以需要C选项打开所有返回的端口。</li><li>临时端口(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a>)</li></ul><blockquote><p>临时端口<br>上一个部分中的网络 ACL 实例使用了临时端口范围 32768-65535。但是，您可能需要根据自己使用的或作为通信目标的客户端的类型为网络 ACL 使用不同的范围。<br>发起请求的客户端会选择临时端口范围。根据客户端的操作系统不同，范围也随之更改。<br>许多 Linux 内核（包括 Amazon Linux 内核）使用端口 32768-61000。<br>生成自 Elastic Load Balancing 的请求使用端口 1024-65535。<br>Windows 操作系统通过 Windows Server 2003 使用端口 1025-5000。<br>Windows Server 2008 及更高版本使用端口 49152-65535。<br>NAT 网关使用端口 1024 - 65535。<br>AWS Lambda 函数使用端口 1024-65535。<br>例如，如果一个来自 Internet 上的 Windows XP 客户端的请求到达您的 VPC 中的 Web 服务器，则您的网络 ACL 必须有相应的出站规则，以支持目标为端口 1025-5000 的数据流。<br>如果您的 VPC 中的一个实例是发起请求的客户端，则您的网络 ACL 必须有入站规则来支持发送到实例类型（Amazon Linux、Windows Server 2008 等）特有的临时端口的数据流。<br>在实际中，为使不同客户端类型可以启动流量进入您 VPC 中的公有实例，您可以开放临时端口 1024-65535。但是，您也可以在 ACL 中添加规则以拒绝任何在此范围内的来自恶意端口的数据流。请务必将拒绝 规则放在表的较前端，先于开放一系列临时端口的允许 规则。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through"><a href="#A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through" class="headerlink" title="A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:"></a>A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:</h2><p>A. VPC peering connection.<br>B. NAT gateway<br>C. VPC endpoint<br>D. AWS Direct Connect</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement"><a href="#A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement" class="headerlink" title="A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?"></a>A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?</h2><p>A. Amazon EFS<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon ElastiCache</p><p>Answer: B</p><ul><li>分析：这道题给出的答案竟然是A，不明白这个网站是不是专门负责坑人的。object-based storage system，很明显是S3.</li></ul><h2 id="A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case"><a href="#A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case" class="headerlink" title="A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?"></a>A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Cached Volumes</p><p>Answer: B</p><h2 id="A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload"><a href="#A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload" class="headerlink" title="A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?"></a>A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?</h2><p>A. Amazon EC2 instance storage<br>B. Amazon EBS General Purpose SSD (gp2) storage<br>C. Amazon S3<br>D. Amazon EBS Provision IOPS SSD (io1) storage</p><p>Answer: B</p><ul><li>分析：原始答案给出的是C，一个legcy application为什么会用S3呢？这可是需要应用改造的。</li></ul><h2 id="争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements"><a href="#争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements" class="headerlink" title="(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?"></a>(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?</h2><p>A. Use AWS IAM authorization and add least-privileged permissions to each respective IAM role.<br>B. Use an API Gateway custom authorizer to invoke an AWS Lambda function to validate each user’s identity.<br>C. Use Amazon Cognito user pools to provide built-in user management.<br>D. Use Amazon Cognito user pools to integrate with external identity providers.</p><p>Answer: B</p><ul><li>分析：正义点在答案D，参考链接：<a href="https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html" target="_blank" rel="noopener">https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html</a></li></ul><h2 id="An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution"><a href="#An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution" class="headerlink" title="An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?"></a>An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?</h2><p>A. Redesign the website to use Amazon API Gateway, and use AWS Lambda to deliver content.<br>B. Add server instances using Amazon EC2 and use Amazon Route 53 with a failover routing policy.<br>C. Serve the images and videos via an Amazon CloudFront distribution created using the news site as the origin.<br>D. Use Amazon ElasticCache for Redis for caching and reducing the load requests from the origin.</p><p>Answer: C</p><h2 id="A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process"><a href="#A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process" class="headerlink" title="A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?"></a>A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?</h2><p>A. Use the AWS CLI to create queues using AWS IAM Access Keys.<br>B. Write a script to create the Amazon SQS queue using AWS Lambda.<br>C. Use AWS Elastic Beanstalk to automatically create the Amazon SQS queues.<br>D. Use AWS CloudFormation Templates to manage the Amazon SQS queue creation.</p><p>Answer: D</p><ul><li>教程：创建 Amazon SQS 队列(<a href="https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation</a>)</li></ul><h2 id="争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone"><a href="#争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone" class="headerlink" title="(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?"></a>(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?</h2><p>A. One public subnet for the load balancer tier, one public subnet for the front-end tier, and one private subnet for the backend tier.<br>B. One shared public subnet for all tiers of the application.<br>C. One public subnet for the load balancer tier and one shared private subnet for the application tiers.<br>D. One shared private subnet for all tiers of the application.</p><p>Answer: C</p><ul><li>分析：答案给出的是A，但是题目中说道only the front-end load balancer should be exposed to the internet，所以A答案中为front-end tier一个公网subnet有点多余了</li></ul><h2 id="A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application"><a href="#A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application" class="headerlink" title="A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?"></a>A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?</h2><p>A. An Amazon EC2 instance store local SSD volume.<br>B. An Amazon EBS provisioned IOPS SSD volume.<br>C. An Amazon EBS throughput optimized HDD volume.<br>D. An Amazon EBS general purpose SSD volume.</p><p>Answer: C</p><ul><li>分析：这道题需要高顺序I/O和低成本，显然C正确</li></ul><h2 id="Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy"><a href="#Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy" class="headerlink" title="Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?"></a>Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?</h2><p>A. Using security groups that reference the security groups of the other application<br>B. Using security groups that reference the application server’s IP addresses<br>C. Using Network Access Control Lists to allow/deny traffic based on application IP addresses<br>D. Migrating the applications to separate subnets from each other</p><p>Answer: A</p><h2 id="Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two"><a href="#Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two" class="headerlink" title="Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)"></a>Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)</h2><p>A. Amazon SNS<br>B. AWS STS<br>C. Amazon SQS<br>D. Amazon Route 53<br>E. AWS Glue</p><p>Answer: AC</p><ul><li>分析：根据题目是要解决消息的问题，消息服务有两种SNS和SQS，因为题目里并没有说消息模式，所以这两种也许能解决需求。</li></ul><h2 id="A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model"><a href="#A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model" class="headerlink" title="A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?"></a>A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?</h2><p>A. Amazon EC2 Spot Instances<br>B. On-Demand Amazon EC2 Instances<br>C. Scheduled Reserved Instances<br>D. Dedicated Amazon EC2 Instances</p><p>Answer: C</p><h2 id="A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available"><a href="#A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available" class="headerlink" title="A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?"></a>A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?</h2><p>A. Change the Amazon EC2 instance to compute optimized.<br>B. Launch a second Amazon EC2 instance to monitor the health of the first.<br>C. Trigger a Lambda function when a new object is uploaded.<br>D. Initially copy the images to an attached Amazon EBS volume.</p><p>Answer: C</p><ul><li>分析：Lambda的触发器很适合做这个</li></ul><h2 id="An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met"><a href="#An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met" class="headerlink" title="An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?"></a>An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?</h2><p>A. Configure a NAT gateway in a public subnet and route all traffic to Amazon Kinesis through the NAT gateway.<br>B. Configure a gateway VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>C. Configure an interface VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>D. Configure an AWS Direct Connect private virtual interface for Kinesis and route all traffic to Kinesis through the virtual interface.</p><p>Answer: C</p><ul><li>分析：误选了B，从题目说是Kinesis需要读取EC2的数据，所以应该是在VPC interface上建立一个endpoint</li></ul><h2 id="A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements" class="headerlink" title="A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?"></a>A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?</h2><p>A. Amazon S3<br>B. Amazon Glacier<br>C. Amazon EFS<br>D. AWS Storage Gateway</p><p>Answer: B</p><blockquote><p>Data stored in Amazon Glacier is immutable, meaning that after an archive is created it cannot be updated. This ensures that data such as compliance and regulatory records cannot be altered after they have been archived.</p></blockquote><ul><li>分析：从这道题我们可以看出，考试的时候选中文的重要性，不会因为一个单词意思不明确导致整个题目判断失误，关键词是immutalbe，不可改变的</li></ul><h2 id="争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted"><a href="#争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted" class="headerlink" title="(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?"></a>(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?</h2><p>A. Set a CORS configuration.<br>B. Set a bucket policy to encrypt all Amazon S3 objects.<br>C. Enable default encryption on the bucket.<br>D. Set permission for users.</p><p>Answer: B</p><ul><li>分析：争议点在于答案C，从界面操作上看BC好像是在做同一件事情</li><li>如何为 Amazon S3 存储桶启用默认加密？(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html</a>)</li><li>How to Prevent Uploads of Unencrypted Objects to Amazon S3(<a href="https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a>)</li></ul><h2 id="An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path"><a href="#An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path" class="headerlink" title="An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?"></a>An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?</h2><p>A. AWS Application Load Balancer<br>B. Amazon CloudFront<br>C. Amazon Classic Load Balancer<br>D. Amazon Route 53</p><p>Answer: A</p><h2 id="A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend"><a href="#A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend" class="headerlink" title="A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?"></a>A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?</h2><p>A. Amazon RDS<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon S3</p><p>Answer: C</p><h2 id="争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows"><a href="#争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows" class="headerlink" title="(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:"></a>(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:</h2><p>✑ Limit access to users origination from the corporate network.<br>✑ Web servers cannot have SSH access directly from the Internet.<br>✑ Web servers reside in a private subnet.<br>Which combination of steps must the Architect complete to meet these requirements? (Choose two.)</p><p>A. Create a bastion host that authenticates users against the corporate directory.<br>B. Create a bastion host with security group rules that only allow traffic from the corporate network.<br>C. Attach an IAM role to the bastion host with relevant permissions.<br>D. Configure the web servers’ security group to allow SSH traffic from a bastion host.<br>E. Deny all SSH traffic from the corporate network in the inbound network ACL.</p><p>Answer: BD</p><ul><li>分析：原有答案给出的是AC，感觉并不能完全解决该问题</li><li>How to Record SSH Sessions Established Through a Bastion Host(<a href="https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/</a>)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考链接：&lt;a href=&quot;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。&lt;/p&gt;
&lt;p&gt;由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>使用阿里云函数计算构建小程序</title>
    <link href="http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/"/>
    <id>http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/</id>
    <published>2019-12-19T15:19:16.000Z</published>
    <updated>2020-01-14T08:51:56.536Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a>1、需求</h1><p>在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。</p><a id="more"></a><img src="/images/blogs/2019-12-19/architecture.png" class=""><p>在原有设计中，使用了Python Flask提供WEB界面，后台使用Celery异步的将用户请求发送至钉钉中，之后采用轮询方式监控审批工单状态，当工单完成审批后，将生成好的License发送至客户提供的邮箱中。</p><p>实现的效果：</p><img src="/images/blogs/2019-12-19/UI.jpeg" class=""><p>这种方式虽然可以满足需求，但是在使用过程中也发现有如下痛点：<br>1、由于对于可用性要求比较高，所以将整套应用以容器化方式部署在云主机上，程序高可用性依赖于底层的平台，基于成本考虑并没有在多可用区进行部署。<br>2、当业务变化时，需要专人将容器从本地容器库上传后进行更新，更新速度慢，敏捷性低。<br>3、需要专人对操作系统层进行维护，并且由于该云主机还运行了其他程序，所以管控上也存在安全风险。</p><p>基于以上出现的问题，决定对原有二维码程序进行重构，并重新部署在阿里云函数计算服务上。<br>1、第一阶段的改造主要是将二维码扫描程序移植到函数计算服务中。<br>2、第二阶段的改造主要是将发送二维码程序改造为函数计算服务，使用钉钉流程接口中的Callback方法调用该接口，在审批结束后触发发送License流程。</p><h1 id="2、函数计算服务——无服务，零运维"><a href="#2、函数计算服务——无服务，零运维" class="headerlink" title="2、函数计算服务——无服务，零运维"></a>2、函数计算服务——无服务，零运维</h1><p>最早接触Serverless的雏形是在2011年开发Cloud Foundry项目时，当时留下一个非常深的印象就是把写好的应用直接上传就完成了部署、扩展等。但是当时Cloud Foundry有一个非常大的局限性，受限于几种开发语言和框架。记得当时的Cloud Foundry只支持Node.js、Python、Java、PHP、Ruby on Rails等，脱离了这个范围则就无法支持，所以当时我其实对这种形态的应用场景存在很大的疑问。<br>这种困惑直到2013年Docker的出现而逐步解开，Docker的出现让开发语言、框架不再是问题，巧妙的解决了Cloud Foundry上述局限性。但是Docker毕竟只是一种工具形态，还不能称得上是平台，紧接着k8s的出现弥补了这一空白，使得Docker从游击队变成了正规军。<br>在这个发展过程中我们不难看出，软件领域发展出现了重大变革，从服务器为王逐渐演进到应用为王的阶段。如果说虚拟化改变了整个物理机的格局，那么无服务化的出现则改变了整个软件开发行业。<br>由于网上各种文档太多了，这里就不对Serverless基本概念进行介绍了，借用一张图说明下。另外还有一点，我们从这里面看到IT行业里的某些岗位，注定要消失的，比如传统运维。</p><img src="/images/blogs/2019-12-19/compare.png" class=""><h1 id="3、应用架构"><a href="#3、应用架构" class="headerlink" title="3、应用架构"></a>3、应用架构</h1><p>整个架构上，分为两个函数计算服务完成：</p><ul><li>二维码前端：主要用于显示页面，并承担HTTP请求转发代理的角色，将请求转发至二维码后端，发给钉钉，采用HTTP触发器，允许公网访问。</li><li>二维码后端：用于将用户请求发送给钉钉，该部分服务仍然采用HTTP触发器，不同于前端，该服务是不允许公网直接访问的，但是需要配置NAT网关，通过网关访问钉钉，实现固定IP访问钉钉的效果。</li></ul><img src="/images/blogs/2019-12-19/new_architecture.png" class=""><p>从逻辑上讲，整个应用并不复杂，但是在实际使用时遇到最大的问题来自钉钉白名单。由于函数服务对外连接的IP并不固定，所以无法在钉钉中添加，那么就要求函数服务对外连接的IP地址一定要固定。社区中提供的方法主要分为：</p><ul><li>ECI（运行Nginx充当Proxy），优势是便宜，劣势是高可用性需要自己维护</li><li>NAT网关，优势是高可用性，劣势是比ECI贵</li></ul><h1 id="4、构建过程"><a href="#4、构建过程" class="headerlink" title="4、构建过程"></a>4、构建过程</h1><p>由于篇幅原因，这里只介绍关键步骤。</p><h2 id="4-1-构建模板"><a href="#4-1-构建模板" class="headerlink" title="4.1 构建模板"></a>4.1 构建模板</h2><p>为了后续管理和扩展方便，选用了阿里云函数计算中使用flask-web模板进行构建，同时可以将前端静态文件模板存放于项目下（出于统一管理的需要，也可以存放于阿里云的OSS中，作为静态网站发布）。</p><p>前端我们使用flask-web作为模板创建函数，后端我们直接采用最简单的HTTP函数。</p><img src="/images/blogs/2019-12-19/create_function_template.png" class=""><p>函数入口配置，及触发器配置：</p><img src="/images/blogs/2019-12-19/create_function.png" class=""><p>服务配置，包含公网访问权限，专有网络配置，日志配置，权限配置。</p><ul><li>前端服务需要公网访问权限，不需要专有网络配置，需要的权限为：AliyunLogFullAccess。</li><li>后端服务不需要公网访问权限，但是需要配置好的NAT映射的专有网络，由于函数服务在北京2区中在cn-beijing-c和cn-beijing-f，所以在新建交换机时需要使用这两个区。还需要选择安全组，由于出方向并没有明确禁止，所以不需要特别的安全组规则设定。需要的权限为：AliyunLogFullAccess/AliyunECSNetworkInterfaceManagementAccess。</li></ul><p>配置好后，通过导出功能，分别下载前端和后端代码和配置，在本地进行开发调试。</p><img src="/images/blogs/2019-12-19/export_function.png" class=""><h2 id="4-2-前端开发"><a href="#4-2-前端开发" class="headerlink" title="4.2 前端开发"></a>4.2 前端开发</h2><p>我们的前端采用Vue.js进行开发，在main.py同级新建templates目录。Vue编译好的静态文件可以放入该目录中，后续Flask会加载该文件作为入口文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── templates</span><br><span class="line">│   ├── index.html</span><br><span class="line">│   ├── static</span><br><span class="line">├── main.py</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># main.py sample</span><br><span class="line">from flask import render_template</span><br><span class="line"></span><br><span class="line">LICENSE_URL &#x3D; &quot;https:&#x2F;&#x2F;[x](https:&#x2F;&#x2F;.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license).cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license&quot;</span><br><span class="line"></span><br><span class="line">@app.route(&#39;&#x2F;qr_code&#39;, methods&#x3D;[&#39;GET&#39;])</span><br><span class="line">def index():</span><br><span class="line">      return render_template(&#39;index.html&#39;)</span><br><span class="line"></span><br><span class="line">      @app.route(&#39;&#x2F;qr_code&#x2F;license&#39;, methods&#x3D;[&#39;POST&#39;])</span><br><span class="line">      def create():</span><br><span class="line">            payload &#x3D; request.json</span><br><span class="line">                resp &#x3D; requests.post(LICENSE_URL,</span><br><span class="line">                                                 json&#x3D;payload,</span><br><span class="line">                                                                              headers&#x3D;DEFAULT_HEADERS)</span><br><span class="line">                return make_response(resp.text, resp.status_code)</span><br></pre></td></tr></table></figure><h2 id="4-3-后端开发"><a href="#4-3-后端开发" class="headerlink" title="4.3 后端开发"></a>4.3 后端开发</h2><p>后端的开发较为简单，实现一个函数支持POST请求，将转发的结果发送至钉钉即可。</p><h2 id="4-4-本地调试"><a href="#4-4-本地调试" class="headerlink" title="4.4 本地调试"></a>4.4 本地调试</h2><p>阿里云在本地开发时提供了fun应用部署和开发工具，详细使用方法见：<a href="https://help.aliyun.com/document_detail/64204.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/64204.html</a>。</p><h3 id="安装fun"><a href="#安装fun" class="headerlink" title="安装fun"></a>安装fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry [https:&#x2F;&#x2F;registry.npm.taobao.org](https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;) --global</span><br><span class="line">npm config set disturl [https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist](https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist) --global</span><br><span class="line"></span><br><span class="line">npm install @alicloud&#x2F;fun -g</span><br></pre></td></tr></table></figure><h3 id="配置fun"><a href="#配置fun" class="headerlink" title="配置fun"></a>配置fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fun config</span><br><span class="line"></span><br><span class="line">(venv) [root@ray-dev test_func]# fun config</span><br><span class="line">? Aliyun Account ID xxxxxxxx</span><br><span class="line">? Aliyun Access Key ID ***********r5Qd</span><br><span class="line">? Aliyun Access Key Secret ***********kCCi</span><br><span class="line">? Default region name cn-beijing</span><br><span class="line">? The timeout in seconds for each SDK client invoking 10</span><br><span class="line">? The maximum number of retries for each SDK client 3</span><br><span class="line">? Allow to anonymously report usage statistics to improve the tool over time? Yes</span><br></pre></td></tr></table></figure><h3 id="Http-Trigger本地运行"><a href="#Http-Trigger本地运行" class="headerlink" title="Http Trigger本地运行"></a>Http Trigger本地运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun local start</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun deploy</span><br></pre></td></tr></table></figure><h2 id="4-5-配置域名解析"><a href="#4-5-配置域名解析" class="headerlink" title="4.5 配置域名解析"></a>4.5 配置域名解析</h2><p>部署完成后有一点需要特别注意，必须要绑定域名，并且设定必要的路由。如果在没有绑定域名的情况下，服务端会为 response header中强制添加 content-disposition: attachment字段，此字段会使得返回结果在浏览器中以附件的方式打开。（<a href="https://www.alibabacloud.com/help/zh/doc-detail/56103.htm" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/56103.htm</a>）</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><ul><li>灵活使用函数计算对开发成本和运行成本具有“双降”的效果</li><li>函数计算除了Http Trigger外，还包含了Event Trigger。Event Trigger中包含了连接各个服务之间的作用，在一些服务衔接上的作用越来越明显</li><li>函数计算在线开发时比较麻烦，并且查看日志不方便，所以尽量在本地开发好在上传的方式</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1、需求&quot;&gt;&lt;a href=&quot;#1、需求&quot; class=&quot;headerlink&quot; title=&quot;1、需求&quot;&gt;&lt;/a&gt;1、需求&lt;/h1&gt;&lt;p&gt;在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="阿里云" scheme="http://sunqi.me/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/tags/Cloud-Computing/"/>
    
      <category term="Serverless" scheme="http://sunqi.me/tags/Serverless/"/>
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Newton国内代码贡献</title>
    <link href="http://sunqi.me/2016/09/30/contricution-in-newton/"/>
    <id>http://sunqi.me/2016/09/30/contricution-in-newton/</id>
    <published>2016-09-30T17:00:49.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。</p><p>这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。</p><p>最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。</p><p>还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><p>OpenStack Mitaka深度解读请见：<a href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>下面是最近三个版本Release的详细对比：</p><img src="/images/blogs/contribution-in-newton-projects.png" class="center"><p>让我们来关注这次Release中的三个新项目：</p><h3 id="Panko-计量服务事件消息存储"><a href="#Panko-计量服务事件消息存储" class="headerlink" title="Panko(计量服务事件消息存储)"></a>Panko(计量服务事件消息存储)</h3><p>Panko是计量模块中的一部分，主要是为了计量模块提供事件消息存储，众所周知，在上一个OpenStack Release中，Ceilometer被一分为三，分别为aodh(告警服务)/Gnocchi(基于时间的数据库服务)/Ceilometer，为了解决当前Ceilometer中存在的性能问题，提高更好的扩展性。</p><p>现在Panko的文档并不是很丰富，如果有需要了解更多详细内容的，可以关注Developer的文档：<a href="http://docs.openstack.org/developer/panko/" target="_blank" rel="noopener">http://docs.openstack.org/developer/panko/</a></p><h3 id="Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务"><a href="#Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务" class="headerlink" title="Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)"></a>Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)</h3><p>Vitrage是一个OpenStack RCA(Root Cause Analysis)服务，用于组织、分析和扩展OpenStack的告警和事件，在真正的问题发生前找到根本原因。</p><p>众所周知，OpenStack平台最大的优势来自于架构的可扩展性，这也是OpenStack能够在基础架构曾一枝独秀的重要原因。分布式架构最大的优势在于扩展，但是过于灵活的扩展性为运维带来的极大的困难，所以Vitrage的出现在一定程度上缓解了OpenStack运维上的痛点。</p><p>我们来简单看一下他的架构，更多详细的介绍请查看WIKI：<a href="https://wiki.openstack.org/wiki/Vitrage" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Vitrage</a></p><img src="/images/blogs/contribution-in-newton-vitrage-architecture.png" class="center"><h3 id="Watcher-OpenStack平台优化服务"><a href="#Watcher-OpenStack平台优化服务" class="headerlink" title="Watcher(OpenStack平台优化服务)"></a>Watcher(OpenStack平台优化服务)</h3><p>从名字上看，我们并不能理解这个模块的具体左右，我们通过文档中用户应用场景来了解一下Watcher的作用：</p><p>作为一名云平台的管理员在云平台使用一段时间后，想根据一些物理特性对云平台虚拟机的分布进行重新平衡，例如服务器的温度、电源的状态等信息，那么这时候就可以通过watcher，利用Nova虚拟机的在线迁移对整个数据中心云平台的虚拟机进行一些优化处理，从而达到某种平衡。我认为这其实类似于VMWare的DRS功能。</p><p>当然Watcher还有更多的应用场景，更多详细的介绍请查看：<a href="https://wiki.openstack.org/wiki/Watcher" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Watcher</a></p><p>我们来简单看一下他的架构，更多架构方面的详细的介绍请查看：<a href="http://docs.openstack.org/developer/watcher/architecture.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/watcher/architecture.html</a></p><img src="/images/blogs/contribution-in-newton-watcher-architecture.svg" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits和blueprints的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司和贡献者来说，都有所上升，这也不难理解，随着OpenStack模块增加，势必涉及更多的领域，所以更多的公司加入了这个生态圈。</p><img src="/images/blogs/contribution-in-newton-companies-contributors.png" class="center"><p>从commits角度进行分析，传统几大好强几乎没有变化，日本的Fujitsu在commits上挤掉了华为，进入了前十名的位置。模块方面，核心模块的贡献仍然位于前十名，也说明是应用最多的模块，所以才会不断的发现问题。本次统计的总项目数量为629个，可能stackalytics在统计策略上有所调整。</p><img src="/images/blogs/contribution-in-newton-companies-modules-commits.png" class="center"><p>单从commits角度统计其实有失偏颇，真正能够体现公司在OpenStack实力的指标应该是Blueprints。我认为完成Blueprints至少具备三个必要条件：英语要好、在社区有一定的影响力、架构设计能力。这些都是需要不断在社区进行积累和沉淀的。</p><p>本次release周期内，能够完成Blueprints的公司为64个，国内的华为和九州云均进入前10名，排名比较靠前的国内企业还包括：Easystack、中兴。</p><p>完成Blueprints最多的仍然是核心模块，排在第二名的是kolla，看来在上一个周期中，kolla项目的活跃程度是较高的。</p><img src="/images/blogs/contribution-in-newton-companies-modules-blueprints.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完总体的状况，再来关注一些国内的贡献情况，与去年相比，今年上榜的国内企业达到了21家，创历年之最，比去年的15家企业整整多了7家，并且我们发现在这些新增企业中大部分都是提供企业服务的公司，说明OpenStack在国内的企业级市场开始站稳脚跟。下面我们来做一个详细的分析：</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在最近的三个版本连续对社区有贡献的企业包括：华为，Easystack，九州云，海云捷迅，华三，Unitedstack，乐视，中国移动和北京休伦科技(Huron)。</p><p>本次爬升最快的企业：中兴，从108位攀升至13位。</p><p>本次统计新增的7家企业：云途腾(t2cloud)，大唐高鸿数据(GohighSec)，华云数据，烽火通信，爱数，北京国电通，云英，中国银联，赛特斯信息。</p><p>本次排名中OpenStack的直接用户：中国移动和中国银联。中国移动更是参选了OpenStack SuperAward的评比，预祝他们能顺利当选。</p><img src="/images/blogs/contribution-in-newton-china-companies.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>我们再来从人员投入来分析贡献情况一下：</p><ul><li>投入人数最多的仍然是华为，有65名工程师贡献了本次的commits</li><li>中兴无疑是本次人员投入增长最快的，从6名工程师一下子扩张到61名，也是唯一能和华为抗衡的</li><li>超过2位数人员投入的包括，Easystack，九州云和Unitedstack，另外海云捷迅有9人，华三有8人，中国移动有7人参与社区贡献</li></ul><img src="/images/blogs/contribution-in-newton-companies-effort.png" class="center"><h3 id="模块贡献分析"><a href="#模块贡献分析" class="headerlink" title="模块贡献分析"></a>模块贡献分析</h3><p>从模块贡献角度来分析，国内企业的贡献仍然没有出现一个统一的趋势，与Mitaka Release相比，贡献涉及模块的总量从192个增加至Newton Release的246个，一方面说明OpenStack本身模块的增加，也说明国内企业使用或开发OpenStack在方向上的多元化。</p><p>从贡献的模块来看，华为主导的dargonflow高居榜首，紧随其后的是手册和clients两个项目，随后的贡献集中在OpenStack的核心模块，与Docker相关的几个模块中。Kolla项目无疑是最近关注的热点，随着Docker的快速发展，OpenStack和Docker不断碰撞出新的火花。</p><img src="/images/blogs/contribution-in-newton-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这个问题仍然是比较敏感的问题，只有每个公司的CEO能够回答这个问题，这里面我从融资的角度来回顾一下2015至2016年之间在OpenStack领域发生过什么。</p><ul><li>2015年9月17日，英特尔投资部门披露了此前投资的中国8家公司名单。投资总额达6700万美元，领域覆盖了新材料、智能设备、物联网、云服务等领域。其中包含九州云和海云捷迅两家OpenStack企业。(<a href="http://tech.qq.com/a/20150917/038604.htm" target="_blank" rel="noopener">http://tech.qq.com/a/20150917/038604.htm</a>)</li><li>2015年10月17日，中国最大的独立公有云提供商UCloud和全球领先的OpenStack厂商Mirantis在东京的OpenStack峰会上正式宣布成立合资公司UMCloud，以求更好的在中国做OpenStack。(<a href="http://www.doit.com.cn/article/1027290510.html" target="_blank" rel="noopener">http://www.doit.com.cn/article/1027290510.html</a>)</li><li>2015年12月16日，UnitedStack有云宣布完成C轮融资，该轮融资由思科和红杉资本投资，具体数额未公布(<a href="http://www.infoq.com/cn/news/2015/12/unitedstack-financing" target="_blank" rel="noopener">http://www.infoq.com/cn/news/2015/12/unitedstack-financing</a>)</li><li>2016年5月20日，云途腾(T2Cloud)完成A轮3650万融资(<a href="http://iimedia.cn/42262.html" target="_blank" rel="noopener">http://iimedia.cn/42262.html</a>)</li><li>2016年9月21日，腾讯与海云捷迅昨日下午在京共同宣布达成战略投资合作关系，海云捷迅接受腾讯的战略投资(<a href="http://www.36dsj.com/archives/62353" target="_blank" rel="noopener">http://www.36dsj.com/archives/62353</a>)</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回到开篇的那句话，OpenStack贡献量只能反应中国企业对于开源项目的重视程度，无法反应真实的用户需求。VMWare花了将近10年的时间教育用户，说服用户把应用从物理机迁移至虚拟机。OpenStack从2011年出生到现在也仅仅短短的5年，可见OpenStack还有很长的路要走。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。&lt;/p&gt;
&lt;p&gt;这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。&lt;/p&gt;
&lt;p&gt;最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。&lt;/p&gt;
&lt;p&gt;还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Mitaka深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>使用国内源部署Ceph</title>
    <link href="http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/"/>
    <id>http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/</id>
    <published>2016-06-19T01:25:37.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。</p><a id="more"></a><h2 id="关于国内源"><a href="#关于国内源" class="headerlink" title="关于国内源"></a>关于国内源</h2><p>根据Ceph中国社区的<a href="http://bbs.ceph.org.cn/?/page/image" target="_blank" rel="noopener">统计</a>，国内已经有四家网站开始同步Ceph源，分别是：</p><ul><li>网易镜像源<a href="http://mirrors.163.com/ceph" target="_blank" rel="noopener">http://mirrors.163.com/ceph</a></li><li>阿里镜像源<a href="http://mirrors.aliyun.com/ceph" target="_blank" rel="noopener">http://mirrors.aliyun.com/ceph</a></li><li>中科大镜像源<a href="http://mirrors.ustc.edu.cn/ceph" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ceph</a></li><li>宝德镜像源 <a href="http://mirrors.plcloud.com/ceph" target="_blank" rel="noopener">http://mirrors.plcloud.com/ceph</a></li></ul><h2 id="国内源分析"><a href="#国内源分析" class="headerlink" title="国内源分析"></a>国内源分析</h2><p>以163为例，是以天为单位向回同步Ceph源，完全可以满足大多数场景的需求，同步的源也非常全，包含了calamari，debian和rpm的全部源，最近几个版本的源也能从中找到。</p><h2 id="安装指定版本的Ceph"><a href="#安装指定版本的Ceph" class="headerlink" title="安装指定版本的Ceph"></a>安装指定版本的Ceph</h2><p>这里以安装最新版本的Jewel为例，由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-jewel&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br></pre></td></tr></table></figure><p>之后的过程就没有任何区别了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create monitor node</span><br><span class="line">ceph-deploy new node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Software Installation</span><br><span class="line">ceph-deploy install deploy node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Gather keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"># Ceph deploy parepare and activate</span><br><span class="line">ceph-deploy osd prepare node1:&#x2F;dev&#x2F;sdb node2:&#x2F;dev&#x2F;sdb node3:&#x2F;dev&#x2F;sdb</span><br><span class="line">ceph-deploy osd activate node1:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 node2:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1 node3:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-2</span><br><span class="line"></span><br><span class="line"># Make 3 copies by default</span><br><span class="line">echo &quot;osd pool default size &#x3D; 3&quot; | tee -a $HOME&#x2F;ceph.conf</span><br><span class="line"></span><br><span class="line"># Copy admin keys and configuration files</span><br><span class="line">ceph-deploy --overwrite-conf admin deploy node1 node2 node3</span><br></pre></td></tr></table></figure><p>这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Docker部署Ceph</title>
    <link href="http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/"/>
    <id>http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/</id>
    <published>2016-06-12T23:20:50.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是根据Sébastien Han的<a href="https://www.youtube.com/watch?v=FUSTjTBA8f8&feature=youtu.be" target="_blank" rel="noopener">演示视频</a>进行整理的，对过程中有问题的部分进行了修复。</p><p>Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。</p><a id="more"></a><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><ul><li>至少需要三台虚拟机或者物理机，每台虚拟机或者物理机至少有两块硬盘，这里我是在一台物理机上用vagrant模拟出三台CentOS 6.6虚拟机进行的实验</li><li>三台虚拟机需要安装docker，本文附带Docker加速方案</li><li>获取ceph/daemon镜像</li></ul><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><img src="/images/blogs/bootstrap-ceph-docker-flow.png" class="center"><h2 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h2><p>主机名和集群的对应关系如下：</p><ul><li>node1 -&gt; 192.168.33.11</li><li>node2 -&gt; 192.168.33.12</li><li>node3 -&gt; 192.168.33.13</li></ul><img src="/images/blogs/bootstrap-ceph-docker-architecture.png" class="center"><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="安装Docker，下载镜像"><a href="#安装Docker，下载镜像" class="headerlink" title="安装Docker，下载镜像"></a>安装Docker，下载镜像</h3><p>国内安装Dcoker还是速度很慢的，这里推荐使用daocloud的加速方案。不但docker安装速度提高了，pull镜像的速度也大幅度提高。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh</span><br></pre></td></tr></table></figure><p>我是在CentOS系统上进行的测试，将docker加入自动启动，并启动docker，接下来pull ceph daemon镜像，该镜像包含了所有的ceph服务和entrypoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chkconfig docker</span><br><span class="line">service docker start</span><br><span class="line">docker pull ceph&#x2F;daemon</span><br></pre></td></tr></table></figure><h3 id="启动第一个Monitor"><a href="#启动第一个Monitor" class="headerlink" title="启动第一个Monitor"></a>启动第一个Monitor</h3><p>在node1上启动第一个Monitor，注意，如果你的环境中IP和我不同，请修改MON_IP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.11 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>验证一下效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES</span><br><span class="line">7babea544ef1        ceph&#x2F;daemon         &quot;&#x2F;entrypoint.sh mon&quot;   3 seconds ago       Up 2 seconds                            backstabbing_brattain</span><br></pre></td></tr></table></figure><p>查看一下集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec 7babea544ef1 ceph -s</span><br></pre></td></tr></table></figure><p>当前集群状态，能看到当前已经有一个mon启动起来了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e1: 1 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 2, quorum 0 node1.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="复制配置文件"><a href="#复制配置文件" class="headerlink" title="复制配置文件"></a>复制配置文件</h3><p>接下来需要将node1的配置文件复制到node2和node3上，复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。这些配置文件非常重要，如果没有这些配置文件的存在，我们在其他节点启动新的docker ceph daemon的时候会被认为是一个新的集群。<br>我们在node1执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssh root@node2 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node2:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node2:&#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line"></span><br><span class="line">ssh root@node3 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node3:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node3:&#x2F;var&#x2F;lib&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="启动第二个和第三个Monitor"><a href="#启动第二个和第三个Monitor" class="headerlink" title="启动第二个和第三个Monitor"></a>启动第二个和第三个Monitor</h3><p>在node2上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.12 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node3上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.13 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node1上查看集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="启动OSD的遇到的问题"><a href="#启动OSD的遇到的问题" class="headerlink" title="启动OSD的遇到的问题"></a>启动OSD的遇到的问题</h3><p>按照原视频的介绍的方法，启动OSD可以直接指定某个分区，然后用osd_ceph_disk作为启动ceph/daemon的参数，之后docker镜像会自动的进行分区等动作。但是经过实际验证却发现在mkjournal创建错误，OSD无法启动。</p><p>经过和社区确认，发现这个Bug在之前版本中得到过修复，但是之后的版本又出现了。根据社区的建议使用jewel版本的ceph daemon进行了再次验证，发现问题依旧，所以这里介绍的方法只能退而求其次，采用手动方式分区、格式化，之后用osd_directory启动ceph/daemon。</p><p>这是github上的相关讨论：<a href="https://github.com/ceph/ceph-docker/issues/171" target="_blank" rel="noopener">https://github.com/ceph/ceph-docker/issues/171</a></p><p>这是用osd_ceph_disk方式启动后的错误日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd --cluster ceph --mkfs --mkkey -i 4 --monmap &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;activate.monmap --osd-data &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG --osd-journal &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal --osd-uuid 89e240e1-17e9-4d6c-8d4f-f1a3e0278b91 --keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;keyring --setuser ceph --setgroup disk</span><br><span class="line">2016-06-12 23:37:26.180610 7f8889654800 -1 filestore(&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG) mkjournal error creating journal on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal: (2) No such file or directory</span><br><span class="line">2016-06-12 23:37:26.180752 7f8889654800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -2</span><br><span class="line">2016-06-12 23:37:26.180918 7f8889654800 -1 ** ERROR: error creating empty object store in &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG: (2) No such file or directory</span><br><span class="line">mount_activate: Failed to activate</span><br><span class="line">unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br><span class="line">command_check_call: Running command: &#x2F;bin&#x2F;umount -- &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br></pre></td></tr></table></figure><h3 id="启动OSD"><a href="#启动OSD" class="headerlink" title="启动OSD"></a>启动OSD</h3><p>第一步先进行分区和格式化，这里只给出node1的操作方式，其他两个节点的方式类似。</p><p>先来安装必要的工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y parted xfsprogs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# parted &#x2F;dev&#x2F;sdb</span><br><span class="line">GNU Parted 2.1</span><br><span class="line">Using &#x2F;dev&#x2F;sdb</span><br><span class="line">(parted) mklabel</span><br><span class="line">New disk label type? gpt</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition name?  []? &quot;Linux filesystem&quot;</span><br><span class="line">File system type?  [ext2]? xfs</span><br><span class="line">Start? 0G</span><br><span class="line">End? 107GB</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name              Flags</span><br><span class="line"> 1      1049kB  107GB  107GB               Linux filesystem</span><br><span class="line"></span><br><span class="line">(parted) quit</span><br></pre></td></tr></table></figure><p>格式化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# mkfs.xfs &#x2F;dev&#x2F;sdb1</span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdb1              isize&#x3D;256    agcount&#x3D;4, agsize&#x3D;6553472 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;0        finobt&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;0</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br></pre></td></tr></table></figure><p>我们把目录在node1上进行挂载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;ceph&#x2F;sdb</span><br><span class="line">mount &#x2F;dev&#x2F;sdb1 &#x2F;ceph&#x2F;sdb</span><br></pre></td></tr></table></figure><p>最后启动OSD，这里最重要的就是把我们刚刚挂载好的OSD的实际路径透传给Docker内部的/var/lib/ceph/osd，如果每个节点有多个OSD的情况下，只需要在Host上映射到不同的目录，启动Docker的时候变更和/var/lib/ceph/osd的映射关系即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -v &#x2F;dev&#x2F;:&#x2F;dev&#x2F; \</span><br><span class="line">    -v &#x2F;ceph&#x2F;sdb:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd \</span><br><span class="line">    --privileged&#x3D;true \</span><br><span class="line">    ceph&#x2F;daemon osd_directory</span><br></pre></td></tr></table></figure><p>按照同样的方法，将node2和node3的OSD也加入到集群，最终的效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        64 pgs degraded</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        64 pgs undersized</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e13: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        4551 MB used, 11306 MB &#x2F; 16720 MB avail</span><br><span class="line">              64 active+undersized+degraded</span><br></pre></td></tr></table></figure><h3 id="创建MDS"><a href="#创建MDS" class="headerlink" title="创建MDS"></a>创建MDS</h3><p>创建好基本的环境，其他的就容易了很多，下面来启动MDS。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -e CEPHFS_CREATE&#x3D;1 \</span><br><span class="line">    ceph&#x2F;daemon mds</span><br></pre></td></tr></table></figure><h3 id="启动RGW，并且映射80端口"><a href="#启动RGW，并且映射80端口" class="headerlink" title="启动RGW，并且映射80端口"></a>启动RGW，并且映射80端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    -p 80:80 \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    ceph&#x2F;daemon rgw</span><br></pre></td></tr></table></figure><h3 id="最终的集群状态"><a href="#最终的集群状态" class="headerlink" title="最终的集群状态"></a>最终的集群状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        48 pgs stuck inactive</span><br><span class="line">        48 pgs stuck unclean</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> mdsmap e5: 1&#x2F;1&#x2F;1 up &#123;0&#x3D;mds-node1.docker.com&#x3D;up:active&#125;</span><br><span class="line"> osdmap e25: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v38: 128 pgs, 9 pools, 588 bytes data, 11 objects</span><br><span class="line">        6791 MB used, 16996 MB &#x2F; 25081 MB avail</span><br><span class="line">              80 active+clean</span><br><span class="line">              45 creating</span><br><span class="line">               3 creating+activating</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Docker中部署Ceph并没有想象中的那么顺利，社区的版本中仍然有Bug需要解决。</p><p>Docker作为一种快捷的部署方式，的确可以大幅度提高Ceph的部署效率，提高扩展的速度。但是从另一个角度我们应该注意到，随着Docker的引入也改变了Ceph的运维方式，比如在OSD增减的时候，需要到容器中对Ceph集群进行维护。再比如配置文件变更后的重启问题等。</p><p>但是无论如何，我相信这些问题都会得到完美的解决，用Docker部署Ceph作为一种新的尝试，值得推广。<br>之后还会为大家带来，如何使用Ansible结合Docker更快速的部署Ceph集群，敬请期待。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是根据Sébastien Han的&lt;a href=&quot;https://www.youtube.com/watch?v=FUSTjTBA8f8&amp;feature=youtu.be&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;演示视频&lt;/a&gt;进行整理的，对过程中有问题的部分进行了修复。&lt;/p&gt;
&lt;p&gt;Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Ceph/Docker/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Mitaka国内代码贡献</title>
    <link href="http://sunqi.me/2016/04/07/contribution-in-mitaka/"/>
    <id>http://sunqi.me/2016/04/07/contribution-in-mitaka/</id>
    <published>2016-04-07T07:19:39.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。</p><p>去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。</p><p>这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。</p><p>对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>在Big Tent公布之后，OpenStack的项目被分为Core Projects和Big Tent Projects。</p><img src="/images/blogs/contribution-in-mitaka-big-tent.jpg" class="center"><p>让我们来看一下在Mitaka版本中，多了哪些新项目。</p><ul><li>几个与Docker相关的项目被发布出来，magnum, senlin, solum</li><li>数据备份容灾的项目：freezer</li><li>计费的项目：cloudkitty</li><li>NFV相关的项目：tracker</li><li>监控相关的项目：monasca</li></ul><p>关于这些新项目的一些介绍，我将放在另外一篇博客里，敬请关注。</p><img src="/images/blogs/contribution-in-mitaka-projects.png" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司数量来看，Mitaka版本略有下降，但是参与的人数多了100多人。</p><img src="/images/blogs/contribution-in-mitaka-companies-contributors.png" class="center"><p>整个社区的公司贡献排名上没有明显的变化，传统的几大豪强仍然霸占公司排名的前十位，华为表现依然强劲，是中国区唯一能进入前十名的公司。</p><p>在模块方面，整体统计的绝大部分比例已经被others所占据，说明在Big Tent计划下，OpenStack正在朝更多元化的方向演进。在Mitaka排名前十位的项目中，fuel相关的两个项目都进入了前十，说明fuel在OpenStack部署的地位已经越来越重要了。同时，核心项目中的nova，neutron，cinder项目仍然在前十名的范围内，贡献量基本保持不变。值得一提的是，在Mitaka统计的项目数量已经从Liberty的708个增长到了829个，可见在短短的6个月内，OpenStack社区的蓬勃发展。</p><img src="/images/blogs/contribution-in-mitaka-companies-modules.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完了整体统计，我们再回到国内，因为已经有文章做了我在Liberty时候的分析，所以这里我换个角度来看国内的社区贡献，首先是统计排名的变化。</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在Liberty中，有13家国内企业为社区做了贡献，在Mitaka中这个数量增加到了15家企业，这里简单的将这些企业做了一下分类：</p><ul><li>互联网用户：乐视、新浪、网易</li><li>电信用户：中国移动</li><li>传统IT服务商：华为、中兴、华三</li><li>私有云服务商：Easystack、九州云、海云捷迅、北京有云、麒麟云、UMCloud、象云、Huron(休伦科技)</li></ul><img src="/images/blogs/contribution-in-mitaka-china-companies.png" class="center"><h3 id="行业分析"><a href="#行业分析" class="headerlink" title="行业分析"></a>行业分析</h3><p>通过行业的分析我们可以看出，国内的主要贡献仍然来自私有云服务商和传统IT服务商，换言之来自于以OpenStack提供产品或者服务的公司。厂商们贡献的目的很明确，主要为了展示自身在开源项目中的积累和专家形象。而用户的贡献主要来自平时在使用OpenStack时候遇到Bug，就是在实际应用过程中出现的问题。</p><img src="/images/blogs/contribution-in-mitaka-china-by-industry.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>单纯的社区贡献排名的比较仅仅是一个维度，下面我们来看一下各个公司的人员投入情况：</p><ul><li>排名前几位的公司对社区投入的人力基本都是两位数，相对于Liberty版本，人员均有所增加</li><li>在人均贡献投入上，99cloud是国内最高的，平均达到了59天，甚至超过了华为，这个统计不仅仅包含了代码贡献，还包含了邮件、Review、Blueprint的时间，基本可以衡量每个公司在OpenStack社区贡献方面的投入力量</li><li>人员投入来看，Easystack和中国移动无疑是最下本的两家，Easystack从Liberty的3人，增长到了23人，一下子增加了20人；中国移动也从最初的4个人，增加到了13个人，可见中国移动未来对OpenStack的野心</li></ul><img src="/images/blogs/contribution-in-mitaka-companies-effort.png" class="center"><h3 id="贡献模块分析"><a href="#贡献模块分析" class="headerlink" title="贡献模块分析"></a>贡献模块分析</h3><p>从模块的角度进行统计，国内企业的贡献情况并未出现一个统一的趋势，总体的贡献项目为193个，项目几乎涉及OpenStack所有最活跃的项目，从排名前十的项目来看：</p><ul><li>得益于华为的主导，dargonflow项目的贡献量超高</li><li>紧随其后的，也是当下的热点，容器相关的两个项目</li><li>几大OpenStack老模块贡献量也高居前十位，说明这些模块是在解决方案中使用频率较高的</li></ul><img src="/images/blogs/contribution-in-mitaka-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这是一个很敏感的话题，每个公司对社区的投入到底换来多少项目上的回报呢？可能这只有每个公司的CEO能够回答的问题了。我在这里就不多做过多的分析，留给大家充分讨论的空间吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>刚刚结束在南京的OpenStack开发培训，也了解到5G的通信网络上已经确定引入了OpenStack，虽然我说不清楚他的具体用途，但是我相信这对OpenStack这个项目、社区是一个重大的利好消息。我也相信，通过国内企业的集体努力，一定能让OpenStack在中国遍地开花结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。&lt;/p&gt;
&lt;p&gt;去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。&lt;/p&gt;
&lt;p&gt;这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。&lt;/p&gt;
&lt;p&gt;对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenStack培训的用户体验</title>
    <link href="http://sunqi.me/2016/03/27/openstack-training-user-experience/"/>
    <id>http://sunqi.me/2016/03/27/openstack-training-user-experience/</id>
    <published>2016-03-27T02:42:38.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？</p><p>这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。</p><a id="more"></a><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>简单来说，我带过OpenStack产品的研发团队，谈过OpenStack的合作，做过OpenStack培训讲师，也卖过OpenStack的私有云产品，也和大量的用户聊过OpenStack，所以还算是对OpenStack这个行业整体上有个清晰认识。</p><h2 id="OpenStack培训的目标群体"><a href="#OpenStack培训的目标群体" class="headerlink" title="OpenStack培训的目标群体"></a>OpenStack培训的目标群体</h2><p>我做过的OpenStack培训大体上分为两类，内训和外训。</p><p>内训是面向公司内部，因为我曾经带过的两个团队都是以开发OpenStack私有云产品为主的，所以我的培训对象主要是研发、运维、售前和销售人员。</p><p>外训的对象很多，包括知名的国企、外企和民营企业以及学校，行业大部分以传统行业为主，涉及通讯、金融、系统集成等，面向的群体主要是研发、IT和售前，培训的内容以OpenStack的基础和研发为主。</p><p>所以我把OpenStack培训的目标群体定义为：研发人员、系统工程师和运维人员、售前、销售人员、学生。</p><h2 id="针对不同的群体，到底需要哪些培训？"><a href="#针对不同的群体，到底需要哪些培训？" class="headerlink" title="针对不同的群体，到底需要哪些培训？"></a>针对不同的群体，到底需要哪些培训？</h2><h3 id="销售人员"><a href="#销售人员" class="headerlink" title="销售人员"></a>销售人员</h3><p>现在做OpenStack生意的无外乎两种：产品和服务。无论是哪一种，对传统的销售人员都是一种极大的挑战。云平台并不像传统软件一样，能够一眼看明白他到底是做什么的，解决了用户的哪些痛点。并且在企业中，能够做决策的人往往并不全是技术出身，所以想和他们解释清楚OpenStack到底能做什么，又是难上艰难。</p><p>所以对于销售人员来讲，培训的重点应该有以下几点：</p><ul><li>使用培训：我觉得无论为哪一类群体培训，演示如何使用OpenStack，都是最有效的帮助人理解的方式。但是这里的演示，必须要设定场景，即传统的业务形态下我们的业务系统是什么样子的，迁移到云平台后该如何部署，从这种比较中，加深对OpenStack的理解。销售人员通过对OpenStack操作，加深对OpenStack或OpenStack产品的理解。毕竟图形是最高效的一种记忆方式。</li><li>理解什么是开源软件：开源软件一定是未来的发展趋势，如果无法对开源软件有一个清晰的认识，也就无法理解清楚OpenStack这个项目出现的价值和意义。</li><li>了解OpenStack的发展历史、OpenStack基金会以及OpenStack社区的运营方式：学习这些的目的是为了给用户讲故事，让用户了解为什么要选择OpenStack，为什么OpenStack项目有持续的生命力，让用户相信使用了OpenStack能够保证未来的基础架构灵活面对业务层面敏捷性的需求。</li><li>案例学习：案例最大的价值就是教育用户，VMWare花了十几年的时间教育了用户，OpenStack不可能在短短的几年时间内就改变这样的局面，所以“学会用别人的案例来教育自己的用户”，是在销售人员OpenStack培训中非常重要的一课。</li></ul><h3 id="售前人员"><a href="#售前人员" class="headerlink" title="售前人员"></a>售前人员</h3><p>售前人员不但要从技术层面让用户信服产品，而且还要结合用户的业务系统需求提供建设方案，外企中的很多售前工程师还要承担搭建POC环境的职责。售前人员沟通的主要对象是企业中有实际需求的业务部门，也是最有可能落地的部门，沟通的成败决定了是否能签单，所以需要更多的专业知识来满足和用户的沟通需要。 培训的重点应该是：</p><ul><li>使用培训：理由同上，但是我觉得售前人员还需要站在用户的角度来思考一下，我的用户到底会如何使用云平台？业务系统迁移到平台后，会有哪些问题？</li><li>如何部署：部署培训向来是各大OpenStack培训必讲的内容，而且90%的内容都是围绕部署展开的，例如某知名企业的OpenStack授证培训。对于售前人员，我认为OpenStack部署训练还是很有必要的。一方面，能够帮助培训对象快速理解OpenStack的架构；另一方面，也能在未来的方案设计上提供参考和依据。由于云平台在使用上与企业传统的IT环境有较大的区别，所以售前人员在学习过程中，应该更多的了解OpenStack部署的特点，服务和服务之间的关系，云平台高可靠等和生产环境部署息息相关的问题。另外还要关注，用户的业务系统迁移到云平台后，可能带来的变化以及应对方式。例如：OpenStack里的网络分为fixed ip和floating ip，但是用户原有的业务系统只会有一个IP，这时候就需要考虑如何为用户选择适当的部署方案。</li><li>OpenStack架构：掌握OpenStack模块的基本工作原理和模块的详细作用。学习这些内容，是为了帮助售前人员在和用户后续交流中，帮助用户选择适当的模块解决用户的需求。</li><li>OpenStack的发展趋势：这部分内容就是能够引导客户未来的项目需求。例如在分布式存储，NFV和SDN方面。</li></ul><h3 id="系统工程师和运维人员"><a href="#系统工程师和运维人员" class="headerlink" title="系统工程师和运维人员"></a>系统工程师和运维人员</h3><p>Iaas云平台不但是对传统的企业IT架构进行了变革，也从管理上对企业原有的流程形成了冲击。需要培训的用户往往集中在自用OpenStack云平台的企业。</p><ul><li>使用培训，不同于上面两种简单的使用，运维人员要求对OpenStack管理部分的使用也要有很深的理解，而且还需要掌握命令行方式的相关操作。</li><li>OpenStack架构，了解OpenStack内部的工作原理，有助于快速定位问题，对系统进行维护。这部分包含的内容比较多，从OpenStack自身的原理到虚拟机，存储，再到虚拟网络的实现都需要有一个系统的了解才可以。</li><li>部署培训，要求详细掌握安装的过程，了解全部配置文件的功能及常用选项和参数。</li><li>自动化部署培训，手动部署即耗费时间又不能保证准确，所以作为运维人员，必须要掌握至少一种自动化部署的方法。这方面的方案有很多，从TripleO、Fuel到Puppet，Salt，Ansible。个人还是推崇应该选择Salt或者Ansible的一种进行学习和掌握。</li><li>运维培训，要求就是在云平台出现问题之后快速定位问题。</li><li>自动化运维培训，DevOps作为未来运维的趋势，反复被提到。云平台自动化运维的内容很多，部署、监控、告警、自动巡检、健康检查等等，使用的工具无外乎上面提到的Salt或者Ansible这样的工具。自动化运维不仅仅是云平台未来培训的一大趋势，也是企业有需求的培训内容。</li></ul><h3 id="开发人员"><a href="#开发人员" class="headerlink" title="开发人员"></a>开发人员</h3><p>开发人员对OpenStack培训的需求主要和未来的工作有关（除了是公司强制或者兴趣之外），从我的经验来看：一种是基于OpenStack API开发，一种是开发OpenStack。所以针对两种不同的需求，培训内容需要单独进行设计，总体来说后一种包含前一种培训。</p><p>与之前几种培训不同，我认为部署培训对开发人员并不是必须的，因为在实际工作中，开发人员很难有机会真正接触到安装过程，这部分工作往往由公司的IT人员去完成，并且其中涉及到大量的Linux基础命令，很多研发人员其实对这部分并不是十分熟悉，所以即使学习了安装内容，也还是一知半解。与其在安装上浪费时间，不如多了解一些架构方面的细节。</p><ul><li>使用培训，帮助开发人员快速了解OpenStack。</li><li>了解社区的开发流程，OpenStack之所以发展到今天的程度，和社区的代码的管理流程密不可分，所以这部分是值得每一名开发人员学习的。</li><li>搭建研发环境，既然要开发OpenStack就应该按照开发的方式搭建研发环境，这样屏蔽了很多安装上的细节，并且让开发人员有个快速能使用和开发的环境。</li><li>基于OpenStack API开发，这部分应该是个重点，我通常会设定一个具体的用户需求，通过解决用户需求来了解API的使用。例如：作为一名用户，我想给我的虚拟机挂卷并自动分区，挂载到/mnt目录。这里的内容包含API文档的使用，通过浏览器REST Client插件详细了解OpenStack API的调用过程，学习使用OpenStack SDK。</li><li>OpenStack编排服务，将API开发中的场景，用编排服务加以实现，还可以包含Scaling和Auto Scaling的场景。这部分很可能是开发人员在未来开发中非常需要的一部分内容。</li><li>OpenStack发展方向，OpenStack的大帐篷展现了对未来的野心，所以了解OpenStack未来的发展方向是很有必要的。</li></ul><p>针对于以后开发OpenStack的研发人员，还需要根据实际的开发内容增加以下的培训内容：</p><ul><li>OpenStack通用技术，学习OpenStack的通用技术有助于理解OpenStack的所有模块，这部分内容主要包括：Eventlet，REST和WSGI，Taskflow，OSLO项目等诸多重要的类库。</li><li>典型模块的架构及开发入门，这里面推荐的模块包含：Nova/Neutron/Horizon/Ceilometer，这几种模块几乎涵盖了OpenStack大部分模块的架构，所以重点理解这些模块的架构和工作原理，对于理解整个OpenStack项目都非常有帮助。直接将代码其实真的很困难，我习惯于使用场景的方式追踪代码的运行轨迹，从而整理出时序图的方式讲解。</li></ul><h3 id="学生"><a href="#学生" class="headerlink" title="学生"></a>学生</h3><p>学生群体事实上是相当有潜力的市场，现在国内OpenStack人才紧缺，所以OpenStack一定要从大学抓起。学生对OpenStack的学习不能仅仅停留在OpenStack本身，与之相关的内容都要学习，但是又不建议完全理论化的学习，强调动手的能力是关键。例如：对Python的学习，虚拟化软件的学习，OpenStack的安装，OpenStack的开发进行循序渐进的学习。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我认为培训中很重要的一环就是让学员动手，否则培训的效果不会很好。以上就是我对OpenStack培训的粗浅认识，还请各位多多指教。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？&lt;/p&gt;
&lt;p&gt;这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul主要使用场景</title>
    <link href="http://sunqi.me/2015/12/24/use-consul/"/>
    <id>http://sunqi.me/2015/12/24/use-consul/</id>
    <published>2015-12-24T02:07:24.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：<a href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/</a></p><p>这篇文章里主要介绍Consul的使用场景，服务和健康检查。</p><a id="more"></a><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>服务注册有点像OpenStack Keystone的Endpoints，可以通过API方式查询到所有服务的端点信息。</p><p>在Agent的节点上添加一个service，之后重启服务。</p><ul><li>添加一个服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80&#125;&#125;&#39; \</span><br><span class="line">    &gt;&#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><p>重新加载新的服务并不需要杀死进程重启服务，只需要给进程直接发送一个SIGHUP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:01:11 [INFO] agent: Synced service &#39;web&#39;</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>我们在任意节点上利用REST API查看服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;catalog&#x2F;service&#x2F;web</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;Address&quot;:&quot;200.21.1.101&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;,&quot;ServiceTags&quot;:[&quot;rails&quot;],&quot;ServiceAddress&quot;:&quot;&quot;,&quot;ServicePort&quot;:80&#125;]</span><br></pre></td></tr></table></figure><h2 id="Health-Check"><a href="#Health-Check" class="headerlink" title="Health Check"></a>Health Check</h2><p>健康检查的方法主要是通过运行一小段脚本的方式，根据运行的结果判断检查对象的健康状况。所以可以通过任意语言定义这个脚本，脚本运行将通过和consul执行的相同用户执行。</p><ul><li>添加一个健康检查</li></ul><p>每30秒ping google.com</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;check&quot;: &#123;&quot;name&quot;: &quot;ping&quot;,</span><br><span class="line">  &quot;script&quot;: &quot;ping -c1 google.com &gt;&#x2F;dev&#x2F;null&quot;, &quot;interval&quot;: &quot;30s&quot;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;ping.json</span><br></pre></td></tr></table></figure><p>为刚才的服务添加健康检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80,</span><br><span class="line">  &quot;check&quot;: &#123;&quot;script&quot;: &quot;curl localhost &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1&quot;, &quot;interval&quot;: &quot;10s&quot;&#125;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced service &#39;web&#39;</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced check &#39;ping&#39;</span><br></pre></td></tr></table></figure><p>经过一段时间后出现了critical和warning日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 12:43:58 [WARN] agent: Check &#39;service:web&#39; is now critical</span><br><span class="line">2015&#x2F;12&#x2F;24 12:44:08 [WARN] agent: Check &#39;ping&#39; is now warning</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>Health check的状态包含了很多种，有any, unkown, passing, warning, critical。any包含了所有状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;health&#x2F;state&#x2F;critical</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;CheckID&quot;:&quot;service:web&quot;,&quot;Name&quot;:&quot;Service &#39;web&#39; check&quot;,&quot;Status&quot;:&quot;critical&quot;,&quot;Notes&quot;:&quot;&quot;,&quot;Output&quot;:&quot;&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;&#125;]</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.consul.io/docs/agent/http/catalog.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/catalog.html</a></li><li><a href="http://www.consul.io/docs/agent/http/health.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/health.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章里主要介绍Consul的使用场景，服务和健康检查。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul的安装方法</title>
    <link href="http://sunqi.me/2015/12/06/consul-installation/"/>
    <id>http://sunqi.me/2015/12/06/consul-installation/</id>
    <published>2015-12-06T18:00:13.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Consul"><a href="#什么是Consul" class="headerlink" title="什么是Consul?"></a>什么是Consul?</h2><p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p><a id="more"></a><h2 id="安装Consul"><a href="#安装Consul" class="headerlink" title="安装Consul"></a>安装Consul</h2><p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html" target="_blank" rel="noopener">https://www.consul.io/downloads.html</a></p><p>我们把consul直接放在/usr/local/bin目录中。</p><h2 id="Consul-Server"><a href="#Consul-Server" class="headerlink" title="Consul Server"></a>Consul Server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -server -bootstrap-expect 3 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;server1 -bind&#x3D;10.10.10.10</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>-server - Serve模式</li><li>-bootstrap-expect - Server数量</li><li>-data-dir - 数据目录</li><li>-node - Node名称</li><li>-bind - 集群通讯地址</li></ul><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;server1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: true (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader</span><br></pre></td></tr></table></figure><h3 id="查看成员"><a href="#查看成员" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="Consul-Agent"><a href="#Consul-Agent" class="headerlink" title="Consul Agent"></a>Consul Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;agent1 -bind&#x3D;10.10.10.100 -config-dir &#x2F;etc&#x2F;consul.d</span><br></pre></td></tr></table></figure><ul><li>输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;agent1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: false (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:41 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br></pre></td></tr></table></figure><ul><li>server日志输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br></pre></td></tr></table></figure><h3 id="查看成员-1"><a href="#查看成员-1" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.consul.io/intro/getting-started/install.html" target="_blank" rel="noopener">https://www.consul.io/intro/getting-started/install.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是Consul&quot;&gt;&lt;a href=&quot;#什么是Consul&quot; class=&quot;headerlink&quot; title=&quot;什么是Consul?&quot;&gt;&lt;/a&gt;什么是Consul?&lt;/h2&gt;&lt;p&gt;Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Grafana+Diamond+Graphite构造完美监控面板</title>
    <link href="http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/"/>
    <id>http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/</id>
    <published>2015-11-30T15:59:46.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。</p><p>本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：</p><img src="/images/blogs/grafana-screenshot.png" class="center" title="800x600"><a id="more"></a><h2 id="安装及配置Dimaond"><a href="#安装及配置Dimaond" class="headerlink" title="安装及配置Dimaond"></a>安装及配置Dimaond</h2><p>安装Diamond最直接和简单的方法就是自己编译RPM或者DEB的安装包, Diamond在这方面提供了比较好的支持。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /root</span></span><br><span class="line"><span class="comment"># yum install -y git rpm-build python-configobj python-setuptools</span></span><br><span class="line"><span class="comment"># git clone https://github.com/python-diamond/Diamond</span></span><br><span class="line"><span class="comment"># cd Diamond</span></span><br><span class="line"><span class="comment"># make rpm</span></span><br><span class="line"><span class="comment"># cd dist</span></span><br><span class="line"><span class="comment"># rpm -ivh diamond-*.noarch.rpm</span></span><br></pre></td></tr></table></figure><p>默认情况下，Diamond开启了基本的监控信息，包括CPU、内存、磁盘的性能数据。当然，我们可以通过配置启动相应的监控项，也能通过自定义的方式进行相应的扩展。这里，我们在/etc/diamond/collectors加载额外的插件，下面的例子中开启了网络的监控。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp -f /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/diamond/diamond.conf</span></span><br><span class="line">[configs]</span><br><span class="line">path = <span class="string">"/etc/diamond/collectors/"</span></span><br><span class="line">extension = <span class="string">".conf"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/net.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[NetworkCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>那么到目前为止，Diamond的基本安装和配置已经完成，但是现在只是简单的采集数据，并没有指明数据要发送给谁，所以下一步我们来开始配置Graphite。</p><h2 id="安装及配置Graphite"><a href="#安装及配置Graphite" class="headerlink" title="安装及配置Graphite"></a>安装及配置Graphite</h2><p>Graphite主要做两件事情：按照时间存储数据、生成图表，在我们的场景里面，实质上就是把Graphite作为数据源给Grafana提供数据。另外还需要安装的是carbon，负责通过网络接受数据并保存到后端存储中；另外还需要whisper，负责生成Graphite样式的基于文件的时间序列的数据库。</p><h3 id="安装软件包"><a href="#安装软件包" class="headerlink" title="安装软件包"></a>安装软件包</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y graphite-web graphite-web-selinux</span></span><br><span class="line"><span class="comment"># yum install -y mysql mysql-server MySQL-python</span></span><br><span class="line"><span class="comment"># yum install -y python-carbon python-whisper</span></span><br></pre></td></tr></table></figure><h3 id="配置MySQL"><a href="#配置MySQL" class="headerlink" title="配置MySQL"></a>配置MySQL</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/init.d/mysqld start</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mysql -e "CREATE DATABASE graphite;" -u root</span></span><br><span class="line"><span class="comment"># mysql -e "GRANT ALL PRIVILEGES ON graphite.* TO 'graphite'@'localhost' IDENTIFIED BY 'sysadmin';" -u root</span></span><br><span class="line"><span class="comment"># mysql -e 'FLUSH PRIVILEGES;' -u root</span></span><br></pre></td></tr></table></figure><h3 id="配置Graphite"><a href="#配置Graphite" class="headerlink" title="配置Graphite"></a>配置Graphite</h3><ul><li>local setting</li></ul><figure class="highlight bash"><figcaption><span>/etc/graphite-web/local_settings.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SECRET_KEY=$(md5sum /etc/passwd | awk &#123;'print $1'&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "SECRET_KEY = '$SECRET_KEY'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"><span class="comment"># echo "TIME_ZONE = 'Asia/Shanghai'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    <span class="string">'default'</span>: &#123;</span><br><span class="line">        <span class="string">'NAME'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'ENGINE'</span>: <span class="string">'django.db.backends.mysql'</span>,</span><br><span class="line">        <span class="string">'USER'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'PASSWORD'</span>: <span class="string">'sysadmin'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cd /usr/lib/python2.6/site-packages/graphite</span></span><br><span class="line"><span class="comment"># ./manage.py syncdb --noinput</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@hihuron.com', 'sysadmin')" | ./manage.py shell</span></span><br></pre></td></tr></table></figure><ul><li>Apache配置</li></ul><figure class="highlight bash"><figcaption><span>/etc/httpd/conf.d/graphite-web.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Listen 0.0.0.0:10000</span><br><span class="line">&lt;VirtualHost *:10000&gt;</span><br><span class="line">    ServerName graphite-web</span><br><span class="line">    DocumentRoot <span class="string">"/usr/share/graphite/webapp"</span></span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/httpd/graphite-web-error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/httpd/graphite-web-access.log common</span><br><span class="line">    Alias /media/ <span class="string">"/usr/lib/python2.6/site-packages/django/contrib/admin/media/"</span></span><br><span class="line"></span><br><span class="line">    WSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi</span><br><span class="line">    WSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%&#123;GLOBAL&#125; application-group=%&#123;GLOBAL&#125;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/content/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/media/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line">&lt;/VirtualHost&gt;</span><br></pre></td></tr></table></figure><ul><li>Diamond配置</li></ul><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HOST_IP=$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.)&#123;3&#125;[0-9]*).*/\2/p' | head -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphiteHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphitePickleHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service carbon-cache restart</span></span><br><span class="line"><span class="comment"># service httpd restart</span></span><br><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure><h2 id="安装和配置Grafana"><a href="#安装和配置Grafana" class="headerlink" title="安装和配置Grafana"></a>安装和配置Grafana</h2><p>Grafana最主要的功能就是对数据的呈现，基于一切可提供time series的后台服务。这里面我们使用Graphite为Grafana提供数据。</p><h3 id="安装及配置"><a href="#安装及配置" class="headerlink" title="安装及配置"></a>安装及配置</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y nodejs</span></span><br><span class="line"><span class="comment"># rpm -ivh https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0-1.x86_64.rpm</span></span><br><span class="line"><span class="comment"># sudo /sbin/chkconfig --add grafana-server</span></span><br><span class="line"><span class="comment"># sed -i 's/^;http_port = 3000$/http_port = 10001/g' /etc/grafana/grafana.ini</span></span><br><span class="line"><span class="comment"># sudo service grafana-server start</span></span><br></pre></td></tr></table></figure><h3 id="添加datasource"><a href="#添加datasource" class="headerlink" title="添加datasource"></a>添加datasource</h3><p>Grafana提供了非常丰富的REST API，我们不仅可以直接利用Grafana作为数据呈现层，还可以利用REST API直接将Grafana的Graph集成在我们的应用中。下面我们利用REST API为Grafana添加datasource。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -i 'http://admin:admin@localhost:10001/api/datasources' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -d '&#123;"name": "graphite", "type": "graphite", "url": "http://localhost:10000", "access": "proxy", "basicAuth": false&#125;'</span></span><br></pre></td></tr></table></figure><h2 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h2><h3 id="修改ceph脚本兼容性"><a href="#修改ceph脚本兼容性" class="headerlink" title="修改ceph脚本兼容性"></a>修改ceph脚本兼容性</h3><p>Diamond是基于Python开发的，但是由于CentOS 6.5的Python版本较低(2.6)，所以直接使用社区版本的Ceph监控时，会导致错误。可以通过简单的修改进行修复。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_stats_from_socket</span><span class="params">(self, name)</span>:</span></span><br><span class="line">    <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">    dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">    an empty result set is returned.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">        <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">        <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">        <span class="comment">#     name,</span></span><br><span class="line">        <span class="comment">#     'perf',</span></span><br><span class="line">        <span class="comment">#     'dump',</span></span><br><span class="line">        <span class="comment">#     ])</span></span><br><span class="line">        cmd = [</span><br><span class="line">             self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">             <span class="string">'--admin-daemon'</span>,</span><br><span class="line">             name,</span><br><span class="line">             <span class="string">'perf'</span>,</span><br><span class="line">             <span class="string">'dump'</span>,</span><br><span class="line">        ]</span><br><span class="line">        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">        json_blob = process.communicate()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="增加对ceph-osd-perf监控"><a href="#增加对ceph-osd-perf监控" class="headerlink" title="增加对ceph osd perf监控"></a>增加对ceph osd perf监控</h3><p>在实际运维Ceph过程中，ceph osd perf是一个非常重要的指令，能够观察出集群中磁盘的latency的信息，通过观察变化，可以辅助判断磁盘出现性能问题。Diamond的设计中，每个Diamond Agent只会采集自己本机的指标，所以我们在添加的时候，只需要在一个节点上增加这个监控就可以了。在ceph.py中结尾处新增加一个类。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CephOsdCollector</span><span class="params">(CephCollector)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_stats</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">        dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">        an empty result set is returned.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">            <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">            <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">            <span class="comment">#     name,</span></span><br><span class="line">            <span class="comment">#     'perf',</span></span><br><span class="line">            <span class="comment">#     'dump',</span></span><br><span class="line">            <span class="comment">#     ])</span></span><br><span class="line">            cmd = [</span><br><span class="line">                 self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">                 <span class="string">'osd'</span>,</span><br><span class="line">                 <span class="string">'perf'</span>,</span><br><span class="line">                 <span class="string">'--format=json'</span>,</span><br><span class="line">            ]</span><br><span class="line">            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">            json_blob = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">except</span> subprocess.CalledProcessError, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not get stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not get stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            json_data = json.loads(json_blob)</span><br><span class="line">        <span class="keyword">except</span> Exception, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not parse stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not parse stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> json_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_publish_stats</span><span class="params">(self, stats)</span>:</span></span><br><span class="line">        <span class="string">"""Given a stats dictionary from _get_stats_from_socket,</span></span><br><span class="line"><span class="string">        publish the individual values.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> perf <span class="keyword">in</span> stats[<span class="string">'osd_perf_infos'</span>]:</span><br><span class="line">            counter_prefix = <span class="string">'osd.'</span> + str(perf[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">for</span> stat_name, stat_value <span class="keyword">in</span> flatten_dictionary(</span><br><span class="line">                perf[<span class="string">'perf_stats'</span>],</span><br><span class="line">                prefix=counter_prefix,</span><br><span class="line">            ):</span><br><span class="line">              self.log.info(<span class="string">'stat_name is %s'</span>, stat_name)</span><br><span class="line">              self.log.info(<span class="string">'stat_value is %s'</span>, stat_value)</span><br><span class="line">              self.publish_gauge(stat_name, stat_value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Collect stats</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.log.info(<span class="string">'in ceph osd collector'</span>)</span><br><span class="line">        stats = self._get_stats()</span><br><span class="line">        self._publish_stats(stats)</span><br></pre></td></tr></table></figure><h3 id="修改Diamond监控配置"><a href="#修改Diamond监控配置" class="headerlink" title="修改Diamond监控配置"></a>修改Diamond监控配置</h3><figure class="highlight bash"><figcaption><span>/etc/diamond/collectors/ceph.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/ceph.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[CephCollector]]</span><br><span class="line">enabled = True</span><br><span class="line"></span><br><span class="line">[[CephOsdCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。&lt;/p&gt;
&lt;p&gt;本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：&lt;/p&gt;
&lt;img src=&quot;/images/blogs/grafana-screenshot.png&quot; class=&quot;center&quot; title=&quot;800x600&quot;&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Liberty国内代码贡献</title>
    <link href="http://sunqi.me/2015/10/29/contribution-in-liberty/"/>
    <id>http://sunqi.me/2015/10/29/contribution-in-liberty/</id>
    <published>2015-10-29T02:56:06.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p><p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p><p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p><a id="more"></a><h2 id="社区贡献分析"><a href="#社区贡献分析" class="headerlink" title="社区贡献分析"></a>社区贡献分析</h2><p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p><p>我们仍然要先看一下模块的贡献情况：</p><img src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" class="left" title="400x300"><p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p><p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p><p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p><h2 id="国内社区贡献分析"><a href="#国内社区贡献分析" class="headerlink" title="国内社区贡献分析"></a>国内社区贡献分析</h2><img src="/images/blogs/contribution-in-liberty-contributor.png" class="center" title="400x300"><p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p><p><strong><em>华为社区贡献统计</em></strong></p><img src="/images/blogs/contribution-in-liberty-huawei.png" class="center" title="800x600"><p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p><p><strong>* 99cloud社区贡献统计*</strong></p><img src="/images/blogs/contribution-in-liberty-99cloud.png" class="center" title="800x600"><p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。<br>更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p><p><strong>* UnitedStack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-unitedstack.png" class="center" title="800x600"><p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p><p><strong>* Kylin Cloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-kylincloud.png" class="center" title="800x600"><p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p><p><strong>* Easystack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-easystack.png" class="center" title="800x600"><p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p><p><strong>* Awcloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-awcloud.png" class="center" title="800x600"><p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于”Neutron &amp; OpenStack漫谈”，非常值得一读。</p><p><strong>* LeTV社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-letv.png" class="center" title="800x600"><p><strong>* Netease社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-netease.png" class="center" title="800x600"><p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p><p><strong>* Huron社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-huron.png" class="center" title="800x600"><p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p><p><strong>* China Mobile社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-chinamobile.png" class="center" title="800x600"><p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p><p><strong>* Lenovo社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-lenovo.png" class="center" title="800x600"><p>排名第135位的是联想。不评论了。</p><p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p><p><strong>* H3C社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-h3c.png" class="center" title="800x600"><p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p><p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p><h2 id="社区贡献内容分析"><a href="#社区贡献内容分析" class="headerlink" title="社区贡献内容分析"></a>社区贡献内容分析</h2><img src="/images/blogs/contribution-in-liberty-complete-blueprints.png" class="center" title="800x600"><p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p><p><strong>* Huawei社区贡献——完成Blueprint *</strong></p><img src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" class="center" title="800x600"><p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p><p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p><p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p><p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty完整版本的翻译可见：&lt;a href=&quot;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 14.04 Server开发者安装指南</title>
    <link href="http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/"/>
    <id>http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/</id>
    <published>2015-09-08T21:50:24.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么会写这篇Blog"><a href="#为什么会写这篇Blog" class="headerlink" title="为什么会写这篇Blog"></a>为什么会写这篇Blog</h2><p>近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。</p><a id="more"></a><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>这篇Blog没有提及的地方：</p><ul><li>网络，需要根据实际情况进行配置，我这里面使用的是DHCP自动获取，所以没有相关步骤</li><li>分区，这里面使用的是默认配置，但是生产环境的配置一般需要手动划分</li></ul><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><ul><li>一定要选择English，否则处理中文的时候太麻烦<img src="/images/blogs/install-ubuntu/1.png" class="center"></li><li>正式开始进入安装<img src="/images/blogs/install-ubuntu/2.png" class="center"></li><li>与上面的原则一致，一定要选择English<img src="/images/blogs/install-ubuntu/3.png" class="center"></li><li>Location一定要选择中国，否则默认不会使用中文的Ubuntu源，影响安装速度，这一步很多初学者不会在意<img src="/images/blogs/install-ubuntu/4.png" class="center"><img src="/images/blogs/install-ubuntu/5.png" class="center"><img src="/images/blogs/install-ubuntu/6.png" class="center"></li><li>这里面主要是字符集的问题，选择United States<img src="/images/blogs/install-ubuntu/7.png" class="center"></li><li>不需要检查键盘布局<img src="/images/blogs/install-ubuntu/8.png" class="center"></li><li>默认使用English布局就好了<img src="/images/blogs/install-ubuntu/9.png" class="center"></li><li>主机名设置，就是hostname<img src="/images/blogs/install-ubuntu/10.png" class="center"></li><li>用户设置，建议建立一个普通用户<img src="/images/blogs/install-ubuntu/11.png" class="center"><img src="/images/blogs/install-ubuntu/12.png" class="center"><img src="/images/blogs/install-ubuntu/13.png" class="center"><img src="/images/blogs/install-ubuntu/15.png" class="center"><img src="/images/blogs/install-ubuntu/16.png" class="center"></li><li>不加密Home目录<img src="/images/blogs/install-ubuntu/17.png" class="center"></li><li>设置时区，这一步也很重要，默认情况下会自动检测到，但是如果不对，一定要修改一下，否则你的系统时间与你实际不一致，你程序里的时间跟着不对，跟调试增加难度<img src="/images/blogs/install-ubuntu/18.png" class="center"></li><li>这里面分区用默认的就好啦，当然如果你知道该如何分区，可以采用Manual方式<img src="/images/blogs/install-ubuntu/19.png" class="center"><img src="/images/blogs/install-ubuntu/20.png" class="center"><img src="/images/blogs/install-ubuntu/21.png" class="center"><img src="/images/blogs/install-ubuntu/22.png" class="center"><img src="/images/blogs/install-ubuntu/23.png" class="center"></li><li>如果访问网络需要使用代理，可以设置一下<img src="/images/blogs/install-ubuntu/24.png" class="center"></li><li>不选择自动更新<img src="/images/blogs/install-ubuntu/25.png" class="center"></li><li>默认只需要选择SSH服务，保证我们在安装后能够SSH登陆服务器即可<img src="/images/blogs/install-ubuntu/26.png" class="center"></li><li>安装grub<img src="/images/blogs/install-ubuntu/27.png" class="center"></li><li>重启完成安装<img src="/images/blogs/install-ubuntu/28.png" class="center"></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>谨记此篇Blog送给我的小徒弟周小球小朋友，希望你能利用利用最后的一年的时间努力学习，找到称心如意的工作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么会写这篇Blog&quot;&gt;&lt;a href=&quot;#为什么会写这篇Blog&quot; class=&quot;headerlink&quot; title=&quot;为什么会写这篇Blog&quot;&gt;&lt;/a&gt;为什么会写这篇Blog&lt;/h2&gt;&lt;p&gt;近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ubuntu" scheme="http://sunqi.me/categories/openstack/ubuntu/"/>
    
    
  </entry>
  
  <entry>
    <title>(Kilo)Devstack完全用户手册</title>
    <link href="http://sunqi.me/2015/09/03/devstack-guide/"/>
    <id>http://sunqi.me/2015/09/03/devstack-guide/</id>
    <published>2015-09-03T02:34:20.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p><p>本篇Blog主要介绍以下几个实用场景：</p><ul><li>如何利用Devstack构建一套完美的开发环境</li><li>提高Devstack安装成功率的方法</li><li>Devstack的实用技巧</li><li>各种场景下的配置和注意事项</li></ul><p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p><a id="more"></a><h2 id="运行环境的选择"><a href="#运行环境的选择" class="headerlink" title="运行环境的选择"></a>运行环境的选择</h2><p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p><ul><li>VMWare Workstation是收费软件，Virtualbox是免费软件</li><li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li><li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li><li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li></ul><h2 id="Virtualbox网络设置"><a href="#Virtualbox网络设置" class="headerlink" title="Virtualbox网络设置"></a>Virtualbox网络设置</h2><img src="/images/blogs/devstack-guide-network-topology.jpg" class="center"><ul><li>Nova Network网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 172.16.0.101</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure><ul><li>Neutron网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth2 inet manual</span><br><span class="line">up ip link set dev $IFACE up</span><br><span class="line">down ip link set dev $IFACE down</span><br></pre></td></tr></table></figure><ul><li>MAC网卡NAT映射</li></ul><p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl net.inet.ip.forwarding&#x3D;1</span><br></pre></td></tr></table></figure><p>在nat-anchor后面添加</p><figure class="highlight plain"><figcaption><span>/etc/pf.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nat on en0 from 172.16.0.0&#x2F;24 -&gt; (en0)</span><br></pre></td></tr></table></figure><p>之后加载</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pfctl -e -f /etc/pf.conf</span><br></pre></td></tr></table></figure><ul><li>Linux网卡NAT映射</li></ul><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward</span><br><span class="line">iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><h2 id="Devstack快速开始"><a href="#Devstack快速开始" class="headerlink" title="Devstack快速开始"></a>Devstack快速开始</h2><p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p><ul><li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li><li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li><li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li></ul><p>下面给出一个最简安装步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># adduser stack</span><br><span class="line"># apt-get install sudo -y</span><br><span class="line"># echo &quot;stack ALL&#x3D;(ALL) NOPASSWD: ALL&quot; &gt;&gt; &#x2F;etc&#x2F;sudoers</span><br><span class="line"># sudo su - stack</span><br><span class="line"></span><br><span class="line">(stack)$ git clone https:&#x2F;&#x2F;git.openstack.org&#x2F;openstack-dev&#x2F;devstack --branch&#x3D;stable&#x2F;kilo</span><br><span class="line">(stack)$ cd devstack &amp;&amp; .&#x2F;stack.sh</span><br></pre></td></tr></table></figure><h2 id="提高Devstack安装成功率"><a href="#提高Devstack安装成功率" class="headerlink" title="提高Devstack安装成功率"></a>提高Devstack安装成功率</h2><p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p><img src="/images/blogs/devstack-guide-flow.png" class="center"><p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p><ul><li>安装依赖时，需要访问Ubuntu的源</li><li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">https://bootstrap.pypa.io/get-pip.py</a></li><li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li><li>安装过程中执行pip install requirements，需要访问pip repo</li><li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li></ul><hr><p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p><ul><li>使用国内源</li></ul><figure class="highlight plain"><figcaption><span>/etc/apt/sources.list</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><ul><li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li></ul><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FILES=<span class="variable">$TOP_DIR</span>/files</span><br><span class="line"></span><br><span class="line">PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py</span><br><span class="line">LOCAL_PIP=<span class="string">"<span class="variable">$FILES</span>/<span class="variable">$(basename $PIP_GET_PIP_URL)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> install_get_pip &#123;</span><br><span class="line">    <span class="comment"># The OpenStack gate and others put a cached version of get-pip.py</span></span><br><span class="line">    <span class="comment"># for this to find, explicitly to avoid download issues.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># However, if DevStack *did* download the file, we want to check</span></span><br><span class="line">    <span class="comment"># for updates; people can leave their stacks around for a long</span></span><br><span class="line">    <span class="comment"># time and in the mean-time pip might get upgraded.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Thus we use curl's "-z" feature to always check the modified</span></span><br><span class="line">    <span class="comment"># since and only download if a new version is out -- but only if</span></span><br><span class="line">    <span class="comment"># it seems we downloaded the file originally.</span></span><br><span class="line">    <span class="keyword">if</span> [[ ! -r <span class="variable">$LOCAL_PIP</span> || -r <span class="variable">$LOCAL_PIP</span>.downloaded ]]; <span class="keyword">then</span></span><br><span class="line">        curl --retry 6 --retry-delay 5 \</span><br><span class="line">            -z <span class="variable">$LOCAL_PIP</span> -o <span class="variable">$LOCAL_PIP</span> <span class="variable">$PIP_GET_PIP_URL</span> || \</span><br><span class="line">            die <span class="variable">$LINENO</span> <span class="string">"Download of get-pip.py failed"</span></span><br><span class="line">        touch <span class="variable">$LOCAL_PIP</span>.downloaded</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    sudo -H -E python <span class="variable">$LOCAL_PIP</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改为我在coding.net上缓存的get-pip脚本</p><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py</span><br></pre></td></tr></table></figure><ul><li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li><li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li><li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li></ul><h2 id="无网络状况下安装Devstack"><a href="#无网络状况下安装Devstack" class="headerlink" title="无网络状况下安装Devstack"></a>无网络状况下安装Devstack</h2><p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p><ul><li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li><li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li><li>建立本地的Gerrit，并且上传所有代码</li><li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li><li>将镜像下载到刚刚创建的Apache服务器</li></ul><p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p><h2 id="OFFLINE模式下安装Devstack"><a href="#OFFLINE模式下安装Devstack" class="headerlink" title="OFFLINE模式下安装Devstack"></a>OFFLINE模式下安装Devstack</h2><p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OFFLINE=True</span><br></pre></td></tr></table></figure><h2 id="虚拟机重启后，如何利用rejoin-stack-sh，免重新安装"><a href="#虚拟机重启后，如何利用rejoin-stack-sh，免重新安装" class="headerlink" title="虚拟机重启后，如何利用rejoin-stack.sh，免重新安装"></a>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2><p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p><figure class="highlight bash"><figcaption><span>/etc/init.d/cinder-setup-backing-file</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file</span><br><span class="line">losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file</span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as root</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /etc/init.d/cinder-setup-backing-file</span><br><span class="line">ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as normal user</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/devstack</span><br><span class="line">./rejoin-stack.sh</span><br></pre></td></tr></table></figure><h2 id="Scenario-0-公共部分"><a href="#Scenario-0-公共部分" class="headerlink" title="Scenario 0: 公共部分"></a>Scenario 0: 公共部分</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Misc</span></span><br><span class="line">ADMIN_PASSWORD=sysadmin</span><br><span class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_TOKEN=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target Path</span></span><br><span class="line">DEST=/opt/stack.kilo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable Logging</span></span><br><span class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</span><br><span class="line">VERBOSE=True</span><br><span class="line">LOG_COLOR=True</span><br><span class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs</span><br></pre></td></tr></table></figure><h2 id="Scenario-1-单节点Nova-Network的安装"><a href="#Scenario-1-单节点Nova-Network的安装" class="headerlink" title="Scenario 1: 单节点Nova-Network的安装"></a>Scenario 1: 单节点Nova-Network的安装</h2><p>这应该就是Devstack默认的模式，有以下几点需要注意：</p><ul><li>根据上面的网卡配置</li></ul><blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p><p>第二块为Host-only Adaptor，用于访问云平台</p><p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p><p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote><ul><li>打开novnc和consoleauth，否则无法访问VNC</li></ul><p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># eth1 address</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=172.24.17.0/24</span><br><span class="line">FIXED_NETWORK_SIZE=254</span><br><span class="line">FLOATING_RANGE=172.16.0.128/25</span><br></pre></td></tr></table></figure><h2 id="Scenario-2-双节点Nova-Network的安装"><a href="#Scenario-2-双节点Nova-Network的安装" class="headerlink" title="Scenario 2: 双节点Nova-Network的安装"></a>Scenario 2: 双节点Nova-Network的安装</h2><ul><li>控制节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line">MULTI_HOST=1</span><br></pre></td></tr></table></figure><ul><li>计算节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># needed by cinder-volume service</span></span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># controller ip</span></span><br><span class="line">SERVICE_HOST=192.168.56.101</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br></pre></td></tr></table></figure><h2 id="Scenario-3-单节点Neutron的安装"><a href="#Scenario-3-单节点Neutron的安装" class="headerlink" title="Scenario 3: 单节点Neutron的安装"></a>Scenario 3: 单节点Neutron的安装</h2><ul><li>基本配置</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron</span></span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line">ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas</span><br><span class="line"></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=20.0.0.0/24</span><br><span class="line">NETWORK_GATEWAY=20.0.0.1</span><br><span class="line">FLOATING_RANGE=172.16.0.0/24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY=172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span><br></pre></td></tr></table></figure><ul><li>OVS设置</li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-4-多节点Neutron的安装-控制-网络-计算节点"><a href="#Scenario-4-多节点Neutron的安装-控制-网络-计算节点" class="headerlink" title="Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)"></a>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2><ul><li><p>控制/网络节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">HOST_IP&#x3D;192.168.56.101</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"># Neutron</span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta</span><br><span class="line">FIXED_RANGE&#x3D;20.0.0.0&#x2F;24</span><br><span class="line">NETWORK_GATEWAY&#x3D;20.0.0.1</span><br><span class="line">FLOATING_RANGE&#x3D;172.16.0.0&#x2F;24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;172.16.0.101,end&#x3D;172.16.0.200</span><br></pre></td></tr></table></figure></li><li><p>计算节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">disable_all_services</span><br><span class="line">ENABLED_SERVICES&#x3D;n-cpu,rabbit,neutron,q-agt,c-vol</span><br><span class="line"></span><br><span class="line"># current host ip</span><br><span class="line">HOST_IP&#x3D;192.168.56.103</span><br><span class="line"># needed by cinder-volume service</span><br><span class="line">DATABASE_TYPE&#x3D;mysql</span><br><span class="line"></span><br><span class="line"># controller ip</span><br><span class="line">SERVICE_HOST&#x3D;192.168.56.101</span><br><span class="line">MYSQL_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">RABBIT_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">GLANCE_HOSTPORT&#x3D;$SERVICE_HOST:9292</span><br><span class="line">NOVA_VNC_ENABLED&#x3D;True</span><br><span class="line">NOVNCPROXY_URL&#x3D;&quot;http:&#x2F;&#x2F;$SERVICE_HOST:6080&#x2F;vnc_auto.html&quot;</span><br><span class="line">VNCSERVER_LISTEN&#x3D;$HOST_IP</span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS&#x3D;$VNCSERVER_LISTEN</span><br><span class="line">Q_HOST&#x3D;$SERVICE_HOST</span><br></pre></td></tr></table></figure></li><li><p>OVS设置</p></li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-5-从源代码安装客户端"><a href="#Scenario-5-从源代码安装客户端" class="headerlink" title="Scenario 5: 从源代码安装客户端"></a>Scenario 5: 从源代码安装客户端</h2><p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEYSTONECLIENT_BRANCH=1.3.1</span><br><span class="line">CINDERCLIENT_BRANCH=1.1.1</span><br><span class="line">GLANCECLIENT_BRANCH=0.17.1</span><br><span class="line">HEATCLIENT_BRANCH=0.4.0</span><br><span class="line">NEUTRONCLIENT_BRANCH=2.4.0</span><br><span class="line">NOVACLIENT_BRANCH=2.23.0</span><br><span class="line">SWIFTCLIENT_BRANCH=2.4.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># client code</span></span><br><span class="line">LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient</span><br></pre></td></tr></table></figure><h2 id="Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift"><a href="#Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift" class="headerlink" title="Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift"></a>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceilometer</span></span><br><span class="line">enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api</span><br><span class="line">enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Heat</span></span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trove</span></span><br><span class="line">enable_service trove tr-api tr-tmgr tr-cond</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sahara</span></span><br><span class="line">enable_service sahara</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swift</span></span><br><span class="line">enable_service s-proxy s-object s-container s-account</span><br><span class="line">SWIFT_REPLICAS=1</span><br><span class="line">SWIFT_HASH=011688b44136573e209e</span><br></pre></td></tr></table></figure><h2 id="Scenario-7-安装Ceph"><a href="#Scenario-7-安装Ceph" class="headerlink" title="Scenario 7: 安装Ceph"></a>Scenario 7: 安装Ceph</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceph</span></span><br><span class="line">ENABLED_SERVICES+=,ceph</span><br><span class="line">CEPH_LOOPBACK_DISK_SIZE=200G</span><br><span class="line">CEPH_CONF=/etc/ceph/ceph.conf</span><br><span class="line">CEPH_REPLICAS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Glance - Image Service</span></span><br><span class="line">GLANCE_CEPH_USER=glance</span><br><span class="line">GLANCE_CEPH_POOL=glance-pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cinder - Block Device Service</span></span><br><span class="line">CINDER_DRIVER=ceph</span><br><span class="line">CINDER_CEPH_USER=cinder</span><br><span class="line">CINDER_CEPH_POOL=cinder-pool</span><br><span class="line">CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73</span><br><span class="line">CINDER_BAK_CEPH_POOL=cinder-backups</span><br><span class="line">CINDER_BAK_CEPH_USER=cinder-backups</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nova - Compute Service</span></span><br><span class="line">NOVA_CEPH_POOL=nova-pool</span><br></pre></td></tr></table></figure><h2 id="Scenario-8-安装Murano"><a href="#Scenario-8-安装Murano" class="headerlink" title="Scenario 8: 安装Murano"></a>Scenario 8: 安装Murano</h2><p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/stack.kilo</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/murano --branch=stable/kilo</span><br><span class="line"><span class="built_in">cd</span> murano/contrib/devstack</span><br><span class="line">cp lib/murano <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp lib/murano-dashboard <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp extras.d/70-murano.sh <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/extras.d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Enable Neutron</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line"></span><br><span class="line"># Enable Heat</span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"># Enable Murano</span><br><span class="line">enable_service murano murano-api murano-engine</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。&lt;/p&gt;
&lt;p&gt;本篇Blog主要介绍以下几个实用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何利用Devstack构建一套完美的开发环境&lt;/li&gt;
&lt;li&gt;提高Devstack安装成功率的方法&lt;/li&gt;
&lt;li&gt;Devstack的实用技巧&lt;/li&gt;
&lt;li&gt;各种场景下的配置和注意事项&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Devstack" scheme="http://sunqi.me/categories/OpenStack/Devstack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Devstack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>为什么叫Monkey Patch？</title>
    <link href="http://sunqi.me/2015/08/18/about-monkey-patch/"/>
    <id>http://sunqi.me/2015/08/18/about-monkey-patch/</id>
    <published>2015-08-18T02:51:21.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？</p><p>估计刚刚看到这个表述的开发人员可能很难理解到底这是什么意思，其实Monkey Patch本与猴子无关，这个词原来为Guerrilla Patch，这样看着好像能明白一些了，游击队嘛，神出鬼没的，好像和运行状态打补丁这个功能贴近点了，但是为什么又变成猴子了。原来老外们都是很顽皮的，他们喜欢一些玩笑式的表述，就像很多技术的文档中一样。在英文里，Guerrilla和Gorilla读音是几乎一样的，Gorilla当什么讲呢？大猩猩。但是大猩猩有点吓人，所以干脆换成了大猩猩的近亲——猴子。就这样Monkey Patch形成了。</p><p>当然这并不是这个词的唯一解释，还有一种解释是说由于这种方式将原来的代码弄乱了(messing with it)，在英文里叫monkeying about(顽皮的)，所以叫做Monkey Patch。这种描述应该是和Monkey Test有异曲同工之妙。但是无论这个词从哪里来，我们只要正确理解Monkey Patch的含义就好了。</p><p>相同的表述还有Duck Typing，描述的是动态类型的一种风格。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？&lt;/p&gt;
&lt;p
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Ceph性能优化总结(v0.94)</title>
    <link href="http://sunqi.me/2015/06/27/ceph-performance-optimization-summary/"/>
    <id>http://sunqi.me/2015/06/27/ceph-performance-optimization-summary/</id>
    <published>2015-06-27T22:30:22.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。</p><h2 id="优化方法论"><a href="#优化方法论" class="headerlink" title="优化方法论"></a>优化方法论</h2><p>做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：</p><h3 id="1-硬件层面"><a href="#1-硬件层面" class="headerlink" title="1. 硬件层面"></a>1. 硬件层面</h3><ul><li>硬件规划</li><li>SSD选择</li><li>BIOS设置</li></ul><h3 id="2-软件层面"><a href="#2-软件层面" class="headerlink" title="2. 软件层面"></a>2. 软件层面</h3><ul><li>Linux OS</li><li>Ceph Configurations</li><li>PG Number调整</li><li>CRUSH Map</li><li>其他因素</li></ul><a id="more"></a><h2 id="硬件优化"><a href="#硬件优化" class="headerlink" title="硬件优化"></a>硬件优化</h2><h3 id="1-硬件规划"><a href="#1-硬件规划" class="headerlink" title="1. 硬件规划"></a>1. 硬件规划</h3><ul><li>Processor</li></ul><p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</p><p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p><p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p><ul><li>内存</li></ul><p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p><ul><li>网络规划</li></ul><p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p><h3 id="2-SSD选择"><a href="#2-SSD选择" class="headerlink" title="2. SSD选择"></a>2. SSD选择</h3><p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="http://www.intel.com/content/www/us/en/solid-state-drives/solid-state-drives-dc-s3500-series.html" target="_blank" rel="noopener">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p><p>如何确定你的SSD是否适合作为SSD Journal，可以参考SÉBASTIEN HAN的<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/" target="_blank" rel="noopener">Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p><h3 id="3-BIOS设置"><a href="#3-BIOS设置" class="headerlink" title="3. BIOS设置"></a>3. BIOS设置</h3><ul><li>Hyper-Threading(HT)</li></ul><p>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p><ul><li>关闭节能</li></ul><p>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请参考<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/" target="_blank" rel="noopener">链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for CPUFREQ in &#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;cpu*&#x2F;cpufreq&#x2F;scaling_governor; do [ -f $CPUFREQ ] || continue; echo -n performance &gt; $CPUFREQ; done</span><br></pre></td></tr></table></figure><ul><li><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/" target="_blank" rel="noopener">NUMA</a></li></ul><p>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html" target="_blank" rel="noopener">NUMA可能会在某些情况下影响ceph-osd</a>。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel &#x2F;vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root&#x3D;UUID&#x3D;870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG&#x3D;en_US.UTF-8 rd_NO_MD SYSFONT&#x3D;latarcyrheb-sun16 crashkernel&#x3D;auto  KEYBOARDTYPE&#x3D;pc KEYTABLE&#x3D;us rd_NO_DM   biosdevname&#x3D;0 numa&#x3D;off</span><br></pre></td></tr></table></figure><h2 id="软件优化"><a href="#软件优化" class="headerlink" title="软件优化"></a>软件优化</h2><h3 id="1-Linux-OS"><a href="#1-Linux-OS" class="headerlink" title="1. Linux OS"></a>1. Linux OS</h3><ul><li>Kernel pid max</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 4194303 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;pid_max</span><br></pre></td></tr></table></figure><ul><li>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 mtu 9000</span><br></pre></td></tr></table></figure><p>永久设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;MTU&#x3D;9000&quot; | tee -a &#x2F;etc&#x2F;sysconfig&#x2F;network-script&#x2F;ifcfg-eth0</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;networking restart</span><br></pre></td></tr></table></figure><ul><li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;read_ahead_kb</span><br></pre></td></tr></table></figure><p>根据一些Ceph的公开分享，8192是比较理想的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;8192&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;read_ahead_kb</span><br></pre></td></tr></table></figure><ul><li>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;vm.swappiness &#x3D; 0&quot; | tee -a &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure><ul><li>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;deadline&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sd[x]&#x2F;queue&#x2F;scheduler</span><br><span class="line">echo &quot;noop&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sd[x]&#x2F;queue&#x2F;scheduler</span><br></pre></td></tr></table></figure><ul><li>cgroup</li></ul><p>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html" target="_blank" rel="noopener">原因</a>。</p><blockquote><ul><li>不在process和thread在不同的core上移动(更好的缓存利用)</li><li>减少NUMA的影响</li><li>网络和存储控制器影响 - 较小</li><li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li><li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li></ul></blockquote><p>这一点具体实现待补充！！！</p><h3 id="2-Ceph-Configurations"><a href="#2-Ceph-Configurations" class="headerlink" title="2. Ceph Configurations"></a>2. Ceph Configurations</h3><h4 id="global"><a href="#global" class="headerlink" title="[global]"></a>[global]</h4><p>| 参数名 | 描述 | 默认值 | 建议值 |<br>| —– | — | —– | —– | ——— |<br>| public network | 客户端访问网络 | | 192.168.100.0/24 |<br>| cluster network | 集群网络 | | 192.168.1.0/24 |<br>| max open files | 如果设置了该选项，Ceph会设置系统的max open fds | 0 | 131072 |</p><hr><ul><li>查看系统最大文件打开数可以使用命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max</span><br></pre></td></tr></table></figure><hr><h4 id="osd-filestore"><a href="#osd-filestore" class="headerlink" title="[osd] - filestore"></a>[osd] - filestore</h4><p>| 参数名 | 描述 | 默认值 | 建议值 |<br>| —– | — | —– | —– | ——— |<br>| filestore xattr use omap | 为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用 | false | true |<br>| filestore max sync interval | 从日志到数据盘最大同步间隔(seconds) | 5 | 15 |<br>| filestore min sync interval | 从日志到数据盘最小同步间隔(seconds) | 0.1 | 10 |<br>| filestore queue max ops | 数据盘最大接受的操作数 | 500 | 25000 |<br>| filestore queue max bytes | 数据盘一次操作最大字节数(bytes) | 100 &lt;&lt; 20 | 10485760 |<br>| filestore queue committing max ops | 数据盘能够commit的操作数 | 500 | 5000 |<br>| filestore queue committing max bytes | 数据盘能够commit的最大字节数(bytes) | 100 &lt;&lt; 20 | 10485760000 |<br>| filestore op threads | 并发文件系统操作数 | 2 | 32 |</p><hr><ul><li>调整omap的原因主要是EXT4文件系统默认仅有4K</li><li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li></ul><hr><h4 id="osd-journal"><a href="#osd-journal" class="headerlink" title="[osd] - journal"></a>[osd] - journal</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd journal size</td><td>OSD日志大小(MB)</td><td>5120</td><td>20000</td></tr><tr><td>journal max write bytes</td><td>journal一次性写入的最大字节数(bytes)</td><td>10 &lt;&lt; 20</td><td>1073714824</td></tr><tr><td>journal max write entries</td><td>journal一次性写入的最大记录数</td><td>100</td><td>10000</td></tr><tr><td>journal queue max ops</td><td>journal一次性最大在队列中的操作数</td><td>500</td><td>50000</td></tr><tr><td>journal queue max bytes</td><td>journal一次性最大在队列中的字节数(bytes)</td><td>10 &lt;&lt; 20</td><td>10485760000</td></tr></tbody></table><hr><ul><li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li><li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li></ul><hr><h4 id="osd-osd-config-tuning"><a href="#osd-osd-config-tuning" class="headerlink" title="[osd] - osd config tuning"></a>[osd] - osd config tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd max write size</td><td>OSD一次可写入的最大值(MB)</td><td>90</td><td>512</td></tr><tr><td>osd client message size cap</td><td>客户端允许在内存中的最大数据(bytes)</td><td>524288000</td><td>2147483648</td></tr><tr><td>osd deep scrub stride</td><td>在Deep Scrub时候允许读取的字节数(bytes)</td><td>524288</td><td>131072</td></tr><tr><td>osd op threads</td><td>OSD进程操作的线程数</td><td>2</td><td>8</td></tr><tr><td>osd disk threads</td><td>OSD密集型操作例如恢复和Scrubbing时的线程</td><td>1</td><td>4</td></tr><tr><td>osd map cache size</td><td>保留OSD Map的缓存(MB)</td><td>500</td><td>1024</td></tr><tr><td>osd map cache bl size</td><td>OSD进程在内存中的OSD Map缓存(MB)</td><td>50</td><td>128</td></tr><tr><td>osd mount options xfs</td><td>Ceph OSD xfs Mount选项</td><td>rw,noatime,inode64</td><td>rw,noexec,nodev,noatime,nodiratime,nobarrier</td></tr></tbody></table><hr><ul><li>增加osd op threads和disk threads会带来额外的CPU开销</li></ul><hr><h4 id="osd-recovery-tuning"><a href="#osd-recovery-tuning" class="headerlink" title="[osd] - recovery tuning"></a>[osd] - recovery tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd recovery op priority</td><td>恢复操作优先级，取值1-63，值越高占用资源越高</td><td>10</td><td>4</td></tr><tr><td>osd recovery max active</td><td>同一时间内活跃的恢复请求数</td><td>15</td><td>10</td></tr><tr><td>osd max backfills</td><td>一个OSD允许的最大backfills数</td><td>10</td><td>4</td></tr></tbody></table><h4 id="osd-client-tuning"><a href="#osd-client-tuning" class="headerlink" title="[osd] - client tuning"></a>[osd] - client tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>rbd cache</td><td>RBD缓存</td><td>true</td><td>true</td></tr><tr><td>rbd cache size</td><td>RBD缓存大小(bytes)</td><td>33554432</td><td>268435456</td></tr><tr><td>rbd cache max dirty</td><td>缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</td><td>25165824</td><td>134217728</td></tr><tr><td>rbd cache max dirty age</td><td>在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</td><td>1</td><td>5</td></tr></tbody></table><h4 id="关闭Debug"><a href="#关闭Debug" class="headerlink" title="关闭Debug"></a>关闭Debug</h4><h3 id="3-PG-Number"><a href="#3-PG-Number" class="headerlink" title="3. PG Number"></a>3. PG Number</h3><p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。</p><pre><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count</code></pre><p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set volumes pg_num 512</span><br><span class="line">ceph osd pool set volumes pgp_num 512</span><br></pre></td></tr></table></figure><h3 id="4-CRUSH-Map"><a href="#4-CRUSH-Map" class="headerlink" title="4. CRUSH Map"></a>4. CRUSH Map</h3><p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p><h3 id="5-其他因素的影响"><a href="#5-其他因素的影响" class="headerlink" title="5. 其他因素的影响"></a>5. 其他因素的影响</h3><p>在今年的(2015年)的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd perf</span><br></pre></td></tr></table></figure><pre><code>osd fs_commit_latency(ms) fs_apply_latency(ms)  0                    14                   17  1                    14                   16  2                    10                   11  3                     4                    5  4                    13                   15  5                    17                   20  6                    15                   18  7                    14                   16  8                   299                  329</code></pre><h2 id="ceph-conf"><a href="#ceph-conf" class="headerlink" title="ceph.conf"></a>ceph.conf</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid &#x3D; 059f27e8-a23f-4587-9033-3e3679d03b31</span><br><span class="line">mon_host &#x3D; 10.10.20.102, 10.10.20.101, 10.10.20.100</span><br><span class="line">auth cluster required &#x3D; cephx</span><br><span class="line">auth service required &#x3D; cephx</span><br><span class="line">auth client required &#x3D; cephx</span><br><span class="line">osd pool default size &#x3D; 3</span><br><span class="line">osd pool default min size &#x3D; 1</span><br><span class="line"></span><br><span class="line">public network &#x3D; 10.10.20.0&#x2F;24</span><br><span class="line">cluster network &#x3D; 10.10.20.0&#x2F;24</span><br><span class="line"></span><br><span class="line">max open files &#x3D; 131072</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon data &#x3D; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-$id</span><br><span class="line"></span><br><span class="line">[osd]</span><br><span class="line">osd data &#x3D; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-$id</span><br><span class="line">osd journal size &#x3D; 20000</span><br><span class="line">osd mkfs type &#x3D; xfs</span><br><span class="line">osd mkfs options xfs &#x3D; -f</span><br><span class="line"></span><br><span class="line">filestore xattr use omap &#x3D; true</span><br><span class="line">filestore min sync interval &#x3D; 10</span><br><span class="line">filestore max sync interval &#x3D; 15</span><br><span class="line">filestore queue max ops &#x3D; 25000</span><br><span class="line">filestore queue max bytes &#x3D; 10485760</span><br><span class="line">filestore queue committing max ops &#x3D; 5000</span><br><span class="line">filestore queue committing max bytes &#x3D; 10485760000</span><br><span class="line"></span><br><span class="line">journal max write bytes &#x3D; 1073714824</span><br><span class="line">journal max write entries &#x3D; 10000</span><br><span class="line">journal queue max ops &#x3D; 50000</span><br><span class="line">journal queue max bytes &#x3D; 10485760000</span><br><span class="line"></span><br><span class="line">osd max write size &#x3D; 512</span><br><span class="line">osd client message size cap &#x3D; 2147483648</span><br><span class="line">osd deep scrub stride &#x3D; 131072</span><br><span class="line">osd op threads &#x3D; 8</span><br><span class="line">osd disk threads &#x3D; 4</span><br><span class="line">osd map cache size &#x3D; 1024</span><br><span class="line">osd map cache bl size &#x3D; 128</span><br><span class="line">osd mount options xfs &#x3D; &quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&quot;</span><br><span class="line">osd recovery op priority &#x3D; 4</span><br><span class="line">osd recovery max active &#x3D; 10</span><br><span class="line">osd max backfills &#x3D; 4</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">rbd cache &#x3D; true</span><br><span class="line">rbd cache size &#x3D; 268435456</span><br><span class="line">rbd cache max dirty &#x3D; 134217728</span><br><span class="line">rbd cache max dirty age &#x3D; 5</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化是一个长期迭代的过程，所有的方法都是别人的，只有在实践过程中才能发现自己的，本篇文章仅仅是一个开始，欢迎各位积极补充，共同完成一篇具有指导性的文章。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。&lt;/p&gt;
&lt;h2 id=&quot;优化方法论&quot;&gt;&lt;a href=&quot;#优化方法论&quot; class=&quot;headerlink&quot; title=&quot;优化方法论&quot;&gt;&lt;/a&gt;优化方法论&lt;/h2&gt;&lt;p&gt;做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：&lt;/p&gt;
&lt;h3 id=&quot;1-硬件层面&quot;&gt;&lt;a href=&quot;#1-硬件层面&quot; class=&quot;headerlink&quot; title=&quot;1. 硬件层面&quot;&gt;&lt;/a&gt;1. 硬件层面&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;硬件规划&lt;/li&gt;
&lt;li&gt;SSD选择&lt;/li&gt;
&lt;li&gt;BIOS设置&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-软件层面&quot;&gt;&lt;a href=&quot;#2-软件层面&quot; class=&quot;headerlink&quot; title=&quot;2. 软件层面&quot;&gt;&lt;/a&gt;2. 软件层面&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Linux OS&lt;/li&gt;
&lt;li&gt;Ceph Configurations&lt;/li&gt;
&lt;li&gt;PG Number调整&lt;/li&gt;
&lt;li&gt;CRUSH Map&lt;/li&gt;
&lt;li&gt;其他因素&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://sunqi.me/categories/ceph/"/>
    
      <category term="openstack" scheme="http://sunqi.me/categories/ceph/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>Ceph集群磁盘没有剩余空间的解决方法</title>
    <link href="http://sunqi.me/2015/05/11/ceph-osd-is-full/"/>
    <id>http://sunqi.me/2015/05/11/ceph-osd-is-full/</id>
    <published>2015-05-11T17:21:42.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障描述"><a href="#故障描述" class="headerlink" title="故障描述"></a>故障描述</h2><p>OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。</p><a id="more"></a><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><ul><li>尝试使用OpenStack重启虚拟机无效</li><li>尝试直接用rbd命令直接删除块失败</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@controller ~]# rbd -p volumes rm volume-c55fd052-212d-4107-a2ac-cf53bfc049be</span><br><span class="line">2015-04-29 05:31:31.719478 7f5fb82f7760  0 client.4781741.objecter  FULL, paused modify 0xe9a9e0 tid 6</span><br></pre></td></tr></table></figure><ul><li>查看ceph健康状态</li></ul><figure class="highlight plain"><figcaption><span>ceph -s</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cluster 059f27e8-a23f-4587-9033-3e3679d03b31</span><br><span class="line"> health HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482&#x2F;129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)</span><br><span class="line"> monmap e6: 4 mons at &#123;node-5e40.cloud.com&#x3D;10.10.20.40:6789&#x2F;0,node-6670.cloud.com&#x3D;10.10.20.31:6789&#x2F;0,node-66c4.cloud.com&#x3D;10.10.20.36:6789&#x2F;0,node-fb27.cloud.com&#x3D;10.10.20.41:6789&#x2F;0&#125;, election epoch 886, quorum 0,1,2,3 node-6670.cloud.com,node-66c4.cloud.com,node-5e40.cloud.com,node-fb27.cloud.com</span><br><span class="line"> osdmap e2743: 3 osds: 3 up, 3 in</span><br><span class="line">        flags full</span><br><span class="line">  pgmap v6564199: 320 pgs, 4 pools, 262 GB data, 43027 objects</span><br><span class="line">        786 GB used, 47785 MB &#x2F; 833 GB avail</span><br><span class="line">        7482&#x2F;129081 objects degraded (5.796%)</span><br><span class="line">             300 active+clean</span><br><span class="line">              20 active+degraded+remapped+backfill_toofull</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>ceph health detail</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482&#x2F;129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)</span><br><span class="line">pg 3.8 is stuck unclean for 7067109.597691, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.7d is stuck unclean for 1852078.505139, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.21 is stuck unclean for 7072842.637848, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.22 is stuck unclean for 7070880.213397, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.a is stuck unclean for 7067057.863562, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.7f is stuck unclean for 7067122.493746, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.5 is stuck unclean for 7067088.369629, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1e is stuck unclean for 7073386.246281, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.19 is stuck unclean for 7068035.310269, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.5d is stuck unclean for 1852078.505949, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1a is stuck unclean for 7067088.429544, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1b is stuck unclean for 7072773.771385, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.3 is stuck unclean for 7067057.864514, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.15 is stuck unclean for 7067088.825483, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.11 is stuck unclean for 7067057.862408, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.6d is stuck unclean for 7067083.634454, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.6e is stuck unclean for 7067098.452576, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.c is stuck unclean for 5658116.678331, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.e is stuck unclean for 7067078.646953, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.20 is stuck unclean for 7067140.530849, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.7d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.7f is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.6d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.6e is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.5d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.20 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.21 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.22 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.1e is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.19 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.1a is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.1b is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.15 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.11 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.c is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.e is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.8 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.a is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.5 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.3 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">recovery 7482&#x2F;129081 objects degraded (5.796%)</span><br><span class="line">osd.0 is full at 95%</span><br><span class="line">osd.2 is full at 95%</span><br><span class="line">osd.1 is near full at 93%</span><br></pre></td></tr></table></figure><h2 id="解决方案一-已验证"><a href="#解决方案一-已验证" class="headerlink" title="解决方案一(已验证)"></a>解决方案一(已验证)</h2><p>增加OSD节点，这也是官方文档中推荐的做法，增加新的节点后，Ceph开始重新平衡数据，OSD使用空间开始下降</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2015-04-29 06:51:58.623262 osd.1 [WRN] OSD near full (91%)</span><br><span class="line">2015-04-29 06:52:01.500813 osd.2 [WRN] OSD near full (92%)</span><br></pre></td></tr></table></figure><h2 id="解决方案二-理论上，没有进行验证"><a href="#解决方案二-理论上，没有进行验证" class="headerlink" title="解决方案二(理论上，没有进行验证)"></a>解决方案二(理论上，没有进行验证)</h2><p>如果在没有新的硬盘的情况下，只能采用另外一种方式。在当前状态下，Ceph不允许任何的读写操作，所以此时任何的Ceph命令都不好使，解决的方案就是尝试降低Ceph对于full的比例定义，我们从上面的日志中可以看到Ceph的full的比例为95%，我们需要做的就是提高full的比例，之后尽快尝试删除数据，将比例下降。</p><ul><li>尝试直接用命令设置，但是失败了，Ceph集群并没有重新同步数据，怀疑可能仍然需要重启服务本身</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon tell \* injectargs &#39;--mon-osd-full-ratio 0.98&#39;</span><br></pre></td></tr></table></figure><ul><li>修改配置文件，之后重启monitor服务，但是担心出问题，所以没有敢尝试该方法，后续经过在邮件列表确认，该方法应该不会对数据产生影响，但是前提是在恢复期间，所有的虚拟机不要向Ceph再写入任何数据。</li></ul><p>默认情况下full的比例是95%，而near full的比例是85%，所以需要根据实际情况对该配置进行调整。</p><figure class="highlight plain"><figcaption><span>/etc/ceph/ceph.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">    mon osd full ratio &#x3D; .98</span><br><span class="line">    mon osd nearfull ratio &#x3D; .80</span><br></pre></td></tr></table></figure><h2 id="分析总结"><a href="#分析总结" class="headerlink" title="分析总结"></a>分析总结</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>根据Ceph官方文档中的描述，当一个OSD full比例达到95%时，集群将不接受任何Ceph Client端的读写数据的请求。所以导致虚拟机在重启时，无法启动的情况。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>从官方的推荐来看，应该比较支持添加新的OSD的方式，当然临时的提高比例是一个解决方案，但是并不推荐，因为需要手动的删除数据去解决，而且一旦再有一个新的节点出现故障，仍然会出现比例变满的状况，所以解决之道最好是扩容。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>在这次故障过程中，有两点是值得思考的：</p><ul><li>监控：由于当时服务器在配置过程中DNS配置错误，导致监控邮件无法正常发出，从而没有收到Ceph WARN的提示信息</li><li>云平台本身： 由于Ceph的机制，在OpenStack平台中分配中，大多时候是超分的，从用户角度看，拷贝大量数据的行为并没有不妥之处，但是由于云平台并没有相应的预警机制，导致了该问题的发生</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity" target="_blank" rel="noopener">http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;故障描述&quot;&gt;&lt;a href=&quot;#故障描述&quot; class=&quot;headerlink&quot; title=&quot;故障描述&quot;&gt;&lt;/a&gt;故障描述&lt;/h2&gt;&lt;p&gt;OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ceph" scheme="http://sunqi.me/categories/openstack/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>(Kilo)Devstack Kilo版本localrc推荐</title>
    <link href="http://sunqi.me/2015/05/10/best-localrc-for-devstack-kilo/"/>
    <id>http://sunqi.me/2015/05/10/best-localrc-for-devstack-kilo/</id>
    <published>2015-05-10T19:33:44.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。</p><a id="more"></a><p>该配置文件中开启了所有的OpenStack的核心模块，以下几点需要注意：</p><ul><li>为了运行Neutron，服务器必须是双网卡，否则外网不会通</li><li>我的实验网段为200.21.0.0/16，eth0的IP为200.21.1.61，eth1与eth0为同一网段</li><li>eth1为公网访问网络，floating网络范围200.21.50.1/24，配置的GATEWAY为200.21.50.2</li><li>保证eth1所处的网段能够连接外网，但是配置为manual模式，配置如下：</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto eth1</span><br><span class="line">iface eth1 inet manual</span><br><span class="line">up ifconfig $IFACE 0.0.0.0 up</span><br><span class="line">down ifconfig $IFACE 0.0.0.0 down</span><br></pre></td></tr></table></figure><ul><li>localrc的配置</li></ul><figure class="highlight plain"><figcaption><span>localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Misc</span><br><span class="line">ADMIN_PASSWORD&#x3D;sysadmin</span><br><span class="line">DATABASE_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">RABBIT_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">SERVICE_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">SERVICE_TOKEN&#x3D;$ADMIN_PASSWORD</span><br><span class="line"></span><br><span class="line"># Target Path</span><br><span class="line">DEST&#x3D;&#x2F;opt&#x2F;stack.kilo</span><br><span class="line"></span><br><span class="line"># Enable Logging</span><br><span class="line">LOGFILE&#x3D;$DEST&#x2F;logs&#x2F;stack.sh.log</span><br><span class="line">VERBOSE&#x3D;True</span><br><span class="line">LOG_COLOR&#x3D;True</span><br><span class="line">SCREEN_LOGDIR&#x3D;$DEST&#x2F;logs</span><br><span class="line"></span><br><span class="line"># Nova</span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line"># Neutron</span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-lbaas,q-vpn,q-fwaas</span><br><span class="line"></span><br><span class="line"># Ceilometer</span><br><span class="line">enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api</span><br><span class="line">enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator</span><br><span class="line"></span><br><span class="line"># Enable Heat</span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"># Trove</span><br><span class="line">enable_service trove tr-api tr-tmgr tr-cond</span><br><span class="line"></span><br><span class="line"># Sahara</span><br><span class="line">enable_service sahara</span><br><span class="line"></span><br><span class="line">#FIXED_RANGE&#x3D;10.0.0.0&#x2F;24</span><br><span class="line">HOST_IP&#x3D;200.21.1.61</span><br><span class="line">FLOATING_RANGE&#x3D;200.21.50.1&#x2F;24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;200.21.50.2</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;200.21.50.100,end&#x3D;200.21.50.150</span><br></pre></td></tr></table></figure><ul><li><p>确认br-ex是否存在</p><figure class="highlight plain"><figcaption><span>sudo ovs-vsctl show</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bridge br-ex</span><br><span class="line">    Port br-ex</span><br><span class="line">        Interface br-ex</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;qg-7ec5be02-69&quot;</span><br><span class="line">        Interface &quot;qg-7ec5be02-69&quot;</span><br><span class="line">            type: internal</span><br><span class="line">ovs_version: &quot;2.0.2&quot;</span><br></pre></td></tr></table></figure></li><li><p>将eth1作为br-ex的接口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth1</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><figcaption><span>sudo ovs-vsctl show</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Bridge br-ex</span><br><span class="line">    Port br-ex</span><br><span class="line">        Interface br-ex</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;qg-7ec5be02-69&quot;</span><br><span class="line">        Interface &quot;qg-7ec5be02-69&quot;</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;eth1&quot;</span><br><span class="line">        Interface &quot;eth1&quot;</span><br><span class="line">ovs_version: &quot;2.0.2&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="cloud computing" scheme="http://sunqi.me/categories/openstack/cloud-computing/"/>
    
      <category term="devstack" scheme="http://sunqi.me/categories/openstack/cloud-computing/devstack/"/>
    
      <category term="kilo" scheme="http://sunqi.me/categories/openstack/cloud-computing/devstack/kilo/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenStack Kilo版本新功能分析</title>
    <link href="http://sunqi.me/2015/05/03/what-is-new-in-kilo/"/>
    <id>http://sunqi.me/2015/05/03/what-is-new-in-kilo/</id>
    <published>2015-05-03T18:37:22.000Z</published>
    <updated>2020-01-14T08:51:56.532Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。</p><p>OpenStack Kilo版本的完整翻译版本可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans</a></p><p>OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。</p><a id="more"></a><h2 id="社区贡献分析"><a href="#社区贡献分析" class="headerlink" title="社区贡献分析"></a>社区贡献分析</h2><p>我们先来看一下OpenStack在最近的4个稳定版本发布中，每一个项目的贡献情况：</p><img src="/images/blogs/what-is-new-in-kilo-contribution-by-modules.jpg" class="left"><p>我们能够很明显的发现，OpenStack最早的几大核心模块(Nova, Cinder, Glance, Keystone, Horizon, Swift)的代码贡献所占比例呈明显下降趋势，这里强调一下，是比例而不是数量，从数量上来看，版本之间相差并不大，以Nova为例，从Havana版本的24%下降到如今的10%。这从一个侧面反映了OpenStack的核心模块日趋稳定，更多的关注集中到更高层次或者功能优化上。</p><p>Neutron模块则一直处于稳中有升的状态,从Havana版本的7%上升到10%，说明Neutron仍然处于需要进一步完善的状态。</p><p>对于Ceilometer，Heat，Sahara，Ironic, Trove等新晋的核心模块，都处于稳步增长的阶段。贡献的比例在四个版本中基本保持持平的态势。在Kilo版本中，Sahara和Heat进入了前十名。</p><p>从Kilo版本的比例来看，Others的比例过半，Others主要包括了OpenStack测试相关项目，例如Rally；开发相关项目，例如Devstack;以及一些新的模块，例如：Manila，Magnum等众多进入孵化器的项目;还包括所有的Client以及Spec等。可以预见，OpenStack的开发重心逐步从底层的核心模块，逐步向更高层次、提供更丰富功能的方向发展。</p><h2 id="国内社区贡献分析"><a href="#国内社区贡献分析" class="headerlink" title="国内社区贡献分析"></a>国内社区贡献分析</h2><img src="/images/blogs/what-is-new-in-kilo-contributor.png" class="center"><p>从企业贡献排名来看，几大巨头企业牢牢占据贡献榜的前几名，OpenStack最成功的公司-Mirantis排名紧追Redhat成为第二贡献大户。排名前几位的公司还包括：IBM, Rackspace, Cisco, Suse, VMware, Intel等。</p><p>国内方面，华为继续稳定在第13名，但Review的数量从Juno版本的1353提升到2548个，贡献的项目几乎涵盖所有的项目，主要贡献来自Heat，Ceilometer, Horizon，Neutron, Nova等项目。</p><p>国内排名第2的贡献企业是九州云，排名达到了21位，看来龚永生的到来为九州云添加了无限活力。九州云的贡献主要来自Horizon和Neutron两个项目，龚永生不愧为Neutron的Core，在网络方面的贡献，九州云的确很给力。</p><p>排名第3的企业是海云捷迅，排名为44位，海云是国内比较早的一批OpenStack创业企业，贡献方面以Sahara，Neutron，Nova，oslo.messaging以及Cinder为主，从之前了解的情况来看，海云的项目不少，可能提交的修改是与在实际项目中遇到的问题有关。</p><p>排名之后的企业还有Kylin Cloud，UnitedStack，EasyStack等。由于是手工统计，在统计过程中如有遗漏，希望大家多多指正。</p><h2 id="Horizon新功能"><a href="#Horizon新功能" class="headerlink" title="Horizon新功能"></a>Horizon新功能</h2><p>Horizon在K版本除了增强了对新增模块的支持，从UE的角度也为我们带来了很多新功能</p><ul><li>支持向导式的创建虚拟机，现在还处于beta版本，如果想在Horizon里激活，可以通过设置local_setting.py的配置实现：</li></ul><figure class="highlight plain"><figcaption><span>local_setting.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LAUNCH_INSTANCE_NG_ENABLED &#x3D; True</span><br></pre></td></tr></table></figure><img src="/images/blogs/what-is-new-in-kilo-instance-guide1.png" class="left"><img src="/images/blogs/what-is-new-in-kilo-instance-guide2.png" class="left"><ul><li>支持简单的主题，主要通过修改_variables.scss和_style.scss完成对主题颜色和简单样式的修改，但是格局不能改变，修改local_settings.py</li></ul><figure class="highlight plain"><figcaption><span>local_setting.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUSTOM_THEME_PATH &#x3D; &#39;static&#x2F;themes&#x2F;blue&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>static/themes/blue/_variables.scss</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$gray:                   #2751DB !default;</span><br><span class="line">$gray-darker:            #94A5F2 !default;</span><br><span class="line">$gray-dark:              #0C0CED !default;</span><br><span class="line">$gray-light:             #C7CFF2 !default;</span><br><span class="line">$gray-lighter:           #DCE1F5 !default;</span><br><span class="line"></span><br><span class="line">$brand-primary:         #375A7F !default;</span><br><span class="line">$brand-success:         #00bc8c !default;</span><br><span class="line">$brand-info:            #34DB98 !default;</span><br><span class="line">$brand-warning:         #F39C12 !default;</span><br><span class="line">$brand-danger:          #E74C3C !default;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>static/themes/blue/_style.scss</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Blue</span><br><span class="line">&#x2F;&#x2F; ----</span><br><span class="line"></span><br><span class="line">@mixin btn-shadow($color) &#123;</span><br><span class="line">  @include gradient-vertical-three-colors(lighten($color, 3%), $color, 6%, darken($color, 3%));</span><br><span class="line">  filter: none;</span><br><span class="line">  border: 1px solid darken($color, 10%);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Buttons &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">.btn-default,</span><br><span class="line">.btn-default:hover &#123;</span><br><span class="line">  @include btn-shadow($btn-default-bg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">.btn-primary,</span><br><span class="line">.btn-primary:hover &#123;</span><br><span class="line">  @include btn-shadow($btn-primary-bg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><img src="/images/blogs/what-is-new-in-kilo-horizon-theme1.png" class="left"><img src="/images/blogs/what-is-new-in-kilo-horizon-theme2.png" class="left"><h2 id="Nova新功能"><a href="#Nova新功能" class="headerlink" title="Nova新功能"></a>Nova新功能</h2><h3 id="Nova-Scheduler"><a href="#Nova-Scheduler" class="headerlink" title="Nova Scheduler"></a>Nova Scheduler</h3><ul><li>标准化了conductor，compute与scheduler的接口，为之后的接口分离做好准备，对于部分直接访问nova数据库的filters进行了优化，不再允许直接访问，参考链接：<a href="https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst" target="_blank" rel="noopener">https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst</a></li><li>对Scheduler做了一些优化，例如：Scheduler对于每一个请求都会重新进行Filters/Weighers，为了优化这个问题，将filter/weighter的初始化从handler移到scheduler，这样每次请求的时候都可以重新使用了。</li></ul><h3 id="Libvirt-NFV相关功能"><a href="#Libvirt-NFV相关功能" class="headerlink" title="Libvirt NFV相关功能"></a>Libvirt NFV相关功能</h3><ul><li>NUMA(Non Uniform Memory Architecture)，在这个架构下，每个处理器都会访问“本地”的内存池，从而在CPU和存储之间有更小的延迟和更大的带宽。</li><li>在Kilo版本中针对此功能的实现包括：基于NUMA的调度的实现；可以将vCPU绑定在物理CPU上；超大页的支持。以上提到的三点都是通过Flavor的Extra Spces完成定义的。</li></ul><h3 id="EC2-API"><a href="#EC2-API" class="headerlink" title="EC2 API"></a>EC2 API</h3><ul><li>EC2 API被从Nova中踢出去了</li><li>取而代之的是在stackforge的EC2 API转换服务</li></ul><h3 id="API-Microversioning"><a href="#API-Microversioning" class="headerlink" title="API Microversioning"></a>API Microversioning</h3><p>先来解释一下为什么需要API的微版本：主要原因在于现在这种API扩展方式，对于API实现的代码的增加或减少管理非常不方便，容易导致不一致性。引入微版本主要目的就是让开发人员在修改API代码时能够向前兼容，而不是加入一个新的API扩展；用户通过指定API的版本，在请求时也能决定是使用的具体的动作。</p><p>包含版本的返回:</p><figure class="highlight plain"><figcaption><span>Result</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;</span><br><span class="line">&#123;</span><br><span class="line">     &quot;versions&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;v2.1&quot;,</span><br><span class="line">            &quot;links&quot;: [</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;href&quot;: &quot;http:&#x2F;&#x2F;localhost:8774&#x2F;v2&#x2F;&quot;,</span><br><span class="line">                    &quot;rel&quot;: &quot;self&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ],</span><br><span class="line">            &quot;status&quot;: &quot;CURRENT&quot;,</span><br><span class="line">            &quot;version&quot;: &quot;5.2&quot;</span><br><span class="line">            &quot;min_version&quot;: &quot;2.1&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>客户端的Header信息：</p><figure class="highlight plain"><figcaption><span>Header</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-OpenStack-Nova-API-Version: 2.114</span><br></pre></td></tr></table></figure><h3 id="一个已知的问题：Evacuate"><a href="#一个已知的问题：Evacuate" class="headerlink" title="一个已知的问题：Evacuate"></a>一个已知的问题：Evacuate</h3><p>这个问题的产生主要是因为Evacuate的清理机制，主机名的变化会导致nova-compute重启过程中误删所有虚拟机，所以一个变通的方法是设置</p><figure class="highlight plain"><figcaption><span>nova.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">destroy_after_evacuate&#x3D;False</span><br></pre></td></tr></table></figure><p>这个问题会在Liberty中得到修复，相关的Spec：<a href="https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst" target="_blank" rel="noopener">https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst</a></p><h2 id="Glance新功能"><a href="#Glance新功能" class="headerlink" title="Glance新功能"></a>Glance新功能</h2><ul><li>自动进行镜像格式转化，例如，Ceph是使用RAW格式的，假如我们上传的是QCOW2，创建虚拟机时，就会经历一番上传下载的过程，速度异常缓慢。而且RAW格式通常都是原始大小，上传时候非常慢，完全可以通过上传小镜像自动转换为指定格式。</li><li>Glance支持多字段排序<figure class="highlight plain"><figcaption><span>API</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;images?sort_key&#x3D;status&amp;sort_dir&#x3D;asc&amp;sort_key&#x3D;name&amp;sort_dir&#x3D;asc&amp;sort_key&#x3D;created_at&amp;sort_dir&#x3D;desc</span><br></pre></td></tr></table></figure></li><li>临时将镜像设置为非活跃状态，假如一个镜像里有病毒，管理员就会将该镜像设置为非活跃状态，在清理后重新发布该镜像，在这个过程中，所有非管理员用户都无法使用或者下载这个镜像</li><li>免重启动态加载配置文件，配置文件改动后重启服务，现在可以给glance服务发送SIGHUP触发，这样升级就可以零当机时间。</li><li>使用多个Swift容器存储镜像，减少大规模部署时的请求瓶颈</li></ul><h2 id="Cinder新功能"><a href="#Cinder新功能" class="headerlink" title="Cinder新功能"></a>Cinder新功能</h2><ul><li>实现服务逻辑代码与数据库结构之间的解耦，支持Rolling更新</li><li>一致性组是指具备公共操作的卷，逻辑上化为一组。在K版本中对增强一致性组的功能：可以添加、删除卷，从已经存在的快照创建新的组，关于一致性组的详细操作可以参考：<a href="http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html" target="_blank" rel="noopener">http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html</a></li></ul><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cinder consisgroup-update</span><br><span class="line">[--name NAME]</span><br><span class="line">[--description DESCRIPTION]</span><br><span class="line">[--add-volumes UUID1,UUID2,......]</span><br><span class="line">[--remove-volumes UUID3,UUID4,......]</span><br><span class="line">CG</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cinder consisgroup-create-from-src</span><br><span class="line">[--cgsnapshot CGSNAPSHOT]</span><br><span class="line">[--name NAME]</span><br><span class="line">[--description DESCRIPTION]</span><br></pre></td></tr></table></figure><ul><li>卷类型的增强功能主要包含两个：为某一项目创建私有的卷类型和为卷类型增加描述信息</li></ul><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cinder type-create &lt;name&gt; --is-public</span><br><span class="line">cinder type-create &lt;name&gt; &lt;description&gt;</span><br></pre></td></tr></table></figure><h2 id="Neutron新功能"><a href="#Neutron新功能" class="headerlink" title="Neutron新功能"></a>Neutron新功能</h2><ul><li>DVR支持OVS中的VLANs</li><li>新的V2版本的LBaas的API</li><li>新的插件的更新，详情请见更新日志中</li><li>一些高级服务的分离，例如：L3, ML2, VPNaaS, LBaaS</li></ul><p>网络方面我不是权威，希望有高人能出来讲讲Kilo中的Neutron新功能。</p><h2 id="Keystone新功能"><a href="#Keystone新功能" class="headerlink" title="Keystone新功能"></a>Keystone新功能</h2><ul><li>项目嵌套，创建一个新的Project时候，可以指定parent的Project</li></ul><figure class="highlight plain"><figcaption><span>keystone</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;projects</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;project&quot;: &#123;</span><br><span class="line">        &quot;description&quot;: &quot;Project space for Test Group&quot;,</span><br><span class="line">        &quot;domain_id&quot;: &quot;1789d1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;name&quot;: &quot;Test Group&quot;,</span><br><span class="line">        &quot;parent_id&quot;: &quot;7fa612&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Keystone与Keystone的联盟，有了这个功能两个或者更多的云服务提供者就可以共享资源，这个功能在J版本引入，在K版本中主要针对该功能的进一步增强，具体的使用方法可参考这篇博文：<a href="http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/" target="_blank" rel="noopener">http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/</a></li><li>针对新人授权的一些增强功能</li><li>keystone的配置中有部分配置发生了变化，例如：keystone.token.backends.memcache被keystone.token.persistence.backends.memcache取代，更多详细内容请参考更新日志</li></ul><h2 id="Swift新功能"><a href="#Swift新功能" class="headerlink" title="Swift新功能"></a>Swift新功能</h2><ul><li>纠删码的加入应该是这个版本最大的亮点，但是纠删码作为beta版本发布，并不推荐应用于生产环境，关于纠删码的详细介绍可以参考：<a href="http://docs.openstack.org/developer/swift/overview_erasure_code.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/swift/overview_erasure_code.html</a></li><li>复合型令牌，简而言之就是需要用户加上服务的Token才能对Swfit存放的内容进行操作，如下图所示：</li></ul><figure class="highlight plain"><figcaption><span>swift</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">client</span><br><span class="line">   \</span><br><span class="line">    \   &lt;request&gt;: &lt;path-specific-to-the-service&gt;</span><br><span class="line">     \  x-auth-token: &lt;user-token&gt;</span><br><span class="line">      \</span><br><span class="line">    SERVICE</span><br><span class="line">       \</span><br><span class="line">        \    PUT: &#x2F;v1&#x2F;SERVICE_1234&#x2F;&lt;container&gt;&#x2F;&lt;object&gt;</span><br><span class="line">         \   x-auth-token: &lt;user-token&gt;</span><br><span class="line">          \  x-service-token: &lt;service-token&gt;</span><br><span class="line">           \</span><br><span class="line">          Swift</span><br></pre></td></tr></table></figure><p>具体的设计文档：<a href="http://docs.openstack.org/developer/swift/overview_backing_store.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/swift/overview_backing_store.html</a></p><ul><li>全局性的集群复制优化，大幅提高效率，避免经过广域网传播的数据</li></ul><h2 id="Ceilometer新功能"><a href="#Ceilometer新功能" class="headerlink" title="Ceilometer新功能"></a>Ceilometer新功能</h2><ul><li>支持Ceph对象存储监控，当对象存储为Ceph而不是Swfit的时候，使用Polling机制，使用Ceph的Rados Gateway的API接口获取数据，具体的设计文档：<a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst" target="_blank" rel="noopener">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst</a></li><li>Ceilometer API RBAC - 更细粒度的权限控制: <a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst" target="_blank" rel="noopener">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst</a><figure class="highlight plain"><figcaption><span>Ceilometer</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;context_is_admin&quot;: [[&quot;role:admin&quot;]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>更细粒度的控制<figure class="highlight plain"><figcaption><span>Ceilometer</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     &quot;context_is_admin&quot;: [[&quot;role:admin&quot;]],</span><br><span class="line">     &quot;admin_or_cloud_admin&quot;: [[&quot;rule:context_is_admin&quot;],</span><br><span class="line">              [&quot;rule:admin_and_matching_project_domain_id&quot;]],</span><br><span class="line">     &quot;telemetry:alarm_delete&quot;: [[&quot;rule:admin_or_cloud_admin&quot;]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>接口中的模糊查询，增加了一个新的查询符号=~</li><li>支持更多的测量，包括Hyper-V，IPMI相关的</li></ul><h2 id="Ironic新功能"><a href="#Ironic新功能" class="headerlink" title="Ironic新功能"></a>Ironic新功能</h2><ul><li>iLO的优化</li><li>使用Config Drive替代Metadata服务</li><li>全盘镜像支持，可以跳过raddisk和kernel，这样就可以部署Windows的镜像了</li><li>使用本地盘启动，替代PXE方式，可以通过设置flavor的capabilities:boot_option实现</li></ul><h2 id="Oslo"><a href="#Oslo" class="headerlink" title="Oslo"></a>Oslo</h2><p>解决了很多之前遗留的技术债，还有一些命名规范的问题。olso.messaging实现了心跳，olso.log在所有项目中使用，优化了oslo.db的代码。</p><h2 id="OpenStack文档"><a href="#OpenStack文档" class="headerlink" title="OpenStack文档"></a>OpenStack文档</h2><p>优化了docs.openstack.org页面，也可以从中选择相应的语言。有专门的团队负责安装、网络和高可靠的文档。</p><h2 id="其他模块"><a href="#其他模块" class="headerlink" title="其他模块"></a>其他模块</h2><p>对于Sahara, Heat, Trove等模块的更新没有在这里Highlight出来，大家可以参考更新日志里的内容，或者查看specs中的具体描述。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过Kilo的一些更新可以看到，Kilo版本在不断优化代码结构的基础上，增加了一些新功能，也偿还了一些技术债，总体来说是一种稳中有升的态势，但是总体感觉并没有太多的惊喜和出人意料。相信随着更多的孵化项目进入正式版本中，OpenStack一定会向更多元化的方向发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。&lt;/p&gt;
&lt;p&gt;OpenStack Kilo版本的完整翻译版本可见：&lt;a href=&quot;https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
</feed>
