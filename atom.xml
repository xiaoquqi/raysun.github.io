<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老孙正经胡说</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.site/"/>
  <updated>2021-02-02T10:06:02.390Z</updated>
  <id>http://sunqi.site/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>CentOS7 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T09:15:02.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：</p><p><img src="/images/pasted-124.png" alt="upload successful"></p><p>原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。</p><p>CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。</p><a id="more"></a><h1 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h1><p>虽然通过yum方式可以安装zsh，但是无法满足powerlevel10k的要求，所以先使用zsh源码进行编译后安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">curl -o zsh.tar.xz https:&#x2F;&#x2F;jaist.dl.sourceforge.net&#x2F;project&#x2F;zsh&#x2F;zsh&#x2F;5.8&#x2F;zsh-5.8.tar.xz</span><br><span class="line">tar -xvf zsh-5.8.tar.xz</span><br><span class="line"></span><br><span class="line">cd zsh</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>zsh会安装在用户目录中/usr/local/bin/zsh中，将zsh设置为默认的系统shell，配置成功后，需要关闭Terminal重新登陆。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chsh -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;zsh</span><br></pre></td></tr></table></figure><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>接下来的流程与MacOS上安装类似，由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-124.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt;原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。&lt;/p&gt;
&lt;p&gt;CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mac iTerm2 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T03:26:14.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。</p><p>我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：</p><p><img src="/images/pasted-123.png" alt="upload successful"></p><a id="more"></a><h1 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h1><p>我们开始配置前，还是有必要讲一下这几个项目的关系，以便了解其工作原理。</p><ul><li>iTerm2不用说了，MacOS上必备的Terminal工具，替代原有系统自带的工具。</li><li>ohmyzsh(<a href="https://github.com/ohmyzsh/ohmyzsh/" target="_blank" rel="noopener">https://github.com/ohmyzsh/ohmyzsh/</a>) 是一套基于zsh深度定制的插件及主题管理的框架，方便定制适合你的zsh环境。</li><li>Nerd Fonts(<a href="https://www.nerdfonts.com/" target="_blank" rel="noopener">https://www.nerdfonts.com/</a>) 我们在截图中看到的那些可爱的小图标就是来自这个项目，让我们原本枯燥的Terminal增添了几分乐趣。</li><li>powerlevel10k(<a href="https://github.com/romkatv/powerlevel10k" target="_blank" rel="noopener">https://github.com/romkatv/powerlevel10k</a>) 是一套zsh皮肤，也是目前我个人比较喜欢的一套皮肤，同时提供了较强的配置能力，包括字体下载，iTerm2的配置都自动完成了，所以也是目前使用最顺手的一套皮肤。</li></ul><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。&lt;/p&gt;
&lt;p&gt;我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-123.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何在微信开发者工具中使用vim编辑模式</title>
    <link href="http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/"/>
    <id>http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-01-26T23:52:10.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。</p><p>作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。</p><a id="more"></a><h1 id="在VS-Code安装vim插件"><a href="#在VS-Code安装vim插件" class="headerlink" title="在VS Code安装vim插件"></a>在VS Code安装vim插件</h1><p>首先在VS Code中安装vim模拟器，如图所示，我安装的是1.18.5版本。我使用的是mac系统，安装完成后，插件会存放在用户HOME目录下的$HOME/.vscode/extensions/vscodevim.vim-1.18.5中。</p><p><img src="/images/pasted-116.png" alt="upload successful"></p><h1 id="在微信开发者工具安装VS-Code插件"><a href="#在微信开发者工具安装VS-Code插件" class="headerlink" title="在微信开发者工具安装VS Code插件"></a>在微信开发者工具安装VS Code插件</h1><p>1、在微信开发者工具中点击“设置”-&gt;”扩展设置”</p><p><img src="/images/pasted-117.png" alt="upload successful"></p><p>2、在打开的窗口中选择“编辑器自定义扩展”，因为我已经安装过了，所以截图中已经包含了vscode.vim插件</p><p><img src="/images/pasted-118.png" alt="upload successful"></p><p>3、点击上方的“打开扩展文件夹”，此时会打开微信开发者插件目录，而你要做的就是将vscode插件拷贝过去。</p><p><img src="/images/pasted-119.png" alt="upload successful"></p><p>但是由于从Finder中无法直接访问隐藏目录，先在左侧选择HOME目录。</p><p><img src="/images/pasted-121.png" alt="upload successful"></p><p>使用“前往文件夹”选项，填入.vscode/extensions。将.vscode/extensions/vscodevim.vim-1.18.5拷贝之刚才打开的微信开发者工具的扩展目录中。</p><p><img src="/images/pasted-120.png" alt="upload successful"></p><p>4、重启微信开发者工具后，就能在“编辑器自定义扩展”中看到vim插件，启动插件后，再次退出重启，此时编辑器里已经可以使用vim模式了。</p><p><img src="/images/pasted-122.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。&lt;/p&gt;
&lt;p&gt;作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>使用镜像源加速Github Clone速度</title>
    <link href="http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/"/>
    <id>http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/</id>
    <published>2021-01-25T01:11:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？</p><a id="more"></a><h1 id="设定gitconfig自动实现替换"><a href="#设定gitconfig自动实现替换" class="headerlink" title="设定gitconfig自动实现替换"></a>设定gitconfig自动实现替换</h1><p>通过在HOME目录下的.gitconfig文件可以实现自动的对github.com进行替换的目的，具体的方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global url.&quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;.insteadOf https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><p>在$HOME/.gitconfig会发现增加了如下行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[url &quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;]</span><br><span class="line">insteadOf &#x3D; https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><h1 id="其他镜像源"><a href="#其他镜像源" class="headerlink" title="其他镜像源"></a>其他镜像源</h1><p>目前国内提供github镜像源还包括以下地址，但是通过网站测速（<a href="https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：" target="_blank" rel="noopener">https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：</a></p><ul><li>fastgit.org: <a href="https://doc.fastgit.org/" target="_blank" rel="noopener">https://doc.fastgit.org/</a></li><li>gitclone.com: <a href="https://gitclone.com/" target="_blank" rel="noopener">https://gitclone.com/</a></li><li>gitee: <a href="https://gitee.com/mirrors" target="_blank" rel="noopener">https://gitee.com/mirrors</a></li><li>cnpmjs.org: <a href="https://github.com.cnpmjs.org/" target="_blank" rel="noopener">https://github.com.cnpmjs.org/</a></li></ul><h1 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h1><p>还有一种情况是要从github下载某个文件，由于raw.githubusercontent.com属于长期被屏蔽状态，所以基本通过wget进行下载，比如要下载的文件为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -O https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure><p>可以替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;raw.staticdn.net&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Git" scheme="http://sunqi.site/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>利用Docker快速搭建Prometheus监控及告警平台</title>
    <link href="http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/"/>
    <id>http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/</id>
    <published>2020-12-25T13:32:48.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。</p><p>目前本文涉及的监控内容：</p><ul><li>主机监控</li><li>容器监控</li><li>Ceph监控</li><li>VMware监控</li></ul><a id="more"></a><h1 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h1><p>我们假设读者已经使用CentOS搭建了容器环境，并配置了国内源的前提下。如果没有设置请参考<a href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/">《CentOS 7和Docker初始化安装》</a>。</p><p>Prometheus快速构建的docker compose原始项目来自<a href="https://github.com/stefanprodan/dockprom" target="_blank" rel="noopener">stefanprodan/dockprom</a>，但是由于原项目中的cAdvisor使用了Google源，所以Fork的项目修改为国内源<a href="https://github.com/xiaoquqi/dockprom" target="_blank" rel="noopener">xiaoquqi/dockprom</a>。</p><p>原项目中包含的组件：</p><ul><li>Prometheus (metrics database) http://<host-ip>:9090</li><li>Prometheus-Pushgateway (push acceptor for ephemeral and batch jobs) http://<host-ip>:9091</li><li>AlertManager (alerts management) http://<host-ip>:9093</li><li>Grafana (visualize metrics) http://<host-ip>:3000</li><li>Caddy (reverse proxy and basic auth provider for prometheus and alertmanager)</li></ul><p>默认包含的采集器：</p><ul><li>NodeExporter (host metrics collector)</li><li>cAdvisor (containers metrics collector)</li></ul><p>在此基础上增加的内容：</p><ul><li>Ceph exporter</li><li>VMware exporter</li><li>钉钉告警webhook</li><li>轻量级http服务，用于内网分发docker-compse.exporter.yml</li></ul><h1 id="环境快速构建"><a href="#环境快速构建" class="headerlink" title="环境快速构建"></a>环境快速构建</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;dockprom</span><br><span class="line">cd dockprom</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>启动完成后，用浏览器访问：</p><ul><li>Prometheus: <a href="http://yourip:9090" target="_blank" rel="noopener">http://yourip:9090</a></li><li>Grafana: <a href="http://yourip:3000" target="_blank" rel="noopener">http://yourip:3000</a></li></ul><p>默认的用户名/密码为: admin/admin，如果需要修改可以在启动之前修改docker-compose.yml文件。</p><p>访问Prometheus，查看metrics是否被正确采集。如果有采集器有红色字样，根据提示查看具体的错误原因，大部分的错误都是因为配置问题，或者网络不通造成的。</p><p><img src="/images/pasted-109.png" alt="upload successful"></p><h1 id="Grafana配置"><a href="#Grafana配置" class="headerlink" title="Grafana配置"></a>Grafana配置</h1><p>访问Grafana的控制面板，其中已经内置了一些模板，也可以选择Import导入Grafana模板库的模板，数据源选择已经配置好的Prometheus即可。</p><p><img src="/images/pasted-110.png" alt="upload successful"></p><h1 id="主机监控"><a href="#主机监控" class="headerlink" title="主机监控"></a>主机监控</h1><p>默认安装情况下，主机层面仅监控了本机，如果需要增加新的监控主机，需要进行以下两步：</p><ul><li>为主机安装node exporter</li><li>修改Prometheus配置文件，并重启服务</li></ul><h2 id="1、安装node-exporter"><a href="#1、安装node-exporter" class="headerlink" title="1、安装node exporter"></a>1、安装node exporter</h2><p>在项目中，内置了一个单独的docker-compose.exporters.yml，如果目标主机安装了容器，可以直接将该yaml文件拷贝至目标节点后，启动监控服务即可。当然也可以通过软件包安装方式，本文不再赘述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>安装完成后，访问metrics接口，即代表安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9100&#x2F;metrics</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;14&quot;&#125; 2.8795339e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;15&quot;&#125; 2.3535384e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;16&quot;&#125; 3.4674675e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;17&quot;&#125; 2.5727501e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;18&quot;&#125; 2.5931391e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;19&quot;&#125; 2.67231846e+08</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;2&quot;&#125; 4.3448998e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;20&quot;&#125; 3.0684276e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;21&quot;&#125; 3.0587632e+07</span><br></pre></td></tr></table></figure><h2 id="2、修改Prometheus配置文件"><a href="#2、修改Prometheus配置文件" class="headerlink" title="2、修改Prometheus配置文件"></a>2、修改Prometheus配置文件</h2><p>回到Prometheus节点，找到dockerprom/prometheus/prometheus.yml进行如下修改，在nodeexporter段的targets增加新的监控节点后重启服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># A scrape configuration containing exactly one endpoint to scrape.</span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#39;nodeexporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;nodeexporter:9100&#39;, &#39;newip:9100&#39;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><h1 id="告警配置"><a href="#告警配置" class="headerlink" title="告警配置"></a>告警配置</h1><p>其实监控并不是最终的目的，往往告警才是监控系统成功与否的关键，在实际运维中对于根因分析和告警收敛是有非常强烈的需求的，本文中暂时还没对此做深入的分析，仅仅提供了常规的告警手段。告警的配置方法有两种方式，一种是通过Prometheus AlertManager，另外一种也可以通过在Grafana上直接进行配置。</p><p>对于告警方式支持多种方式，例如我们常用的邮件或者钉钉等，当然你也可以实现你自己的方式，这里我们使用钉钉的WEBHOOK作为告警方式。</p><h2 id="1-钉钉webhook配置"><a href="#1-钉钉webhook配置" class="headerlink" title="1. 钉钉webhook配置"></a>1. 钉钉webhook配置</h2><p>默认已经启动了钉钉容器，只需要修改dingtalk/config.yaml即可。Targets下面有各种示例，比如配置一个最简单的钉钉告警：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">targets:</span><br><span class="line">  devops:</span><br><span class="line">    url: https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;xxxxx</span><br></pre></td></tr></table></figure><p>这里的devops是自定义的，但是和后面要填入alertmanager的链接地址有关，比如本例中alertmanager回调地址就是http://<yourip>:8060/dingtalk/devops/send</p><h2 id="2-修改AlertManager配置"><a href="#2-修改AlertManager配置" class="headerlink" title="2. 修改AlertManager配置"></a>2. 修改AlertManager配置</h2><p>  修改alertmanager/config.yml</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  route:</span><br><span class="line">    receiver: &#39;dingtalk&#39;</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">  - name: &#39;dingtalk&#39;</span><br><span class="line">    webhook_configs:</span><br><span class="line">    - send_resolved: true</span><br><span class="line">      url: http:&#x2F;&#x2F;&lt;yourip&gt;:8060&#x2F;dingtalk&#x2F;devops&#x2F;send</span><br></pre></td></tr></table></figure><p>  这里不要用localhost，因为部署在容器内。</p><h2 id="3-修改Prometheus配置文件"><a href="#3-修改Prometheus配置文件" class="headerlink" title="3. 修改Prometheus配置文件"></a>3. 修改Prometheus配置文件</h2><p>  修改alert.rules，尝试修改一些规则测试告警，例如：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: host</span><br><span class="line">rules:</span><br><span class="line">- alert: high_cpu_load</span><br><span class="line">  expr: node_load1 &gt; 0.2</span><br><span class="line">  for: 1s</span><br><span class="line">  labels:</span><br><span class="line">    severity: warning</span><br><span class="line">  annotations:</span><br><span class="line">    summary: &quot;Server under high load&quot;</span><br><span class="line">    description: &quot;Docker host is under high load, the avg load 1m is at &#123;&#123; $value&#125;&#125;. Reported by instance &#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125;.&quot;</span><br></pre></td></tr></table></figure><p>此时可以通过AlertManager查看http://<yourip>:9093/#/alerts，检查是否有告警产生。</p><p><img src="/images/pasted-111.png" alt="upload successful"></p><p>如果告警产生了，但是无法触发钉钉，可以通过检查alertmanager容器进行debug，例如上述提到的localhost问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">level&#x3D;warn ts&#x3D;2020-12-29T07:21:56.345Z caller&#x3D;notify.go:674 component&#x3D;dispatcher receiver&#x3D;dingtalk integration&#x3D;webhook[0] msg&#x3D;&quot;Notify attempt failed, will retry later&quot; attempts&#x3D;1 err&#x3D;&quot;Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br><span class="line">level&#x3D;error ts&#x3D;2020-12-29T07:26:56.344Z caller&#x3D;dispatch.go:309 component&#x3D;dispatcher msg&#x3D;&quot;Notify for alerts failed&quot; num_alerts&#x3D;1 err&#x3D;&quot;dingtalk&#x2F;webhook[0]: notify retry canceled after 16 attempts: Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br></pre></td></tr></table></figure><h1 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h1><p>确保Ceph配置文件已经在/etc/ceph目录下，并且能够正常访问Ceph集群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.ceph.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9128/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;ceph-exporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    honor_labels: true</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.201:9128&#39;]</span><br><span class="line">      labels:</span><br><span class="line">        instance: Ceph Cluster</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入三个模板：</p><ul><li>Ceph Cluster Overview: <a href="https://grafana.com/dashboards/917" target="_blank" rel="noopener">https://grafana.com/dashboards/917</a></li><li>Ceph Pools Overview: <a href="https://grafana.com/dashboards/926" target="_blank" rel="noopener">https://grafana.com/dashboards/926</a></li><li>Ceph OSD Overview: <a href="https://grafana.com/dashboards/923" target="_blank" rel="noopener">https://grafana.com/dashboards/923</a></li></ul><p>Ceph Cluster效果：</p><p><img src="/images/pasted-113.png" alt="upload successful"></p><p>Ceph Pool效果：</p><p><img src="/images/pasted-114.png" alt="upload successful"></p><p>Ceph OSD效果：</p><p><img src="/images/pasted-115.png" alt="upload successful"></p><h1 id="VMware监控"><a href="#VMware监控" class="headerlink" title="VMware监控"></a>VMware监控</h1><p>首先修改docker-compose.vmware.exporters.yml中vcenter的连接信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">services:</span><br><span class="line">  vmware-exporter:</span><br><span class="line">    image: pryorda&#x2F;vmware_exporter:v0.11.1</span><br><span class="line">    container_name: vmware-exporter</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    ports:</span><br><span class="line">       - &#39;9272:9272&#39;</span><br><span class="line">    expose:</span><br><span class="line">       - 9272</span><br><span class="line">    environment:</span><br><span class="line">      VSPHERE_HOST: &quot;VC_HOST&quot;</span><br><span class="line">      VSPHERE_IGNORE_SSL: &quot;True&quot;</span><br><span class="line">      VSPHERE_USER: &quot;VC_USERNAME&quot;</span><br><span class="line">      VSPHERE_PASSWORD: &quot;VC_PASSWORD&quot;</span><br><span class="line">    labels:</span><br><span class="line">      org.label-schema.group: &quot;monitoring&quot;</span><br></pre></td></tr></table></figure><p>启动VMware exporter：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.vmware.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9272/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;vmware_vcenter&#39;</span><br><span class="line">    metrics_path: &#39;&#x2F;metrics&#39;</span><br><span class="line">    scrape_timeout: 15s</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.13:9272&#39;]</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入模板：<a href="https://grafana.com/grafana/dashboards/11243" target="_blank" rel="noopener">https://grafana.com/grafana/dashboards/11243</a></p><p>效果如下：</p><p><img src="/images/pasted-112.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。&lt;/p&gt;
&lt;p&gt;目前本文涉及的监控内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主机监控&lt;/li&gt;
&lt;li&gt;容器监控&lt;/li&gt;
&lt;li&gt;Ceph监控&lt;/li&gt;
&lt;li&gt;VMware监控&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我需要一款什么样的网盘？</title>
    <link href="http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/"/>
    <id>http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/</id>
    <published>2020-12-14T13:56:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。</p><a id="more"></a><h1 id="阿里云盘or网盘——傻傻分不清楚"><a href="#阿里云盘or网盘——傻傻分不清楚" class="headerlink" title="阿里云盘or网盘——傻傻分不清楚"></a>阿里云盘or网盘——傻傻分不清楚</h1><p>我相信很多关注了阿里网盘的朋友都会有一个问题，到底哪个是阿里真的网盘？我们通过公开渠道，至少能找到阿里云有至少三款“云盘”或者“网盘”。</p><p><img src="/images/pasted-97.png" alt="upload successful"></p><p><img src="/images/pasted-98.png" alt="upload successful"></p><p><img src="/images/pasted-99.png" alt="upload successful"></p><p>幸运的一点，我除了通过阿里MVP渠道获取到Teambition开发的网盘，同时还获取了阿里云盘的内测码，所以有机会对这两款都进行了充分体验。</p><h2 id="Teambiton网盘"><a href="#Teambiton网盘" class="headerlink" title="Teambiton网盘"></a>Teambiton网盘</h2><p>2019年初阿里耗资1亿美金收购了团队协作平台Teambition，通过收购及整合兼并，阿里不断扩大自身在To B领域的影响力。在推出网盘前，Teambition已经推出了代码托管平台codeUp和Flow，为企业构建完整的DevOps流程垫定了基础。网盘也是顺应这一趋势，阿里也继续丰富着自己的To B版图。</p><p>不过与我们理解的传统网盘有所区别，目前网盘是紧耦合在原有的Teambition体系内，更像是Teambition的一个扩展功能，而不能独立使用，这一点从网页版和手机侧的设计就能看出来。所以，也许Teambition网盘的目标客户也许并不是传统的网盘用户，更像是解决企业成员间文件存储和分享的问题。</p><p>目前Teambition暂不支持企业网盘，还需要切换至个人账户。但是在我测试过程中，有个小Bug，我明明在登陆后是个人账户，但是仍然看到的是企业网盘暂未开放的提示信息。需要先切换到企业，再切换回个人后才能看到。</p><p><img src="/images/pasted-100.png" alt="upload successful"></p><p>先来看一下整体的界面风格，从内测版本的截图看，Teambition的网盘目前还处于非常简单的状态，界面上展现的功能并不多，我认为类似最早期对象存储的基本功能。</p><p><img src="/images/pasted-101.png" alt="upload successful"></p><p>由于目前还处于内测阶段，所以基本不会对上传和下载速度做任何限制，不知道在商用化之后免费权益到底如何？我家的宽带是联通300Mbps，上传之前对网速进行了基本测试，上传的时候我使用的是浏览器直接上传的方式，我的iMac使用网线和路由器直接进行连接。</p><p><img src="/images/pasted-103.png" alt="upload successful"></p><p>我是晚上做的测试，上传了一个4.66GB的MP4文件，上传速度基本稳定在了400KB/s。</p><p><img src="/images/pasted-104.png" alt="upload successful"></p><p>这个和我最早期的一次测试结果是有出入的，感觉速度没有这么慢，并且中途失败了好几次，于是第二天早晨的时候，在失败后进行了重传。这时候速度基本上能把网络的上行速度跑满。</p><p><img src="/images/pasted-107.png" alt="upload successful"></p><p>另外，对于网盘很重要的分享功能也并未开放，只能作为用户自有的网络存储空间。</p><p><img src="/images/pasted-105.png" alt="upload successful"></p><p>手机侧的功能也基本与网页版本相似，并且必须要借助Teambition APP使用，暂时没有提供任何自动备份的功能。</p><p><img src="/images/pasted-106.png" alt="upload successful"></p><h2 id="阿里云盘"><a href="#阿里云盘" class="headerlink" title="阿里云盘"></a>阿里云盘</h2><p>经过与阿里内部确认，阿里的云盘和Teambition是完全独立的两个团队，也就是这是两端独立的产品，所以这也让我怀疑我当初看到的文章到底是在讲网盘还是云盘的？</p><p>阿里的云盘更接近传统对网盘的认知，这一点从用户体验上就能感觉到。与网盘不同的是，阿里云盘相对来说比较独立，之前只开放了手机版本，目前已经可以从网页上进行登陆了。目前只能通过网页版进行内测资格的申请，而所有操作也只能在手机侧完成。阿里云盘的域名是aliyundrive.com。</p><p>目前的客户端方面只提供了手机侧的，对我来说比较重要的客户端的还没有看到。从上传的感受来说，云盘的稳定性要好于网盘，网速基本上稳定在5MB/s，中间没有出现任何断线情况。</p><p><img src="/images/pasted-108.png" alt="upload successful"></p><h1 id="我需要什么样的网盘产品？"><a href="#我需要什么样的网盘产品？" class="headerlink" title="我需要什么样的网盘产品？"></a>我需要什么样的网盘产品？</h1><p>回答这个问题，先要从我用了哪两款付费网盘产品说起。目前我使用的两款付费网盘的产品，一款是苹果的iCloud，一款是百度的网盘。iCloud的费用一个月是21元/月200GB家庭共享空间，而百度网盘超级会员一个月是18元/月空间，空间达到了5TB，但是无法家庭共享。</p><p>可能有人会问为什么每个月要花将近40元在付费网盘呢？为什么不使用同一种网盘呢？这主要源自我最主要的几个需求：</p><p>一、解决不同设备之间的文件同步问题。我目前使用的手机是iPhone、办公电脑是Mac Pro，而家里使用的是iMac。因为经常要在家里工作，而又懒着背笔记本回家，所以重要文件在不同电脑的传输对我来说就非常重要了。另外，因为有时候需要出去交流，所以会从手机侧查看文件，那么手机与PC之间的文件互通也变得非常重要了。百度网盘之前在Mac侧会有个同步盘的应用，但是后来不再维护了，并且同步盘之前在同步过程中经常发生同步冲突，而莫名其妙产生了多个文件的问题。后来还试过类似OneDrive等网盘，都有类似的问题。最终发现只有苹果自身的方案才能最完美的符合我的要求。<br>二、费用成本问题。虽然苹果的方案很完美，但是也是最贵的，苹果的付费储存空间方案跨度太大，200GB之后就是2TB，价格直接从21元/月涨到了68元/月。因为会拍摄一些视频资料，所以对空间消耗比加大，无奈之下，只得选择了百度网盘作为补充，一方面是之前有很多文件都存在了百度网盘上，另外一方面付费后也不受下载限速的影响了，算是作为一种二级存储的方案使用。近期，准备购置百度的智能音箱，还能直接播放网盘内宝宝的音频。<br>三、照片、视频自动备份功能。虽然iCloud也能对照片、视频进行自动备份，但是有了孩子之后发现照片和视频成倍增加，真是不禁用，这时百度网盘的照片、视频自动备份就派上了用场。</p><p>其实，百度网盘还有一些类似照片整理、搜索等功能也非常强大，但是相比前两点并非刚需，所以只是偶尔会用到。</p><p>虽然这种组合使用方式在一定程度基本满足了我的日常需求，只是要定期的将部分数据导入百度网盘略显繁琐。但是另外一个网盘的需求依然困扰着我，就是对于微信的备份功能。虽然钉钉在这一点上要明显好于微信，但是谁让微信是第一社交软件呢？平时的沟通还是在微信，经常遇到一些文件不随手存下来就被微信清理掉的情况。虽然腾讯也有自己的微盘，但是就是不支持微信的自动备份功能，如果支持我估计我会立马付费购买。</p><p>总结一下我对网盘的几个需求：第一，空间足够大，上传下载不限速；第二、设备之间能够互相同步；第三、微信聊天记录和附件的自动备份功能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Docker构建服务器空间占满问题</title>
    <link href="http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/"/>
    <id>http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/</id>
    <published>2020-11-13T06:19:41.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h1><p>今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。</p><a id="more"></a><p><img src="/images/pasted-94.png" alt="upload successful"></p><h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><h2 id="检查容器空间"><a href="#检查容器空间" class="headerlink" title="检查容器空间"></a>检查容器空间</h2><p>首先从容器层面检查一下空间占用情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df</span><br></pre></td></tr></table></figure><p>发现有容器的占用空间达到了1个多TB的空间。</p><p><img src="/images/pasted-95.png" alt="upload successful"></p><h2 id="清理无用的容器和镜像"><a href="#清理无用的容器和镜像" class="headerlink" title="清理无用的容器和镜像"></a>清理无用的容器和镜像</h2><p>先用prune进行一下清理，为了保险起见，过滤一下时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system prune -a -f --filter &quot;until &#x3D; 1h&quot;</span><br></pre></td></tr></table></figure><p>清理完成后，空间仍然没有释放，于是继续排查。</p><h2 id="检查-var-lib下的空间占用"><a href="#检查-var-lib下的空间占用" class="headerlink" title="检查/var/lib下的空间占用"></a>检查/var/lib下的空间占用</h2><p>通过检查发现/var/lib/docker/overlay2中的66d44a19ee93a191cc0585efac45e10696edfd0381d0dc96d9646080337f629e目录空间占用巨大，进入后发现其中有tmp目录没有及时清理。由于没有Jenkins任务在执行，所以手动清理了/tmp/tmp*的目录，空间被立即释放了。</p><p><img src="/images/pasted-96.png" alt="upload successful"></p><p>那么此时问题清晰了，这一层属于Jenkins，进入容器后发现Jenkins的/tmp目录没有被及时清理，属于Build逻辑有缺陷造成了，及时修复Pipeline的Jenkinsfile后，该问题不再出现。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;现象描述&quot;&gt;&lt;a href=&quot;#现象描述&quot; class=&quot;headerlink&quot; title=&quot;现象描述&quot;&gt;&lt;/a&gt;现象描述&lt;/h1&gt;&lt;p&gt;今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MacOS VPN拨号后自动设置路由</title>
    <link href="http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/"/>
    <id>http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/</id>
    <published>2020-11-11T12:04:07.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。</p><a id="more"></a><h1 id="允许L2TP共享密钥为空"><a href="#允许L2TP共享密钥为空" class="headerlink" title="允许L2TP共享密钥为空"></a>允许L2TP共享密钥为空</h1><p>公司L2TP共享密钥为空，默认MacOS是不支持的，所以需要在配置文件中特殊设定，在/etc/ppp下生成options文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tee &#x2F;etc&#x2F;ppp&#x2F;options &lt;&lt; EOF</span><br><span class="line">plugin L2TP.ppp</span><br><span class="line">l2tpnoipsec</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="自动路由设置"><a href="#自动路由设置" class="headerlink" title="自动路由设置"></a>自动路由设置</h1><p>原理很简单，在连接VPN后将指定网段的IP经过VPN虚拟接口即可，具体的实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo touch &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br><span class="line">sudo chmod 0755 &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br></pre></td></tr></table></figure><p>ip-up的内容如下，只需要修改SUBNET网段即可，例如：192.168.10.0/24</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">&#x2F;sbin&#x2F;route add &lt;SUBNET&gt; -interface $1</span><br></pre></td></tr></table></figure><p>其余可利用参数如下：</p><ul><li>$1: VPN接口(例如：ppp0)</li><li>$2: 未知</li><li>$3: VPN服务器地址</li><li>$4: VPN网关地址</li><li>$5: 非VPN网关，本地使用</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h1&gt;&lt;p&gt;公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用钉钉通讯录同步构建本地LDAP服务</title>
    <link href="http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/"/>
    <id>http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/</id>
    <published>2020-10-31T10:24:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？</p><a id="more"></a><p>目前，在我们公司使用OpenLDAP服务作为各个服务统一鉴权的入口，使用的应用系统包括：Gerrit/Jenkins/Yapi/Wiki/进度跟踪等，目前所有的系统都支持LDAP鉴权，所以如果能将钉钉的通讯录定期同步至LDAP中就可以实现统一鉴权的需求。但是由于钉钉的密码无法同步回本地，所以密码层面仍然是独立的。</p><p>本文章的实现思路参考了<a href="https://xujiwei.com/blog/2020/02/internal-authorize-based-on-dingtalk-virtual-ldap-keyclaok/" target="_blank" rel="noopener">《基于钉钉 + Virtual-LDAP + KeyCloak 的内网统一认证系统<br>》</a>，感谢原作者的思路及贡献的virtual-ldap模块，本文所有的优化都是基于此文章基础上进行的优化。</p><h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>简单来说，我们希望能通过钉钉提供的LDAP作为统一鉴权方式，但是由于钉钉没有开放这个能力，那么我们需要将钉钉模拟一个LDAP服务。模拟出的LDAP环境，在内网环境中，我们对于LDAP信息的使用基本上围绕着用户名和密码，其他的信息以钉钉为准。所以，除了开放LDAP接口外，我们还需要提供用户界面，允许用户在内网修改密码。</p><p>整体的实现思路如下：</p><ul><li>钉钉开发者平台：需要在钉钉开发者平台新建一个程序，获取鉴权信息后，赋予通讯录同步权限，提供给VirtualLDAP进行数据同步</li><li>VirtualLDAP：该组件是上面提到的作者开发的虚拟LDAP组件，主要功能为同步钉钉通讯录，并以LDAP协议对外提供服务</li><li>KeyCloak：对于这个场景过重，但是暂时没有发现更好的方案，可以触发自动同步并且可以让内网用户进行密码修改</li><li>本地的全部系统按照LDAP配置方式即可实现鉴权</li></ul><p><img src="/images/pasted-90.png" alt="upload successful"></p><h2 id="钉钉开发者平台"><a href="#钉钉开发者平台" class="headerlink" title="钉钉开发者平台"></a>钉钉开发者平台</h2><p>这里我创建的是H5微应用，配置时有几点需要注意：</p><ul><li>IP地址白名单：需要为你未来运行VirtualLDAP配置访问IP白名单，目前钉钉开发者平台对于同一个IP只能给一个应用使用，但是可以通过通配符进行配置，例如：192.168.10.*的方式，那么192.168.10网段所有地址均可以访问</li><li>权限：需要为该应用开放所有通讯录只读权限即可</li></ul><p><img src="/images/pasted-92.png" alt="upload successful"></p><h2 id="VirtualLDAP"><a href="#VirtualLDAP" class="headerlink" title="VirtualLDAP"></a>VirtualLDAP</h2><p>这是基于Node.js开发的一款组件，主要用于同步钉钉通讯录和模拟LDAP协议。基于原作者版本，为了满足自身应用场景，进行了如下修改：</p><ul><li>由于作者没有提供Docker运行方式，所以在github的pull request中有人进行了改造</li><li>仍然是在同一个pull request中，增加了pinyin组件，在LDAP中增加了一个pinyin属性，方便业务系统使用</li><li>登录名和密码：为了防止公司人员重复，所以特别采用了全拼名称+电话号码后四位方式，作为唯一的用户名，而初始密码为全名名称+电话号码前四位，例如：张三的电话号码为13812345678，则登录名称为zhangsan5678，密码为zhangsan1381,</li><li>在使用VirtualLDAP时，需要使用MySQL存储持久化数据，例如用户修改后的密码，所以对鉴权规则进行了修改，先检查数据库密码是否匹配，再检查LDAP</li><li>实现了整体组件的编排，增加了docker-compose.yaml，方便用户使用，该编排文件中包含了KeyCloak、VirtualLDAP和MySQL</li></ul><h2 id="KeyCloak"><a href="#KeyCloak" class="headerlink" title="KeyCloak"></a>KeyCloak</h2><p>KeyCloak两部分需要进行配置：</p><ul><li>管理员在第一次使用时配置VirtualLDAP的信息，用于用户同步，方便新用户修改密码</li><li>新用户自行修改密码</li></ul><p>正如之前所说，KeyCloak功能过于强大，这里用到的功能非常有限，如果有新的应用场景，欢迎留言。</p><p><img src="/images/pasted-93.png" alt="upload successful"></p><h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><p>这里提供了完整的编排文件，直接使用即可完成整套环境的快速建立。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;virtual-ldap</span><br><span class="line">cd virtual-ldap&#x2F;docker-compose</span><br><span class="line">docker-compose up -d0</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><ul><li>VirtualLDAP配置文件修改。所有配置在virtual-ldap/docker-compose/config.js中进行修改，需要修改钉钉的appKey和appSecret，以及root DN的信息，配置文件有比较详细的介绍，所以这里不再赘述。</li><li>KeyCloak的默认密码修改在docker-compose.yaml中</li></ul><h2 id="登录相关信息"><a href="#登录相关信息" class="headerlink" title="登录相关信息"></a>登录相关信息</h2><ul><li>URL: ldap://ip:1389</li><li>ManageDN: cn=admin,dc=oneprocloud,dc=com</li><li>ManagePassword: password</li><li>User Search Base: ou=People,o=department,dc=oneprocloud,dc=com</li><li>User Search Filter: uid={0}</li><li>Display Name LDAP attribute: cn</li><li>Email Address LDAP attribute: mail</li></ul><h1 id="待优化"><a href="#待优化" class="headerlink" title="待优化"></a>待优化</h1><ul><li>目前对于LDAP的组没有充分利用，配置文件中允许创建特定组，并且通过用户email进行匹配，如果需要可以进行配置</li><li>如果有外部用户，暂时无方法进行创建，例如：如果需要在LDAP中增加一个非钉钉用户暂时无法实现，需要进行开发实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Node.js" scheme="http://sunqi.site/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>使用Kolla部署OpenStack Stein版本</title>
    <link href="http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/</id>
    <published>2020-10-30T14:15:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。</p><a id="more"></a><h1 id="1、云平台规划"><a href="#1、云平台规划" class="headerlink" title="1、云平台规划"></a>1、云平台规划</h1><p>在实际环境中，我们在一台2U的超微四子星服务器上进行了部署。由于是内部使用的研发环境，为了节约成本，我们并没有部署高可靠方案，而是采用了一台作为控制节点+计算节点+存储节点，另外三台作为计算节点+存储节点的方式进行部署。</p><p>由于OpenStack最新的Ussari在使用Kolla部署时，不再支持CentOS 7版本，所以这里我们选定了上一个稳定版本Stein版本进行部署。</p><h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><table><thead><tr><th>硬件名称</th><th>配置规格</th><th>备注</th></tr></thead><tbody><tr><td>CPU</td><td>Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz    x 2</td><td>共40线程</td></tr><tr><td>内存</td><td>DDR4 2400 MHz 64GB</td><td></td></tr><tr><td>硬盘</td><td>板载64 GB x 1 <br/> 240 GB Intel SSD x 1 <br/> 1.2 TB SAS x 5</td><td>经过测试，由于板载64GB空间过小，在控制节点需要损失一块SAS盘空间用于root分区挂载</td></tr><tr><td>网卡</td><td>千兆 x 4 <br/> 万兆 x 4 <br/> IPMI x 1</td><td></td></tr></tbody></table><h3 id="分区规划"><a href="#分区规划" class="headerlink" title="分区规划"></a>分区规划</h3><table><thead><tr><th>磁盘</th><th>规划</th><th>备注</th></tr></thead><tbody><tr><td>64G</td><td>系统盘</td><td>不要使用LVM分区</td></tr><tr><td>SSD 240G</td><td>Ceph Journal<br></td><td>1块盘</td></tr><tr><td>SAS 1.2 T</td><td>Ceph OSD</td><td>5块盘</td></tr></tbody></table><h2 id="网络规划"><a href="#网络规划" class="headerlink" title="网络规划"></a>网络规划</h2><h3 id="交换机配置"><a href="#交换机配置" class="headerlink" title="交换机配置"></a>交换机配置</h3><ul><li>我们默认采用了VLAN模式，所以无须在交换机上进行Trunk配置</li></ul><h3 id="网络规划-1"><a href="#网络规划-1" class="headerlink" title="网络规划"></a>网络规划</h3><table><thead><tr><th>网卡</th><th>网络类型</th><th>VLAN ID</th><th>网段</th><th>说明</th><th>网关</th><th>备注</th></tr></thead><tbody><tr><td></td><td>管理网络</td><td>3</td><td>192.168.10.0/24</td><td>OpenStack管理</td><td>192.168.10.1</td><td>192.168.10.201 - 204</td></tr><tr><td></td><td>存储网络</td><td></td><td>10.0.100.0/24</td><td>Ceph网络</td><td>无需网关</td><td>10.0.100.201 -&nbsp;204</td></tr><tr><td></td><td>External网络</td><td>3</td><td>192.168.10.0/24</td><td>External网络</td><td>192.168.10.1</td><td>可分配地址192.168.10.100 - 192.168.10.200</td></tr><tr><td></td><td>Tunnel网络</td><td></td><td>172.16.100.0/24</td><td>VxLAN通讯网络</td><td><br data-mce-bogus="1"></td><td>172.16.100.201 - 204</td></tr><tr><td>console</td><td>IPMI</td><td>4</td><td>192.168.10.0/24</td><td></td><td></td><td>与管理网地址一一对应, 192.168.10.201</td></tr></tbody></table><h3 id="网卡配置"><a href="#网卡配置" class="headerlink" title="网卡配置"></a>网卡配置</h3><table><thead><tr><th>主机名</th><th>em1(管理网地址)</th><th>em2(存储网)</th><th>em3(External网络)</th><th>em4(Tunnel网络)</th><th>备注</th></tr></thead><tbody><tr><td>control201</td><td>192.168.10.201</td><td>10.0.100.201</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.201</td><td></td></tr><tr><td>compute202</td><td>192.168.10.202</td><td>10.0.100.202</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.202</td><td></td></tr><tr><td>compute203</td><td>192.168.10.203</td><td>10.0.100.203</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr><tr><td>compute204</td><td>192.168.10.204</td><td>10.10.20.204</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr></tbody></table><h2 id="OpenStack规划"><a href="#OpenStack规划" class="headerlink" title="OpenStack规划"></a>OpenStack规划</h2><h3 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h3><p>Ceph采用单独安装方式，这目前也是Kolla项目主推的方式，在U版本中已经彻底不支持通过Kolla安装Ceph了。我们主要安装OpenStack核心模块，另外安装的是日志收集ELK的相关模块，便于运维。</p><ul><li>Horizon</li><li>Nova</li><li>Keystone</li><li>Cinder</li><li>Glance</li><li>Neutron</li><li>Heat</li></ul><h1 id="2、部署准备"><a href="#2、部署准备" class="headerlink" title="2、部署准备"></a>2、部署准备</h1><h2 id="部署架构图"><a href="#部署架构图" class="headerlink" title="部署架构图"></a>部署架构图</h2><p><img src="/images/pasted-60.png" alt="upload successful"></p><h2 id="服务器前期准备"><a href="#服务器前期准备" class="headerlink" title="服务器前期准备"></a>服务器前期准备</h2><ul><li>BIOS配置：在BIOS中打开VT，并且正确配置IPMI地址，方便远程管理</li><li>RAID配置：所有磁盘需要配置成NON-RAID模式</li><li>操作系统安装：<ul><li>使用CentOS 7光盘进行最小化安装</li><li>不要使用LVM分区</li><li>配置主机名</li><li>配置第一块网卡，并配置自动启动</li></ul></li></ul><h2 id="网卡配置-1"><a href="#网卡配置-1" class="headerlink" title="网卡配置"></a>网卡配置</h2><h3 id="em1"><a href="#em1" class="headerlink" title="em1"></a>em1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em1</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em1</span><br><span class="line">DEVICE&#x3D;em1</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;192.168.10.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">GATEWAY&#x3D;192.168.10.1</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em2"><a href="#em2" class="headerlink" title="em2"></a>em2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em2</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">DEFROUTE&#x3D;yes</span><br><span class="line">NAME&#x3D;em2</span><br><span class="line">DEVICE&#x3D;em2</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;10.0.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em3"><a href="#em3" class="headerlink" title="em3"></a>em3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em3</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em3</span><br><span class="line">DEVICE&#x3D;em3</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;172.16.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em4"><a href="#em4" class="headerlink" title="em4"></a>em4</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em4</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;none</span><br><span class="line">NAME&#x3D;em4</span><br><span class="line">DEVICE&#x3D;em4</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h1><h2 id="3-1-准备部署节点"><a href="#3-1-准备部署节点" class="headerlink" title="3.1 准备部署节点"></a>3.1 准备部署节点</h2><p>该节点承担了后续所有的部署流程，该节点可以作为OpenStack控制节点复用，包括运行OpenStack Kolla和Ceph Deploy。</p><p>注意：节点之间可以通过密码或者密钥方式进行访问，附录中提供了自动上传密钥的方式，建议在正式安装前配置完成，这里不提供自动化配置方法。</p><h3 id="下载初始化脚本"><a href="#下载初始化脚本" class="headerlink" title="下载初始化脚本"></a>下载初始化脚本</h3><p>目前已经将常用的操作写成了Ansible脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;my_ansible_playbooks</span><br><span class="line"></span><br><span class="line">cd my_ansible_playbooks</span><br><span class="line">prepare_on_centos7.sh</span><br></pre></td></tr></table></figure><h3 id="修改hosts-ini文件"><a href="#修改hosts-ini文件" class="headerlink" title="修改hosts.ini文件"></a>修改hosts.ini文件</h3><p>修改hosts.ini文件来初始化所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># my_ansible_playbooks&#x2F;hosts.ini</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.201 ip&#x3D;192.168.10.201 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.202 ansible_user&#x3D;root</span><br><span class="line">compute203 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.203 ansible_user&#x3D;root</span><br><span class="line">compute204 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.204 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="初始化节点"><a href="#初始化节点" class="headerlink" title="初始化节点"></a>初始化节点</h3><p>该步骤主要包含了，更新软件，修改主机名，增加/etc/hosts等操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;bootstrap_centos7.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;change_hostname.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_etc_hosts.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_docker.yml</span><br><span class="line"></span><br><span class="line"># 安装pip和系统环境下的python docker模块，否则在precheck的时候会发现没有安装docker模块</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_pip2_package.yml</span><br></pre></td></tr></table></figure><h3 id="安装Ceph-Deploy"><a href="#安装Ceph-Deploy" class="headerlink" title="安装Ceph Deploy"></a>安装Ceph Deploy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h3 id="安装Kolla"><a href="#安装Kolla" class="headerlink" title="安装Kolla"></a>安装Kolla</h3><p>由于Python Warning的提示信息导致在安装时出现如下错误，需要增加忽略Python Warning的环境变量，具体修复信息如下：<a href="https://bugs.launchpad.net/kolla-ansible/+bug/1888657" target="_blank" rel="noopener">https://bugs.launchpad.net/kolla-ansible/+bug/1888657</a></p><p>目前通过pip方式还没有8.2.1这个release，所以kolla的安装从源代码中进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ansible 2.2.0.0 used in Stein kolla-toolbox requires paramiko (no version</span><br><span class="line">constraints), which installs latest cryptography package. It results in</span><br><span class="line">Python deprecation warning about Python 2:</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x2F;cryptography&#x2F;__init__.py:39: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release.</span><br><span class="line"></span><br><span class="line">This warning breaks kolla_toolbox module.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python-devel libffi-devel gcc openssl-devel libselinux-python</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;openstack&#x2F;kolla-ansible --branch stable&#x2F;stein</span><br><span class="line">cd kolla-ansible</span><br><span class="line">pip install . --ignore-installed PyYAML</span><br><span class="line"></span><br><span class="line"># 虚拟环境还需要再安装一次ansible，否则kolla-ansible会提示</span><br><span class="line"># ERROR: kolla_ansible has to be available in the Ansible PYTHONPATH.</span><br><span class="line"># Please install both in the same (virtual) environment.</span><br><span class="line">pip install &#39;ansible&lt;2.10&#39;</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla</span><br><span class="line">cp -r $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;etc_examples&#x2F;kolla&#x2F;* &#x2F;etc&#x2F;kolla</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;root&#x2F;kolla</span><br><span class="line">cp $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;ansible&#x2F;inventory&#x2F;* &#x2F;root&#x2F;kolla</span><br></pre></td></tr></table></figure><p>生成密码，如果需要指定密码，可以到/etc/kolla/password.yml中修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-genpwd</span><br></pre></td></tr></table></figure><h2 id="3-2-部署Ceph"><a href="#3-2-部署Ceph" class="headerlink" title="3.2 部署Ceph"></a>3.2 部署Ceph</h2><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>之前有一篇软文详细介绍了使用Ceph Deploy部署Ceph的方法，这里不再赘述，下面直接给出部署命令，这里我们只部署块服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;root&#x2F;ceph</span><br><span class="line">cd &#x2F;root&#x2F;ceph</span><br><span class="line"></span><br><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 如果阿里源无法使用，可以使用163源，并且可以通过指定rpm-octopus，指定安装的Ceph版本</span><br><span class="line">#export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">#export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">#</span><br><span class="line"># 如果cluster-network和public-network需要分开，可以这样定义：</span><br><span class="line"># ceph-deploy new --cluster-network 172.31.6.0&#x2F;24 --public-network 192.168.4.0&#x2F;24 node1 node2 node3</span><br><span class="line"></span><br><span class="line">ceph-deploy new --public-network 10.0.100.0&#x2F;24 compute201</span><br><span class="line">ceph-deploy install compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create compute201</span><br><span class="line"></span><br><span class="line"># 需要根据实际情况修改，这里模拟的是将RocksDB存放至单独的SSD磁盘，目前文档中并没有特别指出这部分的分配比例，所以DB和WAL都是分配10G，写入的基本顺序为WAL -&gt; DB -&gt; DATA</span><br><span class="line"></span><br><span class="line">pvcreate &#x2F;dev&#x2F;vdb</span><br><span class="line">vgcreate ceph-pool &#x2F;dev&#x2F;vdb</span><br><span class="line"></span><br><span class="line"># 每个OSD分配</span><br><span class="line">lvcreate -n osd0.wal -L 10G ceph-pool</span><br><span class="line">lvcreate -n osd0.db -L 10G ceph-pool</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdd --block-db ceph-pool&#x2F;osd0.db --block-wal ceph-pool&#x2F;osd0.wal compute201</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><h3 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h3><p>为Glance/Nova/Cinder创建资源池并生成鉴权文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create images 128</span><br><span class="line">ceph auth get-or-create client.glance mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create volumes 128</span><br><span class="line">ceph auth get-or-create client.cinder mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;volumes, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create backups 128</span><br><span class="line">ceph auth get-or-create client.cinder-backup mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;backups&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create vms 128</span><br><span class="line">ceph auth get-or-create client.nova mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;vms, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring</span><br></pre></td></tr></table></figure><p>注意：</p><h2 id="3-3-OpenStack部署"><a href="#3-3-OpenStack部署" class="headerlink" title="3.3 OpenStack部署"></a>3.3 OpenStack部署</h2><h3 id="kolla配置文件"><a href="#kolla配置文件" class="headerlink" title="kolla配置文件"></a>kolla配置文件</h3><h4 id="etc-kolla-globals-yml"><a href="#etc-kolla-globals-yml" class="headerlink" title="/etc/kolla/globals.yml"></a>/etc/kolla/globals.yml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;stein&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.10.123&quot;</span><br><span class="line"></span><br><span class="line">docker_registry: registry.cn-beijing.aliyuncs.com</span><br><span class="line">docker_namespace: &quot;openstack-dockers&quot;</span><br><span class="line"></span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">storage_interface: &quot;eth1&quot;</span><br><span class="line">tunnel_interface: &quot;eth3&quot;</span><br><span class="line">neutron_external_interface: &quot;eth2&quot;</span><br><span class="line"></span><br><span class="line">openstack_logging_debug: &quot;True&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_ceph: &quot;no&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">enable_cinder_backup: &quot;yes&quot;</span><br><span class="line">enable_fluentd: &quot;no&quot;</span><br><span class="line">enable_openstack_core: &quot;yes&quot;</span><br><span class="line">glance_backend_ceph: &quot;yes&quot;</span><br><span class="line">glance_backend_file: &quot;no&quot;</span><br><span class="line">glance_enable_rolling_upgrade: &quot;no&quot;</span><br><span class="line">cinder_backend_ceph: &quot;yes&quot;</span><br><span class="line">nova_backend_ceph: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h4 id="multinode"><a href="#multinode" class="headerlink" title="multinode"></a>multinode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[control]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[network]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[compute]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[monitoring]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[storage]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="定制服务配置文件"><a href="#定制服务配置文件" class="headerlink" title="定制服务配置文件"></a>定制服务配置文件</h3><h4 id="Ceph-Glance"><a href="#Ceph-Glance" class="headerlink" title="Ceph Glance"></a>Ceph Glance</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance</span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;glance-api.conf &lt;&lt; EOF</span><br><span class="line">[glance_store]</span><br><span class="line">stores &#x3D; rbd</span><br><span class="line">default_store &#x3D; rbd</span><br><span class="line">rbd_store_pool &#x3D; images</span><br><span class="line">rbd_store_user &#x3D; glance</span><br><span class="line">rbd_store_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.client.glance.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Cinder"><a href="#Ceph-Cinder" class="headerlink" title="Ceph Cinder"></a>Ceph Cinder</h4><p>cinder_rbd_secret_uuid是在passwords.yml中生成的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup</span><br><span class="line"></span><br><span class="line">export cinder_rbd_secret_uuid&#x3D;$(grep cinder_rbd_secret_uuid &#x2F;etc&#x2F;kolla&#x2F;passwords.yml | awk &#39;&#123;print $2&#125;&#39;)</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends&#x3D;rbd-1</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;cinder</span><br><span class="line">backend_host&#x3D;rbd:volumes</span><br><span class="line">rbd_pool&#x3D;volumes</span><br><span class="line">volume_backend_name&#x3D;rbd-1</span><br><span class="line">volume_driver&#x3D;cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_secret_uuid &#x3D; $cinder_rbd_secret_uuid</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">backup_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">backup_ceph_user&#x3D;cinder-backup</span><br><span class="line">backup_ceph_chunk_size &#x3D; 134217728</span><br><span class="line">backup_ceph_pool&#x3D;backups</span><br><span class="line">backup_driver &#x3D; cinder.backup.drivers.ceph.CephBackupDriver</span><br><span class="line">backup_ceph_stripe_unit &#x3D; 0</span><br><span class="line">backup_ceph_stripe_count &#x3D; 0</span><br><span class="line">restore_discard_excess_bytes &#x3D; true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有文件必须命名为ceph.client*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Nova"><a href="#Ceph-Nova" class="headerlink" title="Ceph Nova"></a>Ceph Nova</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;nova-compute.conf &lt;&lt; EOF</span><br><span class="line">[libvirt]</span><br><span class="line">images_rbd_pool&#x3D;vms</span><br><span class="line">images_type&#x3D;rbd</span><br><span class="line">images_rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;nova</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.nova.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.cinder.keyring</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 初始化节点，与上述我们自己的初始化有重复之处</span><br><span class="line">kolla-ansible -i multinode bootstrap-servers</span><br><span class="line"></span><br><span class="line">kolla-ansible -i multinode prechecks</span><br><span class="line"></span><br><span class="line"># 拉取所有镜像</span><br><span class="line">kolla-ansible -i multinode pull</span><br></pre></td></tr></table></figure><h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -i multinode deploy</span><br><span class="line">kolla-ansible -i multinode post-deploy</span><br></pre></td></tr></table></figure><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="节点互信"><a href="#节点互信" class="headerlink" title="节点互信"></a>节点互信</h2><p>节点之间互信建议采用key方式，这里并没有实现完全自动化手段，需要首先在控制节点上生成公钥和私钥。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>然后将~/.ssh/id_rsa.pub文件拷贝至可以正常访问两台节点的环境中的playbooks/keys目录下，再更新所有节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_authorized_keys.yml</span><br></pre></td></tr></table></figure><h2 id="部署出错如何调试"><a href="#部署出错如何调试" class="headerlink" title="部署出错如何调试"></a>部署出错如何调试</h2><p>如果在部署中出现任何错误，可以添加更多的Verbose来判断具体问题，有可能是kolla自身bug，也有可能是配置的问题，具体可以根据详细输出进行判断。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -vvv -i multinode deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>云原生趋势下的迁移与容灾思考</title>
    <link href="http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/"/>
    <id>http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/</id>
    <published>2020-10-18T12:05:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h1><h2 id="云原生发展趋势"><a href="#云原生发展趋势" class="headerlink" title="云原生发展趋势"></a>云原生发展趋势</h2><p>云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。</p><p>2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？</p><a id="more"></a><h2 id="为什么云上需要全新的迁移和容灾"><a href="#为什么云上需要全新的迁移和容灾" class="headerlink" title="为什么云上需要全新的迁移和容灾"></a>为什么云上需要全新的迁移和容灾</h2><h3 id="1、传统方案的局限性"><a href="#1、传统方案的局限性" class="headerlink" title="1、传统方案的局限性"></a>1、传统方案的局限性</h3><p>在这种大的趋势下，传统的迁移和容灾仍然停留在数据搬运的层次上，而忽略了面向云的特性和用户业务重新思考和构建。云计算的愿景是让云资源像水、电一样按需使用，所以基于云上的迁移和容灾也理应顺应这样的历史潮流。Snowflake也是通过这种商业模式的创新，成功打破旧的竞争格局。</p><p>为什么传统容灾的手段无法满足云原生需求呢？简单来说，二者关注的核心不同。传统的容灾往往以存储为核心，拥有对存储的至高无上的控制权。并且在物理时代，对于计算、存储和网络等基础架构层也没有有效的调度方法，无法实现高度自动化的编排。而基于云原生构建的应用，核心变成了云原生服务本身。当用户业务系统全面上云后，用户不再享有对底层存储的绝对控制权，所以传统的容灾手段，就风光不在了。</p><p><img src="/images/pasted-88.png" alt="upload successful"></p><p>我认为在构建云原生容灾的解决方案上，要以业务为核心去思考构建方法，利用云原生服务的编排能力实现业务系统的连续性。</p><h3 id="2、数据安全性"><a href="#2、数据安全性" class="headerlink" title="2、数据安全性"></a>2、数据安全性</h3><p>AWS CTO Werner Vogels曾经说过：Everything fails, all the time。通过AWS的责任共担模型，我们不难发现云商对底层基础架构负责，用户仍然要对自身自身数据安全性和业务连续性负责。</p><p><img src="/images/pasted-74.png" alt="upload successful"></p><p>我认为在云原生趋势下，用户最直接诉求的来自数据安全性即备份，而迁移、恢复、高可靠等都是基于备份表现出的业务形态，而备份能力可能是由云原生能力提供的，也有可能是第三方能力提供的，但最终实现业务形态，是由编排产生的。</p><p>用户上云并不等于高枕无忧，相反用户要学习云的正确打开方式，才能最大程度来保证业务的连续性。虽然云在底层设计上上是高可靠的，但是仍然避免不了外力造成的影响，例如：光缆被挖断、断电、人为误操作导致的云平台可用区无法使用，所以才有了类似“蓝翔决定了中国云计算稳定性”的调侃。我认为用户决定将业务迁移到云上的那一刻开始，备份、迁移、恢复、高可靠是一个连续的过程，如何合理利用云原生服务的特性实现业务连续性，同时进行成本优化，降低总体拥有成本（TCO）。</p><h3 id="3、防止厂商锁定"><a href="#3、防止厂商锁定" class="headerlink" title="3、防止厂商锁定"></a>3、防止厂商锁定</h3><p>某种意义上说，云原生的方向是新一轮厂商锁定，就像当年盛极一时的IOE架构一样，只不过现在换成了云厂商作为底座承载应用。在IOE时代，用户很难找到完美的替代品，但是在云时代，这种差异并不那么明显。所以大部分的客户通常选用混合云作为云建设策略，为了让应用在不同云之间能够平滑移动，利用容灾技术的迁移一定是作为一个常态化需求存在的。Gartnar也在多云管平台定义中，将迁移和DR作为单独的一项能力。充分说明迁移与容灾在多云环境的的常态化趋势。</p><p><img src="/images/pasted-82.png" alt="upload successful"></p><h1 id="云迁移与云容灾的关系"><a href="#云迁移与云容灾的关系" class="headerlink" title="云迁移与云容灾的关系"></a>云迁移与云容灾的关系</h1><h2 id="云迁移需求的产生"><a href="#云迁移需求的产生" class="headerlink" title="云迁移需求的产生"></a>云迁移需求的产生</h2><p>在传统环境下，迁移的需求并不十分突出，除非是遇到机房搬迁或者硬件升级，才会想到迁移，但这里的迁移更像是搬铁，迁移工具化与自动化的需求并不明显。当VMware出现后，从物理环境到虚拟化的迁移需求被放大，但由于是单一的虚拟化平台，基本上虚拟化厂商自身的工具就完全能够满足需求了。在虚拟化平台上，大家突然发现原来只能人工操作的物理环境一下子轻盈起来，简单来说，我们的传统服务器从一堆铁变成了一个文件，并且这个文件还能够被来回移动、复制。再后来，进入云时代，各家云平台风生水起，国内云计算市场更是百家争鸣，上云更是成为了一种刚性需求。随着时间的推移，出于对成本、厂商锁定等诸多因素的影响，在不同云之间的互相迁移更是会成为一种常态化的需求。</p><h2 id="底层技术一致"><a href="#底层技术一致" class="headerlink" title="底层技术一致"></a>底层技术一致</h2><p>这里提到的云迁移和容灾，并不是堆人提供的迁移服务，而是强调的高度自动化的手段。目标就是在迁移过程中保证业务连续性，缩短停机时间甚至不停机的效果。这里就借助了容灾的存储级别同步技术来实现在异构环境下的的“热迁移”。现有解决方案里，既有传统物理机搬迁时代的迁移软件，也有基于云原生开发的工具。但无论何种形式，都在不同程度上都解决了用户上云的基本诉求。最大的区别在于人效比，这一点与你的利益直接相关。</p><p>从另外一个角度也不难发现，所谓的迁移在正式切换之前实质上就是容灾的中间过程。同时，业务系统迁移到云平台后，灾备是一个连续的动作，这里既包含了传统的备份和容灾，还应该包含云上高可靠的概念。这样，用户业务系统在上云后，才能摆脱传统基础架构的负担，做到“零运维”，真正享受到云所带来的的红利。所以，我认为在云原生状态下，云迁移、云容灾、云备份本质上就是一种业务形态，底层采用的技术手段可以是完全一致的。</p><h2 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h2><p>在上述的痛点和趋势下，必然会出现一种全新的平台来帮助客户解决数据的安全性和业务连续性问题，今天就从这个角度来分析一下，在云原生的趋势下如何构建应用系统的迁移与容灾方案。</p><h1 id="云迁移发展趋势"><a href="#云迁移发展趋势" class="headerlink" title="云迁移发展趋势"></a>云迁移发展趋势</h1><h2 id="云迁移方式"><a href="#云迁移方式" class="headerlink" title="云迁移方式"></a>云迁移方式</h2><p>迁移是一项重度的咨询业务，网上各家云商、MSP都有自己的方法论，其实看下来差别都不大，之前也有很多人在分享相关话题，本文就不再赘述。这里我们重点讨论，在实际落地过程中到底该采用哪种工具，哪种方式的效率最高。所谓云迁移工具，就是将源端迁移至目标端，保证源端在目标端正确运行。常见的方式包括：物理机到虚拟化、虚拟化到虚拟化、物理机到云平台、虚拟化到云平台等。</p><p><img src="/images/pasted-62.png" alt="upload successful"></p><p>这是经典的6R迁移理论（现在已经升级为了7R，多了VMware出来搅局），在这个图中与真正迁移相关的其实只有Rehosting, Replatforming, Repurchasing和Refactoring，但是在这4R中，Refactoring明显是一个长期的迭代过程，需要用户和软件开发商共同参与解决，Repurchasing基本上与人为重新部署没有太大的区别。所以真正由用户或MSP在短期完成的只剩下Rehosting和Replatofrming。</p><p>与上面这张经典的迁移理论相比，我更喜欢下面这张图，这张图更能反应一个传统应用到云原生成长的全过程。与上述的结论相似，我们在真正拥抱云的时候，路径基本为上述的三条</p><ul><li>Lift &amp; Shift是Rehost方式的另一种称呼，这种方式路面最宽，寓意这条路是上云的最短路径，应用不需要任何改造直接上云使用</li><li>Evolve和Go Native都属于较窄的路径，寓意为相对于Rehost方式，这两条路径所消耗的时间更久，难度更高</li><li>在图的最右侧，三种形态是存在互相转换的可能，最终演进为彻底的云原生，寓意为迁移并不是一蹴而就，需要循序渐进完成</li></ul><p><img src="/images/pasted-61.png" alt="upload successful"></p><h2 id="重新托管（Rehost）方式"><a href="#重新托管（Rehost）方式" class="headerlink" title="重新托管（Rehost）方式"></a>重新托管（Rehost）方式</h2><p>常用的重新托管方式为冷迁移和热迁移，冷迁移往往涉及到步骤比较繁琐，需要大量人力投入，并且容易出错效率低，对业务连续性有较大的影响，不适合生产系统迁移。而热迁移方案基本都是商用化的解决方案，这里又分为块级别和文件级别，再细分为传统方案与云原生方案。</p><h3 id="冷迁移"><a href="#冷迁移" class="headerlink" title="冷迁移"></a>冷迁移</h3><p>我们先来看一下冷迁移的手动方案，以VMware到OpenStack为例，最简单的方式就是将VMware虚拟机文件(VMDK)通过qemu-img工具进行格式转换，转换为QCOW2或者RAW格式，上传至OpenStack Glance服务，再重新在云平台上进行启动。当然这里面需要进行virtio驱动注入，否则主机无法正常在云平台启动。这个过程中最耗时的应该是虚拟机文件上传至OpenStack Glance服务的过程，在我们最早期的实践中，一台主机从开始迁移到启动完成足足花了24小时。同时，在你迁移这段时间的数据是有增量产生的，除非你将源端关机等待迁移完成，否则，你还要将上述步骤重新来一遍。所以说这种方式真的不适合有业务连续性的生产系统进行迁移。</p><p>那如果是物理机的冷迁移方案怎么做呢？经过我们的最佳实践，这里为大家推荐的是老牌的备份工具CloneZilla，中文名为再生龙。是一款非常老牌的备份软件，常用于进行整机备份与恢复，与我们常见的Norton Ghost原理非常相似。CloneZilla从底层的块级别进行复制，可以进行整盘的备份，并且支持多种目标端，例如我们将磁盘保存至移动硬盘，实际格式就是RAW，你只需要重复上述的方案即可完成迁移。但是在使用CloneZilla过程中，需要使用Live CD方式进行引导，同样会面临长时间业务系统中断的问题，这也是上面我们提到的冷迁移并不适合生产环境迁移的原因。</p><p><img src="/images/pasted-63.png" alt="upload successful"></p><p><img src="/images/pasted-64.png" alt="upload successful"></p><h3 id="传统热迁移方案"><a href="#传统热迁移方案" class="headerlink" title="传统热迁移方案"></a>传统热迁移方案</h3><p>传统的热迁移方案基本分为块级别和文件级别，两者相似之处都是利用差量同步技术进行实现，即全量和增量交叉同步方式。</p><p>文件级别的热迁移方案往往局限性较大，并不能算真正的ReHost方式，因为前期需要准备于源端完全一样的操作系统，无法实现整机搬迁，从操作的复杂性更大和迁移的稳定性来说都不高。我们在Linux上常用的Rsync其实可以作为文件级别热迁移的一种解决方案。</p><p>真正可以实现热迁移的方案，还要使用块级别同步，降低对底层操作系统依赖，实现整机的搬迁效果。传统的块级别热迁移方案基本上来自于传统容灾方案的变种，利用内存操作系统WIN PE或其他Live CD实现，基本原理和过程如下图所示。从过程中我们不难发现这种方式虽然在一定程度解决了迁移的目标，但是作为未来混合云常态化迁移需求来说，仍然有以下几点不足：</p><ul><li>由于传统热迁移方案是基于物理环境构建的，所以我们发现在整个过程中人为介入非常多，对于使用者的技能要求比较高</li><li>无法满足云原生时代多租户、自服务的需求</li><li>安装代理是用户心中永远的芥蒂</li><li>一比一同步方式，从成本角度来说不够经济</li><li>最好的迁移验证方式，就是将业务系统集群在云端完全恢复，但是手动验证的方式，对迁移人力成本是再一次增加</li></ul><p><img src="/images/pasted-67.png" alt="upload successful"></p><h3 id="云原生热迁移方案"><a href="#云原生热迁移方案" class="headerlink" title="云原生热迁移方案"></a>云原生热迁移方案</h3><p>正是由于传统迁移方案的弊端，应运而生了云原生的热迁移方案，这一方面的代表厂商当属AWS在2019年以2.5亿美金击败Google Cloud收购的以色列云原生容灾、迁移厂商CloudEndure。</p><p>云原生热迁移方案是指利用块级别差量同步技术结合云原生API接口和资源实现高度自动化迁移效果，同时提供多租户、API接口满足混合云租户自服务的需求。我们先从原理角度分析一下，为什么相对于传统方案，云原生的方式能够满足高度自动化、用户自服务的用户体验。通过两个方案对比，我们不难发现云原生方式的几个优势：</p><ul><li>利用云原生API接口和资源，操作简便，完全取代了传统方案大量繁琐的人为操作，对使用者技术要求降低，学习陡峭程度大幅度降低</li><li>由于操作简便，迁移效率提高，有效提高迁移实施的人效比</li><li>一对多的同步方式，大幅度降低计算资源使用，计算资源只在验证和最终切换时使用</li><li>能够满足多租户、自服务的要求</li><li>源端也可以支持无代理方式，打消用户疑虑，并且适合大规模批量迁移</li><li>高度自动化的验证手段，在完成迁移切换前，能够反复进行验证</li></ul><p><img src="/images/pasted-69.png" alt="upload successful"></p><p>这是CloudEndure的架构图，当然你也可以利用CloudEndure实现跨区域的容灾。</p><p><img src="/images/pasted-70.png" alt="upload successful"></p><p>不过可惜的一点是由于被AWS收购，CloudEndure目前只能支持迁移至AWS，无法满足国内各种云迁移的需求。所以这里为大家推荐一款纯国产化的迁移平台——万博智云的HyperMotion( <a href="https://hypermotion.oneprocloud.com/" target="_blank" rel="noopener">https://hypermotion.oneprocloud.com/</a> )，从原理上与CloudEndure非常相似，同时支持了VMware及OpenStack无代理的迁移，更重要的是覆盖了国内主流的公有云、专有云和私有云的迁移。</p><p><img src="/images/pasted-71.png" alt="upload successful"></p><h2 id="平台重建（Replatforming）方式"><a href="#平台重建（Replatforming）方式" class="headerlink" title="平台重建（Replatforming）方式"></a>平台重建（Replatforming）方式</h2><p>随着云原生提供越来越多的服务，降低了应用架构的复杂度，使得企业能够更专注自己的业务本身开发。但是研发侧工作量的减少意味着这部分成本被转嫁到部署及运维环节，所以DevOps成为在云原生运用中比不可少的一个缓解，也让企业能够更敏捷的应对业务上的复杂变化。</p><p>正如上面所提到的，用户通过少量的改造可以优先使用一部分云原生服务，这种迁移方式我们成为平台重建（Replatforming），目前选择平台重建方式的迁移，多以与用户数据相关的服务为主。常见的包括：数据库服务RDS、对象存储服务、消息队列服务、容器服务等。这些云原生服务的引入，降低了用户运维成本。但是由于云原生服务自身封装非常严密，底层的基础架构层对于用户完全不可见，所以无法用上述Rehost方式进行迁移，必须采用其他的辅助手段完成。</p><p>以关系型数据库为例，每一种云几乎都提供了迁移工具，像AWS DMS，阿里云的DTS，腾讯云的数据传输服务DTS，这些云原生工具都可以支持 MySQL、MariaDB、PostgreSQL、Redis、MongoDB 等多种关系型数据库及 NoSQL 数据库迁移。以MySQL为例，这些服务都巧妙的利用了binlog复制的方式，实现了数据库的在线迁移。</p><p>再以对象存储为例，几乎每一种云都提供了自己的迁移工具，像阿里云的ossimport，腾讯云COS Migration工具，都可以实现本地到云端对象存储的增量迁移。但是在实际迁移时，还应考虑成本问题，公有云的对象存储在存储数据上比较便宜，但是在读出数据时是要根据网络流量和请求次数进行收费的，这就要求我们在设计迁移方案时，充分考虑成本因素。如果数据量过大，还可以考虑采用离线设备方式，例如：AWS的Snowball，阿里云的闪电立方等。这部分就不展开介绍，以后有机会再单独为大家介绍。</p><p><img src="/images/pasted-72.png" alt="upload successful"></p><p>如果选择平台重建方式上云，除了要进行必要的应用改造，还需要选择一款适合你的迁移工具，保证数据能够平滑上云。结合上面的Rehost方式迁移，能够实现业务系统的整体上云效果。由于涉及的服务较多，这里为大家提供一张迁移工具表格供大家参考。</p><p><img src="/images/pasted-89.png" alt="upload successful"></p><h1 id="云原生下的容灾发展趋势"><a href="#云原生下的容灾发展趋势" class="headerlink" title="云原生下的容灾发展趋势"></a>云原生下的容灾发展趋势</h1><p>目前为止，还没有一套平台能够完全满足云原生状态下的统一容灾需求，我们通过以下场景来分析一下，如何才能构建一套统一的容灾平台满足云原生的需求。</p><h2 id="传统架构"><a href="#传统架构" class="headerlink" title="传统架构"></a>传统架构</h2><p>我们以一个简单的Wordpress + MySQL环境为例，传统下的部署环境一般是这样架构的：</p><p><img src="/images/pasted-58.png" alt="upload successful"></p><p>如果为这套应用架构设计一套容灾方案，可以采用以下的方式：</p><ul><li>负载均衡节点容灾：负载均衡分为硬件和软件层面，硬件负载均衡高可靠和容灾往往通过自身的解决方案实现。如果是软件负载均衡，往往需要安装在基础操作系统上，而同城的容灾可以使用软件高可靠的方式实现，而异地的容灾往往是通过提前建立对等节点，或者干脆采用容灾软件的块或者文件级别容灾实现。是容灾切换（Failover）很重要的一个环节。</li><li>Web Server的容灾：Wordpress的运行环境无非是Apache + PHP，由于分离了用于存放用户上传的文件系统，所以该节点几乎是无状态的，通过扩展节点即可实现高可靠，而异地容灾也比较简单，传统的块级别和文件级别都可以满足容灾的需求</li><li>共享文件系统的容灾，图中采用了Gluster的文件系统，由于分布式系统的一致性通常由内部维护，单纯使用块级别很难保证节点的一致性，所以这里面使用文件级别容灾更为精确</li><li>数据库的容灾，单纯依靠存储层面是无法根本实现数据库0丢失数据的，所以一般采用从数据库层面实现，当然如果为了降低成本，数据库的容灾可以简单的使用周期Dump数据库的方式实现，当然如果对可靠性要求较高，还可以使用CDP方式实现</li></ul><p>从以上的案例分析不难看出，传统基础架构下的容灾往往以存储为核心，无论是磁盘阵列的存储镜像，还是基于I/O数据块、字节级的捕获技术，结合网络、数据库和集群的应用级别技术完成高可靠和容灾体系的构建。在整个容灾过程的参与者主要为：主机、存储、网络和应用软件，相对来说比较单一。所以在传统容灾方案中，如何正确解决存储的容灾也就成为了解决问题的关键。</p><h2 id="混合云容灾"><a href="#混合云容灾" class="headerlink" title="混合云容灾"></a>混合云容灾</h2><p>这应该是目前最常见的混合云的方案，也是各大容灾厂商主推的一种方式。这里我们相当于将云平台当成了一套虚拟化平台，几乎没有利用云平台任何特性。在恢复过程中，需要大量人为的接入才能将业务系统恢复到可用状态。这样的架构并不符合云上的最佳实践，但的确是很多业务系统备份或迁移上云后真实的写照。</p><p><img src="/images/pasted-83.png" alt="upload successful"></p><p>这样的架构确实能解决容灾的问题，但是从成本上来说很高，现在我们来换一种方式。我们利用了对象存储和数据库进行一次优化。我们将原有存储服务存放至对象存储中，而使用数据传输服务来进行实时的数据库复制。云主机仍然采用传统的块级别进行同步。一旦出现故障，则需要自动化编排能力，重新将备份进行恢复，在最短时间内根据我们预设的方案进行恢复，完成容灾。</p><p><img src="/images/pasted-84.png" alt="upload successful"></p><h2 id="云上同城容灾架构"><a href="#云上同城容灾架构" class="headerlink" title="云上同城容灾架构"></a>云上同城容灾架构</h2><p>上述的备份方式，实质上就是利用平台重建的方式进行的迁移，既然已经利用迁移进行了备份，那完全可以对架构进行如下改造，形成同城的容灾架构。我们根据云平台的最佳实践，对架构进行了如下调整：</p><p><img src="/images/pasted-85.png" alt="upload successful"></p><p>这个架构不仅实现了应用级高可靠，还能够支撑一定的高并发性，用户在最少改造代价下就能够在同城实现双活的效果。我们来分析一下在云上利用了多少云原生的服务：</p><ul><li>域名解析服务</li><li>VPC服务</li><li>负载均衡服务</li><li>自动伸缩服务</li><li>云主机服务</li><li>对象存储服务</li><li>关系型数据库RDS服务</li></ul><p>除了云主机外，其他服务均是天然就支持跨可用区的高可用特性，对于云主机我们可以制作镜像方式，由自动伸缩服务负责实例的状态。由于云上可用区就是同城容灾的概念，这里我们就实现了同城的业务系统容灾。</p><p>经过调整的架构在一定程度上满足了业务连续性的要求，但是对于数据的安全性仍然缺乏保障。近几年，勒索病毒横行，大量企业为此蒙受巨大损失，所以数据备份是上云后必须实施的。云原生服务本身提供了备份方案，例如云主机的定期快照等，但往往服务比较分散，不容易统一进行管理。同时，在恢复时往往也是只能每一个服务进行恢复，如果业务系统规模较大，也会增加大量的恢复成本。虽然云原生服务解决了自身备份问题，但是将备份重新组织成应用是需要利用自动化的编排能力实现。</p><h2 id="同云异地容灾架构"><a href="#同云异地容灾架构" class="headerlink" title="同云异地容灾架构"></a>同云异地容灾架构</h2><p>大部分的云原生服务都在可用区内，提供了高可靠能力，但是对于跨区域上通常提供的是备份能力。例如：可以将云主机变为镜像，将镜像复制到其他区域内；关系型数据库和对象存储也具备跨域的备份能力。利用这些组件自身的备份能力，外加上云自身资源的编排能力，我们可以实现在容灾可用域将系统恢复至可用状态。那如何触发切换呢？</p><p>这里我们根据业务系统的特点，在云原生的监控上定制告警，利用告警平台的触发能力触发函数计算，完成业务系统的跨域切换，形成异地容灾的效果。</p><p><img src="/images/pasted-86.png" alt="upload successful"></p><h2 id="跨云容灾"><a href="#跨云容灾" class="headerlink" title="跨云容灾"></a>跨云容灾</h2><p>但跨云容灾不像同云容灾时，在不同的可用区之间至少服务是一致的，那么此时，在同云上使用的方法基本失效，完全需要目标云平台的能力或者中立的第三方的解决方案。这里除了数据的备份，还有一点是服务配置的互相匹配。才能完全满足跨云容灾恢复的需求。另外需要考虑的一点就是成本为例，以对象存储为例，是典型的的“上云容易下云难”。所以如何利用云原生资源特性合理设计容灾方案是对成本的极大考验。</p><p><img src="/images/pasted-87.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>云原生容灾还处于早期阶段，目前尚没有完整的平台能够支持以上各种场景的容灾需求，是值得持续探索的话题。云原生容灾以备份为核心，以迁移、恢复和高可靠为业务场景，实现多云之间的自由流转，最终满足用户的业务需求。</p><p>所以，作为面向云原生的容灾平台要解决好三方面的能力：</p><p>一、以数据为核心，让数据在多云之间互相流转。数据是用户核心价值，所以无论底层基础架构如何变化，数据备份一定是用户的刚醒需求。对于不同云原生服务如何解决好数据备份，是数据流转的必要基础。</p><p>二、利用云原生编排能力，实现高度自动化，在数据基础上构建业务场景。利用自动化编排能力实现更多的基于数据层的应用，帮助用户完成更多的业务创新。</p><p>三、灵活运用云原生资源特点，降低总体拥有成本。解决传统容灾投入巨大的问题，让用户的成本真的能像水、电一样按需付费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;趋势&quot;&gt;&lt;a href=&quot;#趋势&quot; class=&quot;headerlink&quot; title=&quot;趋势&quot;&gt;&lt;/a&gt;趋势&lt;/h1&gt;&lt;h2 id=&quot;云原生发展趋势&quot;&gt;&lt;a href=&quot;#云原生发展趋势&quot; class=&quot;headerlink&quot; title=&quot;云原生发展趋势&quot;&gt;&lt;/a&gt;云原生发展趋势&lt;/h2&gt;&lt;p&gt;云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。&lt;/p&gt;
&lt;p&gt;2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
      <category term="云原生" scheme="http://sunqi.site/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
      <category term="云迁移" scheme="http://sunqi.site/tags/%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    
      <category term="云容灾" scheme="http://sunqi.site/tags/%E4%BA%91%E5%AE%B9%E7%81%BE/"/>
    
      <category term="Cloud Native" scheme="http://sunqi.site/tags/Cloud-Native/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack对接多Ceph资源池</title>
    <link href="http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/"/>
    <id>http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/</id>
    <published>2020-09-14T08:52:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：</p><ul><li>Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系</li><li>Libvirt配置：配置与Ceph之间的鉴权关系</li></ul><a id="more"></a><h1 id="Cinder配置"><a href="#Cinder配置" class="headerlink" title="Cinder配置"></a>Cinder配置</h1><p>其中rbd_secret_uuid可以使用uuidgen命令生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;etc&#x2F;cinder&#x2F;cinder.conf</span><br><span class="line">[DEFAULT]</span><br><span class="line">......</span><br><span class="line"># 与下面的段落对应</span><br><span class="line">enabled_backends &#x3D; rbd-1, rbd-2</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line"></span><br><span class="line"># 与上面的enabled_backends对应</span><br><span class="line">volume_backend_name &#x3D; rbd-1</span><br><span class="line"></span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line"></span><br><span class="line"># 需要从Ceph集群拷贝这两个配置文件到相应目录</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-1.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder1.keyring</span><br><span class="line"></span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 5774b929-0690-4513-a1f7-41aac49cbb31</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br><span class="line"> </span><br><span class="line">[rbd-2]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name &#x3D; rbd-2</span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-2.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder2.keyring</span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 0563c419-bc4c-4794-972a-685498248869</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br></pre></td></tr></table></figure><h2 id="建立与Volume-Type对应关系"><a href="#建立与Volume-Type对应关系" class="headerlink" title="建立与Volume Type对应关系"></a>建立与Volume Type对应关系</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cinder type-create rbd-1</span><br><span class="line">cinder type-key rbd-1 set volume_backend_name&#x3D;rbd-1</span><br><span class="line">cinder extra-specs-list</span><br><span class="line"></span><br><span class="line">cinder type-create rbd-2</span><br><span class="line">cinder type-key rbd-2 set volume_backend_name&#x3D;rbd-2</span><br><span class="line">cinder extra-specs-list</span><br></pre></td></tr></table></figure><h1 id="Libvirt配置"><a href="#Libvirt配置" class="headerlink" title="Libvirt配置"></a>Libvirt配置</h1><p>在/etc/libvirt/secretes建立与上述rbd_secret_uuid同名的两个文件，后缀为.xml和.base64，两个文件的内容为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">&lt;secret ephemeral&#x3D;&#39;no&#39; private&#x3D;&#39;no&#39;&gt;</span><br><span class="line">  &lt;uuid&gt;5774b929-0690-4513-a1f7-41aac49cbb31&lt;&#x2F;uuid&gt;</span><br><span class="line">  &lt;usage type&#x3D;&#39;ceph&#39;&gt;</span><br><span class="line">    &lt;name&gt;client.cinder1 secret&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;&#x2F;usage&gt;</span><br><span class="line">&lt;&#x2F;secret&gt;</span><br></pre></td></tr></table></figure><p>其中base64文件的内容就是keyring文件中key的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.admin]</span><br><span class="line">key &#x3D; AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>最后执行如下命令完成配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh secret-define --file 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">virsh secret-set-value --secret 5774b929-0690-4513-a1f7-41aac49cbb31 --base64 $(cat 5774b929-0690-4513-a1f7-41aac49cbb31.base64)</span><br><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系&lt;/li&gt;
&lt;li&gt;Libvirt配置：配置与Ceph之间的鉴权关系&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="Ceph" scheme="http://sunqi.site/tags/Ceph/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>快速构建Ceph集群</title>
    <link href="http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/"/>
    <id>http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/</id>
    <published>2020-09-12T09:05:53.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。</p><p>本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。</p><a id="more"></a><h1 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h1><p>我们准备四台服务器，其中一台作为部署发起节点和后续Client节点使用。另外三台作为Ceph节点使用，其中第一台节点node01上，除了monitor和osd外，还将运行Manager, MDS和RGW服务，用于提供文件及对象存储服务。每一台Ceph节点都另外挂载了一块单独的磁盘，由于我使用的是虚拟机环境，所以挂载节点为/dev/vdb，如果使用是其他环境需要注意挂载点名称。</p><p><img src="/images/pasted-58-1.png" alt="upload successful"></p><h1 id="部署时序图"><a href="#部署时序图" class="headerlink" title="部署时序图"></a>部署时序图</h1><p>使用Ceph Deploy将大幅度简化安装过程，大体上分为以下安装步骤：</p><ul><li>节点初始化配置</li><li>Ceph Deploy节点安装</li><li>Ceph集群初始化</li><li>ODS节点安装、安装Mgr服务及添加ODS磁盘，完成Ceph基本安装</li><li>CephFS安装，部署Metadata服务</li><li>Ceph RGW安装，部署RGW服务</li></ul><p><img src="/images/pasted-59-1.png" alt="upload successful"></p><h1 id="（全部节点）环境准备"><a href="#（全部节点）环境准备" class="headerlink" title="（全部节点）环境准备"></a>（全部节点）环境准备</h1><p>这是我非常常用的针对CentOS 7的设置，为了测试方便，关闭了防火墙、SELINUX，同时更新了系统和EPEL源为阿里源，最后进行系统更新，保证系统软件包更新到最新版本后，再进行环境安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><p>如果按照正常流程安装后，执行ceph -s，会出现restful模块无法找到，缺少pecan的安装包，所以在初始化阶段直接将缺少的包进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><h1 id="Ceph-Deploy节点安装"><a href="#Ceph-Deploy节点安装" class="headerlink" title="Ceph-Deploy节点安装"></a>Ceph-Deploy节点安装</h1><h2 id="（Ceph-Deploy节点）安装Ceph-Deploy"><a href="#（Ceph-Deploy节点）安装Ceph-Deploy" class="headerlink" title="（Ceph Deploy节点）安装Ceph-Deploy"></a>（Ceph Deploy节点）安装Ceph-Deploy</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h2 id="（全部节点）设置时间同步服务"><a href="#（全部节点）设置时间同步服务" class="headerlink" title="（全部节点）设置时间同步服务"></a>（全部节点）设置时间同步服务</h2><p>时间同步服务是分布式系统的生命线，所以安装时候先要安装NTP或者Chrony。在RHEL 7中，默认的时间同步被替换为Chrony，很多新的安装文档中也开始使用Chrony作为时间同步服务，但是NTP也被同时保留。我的环境中Chrony已经被安装并启动，如果没有请自行安装。</p><h2 id="（Ceph-Deploy节点）无密码登录"><a href="#（Ceph-Deploy节点）无密码登录" class="headerlink" title="（Ceph Deploy节点）无密码登录"></a>（Ceph Deploy节点）无密码登录</h2><p>这里为了简便，使用了root用户进行安装。配置完成后，需要让Ceph Deploy能够无密码的方式访问全部Ceph节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@node1</span><br><span class="line">ssh-copy-id root@node2</span><br><span class="line">ssh-copy-id root@node3</span><br></pre></td></tr></table></figure><h1 id="Ceph集群安装"><a href="#Ceph集群安装" class="headerlink" title="Ceph集群安装"></a>Ceph集群安装</h1><h2 id="（Ceph-Deploy节点）Ceph块存储服务安装"><a href="#（Ceph-Deploy节点）Ceph块存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph块存储服务安装"></a>（Ceph Deploy节点）Ceph块存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">ceph-deploy new node01</span><br><span class="line">ceph-deploy install node01 node02 node03</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin node01 node02 node03</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create node01</span><br><span class="line"></span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node01</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node02</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node03</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>由于默认采用了Bluestore安装方式，如果想使用SSD作为block.db和block.wal，可以这样创建OSD</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb --block-db &#x2F;dev&#x2F;vdc --block-wal &#x2F;dev&#x2F;vdc node01</span><br></pre></td></tr></table></figure><p>在Ceph Deploy节点，将Ceph相关配置文件拷贝至系统的/etc/ceph目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.conf &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.client.admin.keyring &#x2F;etc&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="（Ceph-Deploy节点）增加多个Monitor节点"><a href="#（Ceph-Deploy节点）增加多个Monitor节点" class="headerlink" title="（Ceph Deploy节点）增加多个Monitor节点"></a>（Ceph Deploy节点）增加多个Monitor节点</h3><p>添加多个Monitor节点，可以实现高可靠，但是一定为奇数。先更新配置文件，在刚才初始化集群目录下的ceph.conf中的mon_host添加所有节点IP，之后设定public network，这里我们使用了Ceph节点的网段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ceph.conf</span><br><span class="line"># ...</span><br><span class="line">mon_host &#x3D; 192.168.10.105,192.168.10.176,192.168.10.139</span><br><span class="line">public network &#x3D; 192.168.10.1&#x2F;24</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure><p>分发配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin node01 node02 node03</span><br><span class="line">ceph-deploy mon add node02</span><br><span class="line">ceph-deploy mon add node03</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查quorum状态</span><br><span class="line">ceph quorum_status --format json-pretty</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph文件系统服务安装"><a href="#（Ceph-Deploy节点）Ceph文件系统服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph文件系统服务安装"></a>（Ceph Deploy节点）Ceph文件系统服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds create node01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 添加多个Manager服务，Manager采用的是主从模式</span><br><span class="line">ceph-deploy mgr create node02 node03</span><br><span class="line"></span><br><span class="line"># 可以看到Manager主从节点状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>如果发现Ceph Monitor节点启动失败，需要到相应的节点上查看失败原因，比如我的Monitor使用Start启动，返回这样的提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Job for ceph-mon@node02.service failed because start of the service was attempted too often. See &quot;systemctl status ceph-mon@node02.service&quot; and &quot;journalctl -xe&quot; for details.</span><br><span class="line">To force a start use &quot;systemctl reset-failed ceph-mon@node02.service&quot; followed by &quot;systemctl start ceph-mon@node02.service&quot; again.</span><br></pre></td></tr></table></figure><p>按照提示重新启动即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl reset-failed ceph-mon@node02.service</span><br><span class="line">systemctl start ceph-mon@node02.service</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph对象存储服务安装"><a href="#（Ceph-Deploy节点）Ceph对象存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph对象存储服务安装"></a>（Ceph Deploy节点）Ceph对象存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy rgw create node01</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="块存储测试"><a href="#块存储测试" class="headerlink" title="块存储测试"></a>块存储测试</h2><h3 id="建立存储空间"><a href="#建立存储空间" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h3><h2 id="文件系统测试"><a href="#文件系统测试" class="headerlink" title="文件系统测试"></a>文件系统测试</h2><h3 id="建立存储空间-1"><a href="#建立存储空间-1" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data 16</span><br><span class="line">ceph osd pool create cephfs_metadata 16</span><br><span class="line"></span><br><span class="line"># ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">ceph fs ls</span><br></pre></td></tr></table></figure><h3 id="内核方式挂载"><a href="#内核方式挂载" class="headerlink" title="内核方式挂载"></a>内核方式挂载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">mount -t ceph 192.168.10.11:6789:&#x2F; &#x2F;mnt&#x2F;mycephfs -o name&#x3D;admin,secretfile&#x3D;&#x2F;etc&#x2F;ceph&#x2F;admin.secret</span><br></pre></td></tr></table></figure><h3 id="Fuse方式挂载"><a href="#Fuse方式挂载" class="headerlink" title="Fuse方式挂载"></a>Fuse方式挂载</h3><p>确保/etc/ceph下面已经拷贝了ceph.conf和keyring文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">ceph-fuse -m 192.168.10.11:6789 &#x2F;mnt&#x2F;mycephfs</span><br></pre></td></tr></table></figure><h2 id="对象存储测试"><a href="#对象存储测试" class="headerlink" title="对象存储测试"></a>对象存储测试</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。&lt;/p&gt;
&lt;p&gt;本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>一款云迁移产品的成长史</title>
    <link href="http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/"/>
    <id>http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/</id>
    <published>2020-08-11T06:04:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h1><p>孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。</p><h1 id="关于万博智云"><a href="#关于万博智云" class="headerlink" title="关于万博智云"></a>关于万博智云</h1><p>万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。</p><p>万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。</p><a id="more"></a><h1 id="结缘云迁移"><a href="#结缘云迁移" class="headerlink" title="结缘云迁移"></a>结缘云迁移</h1><p>2011年开始，我一直从事OpenStack在企业私有云应用的研发工作。从2011年一直到2018年，是开源社区最为活跃的时间段，各个公司将自己的主要精力全部投入到OpenStack各个模块的优化中。当时建设私有云平台所提供的服务往往是全方位的，从系统集成、安装实施再到后面的运行维护和定制化开发，基本上就是一整套全栈式解决方案，甚至有时候云平台之上的业务系统出问题，客户也会来找你。这对于任何尚处于初创型规模的OpenStack公司往往是个巨大的挑战。</p><p>2016年的时候，我们为一家农商行客户建设私有云，经过反复的前期验证，最终在2016年底拿下了该项目。当时除了建设云平台的需求外，还有一项作为验收标准的需求是将用户原有运行在各种物理机的业务系统平稳的迁移到新的云平台上，迁移过程不能对现有业务产生任何影响。最后还要将旧的硬件进行必要升级后，重新加入到新的云平台。</p><p>回想起当时云平台的建设过程，架构上并不复杂，就是一个典型的OpenStack使用硬件存储再加上VLAN的简单模式。在实际的项目实施中，从硬件到货到上架安装，再到云平台部署完成，前前后后的时间大约在三周左右。但是由于用户对于热迁移和资源回收的需求，整个项目实际耗时竟然长达半年之久。由于客户所处的位置不直通高铁，我们的工程师从北京出发，要不就是坐一夜的绿皮火车，要不就先高铁到徐州再转长途车的方式。无论哪种方式，路上的时间至少要8个小时以上。从方案验证到最终实施完毕，团队内全体成员总共出差次数超过50次以上，最终的实施成本极高。当我们尝试复盘整个过程时，耗时最久的其实就是解决各种迁移过程中产生的问题。</p><h1 id="挫折中前行"><a href="#挫折中前行" class="headerlink" title="挫折中前行"></a>挫折中前行</h1><p>这个客户的业务系统属于典型的老旧型业务系统，运行在物理机加上硬件存储阵列上，有少量的虚拟化环境，操作系统也是五花八门，最多的是SUSE 11，还有Windows 2003，CentOS等，数据库有DB2，Oracle，还有少量的MySQL。</p><p>由于是银行系统，所以对于业务连续性有非常强烈的诉求，在迁移上对我们提出了以下几点要求：</p><p>第一，风险控制。在任何行业中，稳定、可靠是当仁不让的第一原则，对于关乎民生的金融行业更是如此。所以在实际云平台建设过程中，原有业务系统上云时往往受到的阻力最大。究其原因就是在上云过程中没有一套完整的、科学的方法论及工具让用户打消对上云的顾虑。所以在向云迁移过程中，系统必须是可验证、可回退的。在正式切换到云平台之前，需要让业务系统在云平台之上得到充分的验证；在切换到云平台后，如果一旦发生失败，要马上能够回退到原有系统，继续提供服务。保障在云迁移过程中，风险降到最低。</p><p>第二、保障业务连续性。农商行不同于传统的四大行或者城商行，在IT建设上往往有很大的自主权，除了核心交易系统外，其他的业务系统均运行在本地系统上，所以对本地运维能力提出比较高的要求。在迁移过程中，本地业务系统的连续性非常重要，一旦中断银行就无法开门做生意了。同时，根据银监会印发的相关规定：在业务服务时段导致业务无法正常开展达半个小时(含)以上，属于重大运营中断事件。所以基本上迁移的切换时间窗口，只能在晚间进行，但是晚上银行又会有数据下发、跑批等程序的运行，所以留给迁移的时间窗口非常有限，所以必须采用一种近似于热迁移的效果来满足客户的需求。</p><p>第三，减少人为干预，保障迁移的可靠性。由于很多系统属于服务厂商开发，部分应用时间久远，甚至很多服务厂商已经不存在了，所以迁移过程中尽量减少对应用厂商的依赖很关键，比如重装、重新配置都会导致应用无法运行。同时，在迁移过程中，由于步骤非常复杂，人为操作过多非常容易产生错误。</p><p>在这个过程中，我们走了非常多的弯路，比如从最早采用冷迁移方式的Clonezilla，耗时24个小时才能迁移完一台主机；再比如调研了各种开源的p2v和v2v工具，没有一个好用的；再比如为了解决UEFI启动的问题，修改nova代码，但是加载后发现一台服务器启动过程黑屏了半个小时之久，为了这一个系统我们往返于北京和客户多达五次。这些困难促使我们不得不停下来思考，为什么一个看似简单的迁移，最终却成为影响项目进度和成本的关键因素呢？</p><h1 id="从项目中来，在项目中成长"><a href="#从项目中来，在项目中成长" class="headerlink" title="从项目中来，在项目中成长"></a>从项目中来，在项目中成长</h1><p>为了解决在项目中遇到的问题，我们尝试了各种手段，最终我们发现灾备领域的数据读取技术加上云原生的方式是最佳的组合方案。使用灾备的块级别差量复制技术能够充分保障业务连续性，而最大程度利用云平台原生接口和资源能够实现”两点之间直线最短“的效果，保障迁移的可靠性，大幅度降低人为介入而带来的不确定性，最后二者叠加的效果最终满足了风险可控的终极目标。</p><p>通过2016和2017年近两年的磨练，一个面向OpenStack的热迁移产品具备了初步产品雏形。在紧接着到来的2018年我们迎来了又一次大考，这一次我们面对着是江苏省农信的专有云平台的大规模迁移，我们需要将该省内全部62家二级法人的业务系统迁移上云。很快我们中标的兴奋就淹没在新的困难面前。在之前的项目中，我们的所有迁移行为都是在本地数据中心完成的，至少所有的网络基本都是千兆的。但是在这个项目中，省端和各个二级法人之间的连接变成了以10Mbps的专线，并且这还是最好的情况，还有更糟糕的只有2Mbps。省端与二级法人的专线连接主要用于省端的数据下发，所以用于迁移的数据传输只能在特定时间段进行，同时不能将全部的带宽占满，以防影响业务。但是，每个二级法人的用户数据量很大，大约在30TB - 50TB左右，如果完全依赖网络传输，理论上需要传上一年多的时间。所以完全依赖于网络传输是不可能的，我们需要的是一种硬件加网络的组合方案，由硬件保存全量数据，通过运输方式到省端，将全量数据切换至云端后，再通过网络传输增量，这样形成的效果仍然是热迁移，但是迁移的速度明显提高。</p><p>在解决了大规模数据传输后，我们紧接着遇到的问题就是先迁哪个，后迁哪个？我们都知道应用系统是存在一定的依赖关系的，所以在迁移前必须要梳理清楚应用系统的拓扑结构，同时还要对迁移后的网络、应用配置等变更做出预先分析，保障万无一失。这个过程其实就是在众多迁移方法论中提到的调研分析阶段。在这个过程中，我们也在实践中积累了自己的迁移调研方法和实施方案，对我们后来的项目起到了很大的帮助作用。同时我们也意识到，迁移绝对不是一个工具就解决的问题，而是一个重度的咨询过程，迁移工具只不过解决了最后一公里的问题。</p><p>从2018年初开始，我们和用户方组成的江苏省农信业务专家组，深入每个地市，严格遵照调研、评审、实施、切换进行科学的上云。从基本的系统信息采集、整理到业务系统上下联分析，绘制拓扑图，安全性等进行全面评估，之后根据调研的结论整理实施方案、进度，实施方案中要将一切在迁移后的变更提前进行整理，确保迁移过程中万无一失。通过辅助物理设备进行全量数据拷贝，运输到省端后进行切换上云，最终在合适的时间点完成增量及业务切换过程。在2018年下半年，平均一周就可以有三家农商行的业务系统实现全面上云。</p><p>在这个项目中，我们的产品得到了极大的锤炼，经受了大规模迁移的考验。通过专有云的建设和业务系统迁移，3年共为江苏农信节省IT投资5.6亿元。截止2018年9月30日，总共完成54家二级法人共1200多套系统迁移。同时，云平台的从最初的15个节点增长到了130多个节点，存储从0.2PB增长至3PB。</p><h1 id="从一朵云到一片云"><a href="#从一朵云到一片云" class="headerlink" title="从一朵云到一片云"></a>从一朵云到一片云</h1><p>时间到了2019年，我们产品的云原生的理念逐步得到了更多客户的认可，同时这种基于云原生构建的高度自动化的效果正好填补了云迁移这个市场空白。甚至某些老牌的灾备厂商把我们当成迁移竞争对手，直接在软文中进行”诋毁“，不过这一切恰好证明了我们产品所蕴含的巨大价值。</p><p>但是只能支持单云的迁移已经无法满足市场上越来越多的云迁移需求，所以在2019年上半年，我们准备全面支持更多的公有云和专有云平台。我们首先选择了国内的最大的公有云提供商——阿里云。阿里云在最近10年已经成长为中国云计算领域的标杆，拥有极高的市场占有率，同时提供了最广泛的API接口支持，为合作伙伴提供最大程度的赋能。由于阿里云与OpenStack在一些机制上存在差异，我们通过近3个月的调研和开发，终于突破了阿里云的热迁移。接下来，我们对云平台的支持范围不断扩大，又用了四个月左右时间，覆盖了国内绝大多数的公有云、专有云和私有云平台，成为了名副其实的多云迁移。</p><h1 id="打造极致的用户体验"><a href="#打造极致的用户体验" class="headerlink" title="打造极致的用户体验"></a>打造极致的用户体验</h1><p>很多企业级产品留给人的第一印象就是专业且复杂，不培训你两天你都不会用。在云迁移领域也是如此，很多云迁移产品都是由传统灾备厂商对原有灾备软件进行简单改造后的产物，界面复杂不说，操作还极其繁琐，迁移一台主机下来，十几个、二十几个步骤那是基本配置。所以在我们对产品进行迭代时，希望用To C的思维打造To B的产品。</p><p>在初始阶段，用户只要根据向导配置源端和目标端的信息后，就可以进入迁移流程。我们将迁移流程分成了三个简单的步骤：选择主机、同步数据和开始迁移。通过高度自动化的流程和对云原生API及资源的巧妙利用，初级的Linux工程师基本上几分钟就能完全上手。同时由于自动化程度高，在批量迁移时优势非常明显。</p><p>全新UI.png<br><img src="/images/pasted-52.png" alt="upload successful"></p><p>由于之前一直从事的是私有云领域的产品研发，导致我们的研发团队在产品开发中存在一种惯性。为了满足私有化部署的需要，我们往往需要将安装包做成无网络依赖的ISO格式。这直接导致的后果就是用户在试用我们的产品时往往需要先花很长一段时间去下载我们的安装介质，之后是安装，最后才能试用。这个一来一回的过程，往往就是一天的时间被浪费了。这一点在公有云迁移时，会让人觉得更加繁琐，所以在2019年下半年，我们决定将我们的产品SaaS化，让用户更快速的体验我们的产品而非将时间浪费在安装的环节上。由于人力资源的限制，研发团队和运维团队都受到了极大的挑战。研发团队需要开发新的模块以支持运营、多租户等SaaS需求，同时还要对原有的通讯模式进行改造，避免双向通讯的发生；而实施团队需要兼顾私有项目和线上运维，这就要求平台稳定、高可靠、易运维，所以对云原生的应用就变得尤为关键。我们利用阿里云的Kubernetes容器服务和各种云原生组件完成了SaaS化的改造，在没有增加任何人力的情况下，在2020年初完成SaaS的全面上线。</p><h1 id="在巨人肩膀上一起成长"><a href="#在巨人肩膀上一起成长" class="headerlink" title="在巨人肩膀上一起成长"></a>在巨人肩膀上一起成长</h1><p>2019年初，AWS斥资2.5亿美金收购了以色列灾备初创公司CloudEndure，虽然这家公司以灾备公司名义被收购，但主要业务却是提供向AWS的迁移服务。我们的产品在设计理念和用户体验上与CloudEndure非常相似，同时我们的产品可以支持国内众多的不同的云厂商。</p><p>AWS对CloudEndure的收购给了我们非常大的信心，让我们坚定了走云原生迁移、灾备产品的思路。我们发现这个市场在国内基本上属于空白阶段，虽然传统灾备厂商的工具可以靠堆人解决项目上的问题，但是真正让用户自助式的迁移平台才能让用户自主分配在云端的负载，让云资源得到更快速的消耗，最终让云厂商获益。</p><p>于是一个大胆的想法在脑海中形成，能不能把我们的迁移软件以云原生服务的方式集成在公有云平台中呢？经过几番周折，我们开始与阿里云进行接触。非常感谢阿里云的陈绪博士帮我打开了和阿里云团队的合作大门，在2019年与阿里云对接完成后，我们首先迎来了就是阿里云ECS团队的考验，在对产品充分测试后，我们在杭州与阿里云生态合作伙伴团队、投资部门进行了会面，这次会面彻底打开了我们与阿里云的合作大门。</p><p>2019年底，我被评为阿里云解决方案领域MVP，进一步促进了我们与阿里云之间的合作。2020年初，阿里云控制台上的应用工具市场吸引了我的目光。这种与阿里云深度整合的方式，对于云原生迁移、灾备是绝佳的栖息之地。通过阿里云MVP运营团队的引荐，我们成功的和阿里云应用工具市场团队进行了对接，同时在2月底决定上架阿里云应用工具市场。</p><p><img src="/images/pasted-53.png" alt="upload successful"></p><p>上架阿里云应用工具市场的过程绝非一帆顺利，阿里云对此有严格的安全性要求，上线前必须要通过阿里云安全部门的严格审查。为此，我们做了一些架构上的调整和安全性的加固。最终经过近3个月的努力，终于将我们的平台与2020年7月10日晚8点正式上线。上架后的迁移平台，与阿里云的用户体验保持完全一致。用户使用时毫无违和感。</p><p><img src="/images/pasted-54.png" alt="upload successful"></p><p>紧接着通过MVP运营团队与阿里云Apsara Stack团队取得了联系，开始对接Apsara Stack专有云，截止到8月初已经彻底实现了对Apsara Stack自动化迁移的全面支持。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>2020年4月，国家提出了新基建的发展目标，首当其冲的就是信息基础设施，而云计算作为新基建的底座，重要性不言而喻。2020年初的疫情，让全社会意识到”云上社会“的重要性，可以预见的一点是，全面云化的时代正在到来。</p><p>通过与阿里云的全面合作，为我们的产品带来了顶级流量入口，获取客户信任的时间更短。未来，我们也会将我们的产品打造成基于云原生的备份、容灾产品，为更多的云客户提供完美的用户体验。欢迎各位有志之士加入我们的团队，也欢迎有需求的客户加入我们的迁移群参与讨论（关注微信公众号后回复”支持“）。</p><img src="/images/pasted-55.png" alt="万博智云" title="万博智云" width="300" />]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于作者&quot;&gt;&lt;a href=&quot;#关于作者&quot; class=&quot;headerlink&quot; title=&quot;关于作者&quot;&gt;&lt;/a&gt;关于作者&lt;/h1&gt;&lt;p&gt;孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。&lt;/p&gt;
&lt;h1 id=&quot;关于万博智云&quot;&gt;&lt;a href=&quot;#关于万博智云&quot; class=&quot;headerlink&quot; title=&quot;关于万博智云&quot;&gt;&lt;/a&gt;关于万博智云&lt;/h1&gt;&lt;p&gt;万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。&lt;/p&gt;
&lt;p&gt;万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes All-in-One环境安装</title>
    <link href="http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/"/>
    <id>http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/</id>
    <published>2020-07-31T01:38:24.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes安装及初始化"><a href="#Kubernetes安装及初始化" class="headerlink" title="Kubernetes安装及初始化"></a>Kubernetes安装及初始化</h1><p>研发环境搭建Kubernetes All-in-One环境搭建。</p><a id="more"></a><h2 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h2 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure><h2 id="安装Kubernetes软件"><a href="#安装Kubernetes软件" class="headerlink" title="安装Kubernetes软件"></a>安装Kubernetes软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># step1 Installation Process</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">exclude&#x3D;kube*</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">yum install -y kubelet kubeadm kubectl --disableexcludes&#x3D;kubernetes</span><br><span class="line"> </span><br><span class="line">systemctl enable kubelet</span><br><span class="line"> </span><br><span class="line"># for CentOS 7</span><br><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"> </span><br><span class="line"># from k8s 1.8, swap need to be cloased, otherwise k8s could not be started</span><br><span class="line"># swapoff -a</span><br><span class="line"># Modify &#x2F;etc&#x2F;fstab, comment swap mount</span><br></pre></td></tr></table></figure><h2 id="初始化Kubernetes集群"><a href="#初始化Kubernetes集群" class="headerlink" title="初始化Kubernetes集群"></a>初始化Kubernetes集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Step2 initialization</span><br><span class="line"># Specify kubernetes-version if mirror do not contain latest kubernetes container. ex: if kubeadm is version 1.18.5, you can only</span><br><span class="line"># install kubernetes &#x3D; 1.18.x</span><br><span class="line">kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version&#x3D;1.18.0</span><br><span class="line"> </span><br><span class="line"># Response from output</span><br><span class="line"># You should now deploy a pod network to the cluster.</span><br><span class="line"># Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">#   https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"># Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"># kubeadm join 192.168.10.111:6443 --token 1odaru.0by05advhbu7edgt \</span><br><span class="line">#     --discovery-token-ca-cert-hash sha256:3efb71c40cce36c5ed90fc8b5831233aba06eec26576088e8e7a7a892d272776</span><br><span class="line"> </span><br><span class="line"># Step3 flannel Network</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables&#x3D;1</span><br><span class="line">kubectl apply -f https:&#x2F;&#x2F;gitee.com&#x2F;xiaoquqi&#x2F;flannel&#x2F;raw&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line"> </span><br><span class="line"># Step4 To use cluster</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure><h1 id="允许Master节点运行Pods"><a href="#允许Master节点运行Pods" class="headerlink" title="允许Master节点运行Pods"></a>允许Master节点运行Pods</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="安装Wordpress和MySQL"><a href="#安装Wordpress和MySQL" class="headerlink" title="安装Wordpress和MySQL"></a>安装Wordpress和MySQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;mysql-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;wordpress-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;kustomization.yaml</span><br></pre></td></tr></table></figure><h2 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -k .&#x2F;</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get secrets</span><br><span class="line">kubectl get pvc</span><br><span class="line">kubectl get pods</span><br><span class="line">kubectl get services wordpress</span><br></pre></td></tr></table></figure><h2 id="资源清理"><a href="#资源清理" class="headerlink" title="资源清理"></a>资源清理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -k .&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Kubernetes安装及初始化&quot;&gt;&lt;a href=&quot;#Kubernetes安装及初始化&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes安装及初始化&quot;&gt;&lt;/a&gt;Kubernetes安装及初始化&lt;/h1&gt;&lt;p&gt;研发环境搭建Kubernetes All-in-One环境搭建。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CentOS 7和Docker初始化安装</title>
    <link href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/</id>
    <published>2020-07-31T01:34:49.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h1><p>该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h1 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CentOS-7初始化&quot;&gt;&lt;a href=&quot;#CentOS-7初始化&quot; class=&quot;headerlink&quot; title=&quot;CentOS 7初始化&quot;&gt;&lt;/a&gt;CentOS 7初始化&lt;/h1&gt;&lt;p&gt;该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>OpenStack没落了吗？</title>
    <link href="http://sunqi.site/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/"/>
    <id>http://sunqi.site/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/</id>
    <published>2020-04-29T13:20:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OpenStack回顾"><a href="#OpenStack回顾" class="headerlink" title="OpenStack回顾"></a>OpenStack回顾</h1><p>OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。</p><p><img src="/images/pasted-42.png" alt="upload successful"></p><h1 id="OpenStack没落了吗？"><a href="#OpenStack没落了吗？" class="headerlink" title="OpenStack没落了吗？"></a>OpenStack没落了吗？</h1><p>我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。</p><a id="more"></a><p>随着容器、K8S等新兴技术的崛起，OpenStack无疑受到了很大的冲击，在之前两年经常看到一些唱衰OpenStack的文章。但是不可否认，目前OpenStack已经进入到了一个稳定阶段，很多私有云、专有云项目都是基于OpenStack提供解决方案。所以我认为并不存在OpenStack没落一说，只是技术发展的必经阶段：当底层逐渐稳定后，关注度往上发展。</p><p>同时，我们也看到，OpenStack基金会也在通过吸纳更多的项目来维持自身的影响力，比如：安全容器项目Kata Container，边缘计算项目StarlingX。</p><p>这是我对目前国内云计算市场的一张不完全总结，从这张图中我们应该可以很清晰的看到OpenStack对国内云计算市场深远的影响。同时，大家也能看出来谁才是真正的OpenStack这个开源项目的既得利益者。</p><p><img src="/images/pasted-44.png" alt="upload successful"></p><h1 id="OpenStack社区大数据"><a href="#OpenStack社区大数据" class="headerlink" title="OpenStack社区大数据"></a>OpenStack社区大数据</h1><p>从A版本开始到今天（2020年4月29日），总共有442家公司为OpenStack社区贡献过代码。排名前三位的分别是：Red Hat, Rackspace和Mirantis。中国唯一入选前十的是华为。</p><p><img src="/images/pasted-45.png" alt="upload successful"></p><p>OpenStack总共出现了706个Official项目，提交代码次数最多的是nova, neutron和cinder项目。</p><p><img src="/images/pasted-46.png" alt="upload successful"></p><p>总共有8523名开发者成功提交过1个以上的commits。从名字分析，前十名中有两位中国人：Zhong Shengping（麒麟云，主要贡献在自动化安装OpenStack相关项目puppet和ansible）和Qiming Teng（IBM 滕启明博士，主要贡献在senlin项目）。当然，我知道国内为OpenStack项目贡献的人很多，在这就不一一列举了。</p><p><img src="/images/pasted-47.png" alt="upload successful"></p><h1 id="OpenStack社区贡献变化趋势"><a href="#OpenStack社区贡献变化趋势" class="headerlink" title="OpenStack社区贡献变化趋势"></a>OpenStack社区贡献变化趋势</h1><p>参与贡献的公司已经呈现明显下降趋势，从国内情况来看，很多OpenStack初创公司也在积极投身K8S相关项目的研究，产品上提供基于容器的PaaS平台，丰富自己的解决方案。<br>从图中可以看到，OpenStack参与公司最多的是在Ocata Release中，参与公司达到了210家，从时间看是在2016年到2017年之间的时间点。这也是国内客户对OpenStack普遍接受的时间点。<br>另外从C版本开始一直到O版本（2011年到2017年）基本每个版本迭代维持20%以上的增长，可见在这个阶段绝大多数公司都看好OpenStack的未来。国内开源领域在这个阶段感觉也是最活跃的，毕竟只有当商业利益和开源目标相吻合时，这个开源项目才能得到最大的支持力度。<br>从O版本之后，参与的公司呈现小幅度下降趋势，不是很明显，大概在10%以内，下跌最明显的阶段是在S版本到T版本，也就是2019年。S版本有161家公司提交代码，而T版本只有126家，而目前U版本已经下降到了119家公司。</p><p><img src="/images/pasted-48.png" alt="upload successful"></p><p>从开发者数量看也呈现出相同的趋势，参与人数最多的是N版本，有2422人提交了commit。而到了S版本开发者仅为1189人，下降了一半还多。</p><p><img src="/images/pasted-49.png" alt="upload successful"></p><p>最后一张图，我们来看一下OpenStack模块数量。在早期OpenStack中一个新的项目获得批准是需要技术委员会批准的，也就是TC Approved，这样的项目到今天为止一共只有20个，主要是OpenStack基础的计算、存储和网络服务，包括：Nova, Neutron, Cinder, Heat, Horizon, Keystone, Ironic, Swift, Ceilometer, Glance, Sahara, Trove, Designate, neutron-lib, sahara的各种插件。<br>但是在2015年，社区决定采用Big Tent模式。Big Tent模式本意是基于OpenStack底层的计算、存储和网络等基础组件，构建更庞大的云原生应用场景，类似AWS。但是由于OpenStack自身部署、升级的复杂性，是社区力量更加分散，这样的设计并没有带来意料之中的效果。我个人理解，这样的生态建设更适合K8S。<br>在A版本中仅有8个模块，到了最新的T版本中，模块数量变为609个，还没有Release的U版本中，模块数量增长为627个。</p><p><img src="/images/pasted-50.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于OpenStack提供的服务属于基础架构层，从生态角度看，团结了各个层面的公司。从硬件的服务器、处理器、网络、存储厂商，到操作系统厂商，再到OpenStack创业公司，应用厂商，直到最终用户。<br>之前我们总说OpenStack是仅次于Linux的世界上第二大开源社区，不知道现在这种说法是否还准确。但是不可否认，OpenStack出现给了原来默默耕耘的开发者们走到前台充分展现的机会，也将国内开源的热潮推向了一个新的高度。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;OpenStack回顾&quot;&gt;&lt;a href=&quot;#OpenStack回顾&quot; class=&quot;headerlink&quot; title=&quot;OpenStack回顾&quot;&gt;&lt;/a&gt;OpenStack回顾&lt;/h1&gt;&lt;p&gt;OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-42.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;OpenStack没落了吗？&quot;&gt;&lt;a href=&quot;#OpenStack没落了吗？&quot; class=&quot;headerlink&quot; title=&quot;OpenStack没落了吗？&quot;&gt;&lt;/a&gt;OpenStack没落了吗？&lt;/h1&gt;&lt;p&gt;我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>[阿里云]使用DataQuotient 画像分析筛选优质基金</title>
    <link href="http://sunqi.site/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/"/>
    <id>http://sunqi.site/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/</id>
    <published>2020-04-14T03:10:17.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>该教程是阿里云帮助文档一部分，这里做了进一步完善：<a href="https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK</a></p><h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。</p><pre>4433法则如下：4：代表近一年收益率排名前1/4的基金。4：代表近两年、三年、五年以来，收益率排名前1/4的基金。3：指近六个月收益率排名前1/3的基金。3：指近三个月以来收益率排名前1/3的基金。</pre><p>本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。</p><a id="more"></a><h1 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h1><p><img src="/images/pasted-0.png" alt="upload successful"></p><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul><li>获取基金数据，从天天基金网接口获取股票型基金的不同时间区间的收益率数据</li><li>数据输入MaxCompute</li></ul><h2 id="创建基金信息标签系统"><a href="#创建基金信息标签系统" class="headerlink" title="创建基金信息标签系统"></a>创建基金信息标签系统</h2><ul><li>新建实体，用于将数源和待分析的对象绑定在一起</li><li>绑定标签，将数据源与标签进行绑定</li></ul><h2 id="筛选并导出优质基金群体"><a href="#筛选并导出优质基金群体" class="headerlink" title="筛选并导出优质基金群体"></a>筛选并导出优质基金群体</h2><ul><li>同步标签至RDS，这个例子中因为只是从MaxCompute到RDS，真实环境中有可能从多个数据源同步至目标RDS中，该服务支持数据的合并等</li><li>根据上述预先建立好的模型，新建长期和短期表现较好群体</li><li>计算群体</li><li>导出优质基金列表</li></ul><h1 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h1><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>本教程基于DataQuotient 画像分析、MaxCompute和RDS产品，请确保您已购买该产品。</p><h2 id="获取基础数据"><a href="#获取基础数据" class="headerlink" title="获取基础数据"></a>获取基础数据</h2><p>根据教程提供的线索，从天天基金网获取了全部股票型基金的数据，脚本已经提交到Github上，有需要的可以直接拿过去用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;fund</span><br><span class="line">cd fund</span><br><span class="line">python fund-cli.py -d -v</span><br></pre></td></tr></table></figure><h2 id="在MaxCompute导入数据"><a href="#在MaxCompute导入数据" class="headerlink" title="在MaxCompute导入数据"></a>在MaxCompute导入数据</h2><h3 id="开通MaxCompute服务"><a href="#开通MaxCompute服务" class="headerlink" title="开通MaxCompute服务"></a>开通MaxCompute服务</h3><p>之前我并没有开通过MaxCompute服务，所以需要开通一下DataWorks，才能使用MaxCompute服务。</p><p><img src="/images/pasted-1.png" alt="upload successful"></p><p>这个就是DataWorks控制台，从操作上看局限性很高（比如不支持删除表<br><img src="/images/pasted-2.png" alt="upload successful"></p><p>），所以建议采用IntelliJ IDEA中的MaxCompute Studio插件，安装方式见：<a href="https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1</a><br><img src="/images/pasted-41.png" alt="upload successful"></p><h3 id="创建MaxCompute表"><a href="#创建MaxCompute表" class="headerlink" title="创建MaxCompute表"></a>创建MaxCompute表</h3><p>由于做的时候才发现数据源并没有近五年的数据，所以修改了一下创建语句，先把表名建立起来，之后进入DDL模式进行表创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists fund_profit_stocktype</span><br><span class="line">( &#96;fundid&#96; bigint comment &#39;基金编号&#39;,</span><br><span class="line"> &#96;fundname&#96; string comment &#39;基金名称&#39;,</span><br><span class="line"> &#96;latest3months&#96; double comment &#39;近三月&#39;,</span><br><span class="line"> &#96;latest6months&#96; double comment &#39;近六月&#39;,</span><br><span class="line"> &#96;latest1year&#96; double comment &#39;近一年&#39;,</span><br><span class="line"> &#96;latest2years&#96; double comment &#39;近两年&#39;,</span><br><span class="line"> &#96;latest3years&#96; double comment &#39;近三年&#39;,</span><br><span class="line"> &#96;currentyear&#96; double comment &#39;今年来&#39;,  </span><br><span class="line"> &#96;fromcreated&#96; double comment &#39;成立来&#39;,</span><br><span class="line">) comment &#39;基金信息&#39; ;</span><br></pre></td></tr></table></figure><p><img src="/images/pasted-3.png" alt="upload successful"></p><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>原有文档是通过DDL去创建的数据，由于我们已经将数据保存在CSV文件中，所以我们选择导入方式试一下。</p><p><img src="/images/pasted-4.png" alt="upload successful"></p><p><img src="/images/pasted-5.png" alt="upload successful"></p><p><img src="/images/pasted-7.png" alt="upload successful"></p><p>不知道什么原因，下拉菜单选择并不生效，于是我决定将原有CSV无用字段全部删除，便于后续测试。删除字段的时候，发现我获取的数据并不包含近五年的项，但是包含成立以来的数据，所以对字段进行一下修改。上面的SQL是我修改过的。页面好像并没有删除表的功能，所以为了不影响测试，我重新建了一张表来导入数据。</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><h2 id="关联云计算资源"><a href="#关联云计算资源" class="headerlink" title="关联云计算资源"></a>关联云计算资源</h2><p>回到画像服务，继续进行配置</p><p><img src="/images/pasted-13.png" alt="upload successful"></p><p><img src="/images/pasted-14.png" alt="upload successful"></p><p>我用的主账号的AK/KS竟然提示我权限不足，于是我去查了半天RAM文档，以为MaxCompute需要更精确的授权，结果发现是提示信息误导了我，只是我的project写错了，最后project信息还是回到DataWorks控制台才找到，如下图所示。</p><p>需要添加这么多信息感觉不是特别方便，都是阿里云的资源感觉没有必要使用AK/KS去通讯吧？</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p><img src="/images/pasted-16.png" alt="upload successful"></p><p>配置好后的云计算资源</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><h2 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h2><p>配置好后，可以继续进行标签配置。</p><p><img src="/images/pasted-9.png" alt="upload successful"></p><p><img src="/images/pasted-10.png" alt="upload successful"></p><h3 id="关联MaxCompute表"><a href="#关联MaxCompute表" class="headerlink" title="关联MaxCompute表"></a>关联MaxCompute表</h3><p><img src="/images/pasted-11.png" alt="upload successful"></p><p>如果你没有正确配置云计算资源，是无法看到MaxCompute下的表。<br><img src="/images/pasted-12.png" alt="upload successful"></p><p>绑定表和字段<br><img src="/images/pasted-18.png" alt="upload successful"></p><p><img src="/images/pasted-19.png" alt="upload successful"></p><p><img src="/images/pasted-20.png" alt="upload successful"></p><p>字段绑定后无法删除，需要到详情中删除<br><img src="/images/pasted-21.png" alt="upload successful"></p><p><img src="/images/pasted-22.png" alt="upload successful"></p><p>最开始显示为0，后来又显示出数据总量，但是根据上方提示，MaxCompute是不支持显示的。<br><img src="/images/pasted-23.png" alt="upload successful"></p><p>过了一会又出现了1185的数据总量，不知道为什么<br><img src="/images/pasted-33.png" alt="upload successful"></p><p><img src="/images/pasted-25.png" alt="upload successful"></p><h2 id="筛选出优质基金群体"><a href="#筛选出优质基金群体" class="headerlink" title="筛选出优质基金群体"></a>筛选出优质基金群体</h2><h3 id="同步标签至RDS"><a href="#同步标签至RDS" class="headerlink" title="同步标签至RDS"></a>同步标签至RDS</h3><p><img src="/images/pasted-24.png" alt="upload successful"></p><p><img src="/images/pasted-26.png" alt="upload successful"></p><p><img src="/images/pasted-27.png" alt="upload successful"></p><p><img src="/images/pasted-28.png" alt="upload successful"></p><p><img src="/images/pasted-29.png" alt="upload successful"></p><p>显示同步成功，但是发现输出的信息里有红色的信息，以为是错误，但是仔细一看又是INFO级别的日志，而且显示的太长了。<br><img src="/images/pasted-31.png" alt="upload successful"></p><p><img src="/images/pasted-32.png" alt="upload successful"></p><h3 id="群体画像"><a href="#群体画像" class="headerlink" title="群体画像"></a>群体画像</h3><p>需要切换至群体画像的工作空间。<br><img src="/images/pasted-30.png" alt="upload successful"></p><p>新建短期表现优质的基金。<br><img src="/images/pasted-34.png" alt="upload successful"></p><p>如果配置正确，可以从结果中看出筛选出的结果。<br><img src="/images/pasted-35.png" alt="upload successful"></p><p><img src="/images/pasted-36.png" alt="upload successful"></p><p>新建长期表现优质的基金。<br><img src="/images/pasted-37.png" alt="upload successful"></p><p>使用群体计算，找出二者的交集。这个就是我们希望得到的结果。<br><img src="/images/pasted-38.png" alt="upload successful"></p><p><img src="/images/pasted-39.png" alt="upload successful"></p><p>利用下载功能，可以下载我们筛选出来的基金。<br><img src="/images/pasted-40.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>使用阿里云DataQuotient服务可以快速帮助用户构建用户画像，能够满足多种应用场景，利用函数计算进行数据爬取定期导入到MaxCompute，可以很容易定制出优质基金筛选功能的API接口，供上层业务场景使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该教程是阿里云帮助文档一部分，这里做了进一步完善：&lt;a href=&quot;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h1&gt;&lt;p&gt;4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。&lt;/p&gt;
&lt;pre&gt;
4433法则如下：
4：代表近一年收益率排名前1/4的基金。
4：代表近两年、三年、五年以来，收益率排名前1/4的基金。
3：指近六个月收益率排名前1/3的基金。
3：指近三个月以来收益率排名前1/3的基金。
&lt;/pre&gt;

&lt;p&gt;本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[Python]利用ZooKeeper构建分布式定时任务</title>
    <link href="http://sunqi.site/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/"/>
    <id>http://sunqi.site/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/</id>
    <published>2020-03-24T12:39:06.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>本文涉及的源代码路径：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz</a></p><h1 id="一、目前现状及存在的问题"><a href="#一、目前现状及存在的问题" class="headerlink" title="一、目前现状及存在的问题"></a>一、目前现状及存在的问题</h1><p>在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。</p><img src="/images/blogs/2020-03-24/1.png" class=""><a id="more"></a><h2 id="软件架构设计"><a href="#软件架构设计" class="headerlink" title="软件架构设计"></a>软件架构设计</h2><p>目前云迁移平台的各个服务模块在设计上使用了OpenStack方式，即大部分模块复用了类似Nova的实现框架。即API层直接集成oslo.service中定义好的WSGI Service基类，Worker采用了olso.service中定义好的Service基类，即Eventlet协程方式，API与Worker通讯使用RabbitMQ，API南向接口除少量直接更新数据库操作采用同步接口外，其余所有接口全部使用异步方式。API发送请求后，得到202 Accepted回复，后续通过GET接口不断轮询任务接口等到任务完成。</p><h2 id="高可靠部署"><a href="#高可靠部署" class="headerlink" title="高可靠部署"></a>高可靠部署</h2><p>根据OpenStack官方的HA部署文档（<a href="https://docs.openstack.org/ha-guide/" target="_blank" rel="noopener">https://docs.openstack.org/ha-guide/</a>），将服务分为无状态和有状态两种。无服务状态只需要直接部署多份即可，有状态服务往往需要通过Pacemaker控制副本数量，来保证高可靠。在云迁移平台部署中，我们将全部服务部署于K8S集群中，所以并不需要Pacemaker+Corosync这样的组件（Pacemaker节点上线为16）。但是，由于需要保持定时任务在单一节点被触发（避免任务被重复执行），所以承载定时快照的模块只能同时存在一个容器在运行，无法构成Active-Active（简称AA方式）模式。这样的部署方式，也造成了上述提到的AP模式对扩展性的瓶颈。</p><h1 id="二、问题思路及解决方案"><a href="#二、问题思路及解决方案" class="headerlink" title="二、问题思路及解决方案"></a>二、问题思路及解决方案</h1><h2 id="思路一、利用消息队列解耦任务分配与任务执行"><a href="#思路一、利用消息队列解耦任务分配与任务执行" class="headerlink" title="思路一、利用消息队列解耦任务分配与任务执行"></a>思路一、利用消息队列解耦任务分配与任务执行</h2><p>从上述对现状的描述，我们不难看出，现有任务分配与任务执行是在同一个任务中执行的，当存在大量任务时，任务执行会对任务产生产生很大的影响。同时，由于任务执行唯一性的需要，在部署上只能采用上述的AP模式，导致任务无法由多个任务同时执行。<br />所以，我们可以将任务分解为分配和执行两个阶段。任务分配上，单纯的进行任务生成，由于任务生成相对较快，生成后的任务发送至消息队列，由无状态性的Worker接收后执行。这样就解决了单点执行的效率低下问题。<br />但是这样的解决方案仍然存在缺陷，我们在任务生成的模块仍然必须需要采用AP模式部署，来保证任务的唯一性。如果在任务数量非常庞大时，该部分仍然是一个瓶颈；另外一方面这样的实现方式，我们需要将任务生成部分单独拆分出一个模块，同时增加了开发和部署上的复杂度，所以我们来看一下第二种解决思路。</p><img src="/images/blogs/2020-03-24/2.png" class=""><h2 id="思路二、利用Zookeeper构建可扩展的分布式定时任务"><a href="#思路二、利用Zookeeper构建可扩展的分布式定时任务" class="headerlink" title="思路二、利用Zookeeper构建可扩展的分布式定时任务"></a>思路二、利用Zookeeper构建可扩展的分布式定时任务</h2><p>为了解决思路一的局限性，我们本质上要解决的是任务执行的分布式问题，即如何让Worker不重复的判定任务的归属后再执行，由被动改为主动。</p><p>我们来看以下几种场景：<br />1、假定我们现在有3个Worker可以用于任务生成，在某一个时间点，将同时产生100个任务。如何由这3个Worker主动产生属于自身负责的任务？<br />2、我们知道大部分云平台目前都有云原生的弹性扩展服务，如果我们结合云平台的弹性扩展服务自动将我们用于任务生成的Worker动态进行调整时，例如变为6个时，还能保证这100个任务能够被自动的由6个节点不重复的产生呢？<br />3、当负载降低后，节点数量由6个变为3个后，如何恢复场景1的状态呢？保证任务不漏生成呢？</p><img src="/images/blogs/2020-03-24/3.png" class=""><p>如果想达到以上场景需求，需要以下几个条件：<br />1、节点之间能够准确知道其他节点的存在——利用Zookeeper进行服务发现<br />2、尽量合理的进行任务（对象）分布，同时兼顾节点增加和减少时，降低对象分配时的位移——利用一致性哈希环</p><h1 id="三、技术要点"><a href="#三、技术要点" class="headerlink" title="三、技术要点"></a>三、技术要点</h1><h2 id="1、Zookeeper"><a href="#1、Zookeeper" class="headerlink" title="1、Zookeeper"></a>1、Zookeeper</h2><p>对于Zookeeper的解释网络上有各种各样的详细集成，这里就不再赘述了，这里我直接引用了这篇文章（<a href="https://www.jianshu.com/p/50becf121c66）中开头的内容：" target="_blank" rel="noopener">https://www.jianshu.com/p/50becf121c66）中开头的内容：</a></p><blockquote><p>官方文档上这么解释zookeeper，它是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。<br>上面的解释有点抽象，简单来说zookeeper=文件系统+监听通知机制。</p></blockquote><img src="/images/blogs/2020-03-24/4.png" class=""><p>从我们应用场景的角度看，Zookeeper帮我们解决了Worker之间相互认识的过程，及时、准确的告诉我们：到底现在有多少个和我相同的活跃节点存在。至于底层是如何实现的，感兴趣的同学可以查看具体的Zookeeper实现原理文档，这里只介绍与我们实现相关的内容。</p><h2 id="2、一致性Hash"><a href="#2、一致性Hash" class="headerlink" title="2、一致性Hash"></a>2、一致性Hash</h2><p>又是一个经典的算法，相关的文章也很多，这里推荐大家几篇，这里摘抄出对理解我们实现有价值的内容。 参考文档：</p><ul><li>《面试必备：什么是一致性Hash算法？》<a href="https://zhuanlan.zhihu.com/p/34985026" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34985026</a></li><li>《五分钟看懂一致性哈希算法》<a href="https://juejin.im/post/5ae1476ef265da0b8d419ef2" target="_blank" rel="noopener">https://juejin.im/post/5ae1476ef265da0b8d419ef2</a></li><li>《一致性hash在分布式系统中的应用》<a href="http://www.firefoxbug.com/index.php/archives/2791/" target="_blank" rel="noopener">http://www.firefoxbug.com/index.php/archives/2791/</a></li></ul><h3 id="2-1-关于一致性哈希算法"><a href="#2-1-关于一致性哈希算法" class="headerlink" title="2.1 关于一致性哈希算法"></a>2.1 关于一致性哈希算法</h3><blockquote><p>一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。但现在一致性hash算法在分布式系统中也得到了广泛应用。</p></blockquote><h3 id="2-2-一致性哈希算法在缓存技术中的应用"><a href="#2-2-一致性哈希算法在缓存技术中的应用" class="headerlink" title="2.2 一致性哈希算法在缓存技术中的应用"></a>2.2 一致性哈希算法在缓存技术中的应用</h3><img src="/images/blogs/2020-03-24/5.png" class=""><blockquote><p>上述的方式虽然提升了性能，我们不再需要对整个Redis服务器进行遍历！但是，使用上述Hash算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！<br>试想一下，如果4台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？很简单，多增加几台缓存服务器不就行了！假设：我们增加了一台缓存服务器，那么缓存服务器的数量就由4台变成了5台。那么原本hash(a.png) % 4 = 2 的公式就变成了hash(a.png) % 5 = ？ ， 可想而知这个结果肯定不是2的，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变！换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据（还记得上一篇的《缓存雪崩》吗？）！<br>同样的，假设4台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从4台变为3台，也是会出现上述的问题！<br>所以，我们应该想办法不让这种情况发生，但是由于上述Hash算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，Hash一致性算法（一致性Hash算法）诞生了！</p></blockquote><h3 id="2-3-一致性哈希在缓存中的应用"><a href="#2-3-一致性哈希在缓存中的应用" class="headerlink" title="2.3 一致性哈希在缓存中的应用"></a>2.3 一致性哈希在缓存中的应用</h3><p>初始状态，将节点映射到哈希环中</p><img src="/images/blogs/2020-03-24/6.png" class=""><p>将对象映射到换后，找到负责处理的Node节点。</p><img src="/images/blogs/2020-03-24/7.png" class=""><p>容错性，Node C出现故障后，只需要将Object C迁移到Node D上。</p><img src="/images/blogs/2020-03-24/8.png" class=""><p>增加节点，此时增加了Node X，在Node C右侧，那么此时只有Object C需要移动到Node X节点。</p><img src="/images/blogs/2020-03-24/9.png" class=""><h2 id="3、tooz和kazoo"><a href="#3、tooz和kazoo" class="headerlink" title="3、tooz和kazoo"></a>3、tooz和kazoo</h2><p>Python中操作zookeeper的项目叫kazoo（<a href="https://kazoo.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://kazoo.readthedocs.io/en/latest/</a>）。<br />tooz是OpenStack中为简化开发人员操作分布式系统一致性所开发的组件，利用底层组件抽象出一致性组成员管理、分布式锁、选举、构建哈希环等。tooz除支持zookeeper作为后端，还可以支持Memcached、Redis、IPC、File、PostgreSQL、MySQL、Etcd、Consul等。<br />有关于tooz的发展历史可以参考：<a href="https://julien.danjou.info/python-distributed-membership-lock-with-tooz/" target="_blank" rel="noopener">https://julien.danjou.info/python-distributed-membership-lock-with-tooz/</a><br />这里我们主要使用tooz操作zookeeper实现我们的一致性组及一致性哈希。</p><h2 id="4、oslo相关项目"><a href="#4、oslo相关项目" class="headerlink" title="4、oslo相关项目"></a>4、oslo相关项目</h2><p>这几年一直在做OpenStack项目，从OpenStack项目中学习到很多设计、架构、研发管理等各种新知识、新理念。oslo项目就是在OpenStack不断的迭代中产生的公共项目库，这些库可以让你非常轻松的构建基于Python的构建近似于OpenStack的分布式、可扩展的微服务系统。<br />之前在从事OpenStack开发培训过程中，有专门的一节课去讲解OpenStack中用到的公共库，其中oslo相关项目就是非常重要的一部分内容。olso项目设计的库非常多，在这个内容中会涉及到oslo.config、oslo.log、oslo.service、oslo.utils和oslo.messaging项目。严格意义上来说，为了更精准控制任务，我们还应该引入oslo.db项目由数据库持久化的维护任务运行状态，包括任务回收等工作，但是本次内容主要讲解的是zookeeper，所以这部分的内容需要开发者在实际项目中去实现。<br />关于olso开发的内容，我会以视频课程的形式为大家讲解，敬请期待。</p><h1 id="四、实现过程"><a href="#四、实现过程" class="headerlink" title="四、实现过程"></a>四、实现过程</h1><h2 id="1、Zookeeper部署"><a href="#1、Zookeeper部署" class="headerlink" title="1、Zookeeper部署"></a>1、Zookeeper部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f zookeeper.yml -d up</span><br></pre></td></tr></table></figure><p>启动完成后，将使用本地的三个容器作为zookeeper的三个节点和三个不同的端口（2181/2182/2183）便于zookeeper连接。如果在生产环境中部署时，可以使用云原生服务或部署在多个可用区的方式，保证高可靠。</p><img src="/images/blogs/2020-03-24/10.png" class=""><h3 id="Zookeeper常用命令行"><a href="#Zookeeper常用命令行" class="headerlink" title="Zookeeper常用命令行"></a>Zookeeper常用命令行</h3><p>进入容器，就可以使用zkCli.sh进入zookeeper的CLI模式。如果是初次接触zookeeper，可以把zookeeper理解成一个文件系统，这里我们常用的命令就是ls。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it zookeeper_zoo1_1 bash</span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">zkCli.sh</span><br></pre></td></tr></table></figure><p>看到这样的提示，就表示连接成功了。</p><img src="/images/blogs/2020-03-24/11.png" class=""><p>如上面提到的zookeeper的存储结构所示，我们先从根节点（/）进行获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/12.png" class=""><p>这里zookeeper目录属于保留的目录，我们来看一下tooz的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/13.png" class=""><p>如果我们想继续查看distribution_tasks的内容，可以继续使用ls命令获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz/distribution_tasks</span><br></pre></td></tr></table></figure><p>通常我们会为每一个加入的节点取一个唯一的标识，当节点加入后我们使用ls命令就可以看到，如果离开了，则返回为空。</p><img src="/images/blogs/2020-03-24/14.png" class=""><p>zookeeper常用的命令还包括get，stat等获取value和更详细的信息，还包含更新节点操作set和删除节点rm。这里面就不做一一介绍了，我们直接操作zookeeper主要是为了帮助大家更好的理解程序逻辑。<br />具体的命令行信息可以参考：<a href="https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm</a></p><h2 id="2、tooz基本使用方法"><a href="#2、tooz基本使用方法" class="headerlink" title="2、tooz基本使用方法"></a>2、tooz基本使用方法</h2><p>关于tooz的两个示例主要来自于这篇博客：<a href="https://dzone.com/articles/scaling-a-polling-python-application-with-tooz" target="_blank" rel="noopener">https://dzone.com/articles/scaling-a-polling-python-application-with-tooz</a><br />原文中的例子是有些Bug的，这里面进行重新进行了优化和整理，并且使用zookeeper替代etcd3驱动。</p><h3 id="2-1-组成员（tooz-test-tooz-test-group-members-py）"><a href="#2-1-组成员（tooz-test-tooz-test-group-members-py）" class="headerlink" title="2.1 组成员（tooz/test_tooz/test_group_members.py）"></a>2.1 组成员（tooz/test_tooz/test_group_members.py）</h3><p>在这个例子中，我们主要为大家演示tooz如何进行组成员的管理。结合我们自身的需求，这里的成员就是每一个Worker。通过这个列子我们将观察三种不同场景的变化：<br />1、初始状态下，我们只能看到一个成员；<br />2、当启动了一个新的进程时，第一个成员马上会发现有第二个成员的加入；<br />3、同时，当我们用CTRL + C结束某一个进程时，另外一个活着的进程会立即发现组成员的变化。</p><h4 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h4><p>这里为了更直观表达，用时序图来说明程序的运行逻辑。</p><img src="/images/blogs/2020-03-24/15.png" class=""><h4 id="完整的代码"><a href="#完整的代码" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the group</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    c.create_group(group).get()</span><br><span class="line"><span class="keyword">except</span> coordination.GroupAlreadyExist:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the group</span></span><br><span class="line">c.join_group(group).get()</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Print the members list</span></span><br><span class="line">        <span class="comment">#c.run_watchers()</span></span><br><span class="line">        members = c.get_members(group).get()</span><br><span class="line">        print(<span class="string">"[%s]Current nodes in cluster: %s"</span> % (</span><br><span class="line">            current_time(), members))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line">    print(<span class="string">"[%s]After leave cluster nodes: %s"</span> % (</span><br><span class="line">        current_time(), c.get_members(group).get()))</span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果"><a href="#执行效果" class="headerlink" title="执行效果"></a>执行效果</h4><p>第一个成员</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_group_members.py client1 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/16.png" class=""><p>第二个成员加入，观察第一个成员的标准输出，为了观察加入集群的时间，我们加入了date</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date &amp;&amp; python test_group_members.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/17.png" class=""><p>第一个脚本的标准输出，在16:07:27秒的时候加入了集群:</p><img src="/images/blogs/2020-03-24/18.png" class=""><p>将第二个成员关闭，直接在第二个成员脚本按CTRL + C，首先观察第二个成员的输出：</p><img src="/images/blogs/2020-03-24/19.png" class=""><p>第一个成员的输出，在16:08:51分时，集群中已经没有了第二个成员了：</p><img src="/images/blogs/2020-03-24/20.png" class=""><h3 id="2-2-一致性哈希（tooz-test-tooz-test-ping-py）"><a href="#2-2-一致性哈希（tooz-test-tooz-test-ping-py）" class="headerlink" title="2.2 一致性哈希（tooz/test_tooz/test_ping.py）"></a>2.2 一致性哈希（tooz/test_tooz/test_ping.py）</h3><p>这个模拟测试中，使用分布式任务去ping某一个C类网段(255个IP地址)中的全部IP地址，如果由一个任务去完成，那么只能顺序执行，无法满足并发需求，这里采用一致性哈希算法，让任务分布在各个Worker上。为了节省时间，我们将原有程序中的实际ping换成了time.sleep等待方式。<br />另外在程序启动后，我们默认等待10秒等待其他成员(member)加入，在实际开发过程中，还需要对任务的状态进行严格控制，防止同一任务重复被执行，在演示代码中主要偏重演示分布式，所以并没有在任务状态上增加过多处理。</p><h4 id="时序图-1"><a href="#时序图-1" class="headerlink" title="时序图"></a>时序图</h4><img src="/images/blogs/2020-03-24/21.png" class=""><p>代码需要说明的几点：<br />0、在程序开始时，我们默认等待了10秒，等待其他节点加入，如果在循环开始后，再有新加入的节点时，由于并不知道第一个节点已经处理过的任务，所以在第二个Worker加入后根据当时哈希环对之前的任务重新分配并执行，造成了重复执行，这个问题需要通过额外的手段（例如数据库记录先前执行的任务状态）监控任务状态来防止任务重新执行。<br />1、代码中使用了tooz内置的Hash环，但是也可以在外部自己构建哈希环，我们在后续最终的例子中还是采用了外部构建哈希环的方法。<br />2、Tooz partitioner依赖于watchers，所以在每次循环的时候必须要调用run_watchers即使获取成员的加入和离开。<br />3、无论是group还是member在变量传递时都要变成bytes类型，这样可以确保对象的唯一性，所以在代码处理上都用到了encode()方法。<br />4、<strong>tooz_hash</strong>方法需要在使用Partition时自己实现，能够唯一标识出对象的方法，例如ID、名称等信息。</p><h4 id="完整的代码-1"><a href="#完整的代码-1" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the partitioned group</span></span><br><span class="line">p = c.join_partitioned_group(group)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Host</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hostname)</span>:</span></span><br><span class="line">        self.hostname = hostname</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tooz_hash__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a unique byte identifier so Tooz</span></span><br><span class="line"><span class="string">           can distribute this object."""</span></span><br><span class="line">        <span class="keyword">return</span> self.hostname.encode()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&lt;%s: %s&gt;"</span> % (self.__class__.__name__, self.hostname)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ping</span><span class="params">(self)</span>:</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">hosts_to_ping = [Host(<span class="string">"192.168.10.%d"</span> % i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>)]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"[%s]Waiting 10 seconds for other members..."</span> % current_time())</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"[%s]Current members: %s"</span> % (</span><br><span class="line">    current_time(), c.get_members(group).get()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> host <span class="keyword">in</span> hosts_to_ping:</span><br><span class="line">            c.run_watchers()</span><br><span class="line">            <span class="keyword">if</span> p.belongs_to_self(host):</span><br><span class="line">                print(<span class="string">"[%s]%s belongs to %s"</span> % (</span><br><span class="line">                    current_time(), host, p.members_for_object(host)))</span><br><span class="line">                <span class="keyword">if</span> host.ping():</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        print(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">        print(<span class="string">"Waiting for next loop..."</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果-1"><a href="#执行效果-1" class="headerlink" title="执行效果"></a>执行效果</h4><p>我们分别使用两个不同的窗口，同时启动两个Worker，我们可以很明显的看到主机被分配到两个不同的Worker中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client1 group1</span><br><span class="line">python test_pring.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/22.png" class=""><p>加入第三个Worker，可以看到一部分任务又被分配给了第三个Worker上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client3 group2</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/23.png" class=""><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>暂停第二个Worker，我们看到第二个Worker被停止后，任务重新被平衡到Worker1和Worker2上。</p><img src="/images/blogs/2020-03-24/24.png" class=""><h2 id="3、构建分布式定时任务"><a href="#3、构建分布式定时任务" class="headerlink" title="3、构建分布式定时任务"></a>3、构建分布式定时任务</h2><p>为了保持代码的兼容性，所以这里的实现是基于目前OpenStack体系的实现。另外，将任务发送给消息的部分在这个例子中并没有体现。示例代码仍然重复实现上述ping的例子，部分代码参考于Sahara项目的实现。</p><p>由于代码量较大，这里不贴出全部代码，仅仅对核心实现进行分析，完整代码请参考：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks</a></p><h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── coordinator.py -&gt; 一致性哈希的实现，该类中并没有直接使用上述tooz的partition，而是自己重新实现了HashRing</span><br><span class="line">├── periodic.py -&gt; 定时任务，基于oslo_service的PeriodicTasks基类</span><br><span class="line">├── service.py -&gt; Service类，继承于oslo.service的Service基类</span><br><span class="line">└── test_periodic_task.py -&gt; 程序入口</span><br></pre></td></tr></table></figure><h3 id="coordinator-py"><a href="#coordinator-py" class="headerlink" title="coordinator.py"></a>coordinator.py</h3><p>Coordinator是关键实现，所以这里重点对该类进行解释，在period task中需要调用coordinator即可实现分布式触发定时任务。</p><p>在coordinator.py中共实现了两个类，Coordinator和HashRing。<br />1、Coordinator类主要是针对tooz中对group members相关操作的封装，类似我们在tooz中的第一个例子；<br />2、HashRing是继承于Coordinator类，在功能上接近于tooz中Hash和Partition的实现，但是更简洁，tooz构建HashRing的用的PartitionNumber是32(2^5)，而我们用的是40，更大的数字会带来更均匀的分布但是会导致构建成本增加<br />3、HashRing中最重要的方法就是get_subset，通过映射到HashRing上的ID来判断Object的归属Worker</p><img src="/images/blogs/2020-03-24/25.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashRing</span><span class="params">(Coordinator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backend_url, group_id)</span>:</span></span><br><span class="line">        self.group_id = group_id</span><br><span class="line">        self.replicas = CONF.hash_ring_replicas_count</span><br><span class="line">        super(HashRing, self).__init__(backend_url)</span><br><span class="line">        self.join_group(group_id)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_hash</span><span class="params">(key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> int(</span><br><span class="line">            hashlib.md5(str(key).encode(<span class="string">'utf-8'</span>)).hexdigest(), <span class="number">16</span>)  <span class="comment"># nosec</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_ring</span><span class="params">(self)</span>:</span></span><br><span class="line">        ring = &#123;&#125;</span><br><span class="line">        members = self.get_members(self.group_id)</span><br><span class="line">        LOG.info(<span class="string">"Coordinator members: %s"</span> % members)</span><br><span class="line">        <span class="keyword">for</span> member <span class="keyword">in</span> members:</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> range(self.replicas):</span><br><span class="line">                hashed_key = self._hash(<span class="string">'%s:%s'</span> % (member, r))</span><br><span class="line">                ring[hashed_key] = member</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ring, sorted(ring.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_object</span><span class="params">(self, object, ring, sorted_keys)</span>:</span></span><br><span class="line">        <span class="string">"""Checks if this object belongs to this member or not"""</span></span><br><span class="line">        hashed_key = self._hash(object.id)</span><br><span class="line">        position = bisect.bisect(sorted_keys, hashed_key)</span><br><span class="line">        position = position <span class="keyword">if</span> position &lt; len(sorted_keys) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ring[sorted_keys[position]] == self.member_id</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_subset</span><span class="params">(self, objects)</span>:</span></span><br><span class="line">        <span class="string">"""Returns subset that belongs to this member"""</span></span><br><span class="line">        <span class="keyword">if</span> self.coordinator:</span><br><span class="line">            ring, keys = self._build_ring()</span><br><span class="line">            <span class="keyword">if</span> ring:</span><br><span class="line">                <span class="keyword">return</span> [obj <span class="keyword">for</span> obj <span class="keyword">in</span> objects <span class="keyword">if</span> self._check_object(</span><br><span class="line">                    obj, ring, keys)]</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">return</span> objects</span><br></pre></td></tr></table></figure><h3 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h3><p>分别在两个Terminal中运行脚本，可以看到Host被均匀的分布在两个Worker中执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_periodic_task.py</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/26.png" class=""><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>通过以上实例，我们了解了如何通过zookeeper构建分布式系统并进行任务调度，当然zookeeper在分布式系统还有更多的应用场景值得我们去学习。另外，OpenStack中很多抽象出来的模块对快速构建Python分布式系统是非常有帮助的，值得我们学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文涉及的源代码路径：&lt;a href=&quot;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、目前现状及存在的问题&quot;&gt;&lt;a href=&quot;#一、目前现状及存在的问题&quot; class=&quot;headerlink&quot; title=&quot;一、目前现状及存在的问题&quot;&gt;&lt;/a&gt;一、目前现状及存在的问题&lt;/h1&gt;&lt;p&gt;在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。&lt;/p&gt;
&lt;img src=&quot;/images/blogs/2020-03-24/1.png&quot; class=&quot;&quot;&gt;
    
    </summary>
    
    
    
      <category term="Zookeeper" scheme="http://sunqi.site/tags/Zookeeper/"/>
    
      <category term="微服务" scheme="http://sunqi.site/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="分布式" scheme="http://sunqi.site/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="Python" scheme="http://sunqi.site/tags/Python/"/>
    
      <category term="tooz" scheme="http://sunqi.site/tags/tooz/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architecture Associate Practice</title>
    <link href="http://sunqi.site/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/"/>
    <id>http://sunqi.site/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/</id>
    <published>2020-01-30T13:35:00.000Z</published>
    <updated>2021-02-02T10:06:02.390Z</updated>
    
    <content type="html"><![CDATA[<p>该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。</p><p>总得分: 76%<br>主题得分:<br>1.0. Design Resilient Architectures 89%<br>2.0. Define Performant Architectures 71%<br>3.0. Specify Secure Applications and Architectures 50%<br>4.0. Design Cost-Optimized Architectures 100%<br>5.0. Define Operationally Excellent Architectures 100%</p><p>个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。</p><a id="more"></a><h1 id="您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"><a href="#您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。" class="headerlink" title="(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"></a>(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。</h1><p>该区域有三个可用区（us-west-2a，us-west-2b和us-west-2c)可以使用，如果us-west-2中的任何可用区变得不可用，以下哪种部署可以提供容错功能？（选择两顶◊)<br>A. 在us-west-2a中有2个EC2实例，在us-west-2b中有2个EC2实例，在us-west-2c中有2个EC2实例<br>B. 在 us- west- 2 a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中没有EC2实例<br>C. 在us-west-2a中有4个 EC2实例，在us-west-2b中有2个 EC2实例，在us-west-2c中有2个 EC2实例<br>D. 在 us- west- 2 a中有6个 EC2实例，在us-west-2b中有6个 EC2实例，在us-west-2c中没有EC2实例<br>E. 在us-west-2a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中有3个 EC2实例</p><p>Answer: DE</p><p>该道题的重点是始终运行6个EC2实例，所以当一个区Down掉，仍然能保证有6台实例的答案为正确答案。</p><h1 id="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处"><a href="#一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处" class="headerlink" title="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?"></a>一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?</h1><p>A. Elastic Beanstalk<br>B. CloudFormation<br>C. AMS CodeBuild<br>D. AMS CodeDeploy</p><p>Answer: B</p><h1 id="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"><a href="#解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。" class="headerlink" title="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"></a>解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。</h1><p>Amazon Glacier中的哪种功能可以帮助满足该要求？<br>A. 文件库锁定<br>B. 加速检索<br>c. 批量检索<br>D. 标准检索</p><p>Answer: B</p><blockquote><p>问：如何从该服务检索数据？</p><p>当您请求从 S3 Glacier 检索数据时，即表示您启动了一个存档检索作业。当检索作业完成后，您的数据将在 24 小时内可供下载或通过 Amazon Elastic Compute Cloud (Amazon EC2) 访问。有三种方式可以检索数据，每种具有不同的访问时间和成本：加急、标准和批量检索。</p><p>问：什么是加急检索？</p><p>当您偶尔需要加急请求档案子集时，可以使用加急检索来快速访问您的数据。除了最大的存档 (250MB+) 以外，对于使用加急检索方式访问的所有数据，通常在 1-5 分钟内即可使用。有两种加急检索：按需和预置。当我们可以在 1-5 分钟内完成检索时，就可以实施按需检索。所提供的请求将确保在您需要时能够获得加急检索的能力。</p></blockquote><h1 id="一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"><a href="#一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。" class="headerlink" title="(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"></a>(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。</h1><p>该组织应使用哪种解决方案以满足该要求？<br>A. AMS KMS API<br>B. AMS Certificate Manager<br>C. 具有 STS 的 API Gateway<br>D. IAM访问密钥</p><p>Answer: A</p><blockquote><p>问：什么是 Amazon EBS 加密？</p><p>Amazon EBS 加密提供 EBS 数据卷、引导卷和快照的无缝加密，无需构建和维护安全密钥管理基础设施。EBS 加密可使用 Amazon 托管的密钥或您使用 AWS Key Management Service (KMS) 创建和管理的密钥来给您的数据加密，从而保障静态数据的安全性。加密还发生在托管 EC2 实例的服务器上，当数据在 EC2 实例和 EBS 存储之间移动时提供数据加密。有关详细信息，请参阅 Amazon EC2 用户指南中的“Amazon EBS”加密。</p><p>问：什么是 AWS Key Management Service (KMS)？</p><p>AWS KMS 是一项托管服务，可让您轻松创建和控制加密数据所用的加密密钥。AWS Key Management Service 可与其他 AWS 服务集成，包括 Amazon EBS、Amazon S3 和 Amazon Redshift，可让您轻松使用您管理的加密密钥加密您的数据。AWS Key Management Service 还能与 AWS CloudTrail 集成，从而为您提供所有密钥的使用记录，帮助您满足监管和合规性要求。要了解有关 KMS 的更多信息，请访问 AWS Key Management Service 产品页面。</p></blockquote><h1 id="一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"><a href="#一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。" class="headerlink" title="(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"></a>(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。</h1><p>以下哪种步骤组合满足安全策略要求？（选择两顶)<br>A. 启用 Amazon Redshift 增强 VPC 路由。<br>B. 创建集群安全组以允许Amazon Redshift集群访问Amazon S3<br>C. 在公有子网中创建NAT网关以允许Amazon Redshift集群访问Amazon S3<br>D. 创建并配置Amazon S3 VPC终端节点<br>E. 在私有子网中设置NAT网关以允许Amazon Redshift集群访问AmazonS3</p><p>Answer: AD</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html</a></p><p>使用 VPC 终端节点<br>可以使用 VPC 终端节点创建 VPC 中的 Amazon Redshift 集群与 Amazon Simple Storage Service (Amazon S3) 之间的托管连接。在执行此操作时，您的集群与 Amazon S3 数据之间的 COPY 和 UNLOAD 流量将保留在您的 Amazon VPC 中。可以将终端节点策略附加到您的终端节点，以便更严格地管理对数据的访问。例如，可以向 VPC 终端节点添加策略以仅允许将数据上传到您账户中的特定 Amazon S3 存储桶。</p><p>重要<br>目前，Amazon Redshift 仅支持连接到 Amazon S3 的 VPC 终端节点。当 Amazon VPC 添加对其他 AWS 服务的支持以使用 VPC 终端节点时，Amazon Redshift 也将支持这些 VPC 终端节点连接。要使用 VPC 终端节点连接到 Amazon S3 存储桶，Amazon Redshift 集群与其连接到的 Amazon S3 存储桶必须在同一个 AWS 区域中。</p><p>要使用 VPC 终端节点，请为集群所在的 VPC 创建 VPC 终端节点，然后为集群启用增强型 VPC 路由。可以在 VPC 中创建集群时启用增强型 VPC 路由，也可以修改 VPC 中的集群以使用增强型 VPC 路由。</p><p>VPC 终端节点使用路由表来控制 VPC 中的集群和 Amazon S3 之间的流量路由。与指定路由表关联的子网中的所有集群会自动使用该终端节点来访问服务。</p><p>您的 VPC 使用与集群流量匹配的最具体的/最严格的路由来决定路由流量的方式。例如，假设路由表中有一条路由用于所有指向 Internet 网关和 Amazon S3 终端节点的 Internet 流量 (0.0.0.0/0)。在这种情况下，对所有传送到 Amazon S3 的流量优先使用终端节点路由。这是因为 Amazon S3 服务的 IP 地址范围比 0.0.0.0/0 更具体。在此示例中，所有其他 Internet 流量（包括定位到其他 AWS 区域内的 Amazon S3 存储桶的流量）将流向 Internet 网关。</p><p>有关创建终端节点的更多信息，请参阅 Amazon VPC 用户指南 中的 VPC 终端节点。</p><p>您使用终端节点策略控制从集群到包含数据文件的 Amazon S3 存储桶的访问。默认情况下，创建终端节点向导会附加一个终端节点策略，该策略不会进一步限制来自 VPC 中的任何用户或服务的访问。要实现更具体的控制，您可以选择附加一个自定义终端节点策略。有关更多信息，请参阅 Amazon VPC 用户指南 中的使用终端节点策略。</p><p>使用终端节点不收取任何额外费用。采用标准的数据传输和资源使用计费方式。有关定价的更多信息，请参阅 Amazon EC2 定价。</p></blockquote><blockquote><p><a href="https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html" target="_blank" rel="noopener">https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html</a></p><p>Amazon Redshift 增强型 VPC 路由</p><p>在使用 Amazon Redshift 增强型 VPC 路由时，Amazon Redshift 会强制通过您的 Amazon VPC 路由集群和数据存储库之间的所有 COPY 和 UNLOAD 流量。通过使用增强型 VPC 路由，您可以使用标准 VPC 功能，例如 VPC 安全组、网络访问控制列表 (ACL)、VPC 终端节点、VPC 终端节点策略、Internet 网关和域名系统 (DNS) 服务器，如 Amazon VPC 用户指南 中所述。 您可以使用这些功能来严格管理 Amazon Redshift 集群与其他资源之间的数据流。在使用增强型 VPC 路由通过您的 VPC 路由流量时，也可以使用 VPC 流日志来监视 COPY 和 UNLOAD 流量。</p><p>如果未启用增强型 VPC 路由，则 Amazon Redshift 会通过 Internet 路由流量，包括至 AWS 网络中的其他服务的流量。</p><p>重要<br>由于增强型 VPC 路由影响了 Amazon Redshift 访问其他资源的方式，因此，除非您正确配置 VPC，否则 COPY 和 UNLOAD 命令可能会失败。您必须专门在集群的 VPC 和数据资源之间创建网络路径，如下所述。</p><p>在对已启用增强型 VPC 路由的集群执行 COPY 或 UNLOAD 命令时，您的 VPC 会使用最严格 或最具体的可用网络路径来将流量路由到指定资源。</p></blockquote><h1 id="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"><a href="#一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。" class="headerlink" title="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"></a>一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。</h1><p>哪种存储服务可满足这些要求？<br>A. Amazon Redshift<br>B. Amazon RDS<br>C. ElastiCache<br>D. DynamoDB</p><p>Answer: A</p><h1 id="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"><a href="#解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。" class="headerlink" title="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"></a>解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。</h1><p>架构师应使用哪种AWS服务来实现该目的？<br>A. Amazon STS<br>B. Amazon SQS<br>C. Lambda<br>D. Amazon SNS</p><p>Answer: D</p><blockquote><p>Amazon Simple Notification Service (SNS) 是一种高度可用、持久、安全、完全托管的发布/订阅消息收发服务，可以轻松分离微服务、分布式系统和无服务器应用程序。Amazon SNS 提供了面向高吞吐量、多对多推送式消息收发的主题。借助 Amazon SNS 主题，发布系统可以向大量订阅终端节点（包括 Amazon SQS 队列、AWS Lambda 函数和 HTTP/S Webhook 等）扇出消息，从而实现并行处理。此外，SNS 可用于使用移动推送、短信和电子邮件向最终用户扇出通知。</p></blockquote><h1 id="解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"><a href="#解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。" class="headerlink" title="(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"></a>(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。</h1><p>架构师应使用哪种解决方案以满足这些要求？<br>A. 任务的IAM角色<br>B. EC2实例的 IAM角色<br>C. EC2实例的 IAM实例配置文件<br>D. 安全组规则</p><p>Answer: A</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p>借助 Amazon ECS 任务的 IAM 角色，您可以指定一个可由任务中的容器使用的 IAM 角色。应用程序必须使用 AWS 凭证签署其 AWS API 请求，并且此功能提供了一个管理凭证的策略以供应用程序使用，类似于 Amazon EC2 实例配置文件为 EC2 实例提供凭证的方式。您可以将 IAM 角色与 ECS 任务定义或 RunTask API 操作关联，而不是为容器创建和分配 AWS 凭证或使用 EC2 实例的角色。之后，任务的容器中的应用程序可以使用 AWS 开发工具包或 CLI 向授权的 AWS 服务发出 API 请求。</p></blockquote><h1 id="一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"><a href="#一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。" class="headerlink" title="一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"></a>一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。</h1><p>哪种Amazon RDS引擎满足这些要求？<br>A. MySQL<br>B. Microsoft SQL Server<br>C. Oracle<br>D. Amazon Aurora</p><p>Answer: D</p><blockquote><p><a href="https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6" target="_blank" rel="noopener">https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6</a></p><p>Amazon Aurora 副本复制是毫秒级别，而MySQL是秒级别</p></blockquote><h1 id="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"><a href="#管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。" class="headerlink" title="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"></a>管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。</h1><p>哪种AMS服务可以执行该操作？<br>A. Amazon EBS<br>B. Amazon EFS<br>C. Amazon S3<br>D. Amazon EC2实例存储</p><p>Answer: B</p><h1 id="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"><a href="#一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。" class="headerlink" title="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"></a>一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。</h1><p>哪种解决方案可满足这些要求而不会影响其他运行的应用程序？（选择两顶)<br>A. 在Web服务器的子网上创建一个网络ACL，允许HTTPS端口 443入站流量，并将源指定为0.0.0.0/0。<br>B. 创建一个允许来自Anywhere (0.0.0.0/0)的 HTTPS端口 443入站流量的Web服务器安全组，并将其应用于Web服务器。<br>C. 创建一个允许MySQL端口 3306入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。<br>D. 在数据库子网上创建一个网络ACL，允许Web服务器的MySQL端口 3306入站流量，并拒绝所有出站流量。<br>E. 创建一个允许HTTPS端口 443入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。</p><p>Answer: BC</p><h1 id="一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"><a href="#一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。" class="headerlink" title="一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"></a>一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。</h1><p>备份这些卷的最灵活方法是什么？<br>A. 定期创建EBS快照。<br>B. 启用EBS卷加密。<br>C. 创建脚本以将数据复制到EC2实例存储。<br>D. 在两个EBS卷之间镜像数据。</p><p>Answer: A</p><h1 id="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"><a href="#解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。" class="headerlink" title="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"></a>解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。</h1><p>哪种AMS服务可满足这些要求？<br>A. Amazon S3<br>B. Amazon EBS<br>C. Amazon EFS<br>D. Amazon Storage Gateway VTL</p><p>Answer: A</p><h1 id="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"><a href="#AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。" class="headerlink" title="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"></a>AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。</h1><p>哪种解决方案能满足这些要求？<br>A. 启动一个NAT网关并为0.0.0.0/0添加路由<br>B. 附加一个VPC终端节点并为0.0.0.0/0添加路由<br>C. 附加一个Internet网关并为0.0.0.0/0添加路由<br>D. 在公有子网中部署NAT实例并为0.0.0.0/0添加路由</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p><p>Internet 网关是一种横向扩展、支持冗余且高度可用的 VPC 组件，可实现 VPC 中的实例与 Internet 之间的通信。因此它不会对网络流量造成可用性风险或带宽限制。</p><p>NAT 网关<br>您可以使用网络地址转换 (NAT) 网关允许私有子网中的实例连接到 Internet 或其他 AWS 服务，但阻止 Internet 发起与这些实例的连接。有关 NAT 的更多信息，请参阅NAT。<br>您在账户中创建和使用 NAT 网关会产生费用。NAT 网关小时使用费率和数据处理费率适用于此。Amazon EC2 数据传输费同样适用。有关更多信息，请参阅 Amazon VPC 定价。</p></blockquote><h1 id="待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"><a href="#待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。" class="headerlink" title="(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"></a>(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。</h1><p>您可以选择哪种设计选顶以在保持弹性的同时优化成本？（选择两顶)<br>A. 修改Auto Scaling组终止策略以先终止最老的实例。<br>B. 修改Auto Scaling组终止策略以先终止最新的实例。<br>C. 修改Auto Scaling组冷却计时器。<br>D. 修改Auto Scaling策略以使用计划的缩放操作。<br>E. 修改触发Auto Scaling缩减策略的CloudWatch警报周期。</p><p>Answer: BC</p><p>这道题目前纠结点在于AB两个选项，从文档中可知有一种策略结束类型叫ClosestToNextInstanceHour类型更适合该题目。如果从这个角度说，新的实例好像更靠近最近计费时间点这个选项。</p><h1 id="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶"><a href="#对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶" class="headerlink" title="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)"></a>对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)</h1><p>A. 使用Amazon RDS的Web应用程序<br>B. 企业数据仓库<br>C. 长时间运行的工作进程<br>D. 静态网站<br>E. 每晚运行一次的管理任务</p><p>Answer: AD</p><h1 id="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"><a href="#一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。" class="headerlink" title="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"></a>一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。</h1><p>哪种是满足这些要求的最经济高效的存储选顶？<br>A. 具有加速检索的Amazon Glacier<br>B. Amazon S3标准-低频率访问<br>C. Amazon EFS<br>D. Amazon S3 标准</p><p>Answer: B</p><h1 id="一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"><a href="#一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。" class="headerlink" title="一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"></a>一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。</h1><p>最经济高效的存储类型是什么？<br>A. Amazon EBS 预置 IOPS SSD<br>B. Amazon EBS吞吐量优化HDD<br>C. Amazon EBS 通用型 SSD<br>D. Amazon EFS</p><p>Answer: C</p><h1 id="一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"><a href="#一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。" class="headerlink" title="一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"></a>一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。</h1><p>哪种架构能以最经济高效的方式处理该工作负载？<br>A. 使用Kinesis数据传输流存储文件，并使用Uirtoda进行处理。<br>B. 使用SQS队列存储文件，然后，一组EC2实例访问该文件。<br>C. 将文件存储在EBS卷中，然后，其他EC2实例可以访问该文件以进行处理。<br>D. 将文件存储在S3存储捅中，并使用Amazon S3事件通知调用Lambda函数以处理该文件。</p><p>Answer: D</p><h1 id="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"><a href="#一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。" class="headerlink" title="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"></a>一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。</h1><p>该公司应釆取哪些措施以改进网站的用户体验？<br>A. 将数字资产移到到Amazon Glacier中。<br>B. 使用CloudFront缓存静态内容。<br>C. 调整图像以使其变小。<br>D. 使用保留的EC2实例。</p><p>Answer: B</p><h1 id="您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。"><a href="#您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。" class="headerlink" title="您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。"></a>您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。</h1><p>应使用哪种方法可将凭证安全地传送到该应用程序？<br>A. 使用实例用户数据将API凭证传送到实例。<br>B. 将API凭证作为对象存储在Amazon S3中。<br>C. 将API凭证嵌入到JAR文件中。<br>D. 将IAM角色分配给EC2实例。</p><p>Answer: D</p><h1 id="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的"><a href="#一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的" class="headerlink" title="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:"></a>一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:</h1><p>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=de" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=de</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=en" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=en</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=es" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=es</a><br>应如何配置CloudFront以使用正确的语言提供缓存的数据？<br>A. 将Cookie转发到原始地址。<br>B. 基于查询字符串参数。<br>C. 在原始地址中缓存对象。<br>D. 提供动态内容。</p><p>Answer: B</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html</a></p><p>一些 Web 应用程序使用查询字符串将信息发送到源。查询字符串是 Web 请求的一部分，显示在 ? 字符之后；该字符串可以包含一个或多个使用 &amp; 字符分隔的参数。在以下示例中，查询字符串包括两个参数 color=red 和 size=large：</p></blockquote><h1 id="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"><a href="#解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。" class="headerlink" title="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"></a>解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。</h1><p>最适合该使用案例的决策是什么？<br>A. 将文件存储在Amazon EBS上，并创建一个生命周期策略以在三个月后删除这些文件。<br>B. 将文件存储在Amazon S3中，并创建一个生命周期策略以在三个月后删除这些文件。<br>C. 将文件存储在Amazon Glacier中，并创建一个生命周期策略以在三个月后删除这些文件。<br>D. 将文件存储在Amazon EFS上，并创建一个生命周期策略以在三个月后删除这些文件。</p><p>Answer: B</p><h1 id="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"><a href="#一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。" class="headerlink" title="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"></a>一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。</h1><p>该团队应该选择哪种服务？<br>A. Amazon EFS<br>B. Amazon RedShift<br>C. DynamoDB<br>D. AWS CloudFormation</p><p>Answer: C</p><h1 id="一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。"><a href="#一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。" class="headerlink" title="(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。"></a>(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。</h1><p>架构师应如何最大限度地提高Amazon S3性能？<br>A. 在每个对象名称前面添加随机字符串。<br>B. 使用STANDARD_IA存储类<br>C. 在每个对象名称前面添加当前日期。<br>D. 在S3存储桶上启用版本控制。</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>当从 Amazon S3 上传和检索存储时，您的应用程序可以轻松地实现每秒数千个事务的请求性能。Amazon S3 会自动扩展至高请求速率。例如，您的应用程序可以在存储桶中实现至少每秒每个前缀 3,500 个 PUT/COPY/POST/DELETE 请求和 5,500 个 GET/HEAD 请求。对存储桶中的前缀数量没有限制。您可以通过并行读取来增加读取或写入性能。例如，如果您在 Amazon S3 存储桶中创建 10 个前缀以并行处理读取，则可以将读取性能扩展到每秒 55,000 个读取请求。<br>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><blockquote><p>2018年7月17日 Amazon S3 宣布提高请求速率性能(<a href="https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a>)</p><p>Amazon S3 现在提供了更高的性能，支持每秒至少 3500 个数据添加请求、每秒 5500 个数据检索请求，而且无需额外费用，这可以节省大量处理时间。每个 S3 前缀均支持这些请求速率，因此可以轻松实现显著的性能提升。</p><p>目前在 Amazon S3 上运行的应用程序均可享受此性能改进，而无需实施任何更改；在 S3 上构建新应用程序的客户无需进行任何应用程序自定义即可享受此性能。Amazon S3 对并行请求的支持意味着您可以按照计算集群的系数扩展 S3 性能，而无需对应用程序进行任何自定义。性能按前缀扩展，因此您可以并行使用尽可能多的前缀，从而实现所需的吞吐量。前缀的数量没有限制。</p><p>在这种 S3 请求速率性能提升推出后，先前任何为加速性能而随机化对象前缀的指南均被淘汰。也就是说，您现在可以在 S3 对象命名中使用逻辑或顺序命名模式，而不会产生任何性能影响。所有 AWS 区域现在均已提供此改进。有关更多信息，请访问 Amazon S3 开发人员指南。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。&lt;/p&gt;
&lt;p&gt;总得分: 76%&lt;br&gt;主题得分:&lt;br&gt;1.0. Design Resilient Architectures 89%&lt;br&gt;2.0. Define Performant Architectures 71%&lt;br&gt;3.0. Specify Secure Applications and Architectures 50%&lt;br&gt;4.0. Design Cost-Optimized Architectures 100%&lt;br&gt;5.0. Define Operationally Excellent Architectures 100%&lt;/p&gt;
&lt;p&gt;个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.site/tags/AWS/"/>
    
      <category term="ACA Practice" scheme="http://sunqi.site/tags/ACA-Practice/"/>
    
  </entry>
  
</feed>
