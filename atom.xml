<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>老孙正经胡说</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.site/"/>
  <updated>2021-02-10T05:56:51.789Z</updated>
  <id>http://sunqi.site/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用阿里云VPN服务实现HyperMotion SaaS私有云迁移</title>
    <link href="http://sunqi.site/2021/02/10/%E5%88%A9%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91VPN%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0HyperMotion-SaaS%E7%A7%81%E6%9C%89%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    <id>http://sunqi.site/2021/02/10/%E5%88%A9%E7%94%A8%E9%98%BF%E9%87%8C%E4%BA%91VPN%E6%9C%8D%E5%8A%A1%E5%AE%9E%E7%8E%B0HyperMotion-SaaS%E7%A7%81%E6%9C%89%E4%BA%91%E8%BF%81%E7%A7%BB/</id>
    <published>2021-02-10T02:35:16.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>目前云原生迁移平台HyperMotion SaaS主要应用场景在公有云上，但是在我们平时的测试场景中，由于上行带宽的限制，每次向公有云同步比较消耗时间，特别是在验证启动流程时，需要等待半天到一天的时间进行数据同步，非常不划算。在我们内部环境中，我们经常测试的一种场景是从VMWare迁移到私有化部署的OpenStack上。但是由于网络的限制不可能将OpenStack及Floating IP资源在公网上一一映射（如果是客户场景，通常是私有化部署的HyperMotion解决）。那么，是否可以将线上VPC与本地的机房网络环境利用VPN隧道打通，实现利用HyperMotion SaaS进行私有云环境的迁移呢？本文就为你分享利用阿里云VPN服务实现上述场景的需求。</p><a id="more"></a><h1 id="需求与场景分析"><a href="#需求与场景分析" class="headerlink" title="需求与场景分析"></a>需求与场景分析</h1><p>HyperMotion SaaS是部署在阿里云Kubernetes托管版集群中，即Kubernetes Master节点由阿里云负责，阿里云为我们在指定VPC内启动了两台ECS实例作为Worker节点。在我们自身需求中，需要解决两个流量问题：</p><ul><li>控制流：HyperMotion SaaS每个租户可以添加指定的目标云平台，HyperMotion SaaS后台模块通过VPC关联的NAT网关访问云平台API接口及资源，但是如果添加的是我们内部的OpenStack，则需要SaaS侧与OpenStack控制网络想通；另外HyperMotion会自动利用云平台的云主机资源安装云存储网关，所以也需要访问OpenStack Floating IP的地址（具体看云平台规划，也许是Fixed IP）。</li><li>数据流：在数据层面上，我们仍然希望数据层面通过内网传输，没有必要将数据流入公网，好再HyperMotion SaaS的设计满足了这样的需求</li></ul><p><img src="/images/pasted-133.png" alt="upload successful"></p><p>所以在这个解决方案中，重点是利用阿里云VPN网关和本地打通后（前提是公司出口路由有固定的公网IP），通过合理的设置路由规则实现我们上述的需求。</p><p>注意：文章中使用的截图并非全部都是真实截图，所以在实际配置过程中要根据实际情况进行。</p><h1 id="配置流程"><a href="#配置流程" class="headerlink" title="配置流程"></a>配置流程</h1><p>配置过程中主要涉及阿里云VPN服务和H3C路由器，基本流程如下：</p><ul><li>1、阿里云建立VPN网关，这个最低购买力度是包月</li><li>2、拿到阿里云VPN网关后，在路由器上进行相关配置</li><li>3、回到阿里云配置用户网关及IPsec连接，查看连接是否成功</li><li>4、阿里云侧路由设置</li></ul><h1 id="1、阿里云VPN网关配置"><a href="#1、阿里云VPN网关配置" class="headerlink" title="1、阿里云VPN网关配置"></a>1、阿里云VPN网关配置</h1><p>VPN需要关联到VPC和交换机上，根据带宽的不同，价格也不同，最低是按照包1个月5 Mbps。</p><p><img src="/images/pasted-147.png" alt="upload successful"></p><p>配置好后，会得到一个公网IP，这个公网IP需要在后续配置到路由器上。</p><p><img src="/images/pasted-134.png" alt="upload successful"></p><h1 id="2、H3C路由器设置"><a href="#2、H3C路由器设置" class="headerlink" title="2、H3C路由器设置"></a>2、H3C路由器设置</h1><p>目前我们机房使用的路由器属于非常入门级的企业级路由器（H3C ER3200G2），但是基本能满足我们的需求了，并且支持IPsec VPN方式。之前一直很惧怕配置IPsec VPN，相较于L2TP等简单方案，配置起来太复杂了。但是经过几次折腾，也基本摸清楚是怎么回事了，真是应了那句话：人类的恐惧来自于无知。</p><p>我并不是网络方面的专家，也对IPsec原理没什么研究，我只想记录一下我是怎么配置的。我认为IPsec在配置的时候，最重要的一点是两头配置一样，无法连接往往是由于配置信息不一致导致的。这是一张原理图，加深我们对配置过程的理解。</p><p><img src="/images/pasted-143.png" alt="upload successful"></p><p>H3C配置的基本流程为：虚接口-&gt;IKE安全提议-&gt;IKE对等体-&gt;IPsec安全提议-&gt;IPsec安全策略。</p><h2 id="2-1、虚接口配置"><a href="#2-1、虚接口配置" class="headerlink" title="2.1、虚接口配置"></a>2.1、虚接口配置</h2><p>虚接口应该是定义与外界互连的通道，配置很简单，只要指明对外服务的接口（比如：WAN1）就可以了。</p><p><img src="/images/pasted-134.png" alt="upload successful"></p><h2 id="2-2、IKE安全提议"><a href="#2-2、IKE安全提议" class="headerlink" title="2.2、IKE安全提议"></a>2.2、IKE安全提议</h2><p>IKE是因特网密钥交换的缩写(Internet Key Exchange)，从名字上可以猜出这与互联网进行交换数据时加密有关。验证算法和加密算法一定要与对端配置一致，关于DH组，每一个平台选项不一样，比如截图中叫DH2 modp1024，到了阿里云就叫做group2了，所以也必须要配置一致。</p><p><img src="/images/pasted-139.png" alt="upload successful"></p><p>阿里云侧DH组选项</p><p><img src="/images/pasted-140.png" alt="upload successful"></p><h2 id="2-3、IKE对等体"><a href="#2-3、IKE对等体" class="headerlink" title="2.3、IKE对等体"></a>2.3、IKE对等体</h2><p>和对端的VPN网关进行连接，对端IP是需要首先在对端建立VPN网关后，会得到相应的地址，填入即可。</p><p><img src="/images/pasted-141.png" alt="upload successful"></p><p>协商模式上，阿里云的配置是英文的，主模式叫做main，而野蛮模式被称为aggresive。</p><p><img src="/images/pasted-142.png" alt="upload successful"></p><p>共享密钥是自定义的，两端必须一致，DPD阿里云默认是开启的，而H3C上是关闭的，保持统一即可。</p><h2 id="2-4、IPsec安全提议"><a href="#2-4、IPsec安全提议" class="headerlink" title="2.4、IPsec安全提议"></a>2.4、IPsec安全提议</h2><p>按照我粗浅的认知，IKE主要负责两端连接，同时简化了IPsec交互，而真正的数据交互还是要在IPsec上进行控制。所以要对IPsec也要进行相应的安全配置。安全协议类型，我们选择了默认的ESP，阿里云侧默认也应该采用的是此协议。在配置对端时，仍然是保持一致即可。</p><p><img src="/images/pasted-144.png" alt="upload successful"></p><h2 id="2-5、IPsec安全策略"><a href="#2-5、IPsec安全策略" class="headerlink" title="2.5、IPsec安全策略"></a>2.5、IPsec安全策略</h2><p>这一步最关键的是本地子网IP和对端子网IP及掩码的设置，双方是相反的，如果本地是192.168.0.0/24，源端是172.16.0.0/24。则在阿里云侧的配置就是本地是172.16.0.0/24，远端是192.168.0.0/24。</p><p><img src="/images/pasted-145.png" alt="upload successful"></p><p>还有一个就是PFS的设置，和IKE的DH组是一样的，在阿里云侧也被成为IPsec的DH组。也必须设置一致。</p><p><img src="/images/pasted-146.png" alt="upload successful"></p><h1 id="3、阿里云IPsec连接配置"><a href="#3、阿里云IPsec连接配置" class="headerlink" title="3、阿里云IPsec连接配置"></a>3、阿里云IPsec连接配置</h1><h2 id="3-1、用户网关设置"><a href="#3-1、用户网关设置" class="headerlink" title="3.1、用户网关设置"></a>3.1、用户网关设置</h2><p>用户网关设置比较简单，只要在阿里云测配置你路由的公网IP即可。</p><p><img src="/images/pasted-149.png" alt="upload successful"></p><h2 id="3-2、IPsec连接"><a href="#3-2、IPsec连接" class="headerlink" title="3.2、IPsec连接"></a>3.2、IPsec连接</h2><p>这是最关键的一步，经常在这一步配置失败，提示在第一阶段或者第二阶段失败，目前在我遇到的情况中，基本都是上述配置不一致导致的。配置过程基本分为三个阶段，基本配置、高级配置中的IKE配置和IPsec配置。</p><h3 id="3-2-1-基本配置"><a href="#3-2-1-基本配置" class="headerlink" title="3.2.1 基本配置"></a>3.2.1 基本配置</h3><p>注意图中标出的本端网络、对端网络和预共享密钥的配置，一定要填对。</p><p><img src="/images/pasted-150.png" alt="upload successful"></p><h3 id="3-2-2-IKE配置"><a href="#3-2-2-IKE配置" class="headerlink" title="3.2.2 IKE配置"></a>3.2.2 IKE配置</h3><p>点开下方的高级设置，能够看到IKE和IPsec设置。</p><p>配置只要按照我们在H3C的配置选择相应的内容即可，LocalId和RemoteId都是自动根据VPN填写的，并不需要输入。</p><p><img src="/images/pasted-151.png" alt="upload successful"></p><h3 id="3-2-3-IPsec配置"><a href="#3-2-3-IPsec配置" class="headerlink" title="3.2.3 IPsec配置"></a>3.2.3 IPsec配置</h3><p>图中标注的选项一定要保持一致，提交配置后，等待连接。</p><p><img src="/images/pasted-152.png" alt="upload successful"></p><h2 id="3-3、查看连接状态"><a href="#3-3、查看连接状态" class="headerlink" title="3.3、查看连接状态"></a>3.3、查看连接状态</h2><p>如果连接状态为第二阶段协商成功，就证明VPN已经建立成功，否则请检查配置，多半是由于配置不一致导致的。</p><p><img src="/images/pasted-153.png" alt="upload successful"></p><h1 id="4、设置路由"><a href="#4、设置路由" class="headerlink" title="4、设置路由"></a>4、设置路由</h1><p>我们此此设置路由的目的是为了阿里云侧能够访问我们内网，所以接下来需要在阿里云VPC内设置路由表，当访问我们的内网时，需要使用VPN网关。进入VPC服务的路由表配置中，找到VPC。将目标IP段吓一跳设置为VPN网关。因为阿里云的ACL还处于内测阶段，所以暂时无须考虑ACL的设定。</p><p><img src="/images/pasted-154.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>网络对于未来混合云的场景有至关重要的作用，本文重点描述的是以VPN方式来打通云上和云下环境，但是VPN最大的带宽规格只有200 Mbps，如果真实的需求更大，则需要考虑云联网，通过运营商底层基础设施，实现不同云之间的互联互通。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前云原生迁移平台HyperMotion SaaS主要应用场景在公有云上，但是在我们平时的测试场景中，由于上行带宽的限制，每次向公有云同步比较消耗时间，特别是在验证启动流程时，需要等待半天到一天的时间进行数据同步，非常不划算。在我们内部环境中，我们经常测试的一种场景是从VMWare迁移到私有化部署的OpenStack上。但是由于网络的限制不可能将OpenStack及Floating IP资源在公网上一一映射（如果是客户场景，通常是私有化部署的HyperMotion解决）。那么，是否可以将线上VPC与本地的机房网络环境利用VPN隧道打通，实现利用HyperMotion SaaS进行私有云环境的迁移呢？本文就为你分享利用阿里云VPN服务实现上述场景的需求。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>基于Serverless架构进行应用开发</title>
    <link href="http://sunqi.site/2021/02/06/%E5%9F%BA%E4%BA%8EServerless-Framework%E8%BF%9B%E8%A1%8C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/"/>
    <id>http://sunqi.site/2021/02/06/%E5%9F%BA%E4%BA%8EServerless-Framework%E8%BF%9B%E8%A1%8C%E5%BA%94%E7%94%A8%E5%BC%80%E5%8F%91/</id>
    <published>2021-02-06T01:01:03.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="什么是Serverless"><a href="#什么是Serverless" class="headerlink" title="什么是Serverless"></a>什么是Serverless</h1><p>从过去20年IT基础架构层的发展过程来看，计算、存储和网络三种基础资源得到了不断的发展和抽象。从物理机到虚拟化，从虚拟化到云计算，再从云计算到云原生，孕育出诸多新生概念，无论如何变化，一定是向着有利于业务创新方向发展，开发人员越来越不需要关注底层的基础架构。</p><p>云原生本身是一个非常广义的概念，主要包含：微服务架构，应用容器化、Serverless化以及敏捷的软件交付流程。所以很多人在谈论云原生时与Kubernetes划等号，是非常片面的。</p><a id="more"></a><p>今天我们重点来说说Serverless，这种技术的本质并不是真的“无服务器”，真正的目的是要帮助应用开发者摆脱应用程序后端所需的服务器资源的管理和运维工作。我们在设计一款产品或应用时，往往要从不同的唯独考虑产品。<br>从开发与测试角度更多的考虑的是开发语言、框架的选择、数据库、如何进行压力测试等；而从实施和运维角度考虑则要考虑环境怎么构建、高可靠如何实现，如何升级、如何扩容等诸多基础架构的问题。从Serverless架构看，更希望你专注于你的开发和测试，而将实施和运维彻底交给云来解决，前提是你需要遵循Serverless架构。</p><p>在构建Serverless架构中，通常包含两种常用的服务类型：</p><p>1、后端即服务（Backend-as-a-Servce，BaaS），例如：数据库、消息队列、身份验证由云商直接以服务方式提供的服务<br>2、函数计算（Function-as-a-Service），主要将业务逻辑代码运行在其中，函数计算通常运行在容器环境内，按照CPU和内存的运行时间来计费。函数计算触发往往与时间相关，例如：定时器、HTTP请求。目前各家云商云商都提供了函数计算服务，而这方面的佼佼者无疑是2014年11月就推出的AWS Lambda服务，在很多AWS的最佳实践中都能看到巧妙利用函数计算服务来解决业务架构问题。</p><p>我们在开发一个业务系统时，通常采用传统的架构思想来规划系统建设。但是随着云原生技术的发展，除了逐步打破传统的运维与开发之间的关系，我们的开发架构也随之发生了改变。未来的应用开发架构，让开发、测试与运维的边界越来越模糊，应用开发的迭代速度进一步提升。</p><p>当然Serverless并不是万能的，很多劣势无法满足所有的场景需求，但是随着新技术的不断迭代，一定会有新的技术出现来填补这些空白。</p><h1 id="从业务视角看Serverless"><a href="#从业务视角看Serverless" class="headerlink" title="从业务视角看Serverless"></a>从业务视角看Serverless</h1><p>记得是在2020年12月的微信小程序峰会上一个分享中看到这一组数据，给我的震撼很大。这是一家专门依托于微信小程序从事线上娱乐化社交电商社区。我们从图中数据可以看到1-10月份销售数据为23,909,022.69元，销售在14万笔。</p><p><img src="/images/pasted-136.png" alt="upload successful"></p><p>那么如果是一个传统电商平台，承载这样的销量需要付出多少资源的代价呢？我们来看看这家公司的运营数据。是的，你没看错仅仅是3000元，而研发人力投入仅仅不到10个人。</p><p><img src="/images/pasted-137.png" alt="upload successful"></p><p>2020年双十一销量数据为2,194,203元，而基础架构层为此付出的额外费用仅为10元钱。</p><p><img src="/images/pasted-138.png" alt="upload successful"></p><p>从交易数据看，虽然从并发性上远不及天猫这样每秒几十万的交易量。但是如果从性价比(销量/基础架构投入)看，这样的数据绝对是可以各位同行参考的。为什么可以得到这样惊人的数据，这离不开以Serverless为理念的云开发。</p><h1 id="从开发者视角看Serverless"><a href="#从开发者视角看Serverless" class="headerlink" title="从开发者视角看Serverless"></a>从开发者视角看Serverless</h1><p>这是一张云开发自身发展的版图，这张图还是与厂商利益之间进行了深度绑定，不过我们重点从技术角度去分析一下。Serverless架构的发展主要集中在平台能力和基础能力两个方面，当然扩展能力也很重要，我们也可以将这些归属为平台能力层，并且可以是多云。这些插件能够给我们应用提供更多的想象空间，例如最近大火的Clubhouse，就是利用了中国声网提供的服务。</p><p><img src="/images/pasted-135.png" alt="upload successful"></p><p>去年的时候我曾经专门录制过阿里云的函数计算课程( <a href="https://edu.51cto.com/course/22144.html" target="_blank" rel="noopener">https://edu.51cto.com/course/22144.html</a> )，在这个课程里，我更多的是将函数计算作为串联云原生服务的纽带进行讲解。但是随着Servless开发框架越来越成熟，函数计算在构建应用的地位发生了变化，上述提到的云开发就是一个典型。通过微信这个入口，快速支撑了业务发展的需要，在2020年初紧急的开发的健康码就是利用了这样的特性实现。</p><p>我们设计一款全新的应用架构时，要从云开发能够提供的整体能力角度出发进行思考。简单说，Serverless框架的核心在于围绕着函数计算来设计业务逻辑，通过使用各个云原生服务的能力，满足业务上的需求。Serverless架构的设计更多的是在改变原有的开发框架和开发模式，将原有以架构为核心的代码组织形式打散在各个函数中。将原有架构层需要考虑的并发、高可用等完全交由底层来支撑，开发可以更专注于业务本身。</p><p>那么构建以函数计算为核心的Serverless框架应用时，应当注意哪些问题呢？</p><p>1、函数计算需要事件驱动，例如一个HTTP请求，或者一个上传对象存储的行为都会产生事件，而这些事件产生都是云原生的，也是最及时的。以最常见的WEB类应用为例，基本就是前端及后端对数据库各种CURD的组合实现。前端的静态文件可以考虑使用对象存储，再使用CDN进行加速，通过API网关来驱动后端的函数计算，如果有需要持久化存储的数据可以选择NAS或对象存储服务。通过这套架构可以很轻松的实现一个高并发的业务系统。<br>2、虽然Serverless架构看上去很美好，但是仍然会面临很多挑战。性能问题就是其中之一，因为函数计算是触发式启动，在初始阶段和并发激增的情况下响应请求时会很慢。所以在实际开发过程中，除了要合理优化自己的初始化代码逻辑外，还要结合性能监控指标，合理利用预留实例的功能，达到性能与价格的最优。</p><p><img src="/images/pasted-132.png" alt="upload successful"></p><p>3、那么用户现有的业务是否有必要改造为Serverless架构呢？我觉得应该取决于需求，因为Serverless的开发模式决定了这个改造一定会产生时间和人力投入的成本。首先研发人员要学习Serverless的理念，熟悉开发模式，梳理出当前系统改造的方式，甚至要重构部分代码。这个过程往往要高于容器改造的成本，但是小于微服务改造的成本。</p><p>4、研发管理问题也是在应用Serverless应用中需要考虑的问题，从代码结构如何组织，到如何进行上线前的测试，再到如何优化原有持久化集成的流程，都对研发管理提出了挑战。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>Serverless架构为应用开发带来了新的活力，让研发人员能够更加的专注于业务逻辑的开发工作。同时，让企业的总体拥有成本（TCO）降低，但是提供服务的能力和灵活度大幅度提升。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;什么是Serverless&quot;&gt;&lt;a href=&quot;#什么是Serverless&quot; class=&quot;headerlink&quot; title=&quot;什么是Serverless&quot;&gt;&lt;/a&gt;什么是Serverless&lt;/h1&gt;&lt;p&gt;从过去20年IT基础架构层的发展过程来看，计算、存储和网络三种基础资源得到了不断的发展和抽象。从物理机到虚拟化，从虚拟化到云计算，再从云计算到云原生，孕育出诸多新生概念，无论如何变化，一定是向着有利于业务创新方向发展，开发人员越来越不需要关注底层的基础架构。&lt;/p&gt;
&lt;p&gt;云原生本身是一个非常广义的概念，主要包含：微服务架构，应用容器化、Serverless化以及敏捷的软件交付流程。所以很多人在谈论云原生时与Kubernetes划等号，是非常片面的。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>某股权交易中心业务迁移阿里云最佳实践</title>
    <link href="http://sunqi.site/2021/02/05/%E6%9F%90%E8%82%A1%E6%9D%83%E4%BA%A4%E6%98%93%E4%B8%AD%E5%BF%83%E4%B8%9A%E5%8A%A1%E8%BF%81%E7%A7%BB%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/"/>
    <id>http://sunqi.site/2021/02/05/%E6%9F%90%E8%82%A1%E6%9D%83%E4%BA%A4%E6%98%93%E4%B8%AD%E5%BF%83%E4%B8%9A%E5%8A%A1%E8%BF%81%E7%A7%BB%E9%98%BF%E9%87%8C%E4%BA%91%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/</id>
    <published>2021-02-05T09:35:56.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="项目概述"><a href="#项目概述" class="headerlink" title="项目概述"></a>项目概述</h1><p>某股权交易中心是在深圳地区建设的市场化运作的区域性交易市场。由于业务发展需要，用户需要将主要业务全面上云。最终用户选择使用万博智云HyperMotion将业务系统迁移至阿里公有云平台，在保障用户业务连续性的前提下，实现业务系统全面上云。</p><h1 id="客户面临的挑战"><a href="#客户面临的挑战" class="headerlink" title="客户面临的挑战"></a>客户面临的挑战</h1><p>用户原有业务系统运行在运营商机房内，该机房将整体清退，主要应用系统已切换至备用机房。但备用机房规模较小、总体运维成本较高。由于公司发展需要，运维团队规模由原来的十人缩减为一人。为满足业务快速发展以及系统业务连续性要求，提升整体运维效率，计划将主要生产系统由本地机房迁移至阿里云。</p><p>用户采用传统的VMware与存储阵列的经典组合，业务系统由100多台虚拟机、10+TB数据量构成，用户需要将这100多台VMware虚拟机平滑的迁移至阿里云平台。</p><a id="more"></a><h1 id="为什么选择阿里云"><a href="#为什么选择阿里云" class="headerlink" title="为什么选择阿里云"></a>为什么选择阿里云</h1><p>用户希望将业务系统部署在领先的、高水准的云平台上，并且一定要求云平台为国产化自主可控的公有云平台。实现“一步到位”的信息化建设高起点，同时作为金融交易平台，用户需要得到全方位的技术保障和极为可靠的安全性和稳定性。</p><p>本次在竞标主要竞争是在阿里云和另外一家云商之间展开，由于是金融级别客户，用户重点从【安全产品】和【业务迁移】两个维度对两朵公有云产品及服务能力进行了深度评估。</p><p>经过深度评估，选定阿里云安骑士的高配版本和万博智云HyperMotion云迁移为解决方案。</p><h1 id="为什么选择万博智云"><a href="#为什么选择万博智云" class="headerlink" title="为什么选择万博智云"></a>为什么选择万博智云</h1><p>万博智云是国内最早且目前最优的云原生迁移工具研发的公司，通过与阿里云API接口及云原生资源高度自动化对接，将迁移缩减为简单的三步，满足用户高度自动化、智能化迁移需求。</p><p>同时通过迁移演练能力，满足了用户在切换至云端前从业务维度对系统进行多次生产演练验证的需求，实现了【灾备演练式的渐进迁移体验】。</p><p>通过HyperMotion云迁移工具，实现了：</p><ul><li>业务连续性迁移</li><li>批量/高效/全程可视化迁移</li><li>迁移后IP地址不变</li><li>上云前多次业务级别演练</li></ul><p>充分保障了用户业务上云后的连续性和可靠性。</p><h1 id="解决方案"><a href="#解决方案" class="headerlink" title="解决方案"></a>解决方案</h1><h2 id="网络解决方案"><a href="#网络解决方案" class="headerlink" title="网络解决方案"></a>网络解决方案</h2><p>用户业务上云后，期望保持与原有数据中心内业务系统IP地址保持一致，这就要求在公有云侧VPC需要使用与原有数据中心相同的IP地址规划。同时在上云后，由于云上的地址通常采用DHCP方式进行分配，这就要求在主机启动前就要将端口及IP进行分配，确保在云端启动的主机获得与原有业务系统完全一样的IP地址。</p><p>在该项目中，我们采用【阿里云云联网服务】将用户原有机房网络与云上VPC进行打通，为了避免地址冲突，用户进行数据同步的网络采用单独的地址段，在启动时，通过【HyperMotion指定IP地址】方式进行启动。</p><h2 id="业务连续性"><a href="#业务连续性" class="headerlink" title="业务连续性"></a>业务连续性</h2><p>根据迁移方法论中提到“6R”理论，【重新托管（Re-Host）】方式是上云的最短最高效路径，同时也是对用户原有业务影响最小的方案。</p><p>HyperMotion采用了块级别同步复制技术来实现“热迁移”：</p><ul><li>★源端无代理模式→在源端VMware环境下无入侵操作（不安装agent），对用户业务侧影响几乎为0，实现用户源端业务零停机或者少停机下实现业务系统上云的效果。</li><li>块级别数据的整体复制→用户的操作系统、应用、数据一起被同步到目标侧，无论是WEB应用、数据库或者中间件，都可以通过这种方式完整的迁移至云端，无需针对单独文件或者数据进行操作和配置。</li><li>★异构平台适配技术→通过异构平台智能适配转换驱动，实现跨平台无缝迁移</li><li>★云原生能力→调度云侧API以及逻辑流程，无须繁琐的人为操作，实现高度自动化的用户体验。</li><li>★灾备演练式渐进迁移</li><li>业务系统上云验证是迁移上云前最后一道防线，在云侧完整的对业务系统进行验证是最准确和有效的手段。在传统容灾场景中，通常以定期的灾备演练方式来保证灾备的可用性。</li></ul><p>在HyperMotion中创新的提供了【★迁移演练的能力】，方便用户在上云前通过灾备演练式的体验进行业务系统切换前的校验。HyperMotion通过对阿里云API进行深度整合，以全自动化的方式解决了主机启动、驱动修复、网络修复等多种上云后复杂的人为操作。同时HyperMotion还可以通过指定IP地址启动方式，保证业务系统与源端地址一致。迁移验证系统启动后，并不影响源端业务运行，同时增量数据可以继续同步至云侧。</p><p>本案例中充分发挥了该功能的优势，最大程度满足客户严苛的业务迁移需求。</p><h2 id="成本"><a href="#成本" class="headerlink" title="成本"></a>成本</h2><p>成本因素是迁移到公有云必须要考虑的问题之一，从开始的业务系统同步到迁移演练到最终的迁移上云，均涉及到资源的成本支出，如果无法做到合理的使用云原生资源就会造成大量的浪费。</p><p>HyperMotion创新的采用了云同步网关的概念，实现了多对一的方式进行同步。数据同步阶段，只利用较少的计算资源，而将数据存储于云硬盘中，降低成本消耗。真正的业务主机只在验证或最终切换阶段启动，实现成本最优的效果。</p><p>客户实际在迁移过程中耗时大概在两个月时间，期间将一百多台多台主机拉起进行了三次验证，每次在业务部门确认后，清理掉资源。最终在第四次拉起后，将业务负载全部切换至云侧。目前用户业务系统已经稳定超过半年以上时间。</p><p>以下是我们就该项目中实际消耗的资源&amp;成本 与 备选方案测算进行的比对：</p><p>用户【原有系统资源】统计：</p><p><img src="/images/pasted-130.png" alt="upload successful"></p><p>迁移【中间资源成本】对比：</p><p>项目迁移周期耗时两个月，通过对阿里云账单进行分析，统计出在迁移中间资源、成本、时间如下：</p><p><img src="/images/pasted-131.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;项目概述&quot;&gt;&lt;a href=&quot;#项目概述&quot; class=&quot;headerlink&quot; title=&quot;项目概述&quot;&gt;&lt;/a&gt;项目概述&lt;/h1&gt;&lt;p&gt;某股权交易中心是在深圳地区建设的市场化运作的区域性交易市场。由于业务发展需要，用户需要将主要业务全面上云。最终用户选择使用万博智云HyperMotion将业务系统迁移至阿里公有云平台，在保障用户业务连续性的前提下，实现业务系统全面上云。&lt;/p&gt;
&lt;h1 id=&quot;客户面临的挑战&quot;&gt;&lt;a href=&quot;#客户面临的挑战&quot; class=&quot;headerlink&quot; title=&quot;客户面临的挑战&quot;&gt;&lt;/a&gt;客户面临的挑战&lt;/h1&gt;&lt;p&gt;用户原有业务系统运行在运营商机房内，该机房将整体清退，主要应用系统已切换至备用机房。但备用机房规模较小、总体运维成本较高。由于公司发展需要，运维团队规模由原来的十人缩减为一人。为满足业务快速发展以及系统业务连续性要求，提升整体运维效率，计划将主要生产系统由本地机房迁移至阿里云。&lt;/p&gt;
&lt;p&gt;用户采用传统的VMware与存储阵列的经典组合，业务系统由100多台虚拟机、10+TB数据量构成，用户需要将这100多台VMware虚拟机平滑的迁移至阿里云平台。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用腾讯云开发免费搭建个人博客</title>
    <link href="http://sunqi.site/2021/02/02/%E5%88%A9%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E5%BC%80%E5%8F%91%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <id>http://sunqi.site/2021/02/02/%E5%88%A9%E7%94%A8%E8%85%BE%E8%AE%AF%E4%BA%91%E5%BC%80%E5%8F%91%E5%85%8D%E8%B4%B9%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</id>
    <published>2021-02-02T23:03:10.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>之前一直使用Github Pages搭建个人博客，随着Github访问越来越困难，个人博客国内访问速度越来越慢。之前也试过用cloudflare进行加速，但是收效甚微，所以才考虑将项目迁回到国内。一个偶然的机会，腾讯云开发进入我的视野，起因是他们的9.9元计划，不过后来由于我配置错误，这部分资源无法使用。但是经过研究，原来腾讯云开发提供了最基础的免费资源，恰好可以让我们搭建个人的Blog，经过将近一年多的使用过程中，非常好用，所以就记录下来，供大家参考。</p><a id="more"></a><h1 id="什么是腾讯云开发"><a href="#什么是腾讯云开发" class="headerlink" title="什么是腾讯云开发"></a>什么是腾讯云开发</h1><p>腾讯云开发是我一种非常推崇的开发理念，简单来说就是将Serverless进行了进一步封装，为开发者提供了更便捷的开发体验。目前云开发将轻量级业务中常见的数据库、存储（包括文件存储、对象存储）、云函数（计算资源）、基础运维（告警、监控、日志）进行了整合，同时云开发与微信小程序之间有个非常紧密的整合，能够快速帮助微信小程序构建服务端程序，基本可以承载很多基于微信场景的业务开发，比如电商等应用的开发，并且基于这样的基础架构，支撑千万级并发的需要。</p><p><img src="/images/pasted-126.png" alt="upload successful"></p><p>目前腾讯云开发，主要在广州和上海区域提供服务。</p><p><img src="/images/pasted-125.png" alt="upload successful"></p><p>腾讯云开发目前提供用户可以创建一个免费的环境，其中包含了存储、数据库等免费资源，但是相对于博客场景，主要还是静态资源的托管，每个月有1GB的存储空间，和5GB/月的流量，如果资源不够还可以购买额外的资源包。</p><p>更多的免费额度请参考<a href="https://cloud.tencent.com/document/product/876/47816" target="_blank" rel="noopener">https://cloud.tencent.com/document/product/876/47816</a></p><p><img src="/images/pasted-127.png" alt="upload successful"></p><h1 id="构建过程"><a href="#构建过程" class="headerlink" title="构建过程"></a>构建过程</h1><p>先说一下整体的构建思路：</p><ul><li>我们的博客源代码仍然托管在github上，这样不需要破坏现有逻辑</li><li>如果你有自己的域名，最好申请备案，因为云开发在绑定域名的时候必须要求已备案的域名，但是如果你就不想备案，也有一个Work Around方法，就是通过cloudflare进行跳转的方式实现了，后面会简单介绍</li><li>通过Travis CI自动构建，并上传至云开发中，这样就实现我们在提交代码后，自动进行博客发布的效果了</li></ul><h1 id="云开发购买"><a href="#云开发购买" class="headerlink" title="云开发购买"></a>云开发购买</h1><p>云开发购买的过程，这里不再赘述了，只需要在新建时选择免费资源即可。因为我已经购买过资源了，所以提示我再次购买。</p><p><img src="/images/pasted-128.png" alt="upload successful"></p><p>成功构建后，你会得到这样的环境id，这个id作为你后续使用cli命令行更新环境的参数使用。</p><p><img src="/images/pasted-129.png" alt="upload successful"></p><h1 id="Travis-CI配置文件"><a href="#Travis-CI配置文件" class="headerlink" title="Travis CI配置文件"></a>Travis CI配置文件</h1><p>相信很多人都使用Travis CI构建自己的Github Pages，确实非常方便，虽然Github也提供了自己的CI工具，但是我依然保留着使用Travis CI的习惯。我们无须调整之前的Github Pages的配置或者策略，只需要在你的master分支下，增加或者修改你的.travis.yml即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">sudo: false</span><br><span class="line">language: node_js</span><br><span class="line">node_js:</span><br><span class="line">  - 10 # use nodejs v10 LTS</span><br><span class="line">cache: npm</span><br><span class="line">branches:</span><br><span class="line">  only:</span><br><span class="line">    - master # build master branch only</span><br><span class="line">before_install:</span><br><span class="line">  - npm i -g @cloudbase&#x2F;cli</span><br><span class="line">  - git clone --depth&#x3D;1 https:&#x2F;&#x2F;github.com&#x2F;JoeyBling&#x2F;hexo-theme-yilia-plus.git themes&#x2F;yilia-plus</span><br><span class="line">after_success:</span><br><span class="line">  - cloudbase login --apiKeyId $TECENT_AK --apiKey $TECENT_KS</span><br><span class="line">  - cd public &amp;&amp; echo &#39;y&#39; | tcb hosting deploy -e your-env-id</span><br><span class="line">script:</span><br><span class="line">  - hexo generate # generate static files</span><br><span class="line">deploy:</span><br><span class="line">  repo: xiaoquqi&#x2F;xiaoquqi.github.io</span><br><span class="line">  target_branch: master</span><br><span class="line">  provider: pages</span><br><span class="line">  skip-cleanup: true</span><br><span class="line">  github-token: $GH_TOKEN</span><br><span class="line">  keep-history: true</span><br><span class="line">  on:</span><br><span class="line">    branch: master</span><br><span class="line">  local-dir: public</span><br></pre></td></tr></table></figure><p>在新的配置文件中，我保留了之前deploy到Github Pages的逻辑，主要增加的逻辑是在before_install开始前，安装cloudbase的cli。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">before_install:</span><br><span class="line">  - npm i -g @cloudbase&#x2F;cli</span><br></pre></td></tr></table></figure><p>在hexo generate成功后，增加部署的命令，这里需要在Travis CI中配置腾讯云的鉴权环境变量。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">after_success:</span><br><span class="line">  - cloudbase login --apiKeyId $TECENT_AK --apiKey $TECENT_KS</span><br><span class="line">  - cd public &amp;&amp; echo &#39;y&#39; | tcb hosting deploy -e your-env-id</span><br></pre></td></tr></table></figure><p>目前cloudbase cli(简写：tcb)，有一个问题，如果超过1000个文件上传会有个提示，导致Travis CI认为没有返回任务失败，但是实际上已经提交上去了，这里已经给腾讯团队提交了一个需求，在cloudbase cli中增加一个force-yes的选项。</p><p>这样我们在提交代码后，就可以实现在腾讯云开发中自动发布我们博客的效果了。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;之前一直使用Github Pages搭建个人博客，随着Github访问越来越困难，个人博客国内访问速度越来越慢。之前也试过用cloudflare进行加速，但是收效甚微，所以才考虑将项目迁回到国内。一个偶然的机会，腾讯云开发进入我的视野，起因是他们的9.9元计划，不过后来由于我配置错误，这部分资源无法使用。但是经过研究，原来腾讯云开发提供了最基础的免费资源，恰好可以让我们搭建个人的Blog，经过将近一年多的使用过程中，非常好用，所以就记录下来，供大家参考。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CentOS7 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/CentOS7-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T09:15:02.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：</p><p><img src="/images/pasted-124.png" alt="upload successful"></p><p>原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。</p><p>CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。</p><a id="more"></a><h1 id="安装zsh"><a href="#安装zsh" class="headerlink" title="安装zsh"></a>安装zsh</h1><p>虽然通过yum方式可以安装zsh，但是无法满足powerlevel10k的要求，所以先使用zsh源码进行编译后安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">curl -o zsh.tar.xz https:&#x2F;&#x2F;jaist.dl.sourceforge.net&#x2F;project&#x2F;zsh&#x2F;zsh&#x2F;5.8&#x2F;zsh-5.8.tar.xz</span><br><span class="line">tar -xvf zsh-5.8.tar.xz</span><br><span class="line"></span><br><span class="line">cd zsh</span><br><span class="line">make</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>zsh会安装在用户目录中/usr/local/bin/zsh中，将zsh设置为默认的系统shell，配置成功后，需要关闭Terminal重新登陆。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">chsh -s &#x2F;usr&#x2F;local&#x2F;bin&#x2F;zsh</span><br></pre></td></tr></table></figure><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>接下来的流程与MacOS上安装类似，由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大部分时间里，我还是习惯于ssh到远程的CentOS7服务器上工作，因为Mac配置了漂亮zsh的缘故，所以也想把我的CentOS7切换到zsh模式。这是最终配置好的效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-124.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;p&gt;原理部分不再赘述，有兴趣可以参照MacOS的zsh配置篇。&lt;/p&gt;
&lt;p&gt;CentOS7配置zsh与Mac上还是有一定区别的，因为版本要求，zsh需要自己安装编译，字体也需要自己安装，接下来是详细的步骤。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Mac iTerm2 zshrc快速配置</title>
    <link href="http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/"/>
    <id>http://sunqi.site/2021/02/02/Mac-zshrc%E5%BF%AB%E9%80%9F%E9%85%8D%E7%BD%AE/</id>
    <published>2021-02-02T03:26:14.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。</p><p>我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：</p><p><img src="/images/pasted-123.png" alt="upload successful"></p><a id="more"></a><h1 id="原理解析"><a href="#原理解析" class="headerlink" title="原理解析"></a>原理解析</h1><p>我们开始配置前，还是有必要讲一下这几个项目的关系，以便了解其工作原理。</p><ul><li>iTerm2不用说了，MacOS上必备的Terminal工具，替代原有系统自带的工具。</li><li>ohmyzsh(<a href="https://github.com/ohmyzsh/ohmyzsh/" target="_blank" rel="noopener">https://github.com/ohmyzsh/ohmyzsh/</a>) 是一套基于zsh深度定制的插件及主题管理的框架，方便定制适合你的zsh环境。</li><li>Nerd Fonts(<a href="https://www.nerdfonts.com/" target="_blank" rel="noopener">https://www.nerdfonts.com/</a>) 我们在截图中看到的那些可爱的小图标就是来自这个项目，让我们原本枯燥的Terminal增添了几分乐趣。</li><li>powerlevel10k(<a href="https://github.com/romkatv/powerlevel10k" target="_blank" rel="noopener">https://github.com/romkatv/powerlevel10k</a>) 是一套zsh皮肤，也是目前我个人比较喜欢的一套皮肤，同时提供了较强的配置能力，包括字体下载，iTerm2的配置都自动完成了，所以也是目前使用最顺手的一套皮肤。</li></ul><h2 id="安装流程"><a href="#安装流程" class="headerlink" title="安装流程"></a>安装流程</h2><p>由于以上各个项目帮我们做了大量的优化，所以让zsh的安装过程变得简单了很多，大体的流程为：</p><ul><li>安装oh-my-zsh，其实就是clone回来</li><li>安装powerlevel10k，其实也是clone回来</li><li>powerlevel10k的基本配置，根据我们喜欢进行定制</li><li>最后是zsh的配置，也就是修改.zshrc文件</li></ul><h1 id="oh-my-zsh安装"><a href="#oh-my-zsh安装" class="headerlink" title="oh-my-zsh安装"></a>oh-my-zsh安装</h1><p>官方的方法是通过curl或wget，执行github上的install.sh文件，但是由于raw.githubusercontent.com已经属于常年被墙的状态，所以并不推荐这种方式，这里采用的方式是将oh-my-zsh下载回来后，再执行install.sh。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">export WORKSPACE&#x3D;$HOME&#x2F;workspace&#x2F;zsh</span><br><span class="line">mkdir -p $WORKSPACE</span><br><span class="line"></span><br><span class="line">cd $WORKSPACE</span><br><span class="line">git clone https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br></pre></td></tr></table></figure><p>安装脚本在ohmyzsh/tools/install.sh中，这里我们通过环境变量设置本地源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">export REMOTE&#x3D;https:&#x2F;&#x2F;e.coding.net&#x2F;xiaoquqi&#x2F;github&#x2F;ohmyzsh.git</span><br><span class="line"></span><br><span class="line">$WORKSPACE&#x2F;ohmyzsh&#x2F;tools&#x2F;install.sh</span><br></pre></td></tr></table></figure><h1 id="powerlevel10k安装"><a href="#powerlevel10k安装" class="headerlink" title="powerlevel10k安装"></a>powerlevel10k安装</h1><p>powerlevel10k已经被gitee缓存了，所以我就没再做单独的缓存源，直接利用官方提供的命令获取。powerlevel10k会被clone到ohmyzsh的custom路径中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git clone --depth&#x3D;1 https:&#x2F;&#x2F;gitee.com&#x2F;romkatv&#x2F;powerlevel10k.git $&#123;ZSH_CUSTOM:-$HOME&#x2F;.oh-my-zsh&#x2F;custom&#125;&#x2F;themes&#x2F;powerlevel10k</span><br></pre></td></tr></table></figure><p>替换默认的zsh主题</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;&#39; &quot;s&#x2F;^ZSH_THEME&#x3D;.*&#x2F;ZSH_THEME&#x3D;\&quot;powerlevel10k\&#x2F;powerlevel10k\&quot;&#x2F;g&quot; $HOME&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>在正式启用主题前，还需要对powerlevel下载字体的文件进行优化。由于是从github下载字体，所以powerlevel10k配置一定会失败，必须要进行替换后，才能安装正常。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#39;&#39; &quot;s#^local -r font_base_url&#x3D;.*#local -r font_base_url&#x3D;&#39;https:&#x2F;&#x2F;xiaoquqi.coding.net&#x2F;p&#x2F;github&#x2F;d&#x2F;powerlevel10k-media&#x2F;git&#x2F;raw&#x2F;master&#39;#g&quot; $HOME&#x2F;.oh-my-zsh&#x2F;custom&#x2F;themes&#x2F;powerlevel10k&#x2F;internal&#x2F;wizard.zsh</span><br></pre></td></tr></table></figure><p>source zshrc会自动触发配置，按照向导和喜欢的样式来就好，这里就不再赘述了</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source ~&#x2F;.zshrc</span><br></pre></td></tr></table></figure><p>如果想重新配置，也可以使用命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">p10k configure</span><br></pre></td></tr></table></figure><h1 id="加载插件"><a href="#加载插件" class="headerlink" title="加载插件"></a>加载插件</h1><p>通过修改.zshrc中的plugins变量可以实现插件加载的效果，比如使用virtualenv插件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plugins&#x3D;(git virtualenv)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;zsh基本上已经成为Mac上的标配了，界面美观还有点缀的小图标，非常漂亮。但是网上配置zsh文章很多，配置方法也是五花八门，并且由于github被墙的原因，经过由于网络问题安装失败。经过反复测试，在国内的代码托管网站进行了Github部分关键项目定时缓存后，提高配置效率。这里写一篇自用的配置方法，留给有需要的人。&lt;/p&gt;
&lt;p&gt;我的环境：iTerm2 + oh-my-zsh + powerlevel10k，这是我的配置效果：&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-123.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>如何在微信开发者工具中使用vim编辑模式</title>
    <link href="http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/"/>
    <id>http://sunqi.site/2021/01/26/%E5%A6%82%E4%BD%95%E5%9C%A8%E5%BE%AE%E4%BF%A1%E5%BC%80%E5%8F%91%E8%80%85%E5%B7%A5%E5%85%B7%E4%B8%AD%E4%BD%BF%E7%94%A8vim%E7%BC%96%E8%BE%91%E6%A8%A1%E5%BC%8F/</id>
    <published>2021-01-26T23:52:10.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。</p><p>作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。</p><a id="more"></a><h1 id="在VS-Code安装vim插件"><a href="#在VS-Code安装vim插件" class="headerlink" title="在VS Code安装vim插件"></a>在VS Code安装vim插件</h1><p>首先在VS Code中安装vim模拟器，如图所示，我安装的是1.18.5版本。我使用的是mac系统，安装完成后，插件会存放在用户HOME目录下的$HOME/.vscode/extensions/vscodevim.vim-1.18.5中。</p><p><img src="/images/pasted-116.png" alt="upload successful"></p><h1 id="在微信开发者工具安装VS-Code插件"><a href="#在微信开发者工具安装VS-Code插件" class="headerlink" title="在微信开发者工具安装VS Code插件"></a>在微信开发者工具安装VS Code插件</h1><p>1、在微信开发者工具中点击“设置”-&gt;”扩展设置”</p><p><img src="/images/pasted-117.png" alt="upload successful"></p><p>2、在打开的窗口中选择“编辑器自定义扩展”，因为我已经安装过了，所以截图中已经包含了vscode.vim插件</p><p><img src="/images/pasted-118.png" alt="upload successful"></p><p>3、点击上方的“打开扩展文件夹”，此时会打开微信开发者插件目录，而你要做的就是将vscode插件拷贝过去。</p><p><img src="/images/pasted-119.png" alt="upload successful"></p><p>但是由于从Finder中无法直接访问隐藏目录，先在左侧选择HOME目录。</p><p><img src="/images/pasted-121.png" alt="upload successful"></p><p>使用“前往文件夹”选项，填入.vscode/extensions。将.vscode/extensions/vscodevim.vim-1.18.5拷贝之刚才打开的微信开发者工具的扩展目录中。</p><p><img src="/images/pasted-120.png" alt="upload successful"></p><p>4、重启微信开发者工具后，就能在“编辑器自定义扩展”中看到vim插件，启动插件后，再次退出重启，此时编辑器里已经可以使用vim模式了。</p><p><img src="/images/pasted-122.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;随着云计算技术的发展特别是无服务化的发展，在业务系统的研发上，前端和后端的边界逐步被打破。微信小程序便是这一方面的典型代表，特别是结合了腾讯Serverless云开发的套件后，小程序融会贯通成为业务开发非常重要的载体。今年疫情期间，基于小程序开发的健康码充分发挥了小程序这一方面的特点。小程序上手开发难度不高，基本都是基于Javascript生态构建，对于前端开发或者后端开发来说，无疑都是福音，让大家真正的做一次全栈开发。&lt;/p&gt;
&lt;p&gt;作为一名10多年的开发人员，vim是我最常使用的编辑器，但是在微信开发者工具中并没有直接提供vim的开发模式。经过不断的探索，终于发现微信开发者工具对VS Code插件的兼容模式，于是按照文档将VS Code vim插件安装在微信开发者工具中。果然，我熟悉的vim模式又回来了，这篇文章就为大家简单分享一下。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>使用镜像源加速Github Clone速度</title>
    <link href="http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/"/>
    <id>http://sunqi.site/2021/01/25/%E4%BD%BF%E7%94%A8%E9%95%9C%E5%83%8F%E6%BA%90%E5%8A%A0%E9%80%9FGithub-Clone%E9%80%9F%E5%BA%A6/</id>
    <published>2021-01-25T01:11:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？</p><a id="more"></a><h1 id="设定gitconfig自动实现替换"><a href="#设定gitconfig自动实现替换" class="headerlink" title="设定gitconfig自动实现替换"></a>设定gitconfig自动实现替换</h1><p>通过在HOME目录下的.gitconfig文件可以实现自动的对github.com进行替换的目的，具体的方式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git config --global url.&quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;.insteadOf https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><p>在$HOME/.gitconfig会发现增加了如下行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[url &quot;https:&#x2F;&#x2F;gitclone.com&#x2F;&quot;]</span><br><span class="line">insteadOf &#x3D; https:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure><h1 id="其他镜像源"><a href="#其他镜像源" class="headerlink" title="其他镜像源"></a>其他镜像源</h1><p>目前国内提供github镜像源还包括以下地址，但是通过网站测速（<a href="https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：" target="_blank" rel="noopener">https://tool.chinaz.com/sitespeed）来看，目前相对于北京最稳定和快速的是gitclone.com，所以可以根据不同地域灵活进行选择以下地址：</a></p><ul><li>fastgit.org: <a href="https://doc.fastgit.org/" target="_blank" rel="noopener">https://doc.fastgit.org/</a></li><li>gitclone.com: <a href="https://gitclone.com/" target="_blank" rel="noopener">https://gitclone.com/</a></li><li>gitee: <a href="https://gitee.com/mirrors" target="_blank" rel="noopener">https://gitee.com/mirrors</a></li><li>cnpmjs.org: <a href="https://github.com.cnpmjs.org/" target="_blank" rel="noopener">https://github.com.cnpmjs.org/</a></li></ul><h1 id="文件下载"><a href="#文件下载" class="headerlink" title="文件下载"></a>文件下载</h1><p>还有一种情况是要从github下载某个文件，由于raw.githubusercontent.com属于长期被屏蔽状态，所以基本通过wget进行下载，比如要下载的文件为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -O https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure><p>可以替换为：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https:&#x2F;&#x2F;raw.staticdn.net&#x2F;xiaoquqi&#x2F;dockprom&#x2F;master&#x2F;docker-compose.vmware.exporters.yml</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Github被屏蔽已经不是什么太新鲜的事情了，但是对开发人员下载速度确实造成很大的困扰，所以需要使用镜像源来加速下载速度。但是，我在clone的时候又不想每次破坏原有的链接，那有没有什么自动的方法来帮助我们来修改呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="Git" scheme="http://sunqi.site/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>利用Docker快速搭建Prometheus监控及告警平台</title>
    <link href="http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/"/>
    <id>http://sunqi.site/2020/12/25/%E5%88%A9%E7%94%A8Docker%E5%BF%AB%E9%80%9F%E6%90%AD%E5%BB%BAPrometheus%E7%9B%91%E6%8E%A7%E5%8F%8A%E5%91%8A%E8%AD%A6%E5%B9%B3%E5%8F%B0/</id>
    <published>2020-12-25T13:32:48.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。</p><p>目前本文涉及的监控内容：</p><ul><li>主机监控</li><li>容器监控</li><li>Ceph监控</li><li>VMware监控</li></ul><a id="more"></a><h1 id="项目说明"><a href="#项目说明" class="headerlink" title="项目说明"></a>项目说明</h1><p>我们假设读者已经使用CentOS搭建了容器环境，并配置了国内源的前提下。如果没有设置请参考<a href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/">《CentOS 7和Docker初始化安装》</a>。</p><p>Prometheus快速构建的docker compose原始项目来自<a href="https://github.com/stefanprodan/dockprom" target="_blank" rel="noopener">stefanprodan/dockprom</a>，但是由于原项目中的cAdvisor使用了Google源，所以Fork的项目修改为国内源<a href="https://github.com/xiaoquqi/dockprom" target="_blank" rel="noopener">xiaoquqi/dockprom</a>。</p><p>原项目中包含的组件：</p><ul><li>Prometheus (metrics database) http://<host-ip>:9090</li><li>Prometheus-Pushgateway (push acceptor for ephemeral and batch jobs) http://<host-ip>:9091</li><li>AlertManager (alerts management) http://<host-ip>:9093</li><li>Grafana (visualize metrics) http://<host-ip>:3000</li><li>Caddy (reverse proxy and basic auth provider for prometheus and alertmanager)</li></ul><p>默认包含的采集器：</p><ul><li>NodeExporter (host metrics collector)</li><li>cAdvisor (containers metrics collector)</li></ul><p>在此基础上增加的内容：</p><ul><li>Ceph exporter</li><li>VMware exporter</li><li>钉钉告警webhook</li><li>轻量级http服务，用于内网分发docker-compse.exporter.yml</li></ul><h1 id="环境快速构建"><a href="#环境快速构建" class="headerlink" title="环境快速构建"></a>环境快速构建</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;dockprom</span><br><span class="line">cd dockprom</span><br><span class="line">docker-compose up -d</span><br></pre></td></tr></table></figure><p>启动完成后，用浏览器访问：</p><ul><li>Prometheus: <a href="http://yourip:9090" target="_blank" rel="noopener">http://yourip:9090</a></li><li>Grafana: <a href="http://yourip:3000" target="_blank" rel="noopener">http://yourip:3000</a></li></ul><p>默认的用户名/密码为: admin/admin，如果需要修改可以在启动之前修改docker-compose.yml文件。</p><p>访问Prometheus，查看metrics是否被正确采集。如果有采集器有红色字样，根据提示查看具体的错误原因，大部分的错误都是因为配置问题，或者网络不通造成的。</p><p><img src="/images/pasted-109.png" alt="upload successful"></p><h1 id="Grafana配置"><a href="#Grafana配置" class="headerlink" title="Grafana配置"></a>Grafana配置</h1><p>访问Grafana的控制面板，其中已经内置了一些模板，也可以选择Import导入Grafana模板库的模板，数据源选择已经配置好的Prometheus即可。</p><p><img src="/images/pasted-110.png" alt="upload successful"></p><h1 id="主机监控"><a href="#主机监控" class="headerlink" title="主机监控"></a>主机监控</h1><p>默认安装情况下，主机层面仅监控了本机，如果需要增加新的监控主机，需要进行以下两步：</p><ul><li>为主机安装node exporter</li><li>修改Prometheus配置文件，并重启服务</li></ul><h2 id="1、安装node-exporter"><a href="#1、安装node-exporter" class="headerlink" title="1、安装node exporter"></a>1、安装node exporter</h2><p>在项目中，内置了一个单独的docker-compose.exporters.yml，如果目标主机安装了容器，可以直接将该yaml文件拷贝至目标节点后，启动监控服务即可。当然也可以通过软件包安装方式，本文不再赘述。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>安装完成后，访问metrics接口，即代表安装成功</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl http:&#x2F;&#x2F;localhost:9100&#x2F;metrics</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;14&quot;&#125; 2.8795339e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;15&quot;&#125; 2.3535384e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;16&quot;&#125; 3.4674675e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;17&quot;&#125; 2.5727501e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;18&quot;&#125; 2.5931391e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;19&quot;&#125; 2.67231846e+08</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;2&quot;&#125; 4.3448998e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;20&quot;&#125; 3.0684276e+07</span><br><span class="line">node_softnet_processed_total&#123;cpu&#x3D;&quot;21&quot;&#125; 3.0587632e+07</span><br></pre></td></tr></table></figure><h2 id="2、修改Prometheus配置文件"><a href="#2、修改Prometheus配置文件" class="headerlink" title="2、修改Prometheus配置文件"></a>2、修改Prometheus配置文件</h2><p>回到Prometheus节点，找到dockerprom/prometheus/prometheus.yml进行如下修改，在nodeexporter段的targets增加新的监控节点后重启服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># A scrape configuration containing exactly one endpoint to scrape.</span><br><span class="line">scrape_configs:</span><br><span class="line">  - job_name: &#39;nodeexporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    static_configs:</span><br><span class="line">      - targets: [&#39;nodeexporter:9100&#39;, &#39;newip:9100&#39;]</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><h1 id="告警配置"><a href="#告警配置" class="headerlink" title="告警配置"></a>告警配置</h1><p>其实监控并不是最终的目的，往往告警才是监控系统成功与否的关键，在实际运维中对于根因分析和告警收敛是有非常强烈的需求的，本文中暂时还没对此做深入的分析，仅仅提供了常规的告警手段。告警的配置方法有两种方式，一种是通过Prometheus AlertManager，另外一种也可以通过在Grafana上直接进行配置。</p><p>对于告警方式支持多种方式，例如我们常用的邮件或者钉钉等，当然你也可以实现你自己的方式，这里我们使用钉钉的WEBHOOK作为告警方式。</p><h2 id="1-钉钉webhook配置"><a href="#1-钉钉webhook配置" class="headerlink" title="1. 钉钉webhook配置"></a>1. 钉钉webhook配置</h2><p>默认已经启动了钉钉容器，只需要修改dingtalk/config.yaml即可。Targets下面有各种示例，比如配置一个最简单的钉钉告警：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">targets:</span><br><span class="line">  devops:</span><br><span class="line">    url: https:&#x2F;&#x2F;oapi.dingtalk.com&#x2F;robot&#x2F;send?access_token&#x3D;xxxxx</span><br></pre></td></tr></table></figure><p>这里的devops是自定义的，但是和后面要填入alertmanager的链接地址有关，比如本例中alertmanager回调地址就是http://<yourip>:8060/dingtalk/devops/send</p><h2 id="2-修改AlertManager配置"><a href="#2-修改AlertManager配置" class="headerlink" title="2. 修改AlertManager配置"></a>2. 修改AlertManager配置</h2><p>  修改alertmanager/config.yml</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">  route:</span><br><span class="line">    receiver: &#39;dingtalk&#39;</span><br><span class="line"></span><br><span class="line">receivers:</span><br><span class="line">  - name: &#39;dingtalk&#39;</span><br><span class="line">    webhook_configs:</span><br><span class="line">    - send_resolved: true</span><br><span class="line">      url: http:&#x2F;&#x2F;&lt;yourip&gt;:8060&#x2F;dingtalk&#x2F;devops&#x2F;send</span><br></pre></td></tr></table></figure><p>  这里不要用localhost，因为部署在容器内。</p><h2 id="3-修改Prometheus配置文件"><a href="#3-修改Prometheus配置文件" class="headerlink" title="3. 修改Prometheus配置文件"></a>3. 修改Prometheus配置文件</h2><p>  修改alert.rules，尝试修改一些规则测试告警，例如：</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">- name: host</span><br><span class="line">rules:</span><br><span class="line">- alert: high_cpu_load</span><br><span class="line">  expr: node_load1 &gt; 0.2</span><br><span class="line">  for: 1s</span><br><span class="line">  labels:</span><br><span class="line">    severity: warning</span><br><span class="line">  annotations:</span><br><span class="line">    summary: &quot;Server under high load&quot;</span><br><span class="line">    description: &quot;Docker host is under high load, the avg load 1m is at &#123;&#123; $value&#125;&#125;. Reported by instance &#123;&#123; $labels.instance &#125;&#125; of job &#123;&#123; $labels.job &#125;&#125;.&quot;</span><br></pre></td></tr></table></figure><p>此时可以通过AlertManager查看http://<yourip>:9093/#/alerts，检查是否有告警产生。</p><p><img src="/images/pasted-111.png" alt="upload successful"></p><p>如果告警产生了，但是无法触发钉钉，可以通过检查alertmanager容器进行debug，例如上述提到的localhost问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">level&#x3D;warn ts&#x3D;2020-12-29T07:21:56.345Z caller&#x3D;notify.go:674 component&#x3D;dispatcher receiver&#x3D;dingtalk integration&#x3D;webhook[0] msg&#x3D;&quot;Notify attempt failed, will retry later&quot; attempts&#x3D;1 err&#x3D;&quot;Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br><span class="line">level&#x3D;error ts&#x3D;2020-12-29T07:26:56.344Z caller&#x3D;dispatch.go:309 component&#x3D;dispatcher msg&#x3D;&quot;Notify for alerts failed&quot; num_alerts&#x3D;1 err&#x3D;&quot;dingtalk&#x2F;webhook[0]: notify retry canceled after 16 attempts: Post \&quot;http:&#x2F;&#x2F;localhost:8060&#x2F;dingtalk&#x2F;devops&#x2F;send\&quot;: dial tcp 127.0.0.1:8060: connect: connection refused&quot;</span><br></pre></td></tr></table></figure><h1 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h1><p>确保Ceph配置文件已经在/etc/ceph目录下，并且能够正常访问Ceph集群。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.ceph.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9128/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;ceph-exporter&#39;</span><br><span class="line">    scrape_interval: 5s</span><br><span class="line">    honor_labels: true</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.201:9128&#39;]</span><br><span class="line">      labels:</span><br><span class="line">        instance: Ceph Cluster</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入三个模板：</p><ul><li>Ceph Cluster Overview: <a href="https://grafana.com/dashboards/917" target="_blank" rel="noopener">https://grafana.com/dashboards/917</a></li><li>Ceph Pools Overview: <a href="https://grafana.com/dashboards/926" target="_blank" rel="noopener">https://grafana.com/dashboards/926</a></li><li>Ceph OSD Overview: <a href="https://grafana.com/dashboards/923" target="_blank" rel="noopener">https://grafana.com/dashboards/923</a></li></ul><p>Ceph Cluster效果：</p><p><img src="/images/pasted-113.png" alt="upload successful"></p><p>Ceph Pool效果：</p><p><img src="/images/pasted-114.png" alt="upload successful"></p><p>Ceph OSD效果：</p><p><img src="/images/pasted-115.png" alt="upload successful"></p><h1 id="VMware监控"><a href="#VMware监控" class="headerlink" title="VMware监控"></a>VMware监控</h1><p>首先修改docker-compose.vmware.exporters.yml中vcenter的连接信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">services:</span><br><span class="line">  vmware-exporter:</span><br><span class="line">    image: pryorda&#x2F;vmware_exporter:v0.11.1</span><br><span class="line">    container_name: vmware-exporter</span><br><span class="line">    restart: unless-stopped</span><br><span class="line">    ports:</span><br><span class="line">       - &#39;9272:9272&#39;</span><br><span class="line">    expose:</span><br><span class="line">       - 9272</span><br><span class="line">    environment:</span><br><span class="line">      VSPHERE_HOST: &quot;VC_HOST&quot;</span><br><span class="line">      VSPHERE_IGNORE_SSL: &quot;True&quot;</span><br><span class="line">      VSPHERE_USER: &quot;VC_USERNAME&quot;</span><br><span class="line">      VSPHERE_PASSWORD: &quot;VC_PASSWORD&quot;</span><br><span class="line">    labels:</span><br><span class="line">      org.label-schema.group: &quot;monitoring&quot;</span><br></pre></td></tr></table></figure><p>启动VMware exporter：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f docker-compose.vmware.exporters.yml up -d</span><br></pre></td></tr></table></figure><p>通过访问http://<yourip>:9272/metrics验证是否能够正常获取数据。</p><p>在prometheus/prometheus.yml文件中增加一个新的Job</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">scrape_configs:</span><br><span class="line">  ......</span><br><span class="line">  - job_name: &#39;vmware_vcenter&#39;</span><br><span class="line">    metrics_path: &#39;&#x2F;metrics&#39;</span><br><span class="line">    scrape_timeout: 15s</span><br><span class="line">    static_configs:</span><br><span class="line">    - targets: [&#39;192.168.10.13:9272&#39;]</span><br></pre></td></tr></table></figure><p>最后重启prometheus容器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker restart prometheus</span><br></pre></td></tr></table></figure><p>在Grafana中导入模板：<a href="https://grafana.com/grafana/dashboards/11243" target="_blank" rel="noopener">https://grafana.com/grafana/dashboards/11243</a></p><p>效果如下：</p><p><img src="/images/pasted-112.png" alt="upload successful"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源项目出现让IT产业得到了蓬勃发展的机会，大批的社区贡献者通过向开源社区贡献代码实现自我价值。企业通过使用开源项目，增加了对核心技术的掌控能力。虽然开源项目从功能性上是基本可用的，但是需要从用户体验、运维层面投入人力，本文目的就是帮助读者利用Docker快速构建一套基于Prometheus的监控及告警平台，能够实现对用户环境基本监控，本文将持续更新，收集好用的exporter及Grafana Dashboard。&lt;/p&gt;
&lt;p&gt;目前本文涉及的监控内容：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;主机监控&lt;/li&gt;
&lt;li&gt;容器监控&lt;/li&gt;
&lt;li&gt;Ceph监控&lt;/li&gt;
&lt;li&gt;VMware监控&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>我需要一款什么样的网盘？</title>
    <link href="http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/"/>
    <id>http://sunqi.site/2020/12/14/%E6%88%91%E9%9C%80%E8%A6%81%E4%B8%80%E6%AC%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E7%BD%91%E7%9B%98%EF%BC%9F/</id>
    <published>2020-12-14T13:56:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。</p><a id="more"></a><h1 id="阿里云盘or网盘——傻傻分不清楚"><a href="#阿里云盘or网盘——傻傻分不清楚" class="headerlink" title="阿里云盘or网盘——傻傻分不清楚"></a>阿里云盘or网盘——傻傻分不清楚</h1><p>我相信很多关注了阿里网盘的朋友都会有一个问题，到底哪个是阿里真的网盘？我们通过公开渠道，至少能找到阿里云有至少三款“云盘”或者“网盘”。</p><p><img src="/images/pasted-97.png" alt="upload successful"></p><p><img src="/images/pasted-98.png" alt="upload successful"></p><p><img src="/images/pasted-99.png" alt="upload successful"></p><p>幸运的一点，我除了通过阿里MVP渠道获取到Teambition开发的网盘，同时还获取了阿里云盘的内测码，所以有机会对这两款都进行了充分体验。</p><h2 id="Teambiton网盘"><a href="#Teambiton网盘" class="headerlink" title="Teambiton网盘"></a>Teambiton网盘</h2><p>2019年初阿里耗资1亿美金收购了团队协作平台Teambition，通过收购及整合兼并，阿里不断扩大自身在To B领域的影响力。在推出网盘前，Teambition已经推出了代码托管平台codeUp和Flow，为企业构建完整的DevOps流程垫定了基础。网盘也是顺应这一趋势，阿里也继续丰富着自己的To B版图。</p><p>不过与我们理解的传统网盘有所区别，目前网盘是紧耦合在原有的Teambition体系内，更像是Teambition的一个扩展功能，而不能独立使用，这一点从网页版和手机侧的设计就能看出来。所以，也许Teambition网盘的目标客户也许并不是传统的网盘用户，更像是解决企业成员间文件存储和分享的问题。</p><p>目前Teambition暂不支持企业网盘，还需要切换至个人账户。但是在我测试过程中，有个小Bug，我明明在登陆后是个人账户，但是仍然看到的是企业网盘暂未开放的提示信息。需要先切换到企业，再切换回个人后才能看到。</p><p><img src="/images/pasted-100.png" alt="upload successful"></p><p>先来看一下整体的界面风格，从内测版本的截图看，Teambition的网盘目前还处于非常简单的状态，界面上展现的功能并不多，我认为类似最早期对象存储的基本功能。</p><p><img src="/images/pasted-101.png" alt="upload successful"></p><p>由于目前还处于内测阶段，所以基本不会对上传和下载速度做任何限制，不知道在商用化之后免费权益到底如何？我家的宽带是联通300Mbps，上传之前对网速进行了基本测试，上传的时候我使用的是浏览器直接上传的方式，我的iMac使用网线和路由器直接进行连接。</p><p><img src="/images/pasted-103.png" alt="upload successful"></p><p>我是晚上做的测试，上传了一个4.66GB的MP4文件，上传速度基本稳定在了400KB/s。</p><p><img src="/images/pasted-104.png" alt="upload successful"></p><p>这个和我最早期的一次测试结果是有出入的，感觉速度没有这么慢，并且中途失败了好几次，于是第二天早晨的时候，在失败后进行了重传。这时候速度基本上能把网络的上行速度跑满。</p><p><img src="/images/pasted-107.png" alt="upload successful"></p><p>另外，对于网盘很重要的分享功能也并未开放，只能作为用户自有的网络存储空间。</p><p><img src="/images/pasted-105.png" alt="upload successful"></p><p>手机侧的功能也基本与网页版本相似，并且必须要借助Teambition APP使用，暂时没有提供任何自动备份的功能。</p><p><img src="/images/pasted-106.png" alt="upload successful"></p><h2 id="阿里云盘"><a href="#阿里云盘" class="headerlink" title="阿里云盘"></a>阿里云盘</h2><p>经过与阿里内部确认，阿里的云盘和Teambition是完全独立的两个团队，也就是这是两端独立的产品，所以这也让我怀疑我当初看到的文章到底是在讲网盘还是云盘的？</p><p>阿里的云盘更接近传统对网盘的认知，这一点从用户体验上就能感觉到。与网盘不同的是，阿里云盘相对来说比较独立，之前只开放了手机版本，目前已经可以从网页上进行登陆了。目前只能通过网页版进行内测资格的申请，而所有操作也只能在手机侧完成。阿里云盘的域名是aliyundrive.com。</p><p>目前的客户端方面只提供了手机侧的，对我来说比较重要的客户端的还没有看到。从上传的感受来说，云盘的稳定性要好于网盘，网速基本上稳定在5MB/s，中间没有出现任何断线情况。</p><p><img src="/images/pasted-108.png" alt="upload successful"></p><h1 id="我需要什么样的网盘产品？"><a href="#我需要什么样的网盘产品？" class="headerlink" title="我需要什么样的网盘产品？"></a>我需要什么样的网盘产品？</h1><p>回答这个问题，先要从我用了哪两款付费网盘产品说起。目前我使用的两款付费网盘的产品，一款是苹果的iCloud，一款是百度的网盘。iCloud的费用一个月是21元/月200GB家庭共享空间，而百度网盘超级会员一个月是18元/月空间，空间达到了5TB，但是无法家庭共享。</p><p>可能有人会问为什么每个月要花将近40元在付费网盘呢？为什么不使用同一种网盘呢？这主要源自我最主要的几个需求：</p><p>一、解决不同设备之间的文件同步问题。我目前使用的手机是iPhone、办公电脑是Mac Pro，而家里使用的是iMac。因为经常要在家里工作，而又懒着背笔记本回家，所以重要文件在不同电脑的传输对我来说就非常重要了。另外，因为有时候需要出去交流，所以会从手机侧查看文件，那么手机与PC之间的文件互通也变得非常重要了。百度网盘之前在Mac侧会有个同步盘的应用，但是后来不再维护了，并且同步盘之前在同步过程中经常发生同步冲突，而莫名其妙产生了多个文件的问题。后来还试过类似OneDrive等网盘，都有类似的问题。最终发现只有苹果自身的方案才能最完美的符合我的要求。<br>二、费用成本问题。虽然苹果的方案很完美，但是也是最贵的，苹果的付费储存空间方案跨度太大，200GB之后就是2TB，价格直接从21元/月涨到了68元/月。因为会拍摄一些视频资料，所以对空间消耗比加大，无奈之下，只得选择了百度网盘作为补充，一方面是之前有很多文件都存在了百度网盘上，另外一方面付费后也不受下载限速的影响了，算是作为一种二级存储的方案使用。近期，准备购置百度的智能音箱，还能直接播放网盘内宝宝的音频。<br>三、照片、视频自动备份功能。虽然iCloud也能对照片、视频进行自动备份，但是有了孩子之后发现照片和视频成倍增加，真是不禁用，这时百度网盘的照片、视频自动备份就派上了用场。</p><p>其实，百度网盘还有一些类似照片整理、搜索等功能也非常强大，但是相比前两点并非刚需，所以只是偶尔会用到。</p><p>虽然这种组合使用方式在一定程度基本满足了我的日常需求，只是要定期的将部分数据导入百度网盘略显繁琐。但是另外一个网盘的需求依然困扰着我，就是对于微信的备份功能。虽然钉钉在这一点上要明显好于微信，但是谁让微信是第一社交软件呢？平时的沟通还是在微信，经常遇到一些文件不随手存下来就被微信清理掉的情况。虽然腾讯也有自己的微盘，但是就是不支持微信的自动备份功能，如果支持我估计我会立马付费购买。</p><p>总结一下我对网盘的几个需求：第一，空间足够大，上传下载不限速；第二、设备之间能够互相同步；第三、微信聊天记录和附件的自动备份功能。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;大概是在8月份的时候，收到阿里要做网盘的消息，那篇文章以“免费”、“不限速”这样的噱头来吸引读者的眼球，直击目前网盘的痛点，当时确实赚足了流量。不过两个月过后，阿里的网盘仍然是犹抱琵琶半遮面的感觉，始终让人看不清楚阿里网盘的端倪。十月底的时候，依靠阿里MVP这一“天时”，搞到了阿里云盘的内测码，并进行了深度体验，所以这篇文章主要是两个目的：第一，是交个作业，毕竟“拿人内测码，与人评测”；第二，网盘我用了不少，也从我的需求角度来讲一下，我对网盘的需求到底是什么，希望能引起读者的一些共鸣。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Docker构建服务器空间占满问题</title>
    <link href="http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/"/>
    <id>http://sunqi.site/2020/11/13/Docker%E6%9E%84%E5%BB%BA%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%A9%BA%E9%97%B4%E5%8D%A0%E6%BB%A1%E9%97%AE%E9%A2%98/</id>
    <published>2020-11-13T06:19:41.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="现象描述"><a href="#现象描述" class="headerlink" title="现象描述"></a>现象描述</h1><p>今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。</p><a id="more"></a><p><img src="/images/pasted-94.png" alt="upload successful"></p><h1 id="排查过程"><a href="#排查过程" class="headerlink" title="排查过程"></a>排查过程</h1><h2 id="检查容器空间"><a href="#检查容器空间" class="headerlink" title="检查容器空间"></a>检查容器空间</h2><p>首先从容器层面检查一下空间占用情况：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system df</span><br></pre></td></tr></table></figure><p>发现有容器的占用空间达到了1个多TB的空间。</p><p><img src="/images/pasted-95.png" alt="upload successful"></p><h2 id="清理无用的容器和镜像"><a href="#清理无用的容器和镜像" class="headerlink" title="清理无用的容器和镜像"></a>清理无用的容器和镜像</h2><p>先用prune进行一下清理，为了保险起见，过滤一下时间</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker system prune -a -f --filter &quot;until &#x3D; 1h&quot;</span><br></pre></td></tr></table></figure><p>清理完成后，空间仍然没有释放，于是继续排查。</p><h2 id="检查-var-lib下的空间占用"><a href="#检查-var-lib下的空间占用" class="headerlink" title="检查/var/lib下的空间占用"></a>检查/var/lib下的空间占用</h2><p>通过检查发现/var/lib/docker/overlay2中的66d44a19ee93a191cc0585efac45e10696edfd0381d0dc96d9646080337f629e目录空间占用巨大，进入后发现其中有tmp目录没有及时清理。由于没有Jenkins任务在执行，所以手动清理了/tmp/tmp*的目录，空间被立即释放了。</p><p><img src="/images/pasted-96.png" alt="upload successful"></p><p>那么此时问题清晰了，这一层属于Jenkins，进入容器后发现Jenkins的/tmp目录没有被及时清理，属于Build逻辑有缺陷造成了，及时修复Pipeline的Jenkinsfile后，该问题不再出现。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;现象描述&quot;&gt;&lt;a href=&quot;#现象描述&quot; class=&quot;headerlink&quot; title=&quot;现象描述&quot;&gt;&lt;/a&gt;现象描述&lt;/h1&gt;&lt;p&gt;今天Jenkins构建突然出现问题，检查Jenkins Job日志发现no space left，于是登录到Jenkins Build服务器上，发现容器所在的/var/lib空间被完全满了。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>MacOS VPN拨号后自动设置路由</title>
    <link href="http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/"/>
    <id>http://sunqi.site/2020/11/11/MacOS-VPN%E6%8B%A8%E5%8F%B7%E5%90%8E%E8%87%AA%E5%8A%A8%E8%AE%BE%E7%BD%AE%E8%B7%AF%E7%94%B1/</id>
    <published>2020-11-11T12:04:07.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="目的"><a href="#目的" class="headerlink" title="目的"></a>目的</h1><p>公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。</p><a id="more"></a><h1 id="允许L2TP共享密钥为空"><a href="#允许L2TP共享密钥为空" class="headerlink" title="允许L2TP共享密钥为空"></a>允许L2TP共享密钥为空</h1><p>公司L2TP共享密钥为空，默认MacOS是不支持的，所以需要在配置文件中特殊设定，在/etc/ppp下生成options文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo tee &#x2F;etc&#x2F;ppp&#x2F;options &lt;&lt; EOF</span><br><span class="line">plugin L2TP.ppp</span><br><span class="line">l2tpnoipsec</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="自动路由设置"><a href="#自动路由设置" class="headerlink" title="自动路由设置"></a>自动路由设置</h1><p>原理很简单，在连接VPN后将指定网段的IP经过VPN虚拟接口即可，具体的实现如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo touch &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br><span class="line">sudo chmod 0755 &#x2F;etc&#x2F;ppp&#x2F;ip-up</span><br></pre></td></tr></table></figure><p>ip-up的内容如下，只需要修改SUBNET网段即可，例如：192.168.10.0/24</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#!&#x2F;bin&#x2F;sh</span><br><span class="line">&#x2F;sbin&#x2F;route add &lt;SUBNET&gt; -interface $1</span><br></pre></td></tr></table></figure><p>其余可利用参数如下：</p><ul><li>$1: VPN接口(例如：ppp0)</li><li>$2: 未知</li><li>$3: VPN服务器地址</li><li>$4: VPN网关地址</li><li>$5: 非VPN网关，本地使用</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;目的&quot;&gt;&lt;a href=&quot;#目的&quot; class=&quot;headerlink&quot; title=&quot;目的&quot;&gt;&lt;/a&gt;目的&lt;/h1&gt;&lt;p&gt;公司使用的VPN是L2TP协议的，平时在家远程工作时需要VPN拨入，但是又不想所有的流量都经过VPN，需要使用路由表来路由指定的网段。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>利用钉钉通讯录同步构建本地LDAP服务</title>
    <link href="http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/"/>
    <id>http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/</id>
    <published>2020-10-31T10:24:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？</p><a id="more"></a><p>目前，在我们公司使用OpenLDAP服务作为各个服务统一鉴权的入口，使用的应用系统包括：Gerrit/Jenkins/Yapi/Wiki/进度跟踪等，目前所有的系统都支持LDAP鉴权，所以如果能将钉钉的通讯录定期同步至LDAP中就可以实现统一鉴权的需求。但是由于钉钉的密码无法同步回本地，所以密码层面仍然是独立的。</p><p>本文章的实现思路参考了<a href="https://xujiwei.com/blog/2020/02/internal-authorize-based-on-dingtalk-virtual-ldap-keyclaok/" target="_blank" rel="noopener">《基于钉钉 + Virtual-LDAP + KeyCloak 的内网统一认证系统<br>》</a>，感谢原作者的思路及贡献的virtual-ldap模块，本文所有的优化都是基于此文章基础上进行的优化。</p><h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>简单来说，我们希望能通过钉钉提供的LDAP作为统一鉴权方式，但是由于钉钉没有开放这个能力，那么我们需要将钉钉模拟一个LDAP服务。模拟出的LDAP环境，在内网环境中，我们对于LDAP信息的使用基本上围绕着用户名和密码，其他的信息以钉钉为准。所以，除了开放LDAP接口外，我们还需要提供用户界面，允许用户在内网修改密码。</p><p>整体的实现思路如下：</p><ul><li>钉钉开发者平台：需要在钉钉开发者平台新建一个程序，获取鉴权信息后，赋予通讯录同步权限，提供给VirtualLDAP进行数据同步</li><li>VirtualLDAP：该组件是上面提到的作者开发的虚拟LDAP组件，主要功能为同步钉钉通讯录，并以LDAP协议对外提供服务</li><li>KeyCloak：对于这个场景过重，但是暂时没有发现更好的方案，可以触发自动同步并且可以让内网用户进行密码修改</li><li>本地的全部系统按照LDAP配置方式即可实现鉴权</li></ul><p><img src="/images/pasted-90.png" alt="upload successful"></p><h2 id="钉钉开发者平台"><a href="#钉钉开发者平台" class="headerlink" title="钉钉开发者平台"></a>钉钉开发者平台</h2><p>这里我创建的是H5微应用，配置时有几点需要注意：</p><ul><li>IP地址白名单：需要为你未来运行VirtualLDAP配置访问IP白名单，目前钉钉开发者平台对于同一个IP只能给一个应用使用，但是可以通过通配符进行配置，例如：192.168.10.*的方式，那么192.168.10网段所有地址均可以访问</li><li>权限：需要为该应用开放所有通讯录只读权限即可</li></ul><p><img src="/images/pasted-92.png" alt="upload successful"></p><h2 id="VirtualLDAP"><a href="#VirtualLDAP" class="headerlink" title="VirtualLDAP"></a>VirtualLDAP</h2><p>这是基于Node.js开发的一款组件，主要用于同步钉钉通讯录和模拟LDAP协议。基于原作者版本，为了满足自身应用场景，进行了如下修改：</p><ul><li>由于作者没有提供Docker运行方式，所以在github的pull request中有人进行了改造</li><li>仍然是在同一个pull request中，增加了pinyin组件，在LDAP中增加了一个pinyin属性，方便业务系统使用</li><li>登录名和密码：为了防止公司人员重复，所以特别采用了全拼名称+电话号码后四位方式，作为唯一的用户名，而初始密码为全名名称+电话号码前四位，例如：张三的电话号码为13812345678，则登录名称为zhangsan5678，密码为zhangsan1381,</li><li>在使用VirtualLDAP时，需要使用MySQL存储持久化数据，例如用户修改后的密码，所以对鉴权规则进行了修改，先检查数据库密码是否匹配，再检查LDAP</li><li>实现了整体组件的编排，增加了docker-compose.yaml，方便用户使用，该编排文件中包含了KeyCloak、VirtualLDAP和MySQL</li></ul><h2 id="KeyCloak"><a href="#KeyCloak" class="headerlink" title="KeyCloak"></a>KeyCloak</h2><p>KeyCloak两部分需要进行配置：</p><ul><li>管理员在第一次使用时配置VirtualLDAP的信息，用于用户同步，方便新用户修改密码</li><li>新用户自行修改密码</li></ul><p>正如之前所说，KeyCloak功能过于强大，这里用到的功能非常有限，如果有新的应用场景，欢迎留言。</p><p><img src="/images/pasted-93.png" alt="upload successful"></p><h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><p>这里提供了完整的编排文件，直接使用即可完成整套环境的快速建立。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;virtual-ldap</span><br><span class="line">cd virtual-ldap&#x2F;docker-compose</span><br><span class="line">docker-compose up -d0</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><ul><li>VirtualLDAP配置文件修改。所有配置在virtual-ldap/docker-compose/config.js中进行修改，需要修改钉钉的appKey和appSecret，以及root DN的信息，配置文件有比较详细的介绍，所以这里不再赘述。</li><li>KeyCloak的默认密码修改在docker-compose.yaml中</li></ul><h2 id="登录相关信息"><a href="#登录相关信息" class="headerlink" title="登录相关信息"></a>登录相关信息</h2><ul><li>URL: ldap://ip:1389</li><li>ManageDN: cn=admin,dc=oneprocloud,dc=com</li><li>ManagePassword: password</li><li>User Search Base: ou=People,o=department,dc=oneprocloud,dc=com</li><li>User Search Filter: uid={0}</li><li>Display Name LDAP attribute: cn</li><li>Email Address LDAP attribute: mail</li></ul><h1 id="待优化"><a href="#待优化" class="headerlink" title="待优化"></a>待优化</h1><ul><li>目前对于LDAP的组没有充分利用，配置文件中允许创建特定组，并且通过用户email进行匹配，如果需要可以进行配置</li><li>如果有外部用户，暂时无方法进行创建，例如：如果需要在LDAP中增加一个非钉钉用户暂时无法实现，需要进行开发实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Node.js" scheme="http://sunqi.site/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>使用Kolla部署OpenStack Stein版本</title>
    <link href="http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/</id>
    <published>2020-10-30T14:15:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。</p><a id="more"></a><h1 id="1、云平台规划"><a href="#1、云平台规划" class="headerlink" title="1、云平台规划"></a>1、云平台规划</h1><p>在实际环境中，我们在一台2U的超微四子星服务器上进行了部署。由于是内部使用的研发环境，为了节约成本，我们并没有部署高可靠方案，而是采用了一台作为控制节点+计算节点+存储节点，另外三台作为计算节点+存储节点的方式进行部署。</p><p>由于OpenStack最新的Ussari在使用Kolla部署时，不再支持CentOS 7版本，所以这里我们选定了上一个稳定版本Stein版本进行部署。</p><h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><table><thead><tr><th>硬件名称</th><th>配置规格</th><th>备注</th></tr></thead><tbody><tr><td>CPU</td><td>Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz    x 2</td><td>共40线程</td></tr><tr><td>内存</td><td>DDR4 2400 MHz 64GB</td><td></td></tr><tr><td>硬盘</td><td>板载64 GB x 1 <br/> 240 GB Intel SSD x 1 <br/> 1.2 TB SAS x 5</td><td>经过测试，由于板载64GB空间过小，在控制节点需要损失一块SAS盘空间用于root分区挂载</td></tr><tr><td>网卡</td><td>千兆 x 4 <br/> 万兆 x 4 <br/> IPMI x 1</td><td></td></tr></tbody></table><h3 id="分区规划"><a href="#分区规划" class="headerlink" title="分区规划"></a>分区规划</h3><table><thead><tr><th>磁盘</th><th>规划</th><th>备注</th></tr></thead><tbody><tr><td>64G</td><td>系统盘</td><td>不要使用LVM分区</td></tr><tr><td>SSD 240G</td><td>Ceph Journal<br></td><td>1块盘</td></tr><tr><td>SAS 1.2 T</td><td>Ceph OSD</td><td>5块盘</td></tr></tbody></table><h2 id="网络规划"><a href="#网络规划" class="headerlink" title="网络规划"></a>网络规划</h2><h3 id="交换机配置"><a href="#交换机配置" class="headerlink" title="交换机配置"></a>交换机配置</h3><ul><li>我们默认采用了VLAN模式，所以无须在交换机上进行Trunk配置</li></ul><h3 id="网络规划-1"><a href="#网络规划-1" class="headerlink" title="网络规划"></a>网络规划</h3><table><thead><tr><th>网卡</th><th>网络类型</th><th>VLAN ID</th><th>网段</th><th>说明</th><th>网关</th><th>备注</th></tr></thead><tbody><tr><td></td><td>管理网络</td><td>3</td><td>192.168.10.0/24</td><td>OpenStack管理</td><td>192.168.10.1</td><td>192.168.10.201 - 204</td></tr><tr><td></td><td>存储网络</td><td></td><td>10.0.100.0/24</td><td>Ceph网络</td><td>无需网关</td><td>10.0.100.201 -&nbsp;204</td></tr><tr><td></td><td>External网络</td><td>3</td><td>192.168.10.0/24</td><td>External网络</td><td>192.168.10.1</td><td>可分配地址192.168.10.100 - 192.168.10.200</td></tr><tr><td></td><td>Tunnel网络</td><td></td><td>172.16.100.0/24</td><td>VxLAN通讯网络</td><td><br data-mce-bogus="1"></td><td>172.16.100.201 - 204</td></tr><tr><td>console</td><td>IPMI</td><td>4</td><td>192.168.10.0/24</td><td></td><td></td><td>与管理网地址一一对应, 192.168.10.201</td></tr></tbody></table><h3 id="网卡配置"><a href="#网卡配置" class="headerlink" title="网卡配置"></a>网卡配置</h3><table><thead><tr><th>主机名</th><th>em1(管理网地址)</th><th>em2(存储网)</th><th>em3(External网络)</th><th>em4(Tunnel网络)</th><th>备注</th></tr></thead><tbody><tr><td>control201</td><td>192.168.10.201</td><td>10.0.100.201</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.201</td><td></td></tr><tr><td>compute202</td><td>192.168.10.202</td><td>10.0.100.202</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.202</td><td></td></tr><tr><td>compute203</td><td>192.168.10.203</td><td>10.0.100.203</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr><tr><td>compute204</td><td>192.168.10.204</td><td>10.10.20.204</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr></tbody></table><h2 id="OpenStack规划"><a href="#OpenStack规划" class="headerlink" title="OpenStack规划"></a>OpenStack规划</h2><h3 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h3><p>Ceph采用单独安装方式，这目前也是Kolla项目主推的方式，在U版本中已经彻底不支持通过Kolla安装Ceph了。我们主要安装OpenStack核心模块，另外安装的是日志收集ELK的相关模块，便于运维。</p><ul><li>Horizon</li><li>Nova</li><li>Keystone</li><li>Cinder</li><li>Glance</li><li>Neutron</li><li>Heat</li></ul><h1 id="2、部署准备"><a href="#2、部署准备" class="headerlink" title="2、部署准备"></a>2、部署准备</h1><h2 id="部署架构图"><a href="#部署架构图" class="headerlink" title="部署架构图"></a>部署架构图</h2><p><img src="/images/pasted-60.png" alt="upload successful"></p><h2 id="服务器前期准备"><a href="#服务器前期准备" class="headerlink" title="服务器前期准备"></a>服务器前期准备</h2><ul><li>BIOS配置：在BIOS中打开VT，并且正确配置IPMI地址，方便远程管理</li><li>RAID配置：所有磁盘需要配置成NON-RAID模式</li><li>操作系统安装：<ul><li>使用CentOS 7光盘进行最小化安装</li><li>不要使用LVM分区</li><li>配置主机名</li><li>配置第一块网卡，并配置自动启动</li></ul></li></ul><h2 id="网卡配置-1"><a href="#网卡配置-1" class="headerlink" title="网卡配置"></a>网卡配置</h2><h3 id="em1"><a href="#em1" class="headerlink" title="em1"></a>em1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em1</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em1</span><br><span class="line">DEVICE&#x3D;em1</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;192.168.10.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">GATEWAY&#x3D;192.168.10.1</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em2"><a href="#em2" class="headerlink" title="em2"></a>em2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em2</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">DEFROUTE&#x3D;yes</span><br><span class="line">NAME&#x3D;em2</span><br><span class="line">DEVICE&#x3D;em2</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;10.0.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em3"><a href="#em3" class="headerlink" title="em3"></a>em3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em3</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em3</span><br><span class="line">DEVICE&#x3D;em3</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;172.16.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em4"><a href="#em4" class="headerlink" title="em4"></a>em4</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em4</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;none</span><br><span class="line">NAME&#x3D;em4</span><br><span class="line">DEVICE&#x3D;em4</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h1><h2 id="3-1-准备部署节点"><a href="#3-1-准备部署节点" class="headerlink" title="3.1 准备部署节点"></a>3.1 准备部署节点</h2><p>该节点承担了后续所有的部署流程，该节点可以作为OpenStack控制节点复用，包括运行OpenStack Kolla和Ceph Deploy。</p><p>注意：节点之间可以通过密码或者密钥方式进行访问，附录中提供了自动上传密钥的方式，建议在正式安装前配置完成，这里不提供自动化配置方法。</p><h3 id="下载初始化脚本"><a href="#下载初始化脚本" class="headerlink" title="下载初始化脚本"></a>下载初始化脚本</h3><p>目前已经将常用的操作写成了Ansible脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;my_ansible_playbooks</span><br><span class="line"></span><br><span class="line">cd my_ansible_playbooks</span><br><span class="line">prepare_on_centos7.sh</span><br></pre></td></tr></table></figure><h3 id="修改hosts-ini文件"><a href="#修改hosts-ini文件" class="headerlink" title="修改hosts.ini文件"></a>修改hosts.ini文件</h3><p>修改hosts.ini文件来初始化所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># my_ansible_playbooks&#x2F;hosts.ini</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.201 ip&#x3D;192.168.10.201 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.202 ansible_user&#x3D;root</span><br><span class="line">compute203 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.203 ansible_user&#x3D;root</span><br><span class="line">compute204 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.204 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="初始化节点"><a href="#初始化节点" class="headerlink" title="初始化节点"></a>初始化节点</h3><p>该步骤主要包含了，更新软件，修改主机名，增加/etc/hosts等操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;bootstrap_centos7.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;change_hostname.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_etc_hosts.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_docker.yml</span><br><span class="line"></span><br><span class="line"># 安装pip和系统环境下的python docker模块，否则在precheck的时候会发现没有安装docker模块</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_pip2_package.yml</span><br></pre></td></tr></table></figure><h3 id="安装Ceph-Deploy"><a href="#安装Ceph-Deploy" class="headerlink" title="安装Ceph Deploy"></a>安装Ceph Deploy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h3 id="安装Kolla"><a href="#安装Kolla" class="headerlink" title="安装Kolla"></a>安装Kolla</h3><p>由于Python Warning的提示信息导致在安装时出现如下错误，需要增加忽略Python Warning的环境变量，具体修复信息如下：<a href="https://bugs.launchpad.net/kolla-ansible/+bug/1888657" target="_blank" rel="noopener">https://bugs.launchpad.net/kolla-ansible/+bug/1888657</a></p><p>目前通过pip方式还没有8.2.1这个release，所以kolla的安装从源代码中进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ansible 2.2.0.0 used in Stein kolla-toolbox requires paramiko (no version</span><br><span class="line">constraints), which installs latest cryptography package. It results in</span><br><span class="line">Python deprecation warning about Python 2:</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x2F;cryptography&#x2F;__init__.py:39: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release.</span><br><span class="line"></span><br><span class="line">This warning breaks kolla_toolbox module.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python-devel libffi-devel gcc openssl-devel libselinux-python</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;openstack&#x2F;kolla-ansible --branch stable&#x2F;stein</span><br><span class="line">cd kolla-ansible</span><br><span class="line">pip install . --ignore-installed PyYAML</span><br><span class="line"></span><br><span class="line"># 虚拟环境还需要再安装一次ansible，否则kolla-ansible会提示</span><br><span class="line"># ERROR: kolla_ansible has to be available in the Ansible PYTHONPATH.</span><br><span class="line"># Please install both in the same (virtual) environment.</span><br><span class="line">pip install &#39;ansible&lt;2.10&#39;</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla</span><br><span class="line">cp -r $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;etc_examples&#x2F;kolla&#x2F;* &#x2F;etc&#x2F;kolla</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;root&#x2F;kolla</span><br><span class="line">cp $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;ansible&#x2F;inventory&#x2F;* &#x2F;root&#x2F;kolla</span><br></pre></td></tr></table></figure><p>生成密码，如果需要指定密码，可以到/etc/kolla/password.yml中修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-genpwd</span><br></pre></td></tr></table></figure><h2 id="3-2-部署Ceph"><a href="#3-2-部署Ceph" class="headerlink" title="3.2 部署Ceph"></a>3.2 部署Ceph</h2><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>之前有一篇软文详细介绍了使用Ceph Deploy部署Ceph的方法，这里不再赘述，下面直接给出部署命令，这里我们只部署块服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;root&#x2F;ceph</span><br><span class="line">cd &#x2F;root&#x2F;ceph</span><br><span class="line"></span><br><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 如果阿里源无法使用，可以使用163源，并且可以通过指定rpm-octopus，指定安装的Ceph版本</span><br><span class="line">#export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">#export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">#</span><br><span class="line"># 如果cluster-network和public-network需要分开，可以这样定义：</span><br><span class="line"># ceph-deploy new --cluster-network 172.31.6.0&#x2F;24 --public-network 192.168.4.0&#x2F;24 node1 node2 node3</span><br><span class="line"></span><br><span class="line">ceph-deploy new --public-network 10.0.100.0&#x2F;24 compute201</span><br><span class="line">ceph-deploy install compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create compute201</span><br><span class="line"></span><br><span class="line"># 需要根据实际情况修改，这里模拟的是将RocksDB存放至单独的SSD磁盘，目前文档中并没有特别指出这部分的分配比例，所以DB和WAL都是分配10G，写入的基本顺序为WAL -&gt; DB -&gt; DATA</span><br><span class="line"></span><br><span class="line">pvcreate &#x2F;dev&#x2F;vdb</span><br><span class="line">vgcreate ceph-pool &#x2F;dev&#x2F;vdb</span><br><span class="line"></span><br><span class="line"># 每个OSD分配</span><br><span class="line">lvcreate -n osd0.wal -L 10G ceph-pool</span><br><span class="line">lvcreate -n osd0.db -L 10G ceph-pool</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdd --block-db ceph-pool&#x2F;osd0.db --block-wal ceph-pool&#x2F;osd0.wal compute201</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><h3 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h3><p>为Glance/Nova/Cinder创建资源池并生成鉴权文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create images 128</span><br><span class="line">ceph auth get-or-create client.glance mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create volumes 128</span><br><span class="line">ceph auth get-or-create client.cinder mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;volumes, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create backups 128</span><br><span class="line">ceph auth get-or-create client.cinder-backup mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;backups&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create vms 128</span><br><span class="line">ceph auth get-or-create client.nova mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;vms, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring</span><br></pre></td></tr></table></figure><p>注意：</p><h2 id="3-3-OpenStack部署"><a href="#3-3-OpenStack部署" class="headerlink" title="3.3 OpenStack部署"></a>3.3 OpenStack部署</h2><h3 id="kolla配置文件"><a href="#kolla配置文件" class="headerlink" title="kolla配置文件"></a>kolla配置文件</h3><h4 id="etc-kolla-globals-yml"><a href="#etc-kolla-globals-yml" class="headerlink" title="/etc/kolla/globals.yml"></a>/etc/kolla/globals.yml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;stein&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.10.123&quot;</span><br><span class="line"></span><br><span class="line">docker_registry: registry.cn-beijing.aliyuncs.com</span><br><span class="line">docker_namespace: &quot;openstack-dockers&quot;</span><br><span class="line"></span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">storage_interface: &quot;eth1&quot;</span><br><span class="line">tunnel_interface: &quot;eth3&quot;</span><br><span class="line">neutron_external_interface: &quot;eth2&quot;</span><br><span class="line"></span><br><span class="line">openstack_logging_debug: &quot;True&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_ceph: &quot;no&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">enable_cinder_backup: &quot;yes&quot;</span><br><span class="line">enable_fluentd: &quot;no&quot;</span><br><span class="line">enable_openstack_core: &quot;yes&quot;</span><br><span class="line">glance_backend_ceph: &quot;yes&quot;</span><br><span class="line">glance_backend_file: &quot;no&quot;</span><br><span class="line">glance_enable_rolling_upgrade: &quot;no&quot;</span><br><span class="line">cinder_backend_ceph: &quot;yes&quot;</span><br><span class="line">nova_backend_ceph: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h4 id="multinode"><a href="#multinode" class="headerlink" title="multinode"></a>multinode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[control]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[network]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[compute]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[monitoring]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[storage]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="定制服务配置文件"><a href="#定制服务配置文件" class="headerlink" title="定制服务配置文件"></a>定制服务配置文件</h3><h4 id="Ceph-Glance"><a href="#Ceph-Glance" class="headerlink" title="Ceph Glance"></a>Ceph Glance</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance</span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;glance-api.conf &lt;&lt; EOF</span><br><span class="line">[glance_store]</span><br><span class="line">stores &#x3D; rbd</span><br><span class="line">default_store &#x3D; rbd</span><br><span class="line">rbd_store_pool &#x3D; images</span><br><span class="line">rbd_store_user &#x3D; glance</span><br><span class="line">rbd_store_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.client.glance.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Cinder"><a href="#Ceph-Cinder" class="headerlink" title="Ceph Cinder"></a>Ceph Cinder</h4><p>cinder_rbd_secret_uuid是在passwords.yml中生成的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup</span><br><span class="line"></span><br><span class="line">export cinder_rbd_secret_uuid&#x3D;$(grep cinder_rbd_secret_uuid &#x2F;etc&#x2F;kolla&#x2F;passwords.yml | awk &#39;&#123;print $2&#125;&#39;)</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends&#x3D;rbd-1</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;cinder</span><br><span class="line">backend_host&#x3D;rbd:volumes</span><br><span class="line">rbd_pool&#x3D;volumes</span><br><span class="line">volume_backend_name&#x3D;rbd-1</span><br><span class="line">volume_driver&#x3D;cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_secret_uuid &#x3D; $cinder_rbd_secret_uuid</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">backup_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">backup_ceph_user&#x3D;cinder-backup</span><br><span class="line">backup_ceph_chunk_size &#x3D; 134217728</span><br><span class="line">backup_ceph_pool&#x3D;backups</span><br><span class="line">backup_driver &#x3D; cinder.backup.drivers.ceph.CephBackupDriver</span><br><span class="line">backup_ceph_stripe_unit &#x3D; 0</span><br><span class="line">backup_ceph_stripe_count &#x3D; 0</span><br><span class="line">restore_discard_excess_bytes &#x3D; true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有文件必须命名为ceph.client*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Nova"><a href="#Ceph-Nova" class="headerlink" title="Ceph Nova"></a>Ceph Nova</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;nova-compute.conf &lt;&lt; EOF</span><br><span class="line">[libvirt]</span><br><span class="line">images_rbd_pool&#x3D;vms</span><br><span class="line">images_type&#x3D;rbd</span><br><span class="line">images_rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;nova</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.nova.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.cinder.keyring</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 初始化节点，与上述我们自己的初始化有重复之处</span><br><span class="line">kolla-ansible -i multinode bootstrap-servers</span><br><span class="line"></span><br><span class="line">kolla-ansible -i multinode prechecks</span><br><span class="line"></span><br><span class="line"># 拉取所有镜像</span><br><span class="line">kolla-ansible -i multinode pull</span><br></pre></td></tr></table></figure><h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -i multinode deploy</span><br><span class="line">kolla-ansible -i multinode post-deploy</span><br></pre></td></tr></table></figure><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="节点互信"><a href="#节点互信" class="headerlink" title="节点互信"></a>节点互信</h2><p>节点之间互信建议采用key方式，这里并没有实现完全自动化手段，需要首先在控制节点上生成公钥和私钥。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>然后将~/.ssh/id_rsa.pub文件拷贝至可以正常访问两台节点的环境中的playbooks/keys目录下，再更新所有节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_authorized_keys.yml</span><br></pre></td></tr></table></figure><h2 id="部署出错如何调试"><a href="#部署出错如何调试" class="headerlink" title="部署出错如何调试"></a>部署出错如何调试</h2><p>如果在部署中出现任何错误，可以添加更多的Verbose来判断具体问题，有可能是kolla自身bug，也有可能是配置的问题，具体可以根据详细输出进行判断。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -vvv -i multinode deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>云原生趋势下的迁移与容灾思考</title>
    <link href="http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/"/>
    <id>http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/</id>
    <published>2020-10-18T12:05:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h1><h2 id="云原生发展趋势"><a href="#云原生发展趋势" class="headerlink" title="云原生发展趋势"></a>云原生发展趋势</h2><p>云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。</p><p>2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？</p><a id="more"></a><h2 id="为什么云上需要全新的迁移和容灾"><a href="#为什么云上需要全新的迁移和容灾" class="headerlink" title="为什么云上需要全新的迁移和容灾"></a>为什么云上需要全新的迁移和容灾</h2><h3 id="1、传统方案的局限性"><a href="#1、传统方案的局限性" class="headerlink" title="1、传统方案的局限性"></a>1、传统方案的局限性</h3><p>在这种大的趋势下，传统的迁移和容灾仍然停留在数据搬运的层次上，而忽略了面向云的特性和用户业务重新思考和构建。云计算的愿景是让云资源像水、电一样按需使用，所以基于云上的迁移和容灾也理应顺应这样的历史潮流。Snowflake也是通过这种商业模式的创新，成功打破旧的竞争格局。</p><p>为什么传统容灾的手段无法满足云原生需求呢？简单来说，二者关注的核心不同。传统的容灾往往以存储为核心，拥有对存储的至高无上的控制权。并且在物理时代，对于计算、存储和网络等基础架构层也没有有效的调度方法，无法实现高度自动化的编排。而基于云原生构建的应用，核心变成了云原生服务本身。当用户业务系统全面上云后，用户不再享有对底层存储的绝对控制权，所以传统的容灾手段，就风光不在了。</p><p><img src="/images/pasted-88.png" alt="upload successful"></p><p>我认为在构建云原生容灾的解决方案上，要以业务为核心去思考构建方法，利用云原生服务的编排能力实现业务系统的连续性。</p><h3 id="2、数据安全性"><a href="#2、数据安全性" class="headerlink" title="2、数据安全性"></a>2、数据安全性</h3><p>AWS CTO Werner Vogels曾经说过：Everything fails, all the time。通过AWS的责任共担模型，我们不难发现云商对底层基础架构负责，用户仍然要对自身自身数据安全性和业务连续性负责。</p><p><img src="/images/pasted-74.png" alt="upload successful"></p><p>我认为在云原生趋势下，用户最直接诉求的来自数据安全性即备份，而迁移、恢复、高可靠等都是基于备份表现出的业务形态，而备份能力可能是由云原生能力提供的，也有可能是第三方能力提供的，但最终实现业务形态，是由编排产生的。</p><p>用户上云并不等于高枕无忧，相反用户要学习云的正确打开方式，才能最大程度来保证业务的连续性。虽然云在底层设计上上是高可靠的，但是仍然避免不了外力造成的影响，例如：光缆被挖断、断电、人为误操作导致的云平台可用区无法使用，所以才有了类似“蓝翔决定了中国云计算稳定性”的调侃。我认为用户决定将业务迁移到云上的那一刻开始，备份、迁移、恢复、高可靠是一个连续的过程，如何合理利用云原生服务的特性实现业务连续性，同时进行成本优化，降低总体拥有成本（TCO）。</p><h3 id="3、防止厂商锁定"><a href="#3、防止厂商锁定" class="headerlink" title="3、防止厂商锁定"></a>3、防止厂商锁定</h3><p>某种意义上说，云原生的方向是新一轮厂商锁定，就像当年盛极一时的IOE架构一样，只不过现在换成了云厂商作为底座承载应用。在IOE时代，用户很难找到完美的替代品，但是在云时代，这种差异并不那么明显。所以大部分的客户通常选用混合云作为云建设策略，为了让应用在不同云之间能够平滑移动，利用容灾技术的迁移一定是作为一个常态化需求存在的。Gartnar也在多云管平台定义中，将迁移和DR作为单独的一项能力。充分说明迁移与容灾在多云环境的的常态化趋势。</p><p><img src="/images/pasted-82.png" alt="upload successful"></p><h1 id="云迁移与云容灾的关系"><a href="#云迁移与云容灾的关系" class="headerlink" title="云迁移与云容灾的关系"></a>云迁移与云容灾的关系</h1><h2 id="云迁移需求的产生"><a href="#云迁移需求的产生" class="headerlink" title="云迁移需求的产生"></a>云迁移需求的产生</h2><p>在传统环境下，迁移的需求并不十分突出，除非是遇到机房搬迁或者硬件升级，才会想到迁移，但这里的迁移更像是搬铁，迁移工具化与自动化的需求并不明显。当VMware出现后，从物理环境到虚拟化的迁移需求被放大，但由于是单一的虚拟化平台，基本上虚拟化厂商自身的工具就完全能够满足需求了。在虚拟化平台上，大家突然发现原来只能人工操作的物理环境一下子轻盈起来，简单来说，我们的传统服务器从一堆铁变成了一个文件，并且这个文件还能够被来回移动、复制。再后来，进入云时代，各家云平台风生水起，国内云计算市场更是百家争鸣，上云更是成为了一种刚性需求。随着时间的推移，出于对成本、厂商锁定等诸多因素的影响，在不同云之间的互相迁移更是会成为一种常态化的需求。</p><h2 id="底层技术一致"><a href="#底层技术一致" class="headerlink" title="底层技术一致"></a>底层技术一致</h2><p>这里提到的云迁移和容灾，并不是堆人提供的迁移服务，而是强调的高度自动化的手段。目标就是在迁移过程中保证业务连续性，缩短停机时间甚至不停机的效果。这里就借助了容灾的存储级别同步技术来实现在异构环境下的的“热迁移”。现有解决方案里，既有传统物理机搬迁时代的迁移软件，也有基于云原生开发的工具。但无论何种形式，都在不同程度上都解决了用户上云的基本诉求。最大的区别在于人效比，这一点与你的利益直接相关。</p><p>从另外一个角度也不难发现，所谓的迁移在正式切换之前实质上就是容灾的中间过程。同时，业务系统迁移到云平台后，灾备是一个连续的动作，这里既包含了传统的备份和容灾，还应该包含云上高可靠的概念。这样，用户业务系统在上云后，才能摆脱传统基础架构的负担，做到“零运维”，真正享受到云所带来的的红利。所以，我认为在云原生状态下，云迁移、云容灾、云备份本质上就是一种业务形态，底层采用的技术手段可以是完全一致的。</p><h2 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h2><p>在上述的痛点和趋势下，必然会出现一种全新的平台来帮助客户解决数据的安全性和业务连续性问题，今天就从这个角度来分析一下，在云原生的趋势下如何构建应用系统的迁移与容灾方案。</p><h1 id="云迁移发展趋势"><a href="#云迁移发展趋势" class="headerlink" title="云迁移发展趋势"></a>云迁移发展趋势</h1><h2 id="云迁移方式"><a href="#云迁移方式" class="headerlink" title="云迁移方式"></a>云迁移方式</h2><p>迁移是一项重度的咨询业务，网上各家云商、MSP都有自己的方法论，其实看下来差别都不大，之前也有很多人在分享相关话题，本文就不再赘述。这里我们重点讨论，在实际落地过程中到底该采用哪种工具，哪种方式的效率最高。所谓云迁移工具，就是将源端迁移至目标端，保证源端在目标端正确运行。常见的方式包括：物理机到虚拟化、虚拟化到虚拟化、物理机到云平台、虚拟化到云平台等。</p><p><img src="/images/pasted-62.png" alt="upload successful"></p><p>这是经典的6R迁移理论（现在已经升级为了7R，多了VMware出来搅局），在这个图中与真正迁移相关的其实只有Rehosting, Replatforming, Repurchasing和Refactoring，但是在这4R中，Refactoring明显是一个长期的迭代过程，需要用户和软件开发商共同参与解决，Repurchasing基本上与人为重新部署没有太大的区别。所以真正由用户或MSP在短期完成的只剩下Rehosting和Replatofrming。</p><p>与上面这张经典的迁移理论相比，我更喜欢下面这张图，这张图更能反应一个传统应用到云原生成长的全过程。与上述的结论相似，我们在真正拥抱云的时候，路径基本为上述的三条</p><ul><li>Lift &amp; Shift是Rehost方式的另一种称呼，这种方式路面最宽，寓意这条路是上云的最短路径，应用不需要任何改造直接上云使用</li><li>Evolve和Go Native都属于较窄的路径，寓意为相对于Rehost方式，这两条路径所消耗的时间更久，难度更高</li><li>在图的最右侧，三种形态是存在互相转换的可能，最终演进为彻底的云原生，寓意为迁移并不是一蹴而就，需要循序渐进完成</li></ul><p><img src="/images/pasted-61.png" alt="upload successful"></p><h2 id="重新托管（Rehost）方式"><a href="#重新托管（Rehost）方式" class="headerlink" title="重新托管（Rehost）方式"></a>重新托管（Rehost）方式</h2><p>常用的重新托管方式为冷迁移和热迁移，冷迁移往往涉及到步骤比较繁琐，需要大量人力投入，并且容易出错效率低，对业务连续性有较大的影响，不适合生产系统迁移。而热迁移方案基本都是商用化的解决方案，这里又分为块级别和文件级别，再细分为传统方案与云原生方案。</p><h3 id="冷迁移"><a href="#冷迁移" class="headerlink" title="冷迁移"></a>冷迁移</h3><p>我们先来看一下冷迁移的手动方案，以VMware到OpenStack为例，最简单的方式就是将VMware虚拟机文件(VMDK)通过qemu-img工具进行格式转换，转换为QCOW2或者RAW格式，上传至OpenStack Glance服务，再重新在云平台上进行启动。当然这里面需要进行virtio驱动注入，否则主机无法正常在云平台启动。这个过程中最耗时的应该是虚拟机文件上传至OpenStack Glance服务的过程，在我们最早期的实践中，一台主机从开始迁移到启动完成足足花了24小时。同时，在你迁移这段时间的数据是有增量产生的，除非你将源端关机等待迁移完成，否则，你还要将上述步骤重新来一遍。所以说这种方式真的不适合有业务连续性的生产系统进行迁移。</p><p>那如果是物理机的冷迁移方案怎么做呢？经过我们的最佳实践，这里为大家推荐的是老牌的备份工具CloneZilla，中文名为再生龙。是一款非常老牌的备份软件，常用于进行整机备份与恢复，与我们常见的Norton Ghost原理非常相似。CloneZilla从底层的块级别进行复制，可以进行整盘的备份，并且支持多种目标端，例如我们将磁盘保存至移动硬盘，实际格式就是RAW，你只需要重复上述的方案即可完成迁移。但是在使用CloneZilla过程中，需要使用Live CD方式进行引导，同样会面临长时间业务系统中断的问题，这也是上面我们提到的冷迁移并不适合生产环境迁移的原因。</p><p><img src="/images/pasted-63.png" alt="upload successful"></p><p><img src="/images/pasted-64.png" alt="upload successful"></p><h3 id="传统热迁移方案"><a href="#传统热迁移方案" class="headerlink" title="传统热迁移方案"></a>传统热迁移方案</h3><p>传统的热迁移方案基本分为块级别和文件级别，两者相似之处都是利用差量同步技术进行实现，即全量和增量交叉同步方式。</p><p>文件级别的热迁移方案往往局限性较大，并不能算真正的ReHost方式，因为前期需要准备于源端完全一样的操作系统，无法实现整机搬迁，从操作的复杂性更大和迁移的稳定性来说都不高。我们在Linux上常用的Rsync其实可以作为文件级别热迁移的一种解决方案。</p><p>真正可以实现热迁移的方案，还要使用块级别同步，降低对底层操作系统依赖，实现整机的搬迁效果。传统的块级别热迁移方案基本上来自于传统容灾方案的变种，利用内存操作系统WIN PE或其他Live CD实现，基本原理和过程如下图所示。从过程中我们不难发现这种方式虽然在一定程度解决了迁移的目标，但是作为未来混合云常态化迁移需求来说，仍然有以下几点不足：</p><ul><li>由于传统热迁移方案是基于物理环境构建的，所以我们发现在整个过程中人为介入非常多，对于使用者的技能要求比较高</li><li>无法满足云原生时代多租户、自服务的需求</li><li>安装代理是用户心中永远的芥蒂</li><li>一比一同步方式，从成本角度来说不够经济</li><li>最好的迁移验证方式，就是将业务系统集群在云端完全恢复，但是手动验证的方式，对迁移人力成本是再一次增加</li></ul><p><img src="/images/pasted-67.png" alt="upload successful"></p><h3 id="云原生热迁移方案"><a href="#云原生热迁移方案" class="headerlink" title="云原生热迁移方案"></a>云原生热迁移方案</h3><p>正是由于传统迁移方案的弊端，应运而生了云原生的热迁移方案，这一方面的代表厂商当属AWS在2019年以2.5亿美金击败Google Cloud收购的以色列云原生容灾、迁移厂商CloudEndure。</p><p>云原生热迁移方案是指利用块级别差量同步技术结合云原生API接口和资源实现高度自动化迁移效果，同时提供多租户、API接口满足混合云租户自服务的需求。我们先从原理角度分析一下，为什么相对于传统方案，云原生的方式能够满足高度自动化、用户自服务的用户体验。通过两个方案对比，我们不难发现云原生方式的几个优势：</p><ul><li>利用云原生API接口和资源，操作简便，完全取代了传统方案大量繁琐的人为操作，对使用者技术要求降低，学习陡峭程度大幅度降低</li><li>由于操作简便，迁移效率提高，有效提高迁移实施的人效比</li><li>一对多的同步方式，大幅度降低计算资源使用，计算资源只在验证和最终切换时使用</li><li>能够满足多租户、自服务的要求</li><li>源端也可以支持无代理方式，打消用户疑虑，并且适合大规模批量迁移</li><li>高度自动化的验证手段，在完成迁移切换前，能够反复进行验证</li></ul><p><img src="/images/pasted-69.png" alt="upload successful"></p><p>这是CloudEndure的架构图，当然你也可以利用CloudEndure实现跨区域的容灾。</p><p><img src="/images/pasted-70.png" alt="upload successful"></p><p>不过可惜的一点是由于被AWS收购，CloudEndure目前只能支持迁移至AWS，无法满足国内各种云迁移的需求。所以这里为大家推荐一款纯国产化的迁移平台——万博智云的HyperMotion( <a href="https://hypermotion.oneprocloud.com/" target="_blank" rel="noopener">https://hypermotion.oneprocloud.com/</a> )，从原理上与CloudEndure非常相似，同时支持了VMware及OpenStack无代理的迁移，更重要的是覆盖了国内主流的公有云、专有云和私有云的迁移。</p><p><img src="/images/pasted-71.png" alt="upload successful"></p><h2 id="平台重建（Replatforming）方式"><a href="#平台重建（Replatforming）方式" class="headerlink" title="平台重建（Replatforming）方式"></a>平台重建（Replatforming）方式</h2><p>随着云原生提供越来越多的服务，降低了应用架构的复杂度，使得企业能够更专注自己的业务本身开发。但是研发侧工作量的减少意味着这部分成本被转嫁到部署及运维环节，所以DevOps成为在云原生运用中比不可少的一个缓解，也让企业能够更敏捷的应对业务上的复杂变化。</p><p>正如上面所提到的，用户通过少量的改造可以优先使用一部分云原生服务，这种迁移方式我们成为平台重建（Replatforming），目前选择平台重建方式的迁移，多以与用户数据相关的服务为主。常见的包括：数据库服务RDS、对象存储服务、消息队列服务、容器服务等。这些云原生服务的引入，降低了用户运维成本。但是由于云原生服务自身封装非常严密，底层的基础架构层对于用户完全不可见，所以无法用上述Rehost方式进行迁移，必须采用其他的辅助手段完成。</p><p>以关系型数据库为例，每一种云几乎都提供了迁移工具，像AWS DMS，阿里云的DTS，腾讯云的数据传输服务DTS，这些云原生工具都可以支持 MySQL、MariaDB、PostgreSQL、Redis、MongoDB 等多种关系型数据库及 NoSQL 数据库迁移。以MySQL为例，这些服务都巧妙的利用了binlog复制的方式，实现了数据库的在线迁移。</p><p>再以对象存储为例，几乎每一种云都提供了自己的迁移工具，像阿里云的ossimport，腾讯云COS Migration工具，都可以实现本地到云端对象存储的增量迁移。但是在实际迁移时，还应考虑成本问题，公有云的对象存储在存储数据上比较便宜，但是在读出数据时是要根据网络流量和请求次数进行收费的，这就要求我们在设计迁移方案时，充分考虑成本因素。如果数据量过大，还可以考虑采用离线设备方式，例如：AWS的Snowball，阿里云的闪电立方等。这部分就不展开介绍，以后有机会再单独为大家介绍。</p><p><img src="/images/pasted-72.png" alt="upload successful"></p><p>如果选择平台重建方式上云，除了要进行必要的应用改造，还需要选择一款适合你的迁移工具，保证数据能够平滑上云。结合上面的Rehost方式迁移，能够实现业务系统的整体上云效果。由于涉及的服务较多，这里为大家提供一张迁移工具表格供大家参考。</p><p><img src="/images/pasted-89.png" alt="upload successful"></p><h1 id="云原生下的容灾发展趋势"><a href="#云原生下的容灾发展趋势" class="headerlink" title="云原生下的容灾发展趋势"></a>云原生下的容灾发展趋势</h1><p>目前为止，还没有一套平台能够完全满足云原生状态下的统一容灾需求，我们通过以下场景来分析一下，如何才能构建一套统一的容灾平台满足云原生的需求。</p><h2 id="传统架构"><a href="#传统架构" class="headerlink" title="传统架构"></a>传统架构</h2><p>我们以一个简单的Wordpress + MySQL环境为例，传统下的部署环境一般是这样架构的：</p><p><img src="/images/pasted-58.png" alt="upload successful"></p><p>如果为这套应用架构设计一套容灾方案，可以采用以下的方式：</p><ul><li>负载均衡节点容灾：负载均衡分为硬件和软件层面，硬件负载均衡高可靠和容灾往往通过自身的解决方案实现。如果是软件负载均衡，往往需要安装在基础操作系统上，而同城的容灾可以使用软件高可靠的方式实现，而异地的容灾往往是通过提前建立对等节点，或者干脆采用容灾软件的块或者文件级别容灾实现。是容灾切换（Failover）很重要的一个环节。</li><li>Web Server的容灾：Wordpress的运行环境无非是Apache + PHP，由于分离了用于存放用户上传的文件系统，所以该节点几乎是无状态的，通过扩展节点即可实现高可靠，而异地容灾也比较简单，传统的块级别和文件级别都可以满足容灾的需求</li><li>共享文件系统的容灾，图中采用了Gluster的文件系统，由于分布式系统的一致性通常由内部维护，单纯使用块级别很难保证节点的一致性，所以这里面使用文件级别容灾更为精确</li><li>数据库的容灾，单纯依靠存储层面是无法根本实现数据库0丢失数据的，所以一般采用从数据库层面实现，当然如果为了降低成本，数据库的容灾可以简单的使用周期Dump数据库的方式实现，当然如果对可靠性要求较高，还可以使用CDP方式实现</li></ul><p>从以上的案例分析不难看出，传统基础架构下的容灾往往以存储为核心，无论是磁盘阵列的存储镜像，还是基于I/O数据块、字节级的捕获技术，结合网络、数据库和集群的应用级别技术完成高可靠和容灾体系的构建。在整个容灾过程的参与者主要为：主机、存储、网络和应用软件，相对来说比较单一。所以在传统容灾方案中，如何正确解决存储的容灾也就成为了解决问题的关键。</p><h2 id="混合云容灾"><a href="#混合云容灾" class="headerlink" title="混合云容灾"></a>混合云容灾</h2><p>这应该是目前最常见的混合云的方案，也是各大容灾厂商主推的一种方式。这里我们相当于将云平台当成了一套虚拟化平台，几乎没有利用云平台任何特性。在恢复过程中，需要大量人为的接入才能将业务系统恢复到可用状态。这样的架构并不符合云上的最佳实践，但的确是很多业务系统备份或迁移上云后真实的写照。</p><p><img src="/images/pasted-83.png" alt="upload successful"></p><p>这样的架构确实能解决容灾的问题，但是从成本上来说很高，现在我们来换一种方式。我们利用了对象存储和数据库进行一次优化。我们将原有存储服务存放至对象存储中，而使用数据传输服务来进行实时的数据库复制。云主机仍然采用传统的块级别进行同步。一旦出现故障，则需要自动化编排能力，重新将备份进行恢复，在最短时间内根据我们预设的方案进行恢复，完成容灾。</p><p><img src="/images/pasted-84.png" alt="upload successful"></p><h2 id="云上同城容灾架构"><a href="#云上同城容灾架构" class="headerlink" title="云上同城容灾架构"></a>云上同城容灾架构</h2><p>上述的备份方式，实质上就是利用平台重建的方式进行的迁移，既然已经利用迁移进行了备份，那完全可以对架构进行如下改造，形成同城的容灾架构。我们根据云平台的最佳实践，对架构进行了如下调整：</p><p><img src="/images/pasted-85.png" alt="upload successful"></p><p>这个架构不仅实现了应用级高可靠，还能够支撑一定的高并发性，用户在最少改造代价下就能够在同城实现双活的效果。我们来分析一下在云上利用了多少云原生的服务：</p><ul><li>域名解析服务</li><li>VPC服务</li><li>负载均衡服务</li><li>自动伸缩服务</li><li>云主机服务</li><li>对象存储服务</li><li>关系型数据库RDS服务</li></ul><p>除了云主机外，其他服务均是天然就支持跨可用区的高可用特性，对于云主机我们可以制作镜像方式，由自动伸缩服务负责实例的状态。由于云上可用区就是同城容灾的概念，这里我们就实现了同城的业务系统容灾。</p><p>经过调整的架构在一定程度上满足了业务连续性的要求，但是对于数据的安全性仍然缺乏保障。近几年，勒索病毒横行，大量企业为此蒙受巨大损失，所以数据备份是上云后必须实施的。云原生服务本身提供了备份方案，例如云主机的定期快照等，但往往服务比较分散，不容易统一进行管理。同时，在恢复时往往也是只能每一个服务进行恢复，如果业务系统规模较大，也会增加大量的恢复成本。虽然云原生服务解决了自身备份问题，但是将备份重新组织成应用是需要利用自动化的编排能力实现。</p><h2 id="同云异地容灾架构"><a href="#同云异地容灾架构" class="headerlink" title="同云异地容灾架构"></a>同云异地容灾架构</h2><p>大部分的云原生服务都在可用区内，提供了高可靠能力，但是对于跨区域上通常提供的是备份能力。例如：可以将云主机变为镜像，将镜像复制到其他区域内；关系型数据库和对象存储也具备跨域的备份能力。利用这些组件自身的备份能力，外加上云自身资源的编排能力，我们可以实现在容灾可用域将系统恢复至可用状态。那如何触发切换呢？</p><p>这里我们根据业务系统的特点，在云原生的监控上定制告警，利用告警平台的触发能力触发函数计算，完成业务系统的跨域切换，形成异地容灾的效果。</p><p><img src="/images/pasted-86.png" alt="upload successful"></p><h2 id="跨云容灾"><a href="#跨云容灾" class="headerlink" title="跨云容灾"></a>跨云容灾</h2><p>但跨云容灾不像同云容灾时，在不同的可用区之间至少服务是一致的，那么此时，在同云上使用的方法基本失效，完全需要目标云平台的能力或者中立的第三方的解决方案。这里除了数据的备份，还有一点是服务配置的互相匹配。才能完全满足跨云容灾恢复的需求。另外需要考虑的一点就是成本为例，以对象存储为例，是典型的的“上云容易下云难”。所以如何利用云原生资源特性合理设计容灾方案是对成本的极大考验。</p><p><img src="/images/pasted-87.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>云原生容灾还处于早期阶段，目前尚没有完整的平台能够支持以上各种场景的容灾需求，是值得持续探索的话题。云原生容灾以备份为核心，以迁移、恢复和高可靠为业务场景，实现多云之间的自由流转，最终满足用户的业务需求。</p><p>所以，作为面向云原生的容灾平台要解决好三方面的能力：</p><p>一、以数据为核心，让数据在多云之间互相流转。数据是用户核心价值，所以无论底层基础架构如何变化，数据备份一定是用户的刚醒需求。对于不同云原生服务如何解决好数据备份，是数据流转的必要基础。</p><p>二、利用云原生编排能力，实现高度自动化，在数据基础上构建业务场景。利用自动化编排能力实现更多的基于数据层的应用，帮助用户完成更多的业务创新。</p><p>三、灵活运用云原生资源特点，降低总体拥有成本。解决传统容灾投入巨大的问题，让用户的成本真的能像水、电一样按需付费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;趋势&quot;&gt;&lt;a href=&quot;#趋势&quot; class=&quot;headerlink&quot; title=&quot;趋势&quot;&gt;&lt;/a&gt;趋势&lt;/h1&gt;&lt;h2 id=&quot;云原生发展趋势&quot;&gt;&lt;a href=&quot;#云原生发展趋势&quot; class=&quot;headerlink&quot; title=&quot;云原生发展趋势&quot;&gt;&lt;/a&gt;云原生发展趋势&lt;/h2&gt;&lt;p&gt;云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。&lt;/p&gt;
&lt;p&gt;2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
      <category term="云原生" scheme="http://sunqi.site/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
      <category term="云迁移" scheme="http://sunqi.site/tags/%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    
      <category term="云容灾" scheme="http://sunqi.site/tags/%E4%BA%91%E5%AE%B9%E7%81%BE/"/>
    
      <category term="Cloud Native" scheme="http://sunqi.site/tags/Cloud-Native/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack对接多Ceph资源池</title>
    <link href="http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/"/>
    <id>http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/</id>
    <published>2020-09-14T08:52:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：</p><ul><li>Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系</li><li>Libvirt配置：配置与Ceph之间的鉴权关系</li></ul><a id="more"></a><h1 id="Cinder配置"><a href="#Cinder配置" class="headerlink" title="Cinder配置"></a>Cinder配置</h1><p>其中rbd_secret_uuid可以使用uuidgen命令生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;etc&#x2F;cinder&#x2F;cinder.conf</span><br><span class="line">[DEFAULT]</span><br><span class="line">......</span><br><span class="line"># 与下面的段落对应</span><br><span class="line">enabled_backends &#x3D; rbd-1, rbd-2</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line"></span><br><span class="line"># 与上面的enabled_backends对应</span><br><span class="line">volume_backend_name &#x3D; rbd-1</span><br><span class="line"></span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line"></span><br><span class="line"># 需要从Ceph集群拷贝这两个配置文件到相应目录</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-1.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder1.keyring</span><br><span class="line"></span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 5774b929-0690-4513-a1f7-41aac49cbb31</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br><span class="line"> </span><br><span class="line">[rbd-2]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name &#x3D; rbd-2</span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-2.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder2.keyring</span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 0563c419-bc4c-4794-972a-685498248869</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br></pre></td></tr></table></figure><h2 id="建立与Volume-Type对应关系"><a href="#建立与Volume-Type对应关系" class="headerlink" title="建立与Volume Type对应关系"></a>建立与Volume Type对应关系</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cinder type-create rbd-1</span><br><span class="line">cinder type-key rbd-1 set volume_backend_name&#x3D;rbd-1</span><br><span class="line">cinder extra-specs-list</span><br><span class="line"></span><br><span class="line">cinder type-create rbd-2</span><br><span class="line">cinder type-key rbd-2 set volume_backend_name&#x3D;rbd-2</span><br><span class="line">cinder extra-specs-list</span><br></pre></td></tr></table></figure><h1 id="Libvirt配置"><a href="#Libvirt配置" class="headerlink" title="Libvirt配置"></a>Libvirt配置</h1><p>在/etc/libvirt/secretes建立与上述rbd_secret_uuid同名的两个文件，后缀为.xml和.base64，两个文件的内容为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">&lt;secret ephemeral&#x3D;&#39;no&#39; private&#x3D;&#39;no&#39;&gt;</span><br><span class="line">  &lt;uuid&gt;5774b929-0690-4513-a1f7-41aac49cbb31&lt;&#x2F;uuid&gt;</span><br><span class="line">  &lt;usage type&#x3D;&#39;ceph&#39;&gt;</span><br><span class="line">    &lt;name&gt;client.cinder1 secret&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;&#x2F;usage&gt;</span><br><span class="line">&lt;&#x2F;secret&gt;</span><br></pre></td></tr></table></figure><p>其中base64文件的内容就是keyring文件中key的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.admin]</span><br><span class="line">key &#x3D; AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>最后执行如下命令完成配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh secret-define --file 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">virsh secret-set-value --secret 5774b929-0690-4513-a1f7-41aac49cbb31 --base64 $(cat 5774b929-0690-4513-a1f7-41aac49cbb31.base64)</span><br><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系&lt;/li&gt;
&lt;li&gt;Libvirt配置：配置与Ceph之间的鉴权关系&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="Ceph" scheme="http://sunqi.site/tags/Ceph/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>快速构建Ceph集群</title>
    <link href="http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/"/>
    <id>http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/</id>
    <published>2020-09-12T09:05:53.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<p>虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。</p><p>本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。</p><a id="more"></a><h1 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h1><p>我们准备四台服务器，其中一台作为部署发起节点和后续Client节点使用。另外三台作为Ceph节点使用，其中第一台节点node01上，除了monitor和osd外，还将运行Manager, MDS和RGW服务，用于提供文件及对象存储服务。每一台Ceph节点都另外挂载了一块单独的磁盘，由于我使用的是虚拟机环境，所以挂载节点为/dev/vdb，如果使用是其他环境需要注意挂载点名称。</p><p><img src="/images/pasted-58-1.png" alt="upload successful"></p><h1 id="部署时序图"><a href="#部署时序图" class="headerlink" title="部署时序图"></a>部署时序图</h1><p>使用Ceph Deploy将大幅度简化安装过程，大体上分为以下安装步骤：</p><ul><li>节点初始化配置</li><li>Ceph Deploy节点安装</li><li>Ceph集群初始化</li><li>ODS节点安装、安装Mgr服务及添加ODS磁盘，完成Ceph基本安装</li><li>CephFS安装，部署Metadata服务</li><li>Ceph RGW安装，部署RGW服务</li></ul><p><img src="/images/pasted-59-1.png" alt="upload successful"></p><h1 id="（全部节点）环境准备"><a href="#（全部节点）环境准备" class="headerlink" title="（全部节点）环境准备"></a>（全部节点）环境准备</h1><p>这是我非常常用的针对CentOS 7的设置，为了测试方便，关闭了防火墙、SELINUX，同时更新了系统和EPEL源为阿里源，最后进行系统更新，保证系统软件包更新到最新版本后，再进行环境安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><p>如果按照正常流程安装后，执行ceph -s，会出现restful模块无法找到，缺少pecan的安装包，所以在初始化阶段直接将缺少的包进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><h1 id="Ceph-Deploy节点安装"><a href="#Ceph-Deploy节点安装" class="headerlink" title="Ceph-Deploy节点安装"></a>Ceph-Deploy节点安装</h1><h2 id="（Ceph-Deploy节点）安装Ceph-Deploy"><a href="#（Ceph-Deploy节点）安装Ceph-Deploy" class="headerlink" title="（Ceph Deploy节点）安装Ceph-Deploy"></a>（Ceph Deploy节点）安装Ceph-Deploy</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h2 id="（全部节点）设置时间同步服务"><a href="#（全部节点）设置时间同步服务" class="headerlink" title="（全部节点）设置时间同步服务"></a>（全部节点）设置时间同步服务</h2><p>时间同步服务是分布式系统的生命线，所以安装时候先要安装NTP或者Chrony。在RHEL 7中，默认的时间同步被替换为Chrony，很多新的安装文档中也开始使用Chrony作为时间同步服务，但是NTP也被同时保留。我的环境中Chrony已经被安装并启动，如果没有请自行安装。</p><h2 id="（Ceph-Deploy节点）无密码登录"><a href="#（Ceph-Deploy节点）无密码登录" class="headerlink" title="（Ceph Deploy节点）无密码登录"></a>（Ceph Deploy节点）无密码登录</h2><p>这里为了简便，使用了root用户进行安装。配置完成后，需要让Ceph Deploy能够无密码的方式访问全部Ceph节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@node1</span><br><span class="line">ssh-copy-id root@node2</span><br><span class="line">ssh-copy-id root@node3</span><br></pre></td></tr></table></figure><h1 id="Ceph集群安装"><a href="#Ceph集群安装" class="headerlink" title="Ceph集群安装"></a>Ceph集群安装</h1><h2 id="（Ceph-Deploy节点）Ceph块存储服务安装"><a href="#（Ceph-Deploy节点）Ceph块存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph块存储服务安装"></a>（Ceph Deploy节点）Ceph块存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">ceph-deploy new node01</span><br><span class="line">ceph-deploy install node01 node02 node03</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin node01 node02 node03</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create node01</span><br><span class="line"></span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node01</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node02</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node03</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>由于默认采用了Bluestore安装方式，如果想使用SSD作为block.db和block.wal，可以这样创建OSD</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb --block-db &#x2F;dev&#x2F;vdc --block-wal &#x2F;dev&#x2F;vdc node01</span><br></pre></td></tr></table></figure><p>在Ceph Deploy节点，将Ceph相关配置文件拷贝至系统的/etc/ceph目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.conf &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.client.admin.keyring &#x2F;etc&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="（Ceph-Deploy节点）增加多个Monitor节点"><a href="#（Ceph-Deploy节点）增加多个Monitor节点" class="headerlink" title="（Ceph Deploy节点）增加多个Monitor节点"></a>（Ceph Deploy节点）增加多个Monitor节点</h3><p>添加多个Monitor节点，可以实现高可靠，但是一定为奇数。先更新配置文件，在刚才初始化集群目录下的ceph.conf中的mon_host添加所有节点IP，之后设定public network，这里我们使用了Ceph节点的网段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ceph.conf</span><br><span class="line"># ...</span><br><span class="line">mon_host &#x3D; 192.168.10.105,192.168.10.176,192.168.10.139</span><br><span class="line">public network &#x3D; 192.168.10.1&#x2F;24</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure><p>分发配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin node01 node02 node03</span><br><span class="line">ceph-deploy mon add node02</span><br><span class="line">ceph-deploy mon add node03</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查quorum状态</span><br><span class="line">ceph quorum_status --format json-pretty</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph文件系统服务安装"><a href="#（Ceph-Deploy节点）Ceph文件系统服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph文件系统服务安装"></a>（Ceph Deploy节点）Ceph文件系统服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds create node01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 添加多个Manager服务，Manager采用的是主从模式</span><br><span class="line">ceph-deploy mgr create node02 node03</span><br><span class="line"></span><br><span class="line"># 可以看到Manager主从节点状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>如果发现Ceph Monitor节点启动失败，需要到相应的节点上查看失败原因，比如我的Monitor使用Start启动，返回这样的提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Job for ceph-mon@node02.service failed because start of the service was attempted too often. See &quot;systemctl status ceph-mon@node02.service&quot; and &quot;journalctl -xe&quot; for details.</span><br><span class="line">To force a start use &quot;systemctl reset-failed ceph-mon@node02.service&quot; followed by &quot;systemctl start ceph-mon@node02.service&quot; again.</span><br></pre></td></tr></table></figure><p>按照提示重新启动即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl reset-failed ceph-mon@node02.service</span><br><span class="line">systemctl start ceph-mon@node02.service</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph对象存储服务安装"><a href="#（Ceph-Deploy节点）Ceph对象存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph对象存储服务安装"></a>（Ceph Deploy节点）Ceph对象存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy rgw create node01</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="块存储测试"><a href="#块存储测试" class="headerlink" title="块存储测试"></a>块存储测试</h2><h3 id="建立存储空间"><a href="#建立存储空间" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h3><h2 id="文件系统测试"><a href="#文件系统测试" class="headerlink" title="文件系统测试"></a>文件系统测试</h2><h3 id="建立存储空间-1"><a href="#建立存储空间-1" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data 16</span><br><span class="line">ceph osd pool create cephfs_metadata 16</span><br><span class="line"></span><br><span class="line"># ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">ceph fs ls</span><br></pre></td></tr></table></figure><h3 id="内核方式挂载"><a href="#内核方式挂载" class="headerlink" title="内核方式挂载"></a>内核方式挂载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">mount -t ceph 192.168.10.11:6789:&#x2F; &#x2F;mnt&#x2F;mycephfs -o name&#x3D;admin,secretfile&#x3D;&#x2F;etc&#x2F;ceph&#x2F;admin.secret</span><br></pre></td></tr></table></figure><h3 id="Fuse方式挂载"><a href="#Fuse方式挂载" class="headerlink" title="Fuse方式挂载"></a>Fuse方式挂载</h3><p>确保/etc/ceph下面已经拷贝了ceph.conf和keyring文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">ceph-fuse -m 192.168.10.11:6789 &#x2F;mnt&#x2F;mycephfs</span><br></pre></td></tr></table></figure><h2 id="对象存储测试"><a href="#对象存储测试" class="headerlink" title="对象存储测试"></a>对象存储测试</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。&lt;/p&gt;
&lt;p&gt;本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>一款云迁移产品的成长史</title>
    <link href="http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/"/>
    <id>http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/</id>
    <published>2020-08-11T06:04:00.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h1><p>孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。</p><h1 id="关于万博智云"><a href="#关于万博智云" class="headerlink" title="关于万博智云"></a>关于万博智云</h1><p>万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。</p><p>万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。</p><a id="more"></a><h1 id="结缘云迁移"><a href="#结缘云迁移" class="headerlink" title="结缘云迁移"></a>结缘云迁移</h1><p>2011年开始，我一直从事OpenStack在企业私有云应用的研发工作。从2011年一直到2018年，是开源社区最为活跃的时间段，各个公司将自己的主要精力全部投入到OpenStack各个模块的优化中。当时建设私有云平台所提供的服务往往是全方位的，从系统集成、安装实施再到后面的运行维护和定制化开发，基本上就是一整套全栈式解决方案，甚至有时候云平台之上的业务系统出问题，客户也会来找你。这对于任何尚处于初创型规模的OpenStack公司往往是个巨大的挑战。</p><p>2016年的时候，我们为一家农商行客户建设私有云，经过反复的前期验证，最终在2016年底拿下了该项目。当时除了建设云平台的需求外，还有一项作为验收标准的需求是将用户原有运行在各种物理机的业务系统平稳的迁移到新的云平台上，迁移过程不能对现有业务产生任何影响。最后还要将旧的硬件进行必要升级后，重新加入到新的云平台。</p><p>回想起当时云平台的建设过程，架构上并不复杂，就是一个典型的OpenStack使用硬件存储再加上VLAN的简单模式。在实际的项目实施中，从硬件到货到上架安装，再到云平台部署完成，前前后后的时间大约在三周左右。但是由于用户对于热迁移和资源回收的需求，整个项目实际耗时竟然长达半年之久。由于客户所处的位置不直通高铁，我们的工程师从北京出发，要不就是坐一夜的绿皮火车，要不就先高铁到徐州再转长途车的方式。无论哪种方式，路上的时间至少要8个小时以上。从方案验证到最终实施完毕，团队内全体成员总共出差次数超过50次以上，最终的实施成本极高。当我们尝试复盘整个过程时，耗时最久的其实就是解决各种迁移过程中产生的问题。</p><h1 id="挫折中前行"><a href="#挫折中前行" class="headerlink" title="挫折中前行"></a>挫折中前行</h1><p>这个客户的业务系统属于典型的老旧型业务系统，运行在物理机加上硬件存储阵列上，有少量的虚拟化环境，操作系统也是五花八门，最多的是SUSE 11，还有Windows 2003，CentOS等，数据库有DB2，Oracle，还有少量的MySQL。</p><p>由于是银行系统，所以对于业务连续性有非常强烈的诉求，在迁移上对我们提出了以下几点要求：</p><p>第一，风险控制。在任何行业中，稳定、可靠是当仁不让的第一原则，对于关乎民生的金融行业更是如此。所以在实际云平台建设过程中，原有业务系统上云时往往受到的阻力最大。究其原因就是在上云过程中没有一套完整的、科学的方法论及工具让用户打消对上云的顾虑。所以在向云迁移过程中，系统必须是可验证、可回退的。在正式切换到云平台之前，需要让业务系统在云平台之上得到充分的验证；在切换到云平台后，如果一旦发生失败，要马上能够回退到原有系统，继续提供服务。保障在云迁移过程中，风险降到最低。</p><p>第二、保障业务连续性。农商行不同于传统的四大行或者城商行，在IT建设上往往有很大的自主权，除了核心交易系统外，其他的业务系统均运行在本地系统上，所以对本地运维能力提出比较高的要求。在迁移过程中，本地业务系统的连续性非常重要，一旦中断银行就无法开门做生意了。同时，根据银监会印发的相关规定：在业务服务时段导致业务无法正常开展达半个小时(含)以上，属于重大运营中断事件。所以基本上迁移的切换时间窗口，只能在晚间进行，但是晚上银行又会有数据下发、跑批等程序的运行，所以留给迁移的时间窗口非常有限，所以必须采用一种近似于热迁移的效果来满足客户的需求。</p><p>第三，减少人为干预，保障迁移的可靠性。由于很多系统属于服务厂商开发，部分应用时间久远，甚至很多服务厂商已经不存在了，所以迁移过程中尽量减少对应用厂商的依赖很关键，比如重装、重新配置都会导致应用无法运行。同时，在迁移过程中，由于步骤非常复杂，人为操作过多非常容易产生错误。</p><p>在这个过程中，我们走了非常多的弯路，比如从最早采用冷迁移方式的Clonezilla，耗时24个小时才能迁移完一台主机；再比如调研了各种开源的p2v和v2v工具，没有一个好用的；再比如为了解决UEFI启动的问题，修改nova代码，但是加载后发现一台服务器启动过程黑屏了半个小时之久，为了这一个系统我们往返于北京和客户多达五次。这些困难促使我们不得不停下来思考，为什么一个看似简单的迁移，最终却成为影响项目进度和成本的关键因素呢？</p><h1 id="从项目中来，在项目中成长"><a href="#从项目中来，在项目中成长" class="headerlink" title="从项目中来，在项目中成长"></a>从项目中来，在项目中成长</h1><p>为了解决在项目中遇到的问题，我们尝试了各种手段，最终我们发现灾备领域的数据读取技术加上云原生的方式是最佳的组合方案。使用灾备的块级别差量复制技术能够充分保障业务连续性，而最大程度利用云平台原生接口和资源能够实现”两点之间直线最短“的效果，保障迁移的可靠性，大幅度降低人为介入而带来的不确定性，最后二者叠加的效果最终满足了风险可控的终极目标。</p><p>通过2016和2017年近两年的磨练，一个面向OpenStack的热迁移产品具备了初步产品雏形。在紧接着到来的2018年我们迎来了又一次大考，这一次我们面对着是江苏省农信的专有云平台的大规模迁移，我们需要将该省内全部62家二级法人的业务系统迁移上云。很快我们中标的兴奋就淹没在新的困难面前。在之前的项目中，我们的所有迁移行为都是在本地数据中心完成的，至少所有的网络基本都是千兆的。但是在这个项目中，省端和各个二级法人之间的连接变成了以10Mbps的专线，并且这还是最好的情况，还有更糟糕的只有2Mbps。省端与二级法人的专线连接主要用于省端的数据下发，所以用于迁移的数据传输只能在特定时间段进行，同时不能将全部的带宽占满，以防影响业务。但是，每个二级法人的用户数据量很大，大约在30TB - 50TB左右，如果完全依赖网络传输，理论上需要传上一年多的时间。所以完全依赖于网络传输是不可能的，我们需要的是一种硬件加网络的组合方案，由硬件保存全量数据，通过运输方式到省端，将全量数据切换至云端后，再通过网络传输增量，这样形成的效果仍然是热迁移，但是迁移的速度明显提高。</p><p>在解决了大规模数据传输后，我们紧接着遇到的问题就是先迁哪个，后迁哪个？我们都知道应用系统是存在一定的依赖关系的，所以在迁移前必须要梳理清楚应用系统的拓扑结构，同时还要对迁移后的网络、应用配置等变更做出预先分析，保障万无一失。这个过程其实就是在众多迁移方法论中提到的调研分析阶段。在这个过程中，我们也在实践中积累了自己的迁移调研方法和实施方案，对我们后来的项目起到了很大的帮助作用。同时我们也意识到，迁移绝对不是一个工具就解决的问题，而是一个重度的咨询过程，迁移工具只不过解决了最后一公里的问题。</p><p>从2018年初开始，我们和用户方组成的江苏省农信业务专家组，深入每个地市，严格遵照调研、评审、实施、切换进行科学的上云。从基本的系统信息采集、整理到业务系统上下联分析，绘制拓扑图，安全性等进行全面评估，之后根据调研的结论整理实施方案、进度，实施方案中要将一切在迁移后的变更提前进行整理，确保迁移过程中万无一失。通过辅助物理设备进行全量数据拷贝，运输到省端后进行切换上云，最终在合适的时间点完成增量及业务切换过程。在2018年下半年，平均一周就可以有三家农商行的业务系统实现全面上云。</p><p>在这个项目中，我们的产品得到了极大的锤炼，经受了大规模迁移的考验。通过专有云的建设和业务系统迁移，3年共为江苏农信节省IT投资5.6亿元。截止2018年9月30日，总共完成54家二级法人共1200多套系统迁移。同时，云平台的从最初的15个节点增长到了130多个节点，存储从0.2PB增长至3PB。</p><h1 id="从一朵云到一片云"><a href="#从一朵云到一片云" class="headerlink" title="从一朵云到一片云"></a>从一朵云到一片云</h1><p>时间到了2019年，我们产品的云原生的理念逐步得到了更多客户的认可，同时这种基于云原生构建的高度自动化的效果正好填补了云迁移这个市场空白。甚至某些老牌的灾备厂商把我们当成迁移竞争对手，直接在软文中进行”诋毁“，不过这一切恰好证明了我们产品所蕴含的巨大价值。</p><p>但是只能支持单云的迁移已经无法满足市场上越来越多的云迁移需求，所以在2019年上半年，我们准备全面支持更多的公有云和专有云平台。我们首先选择了国内的最大的公有云提供商——阿里云。阿里云在最近10年已经成长为中国云计算领域的标杆，拥有极高的市场占有率，同时提供了最广泛的API接口支持，为合作伙伴提供最大程度的赋能。由于阿里云与OpenStack在一些机制上存在差异，我们通过近3个月的调研和开发，终于突破了阿里云的热迁移。接下来，我们对云平台的支持范围不断扩大，又用了四个月左右时间，覆盖了国内绝大多数的公有云、专有云和私有云平台，成为了名副其实的多云迁移。</p><h1 id="打造极致的用户体验"><a href="#打造极致的用户体验" class="headerlink" title="打造极致的用户体验"></a>打造极致的用户体验</h1><p>很多企业级产品留给人的第一印象就是专业且复杂，不培训你两天你都不会用。在云迁移领域也是如此，很多云迁移产品都是由传统灾备厂商对原有灾备软件进行简单改造后的产物，界面复杂不说，操作还极其繁琐，迁移一台主机下来，十几个、二十几个步骤那是基本配置。所以在我们对产品进行迭代时，希望用To C的思维打造To B的产品。</p><p>在初始阶段，用户只要根据向导配置源端和目标端的信息后，就可以进入迁移流程。我们将迁移流程分成了三个简单的步骤：选择主机、同步数据和开始迁移。通过高度自动化的流程和对云原生API及资源的巧妙利用，初级的Linux工程师基本上几分钟就能完全上手。同时由于自动化程度高，在批量迁移时优势非常明显。</p><p>全新UI.png<br><img src="/images/pasted-52.png" alt="upload successful"></p><p>由于之前一直从事的是私有云领域的产品研发，导致我们的研发团队在产品开发中存在一种惯性。为了满足私有化部署的需要，我们往往需要将安装包做成无网络依赖的ISO格式。这直接导致的后果就是用户在试用我们的产品时往往需要先花很长一段时间去下载我们的安装介质，之后是安装，最后才能试用。这个一来一回的过程，往往就是一天的时间被浪费了。这一点在公有云迁移时，会让人觉得更加繁琐，所以在2019年下半年，我们决定将我们的产品SaaS化，让用户更快速的体验我们的产品而非将时间浪费在安装的环节上。由于人力资源的限制，研发团队和运维团队都受到了极大的挑战。研发团队需要开发新的模块以支持运营、多租户等SaaS需求，同时还要对原有的通讯模式进行改造，避免双向通讯的发生；而实施团队需要兼顾私有项目和线上运维，这就要求平台稳定、高可靠、易运维，所以对云原生的应用就变得尤为关键。我们利用阿里云的Kubernetes容器服务和各种云原生组件完成了SaaS化的改造，在没有增加任何人力的情况下，在2020年初完成SaaS的全面上线。</p><h1 id="在巨人肩膀上一起成长"><a href="#在巨人肩膀上一起成长" class="headerlink" title="在巨人肩膀上一起成长"></a>在巨人肩膀上一起成长</h1><p>2019年初，AWS斥资2.5亿美金收购了以色列灾备初创公司CloudEndure，虽然这家公司以灾备公司名义被收购，但主要业务却是提供向AWS的迁移服务。我们的产品在设计理念和用户体验上与CloudEndure非常相似，同时我们的产品可以支持国内众多的不同的云厂商。</p><p>AWS对CloudEndure的收购给了我们非常大的信心，让我们坚定了走云原生迁移、灾备产品的思路。我们发现这个市场在国内基本上属于空白阶段，虽然传统灾备厂商的工具可以靠堆人解决项目上的问题，但是真正让用户自助式的迁移平台才能让用户自主分配在云端的负载，让云资源得到更快速的消耗，最终让云厂商获益。</p><p>于是一个大胆的想法在脑海中形成，能不能把我们的迁移软件以云原生服务的方式集成在公有云平台中呢？经过几番周折，我们开始与阿里云进行接触。非常感谢阿里云的陈绪博士帮我打开了和阿里云团队的合作大门，在2019年与阿里云对接完成后，我们首先迎来了就是阿里云ECS团队的考验，在对产品充分测试后，我们在杭州与阿里云生态合作伙伴团队、投资部门进行了会面，这次会面彻底打开了我们与阿里云的合作大门。</p><p>2019年底，我被评为阿里云解决方案领域MVP，进一步促进了我们与阿里云之间的合作。2020年初，阿里云控制台上的应用工具市场吸引了我的目光。这种与阿里云深度整合的方式，对于云原生迁移、灾备是绝佳的栖息之地。通过阿里云MVP运营团队的引荐，我们成功的和阿里云应用工具市场团队进行了对接，同时在2月底决定上架阿里云应用工具市场。</p><p><img src="/images/pasted-53.png" alt="upload successful"></p><p>上架阿里云应用工具市场的过程绝非一帆顺利，阿里云对此有严格的安全性要求，上线前必须要通过阿里云安全部门的严格审查。为此，我们做了一些架构上的调整和安全性的加固。最终经过近3个月的努力，终于将我们的平台与2020年7月10日晚8点正式上线。上架后的迁移平台，与阿里云的用户体验保持完全一致。用户使用时毫无违和感。</p><p><img src="/images/pasted-54.png" alt="upload successful"></p><p>紧接着通过MVP运营团队与阿里云Apsara Stack团队取得了联系，开始对接Apsara Stack专有云，截止到8月初已经彻底实现了对Apsara Stack自动化迁移的全面支持。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>2020年4月，国家提出了新基建的发展目标，首当其冲的就是信息基础设施，而云计算作为新基建的底座，重要性不言而喻。2020年初的疫情，让全社会意识到”云上社会“的重要性，可以预见的一点是，全面云化的时代正在到来。</p><p>通过与阿里云的全面合作，为我们的产品带来了顶级流量入口，获取客户信任的时间更短。未来，我们也会将我们的产品打造成基于云原生的备份、容灾产品，为更多的云客户提供完美的用户体验。欢迎各位有志之士加入我们的团队，也欢迎有需求的客户加入我们的迁移群参与讨论（关注微信公众号后回复”支持“）。</p><img src="/images/pasted-55.png" alt="万博智云" title="万博智云" width="300" />]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于作者&quot;&gt;&lt;a href=&quot;#关于作者&quot; class=&quot;headerlink&quot; title=&quot;关于作者&quot;&gt;&lt;/a&gt;关于作者&lt;/h1&gt;&lt;p&gt;孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。&lt;/p&gt;
&lt;h1 id=&quot;关于万博智云&quot;&gt;&lt;a href=&quot;#关于万博智云&quot; class=&quot;headerlink&quot; title=&quot;关于万博智云&quot;&gt;&lt;/a&gt;关于万博智云&lt;/h1&gt;&lt;p&gt;万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。&lt;/p&gt;
&lt;p&gt;万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="趋势分析" scheme="http://sunqi.site/tags/%E8%B6%8B%E5%8A%BF%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>Kubernetes All-in-One环境安装</title>
    <link href="http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/"/>
    <id>http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/</id>
    <published>2020-07-31T01:38:24.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes安装及初始化"><a href="#Kubernetes安装及初始化" class="headerlink" title="Kubernetes安装及初始化"></a>Kubernetes安装及初始化</h1><p>研发环境搭建Kubernetes All-in-One环境搭建。</p><a id="more"></a><h2 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h2 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure><h2 id="安装Kubernetes软件"><a href="#安装Kubernetes软件" class="headerlink" title="安装Kubernetes软件"></a>安装Kubernetes软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># step1 Installation Process</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">exclude&#x3D;kube*</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">yum install -y kubelet kubeadm kubectl --disableexcludes&#x3D;kubernetes</span><br><span class="line"> </span><br><span class="line">systemctl enable kubelet</span><br><span class="line"> </span><br><span class="line"># for CentOS 7</span><br><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"> </span><br><span class="line"># from k8s 1.8, swap need to be cloased, otherwise k8s could not be started</span><br><span class="line"># swapoff -a</span><br><span class="line"># Modify &#x2F;etc&#x2F;fstab, comment swap mount</span><br></pre></td></tr></table></figure><h2 id="初始化Kubernetes集群"><a href="#初始化Kubernetes集群" class="headerlink" title="初始化Kubernetes集群"></a>初始化Kubernetes集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Step2 initialization</span><br><span class="line"># Specify kubernetes-version if mirror do not contain latest kubernetes container. ex: if kubeadm is version 1.18.5, you can only</span><br><span class="line"># install kubernetes &#x3D; 1.18.x</span><br><span class="line">kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version&#x3D;1.18.0</span><br><span class="line"> </span><br><span class="line"># Response from output</span><br><span class="line"># You should now deploy a pod network to the cluster.</span><br><span class="line"># Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">#   https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"># Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"># kubeadm join 192.168.10.111:6443 --token 1odaru.0by05advhbu7edgt \</span><br><span class="line">#     --discovery-token-ca-cert-hash sha256:3efb71c40cce36c5ed90fc8b5831233aba06eec26576088e8e7a7a892d272776</span><br><span class="line"> </span><br><span class="line"># Step3 flannel Network</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables&#x3D;1</span><br><span class="line">kubectl apply -f https:&#x2F;&#x2F;gitee.com&#x2F;xiaoquqi&#x2F;flannel&#x2F;raw&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line"> </span><br><span class="line"># Step4 To use cluster</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure><h1 id="允许Master节点运行Pods"><a href="#允许Master节点运行Pods" class="headerlink" title="允许Master节点运行Pods"></a>允许Master节点运行Pods</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="安装Wordpress和MySQL"><a href="#安装Wordpress和MySQL" class="headerlink" title="安装Wordpress和MySQL"></a>安装Wordpress和MySQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;mysql-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;wordpress-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;kustomization.yaml</span><br></pre></td></tr></table></figure><h2 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -k .&#x2F;</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get secrets</span><br><span class="line">kubectl get pvc</span><br><span class="line">kubectl get pods</span><br><span class="line">kubectl get services wordpress</span><br></pre></td></tr></table></figure><h2 id="资源清理"><a href="#资源清理" class="headerlink" title="资源清理"></a>资源清理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -k .&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Kubernetes安装及初始化&quot;&gt;&lt;a href=&quot;#Kubernetes安装及初始化&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes安装及初始化&quot;&gt;&lt;/a&gt;Kubernetes安装及初始化&lt;/h1&gt;&lt;p&gt;研发环境搭建Kubernetes All-in-One环境搭建。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CentOS 7和Docker初始化安装</title>
    <link href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/</id>
    <published>2020-07-31T01:34:49.000Z</published>
    <updated>2021-02-10T05:56:51.789Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h1><p>该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h1 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CentOS-7初始化&quot;&gt;&lt;a href=&quot;#CentOS-7初始化&quot; class=&quot;headerlink&quot; title=&quot;CentOS 7初始化&quot;&gt;&lt;/a&gt;CentOS 7初始化&lt;/h1&gt;&lt;p&gt;该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
</feed>
