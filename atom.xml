<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Ray&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.me/"/>
  <updated>2020-01-03T03:14:17.843Z</updated>
  <id>http://sunqi.me/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam</title>
    <link href="http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/"/>
    <id>http://sunqi.me/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/</id>
    <published>2019-12-31T01:11:55.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接：<a href="https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/" target="_blank" rel="noopener">https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/</a></p><p>由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。</p><a id="more"></a><h2 id="A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest"><a href="#A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest" class="headerlink" title="A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?"></a>A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?</h2><p>A. Place the Redshift cluster in a private subnet.<br>B. Use the AWS KMS Default Customer master key.<br>C. Encrypt the Amazon EBS volumes.<br>D. Encrypt the data using SSL/TLS.</p><p>Answer: B</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html</a></li><li>分析：Amazon Redshift 使用加密密钥层次结构来加密数据库。您可以使用 AWS Key Management Service (AWS KMS) 或硬件安全模块 (HSM) 来管理该层次结构中的顶级加密密钥。Amazon Redshift 用于加密的流程因您管理密钥的方式而异。Amazon Redshift 自动与 AWS KMS 集成，而不与 HSM 集成。当您使用 HSM 时，必须使用客户端和服务器证书在 Amazon Redshift 和 HSM 之间配置受信任的连接。</li></ul><h2 id="A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database"><a href="#A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database" class="headerlink" title="A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?"></a>A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?</h2><p>A. Amazon SQS<br>B. Amazon EFS<br>C. Amazon S3<br>D. AWS Lambda</p><p>Answer: A</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/sqs/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/cn/sqs/faqs/</a></li><li>分析：关键词是unpredictable traffic, keep up with write request, decouple the web application, 所以通过消息队列服务可以让写入请求排队，从而实现前端应用和后端数据库的解耦。</li></ul><h2 id="A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements"><a href="#A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements" class="headerlink" title="A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?"></a>A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?</h2><p>A. AWS Snowball storage for the legacy application until the application can be re-architected.<br>B. AWS Storage Gateway in cached mode for the legacy application storage to write data to Amazon S3.<br>C. AWS Storage Gateway in stored mode for the legacy application storage to write data to Amazon S3.<br>D. An Amazon S3 volume mounted on the legacy application server locally using the File Gateway service.</p><p>Answer: C</p><ul><li>分析：关键词是local stroage with iSCSI, 并且需要将所有新的存储用AWS提供，所以排除A选项；因为用到了iSCSI协议，所以S3使用文件网关方式也不适用，排除D；剩下的B和C区别在于存储模式，因为需要本地应用请求，所以需要使用存储模式，而不能用缓存模式，所以最终选择C。</li></ul><h2 id="A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck"><a href="#A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck" class="headerlink" title="A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?"></a>A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?</h2><p>A. Multi-AZ feature on the RDS MySQL Database<br>B. ELB Classic Load Balancer in front of the web application tier<br>C. Amazon SQS in front of RDS MySQL Database<br>D. Amazon ElastiCache in front of the RDS MySQL Database</p><p>Answer: D</p><ul><li>分析：该问题的关键在于bottleneck due to frequently accessed queries，查询变成瓶颈，可以使用ElastiCache服务作为缓存，降低读取频率解决问题。</li></ul><h2 id="A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives"><a href="#A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives" class="headerlink" title="A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?"></a>A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?</h2><p>A. Amazon Redshift<br>B. Amazon DynamoDB<br>C. Amazon RDS MySQL<br>D. Amazon Aurora</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，但是经过分析觉得有些问题，这道题的几个关键词：scale with the rest of the application, schema will change frequently, cannot afford any downtime for database changes. 首先，schema总是变更，所以这里需要的非关系型数据库，排除C和D。Redshift是数据仓库，其实也是数据仓库，从第一点上就可以排除。另外从这个链接（<a href="http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero" target="_blank" rel="noopener">http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero</a> downtime，其他的都不行。所以原网站给出的答案是错误的。</li></ul><h2 id="A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements"><a href="#A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements" class="headerlink" title="A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?"></a>A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?</h2><p>A. Use AWS CloudFormation to deploy the cluster in a second region.<br>B. Take a snapshot of the cluster and copy it to another Availability Zone.<br>C. Modify the Redshift cluster to span two regions.<br>D. Enable cross-region snapshots to a different region.</p><p>Answer: D</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/</a></li></ul><h2 id="A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct"><a href="#A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct" class="headerlink" title="A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?"></a>A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?</h2><p>A. The application is reading parts of objects from Amazon S3 using a range header.<br>B. The application is reading objects from Amazon S3 using parallel object requests.<br>C. The application is updating records by writing new objects with unique keys.<br>D. The application is updating records by overwriting existing objects with the same keys.</p><p>Answer: D</p><ul><li>分析：这道题也是争论很大的一道题，原网站答案为A。问题简单描述为客户端访问不到最新的数据，发生的时间点在于应用上传信息时候速率提高导致的，所以问题应该出现在写入的时候，这样排除A和B读取的问题。因为S3同一object永远是覆盖，所以最有可能的问题是在same key的情况下，所以选择D。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements" class="headerlink" title="A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?"></a>A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?</h2><p>A. Use Amazon Kinesis with AWS CloudTrail for auditing the specific times when profile photos are uploaded.<br>B. Use Amazon EBS volumes with IAM policies restricting user access to specific time periods.<br>C. Use Amazon S3 with the default private access policy and generate pre-signed URLs each time a new site profile is created.<br>D. Use Amazon CloudFront with AWS CloudTrail for auditing the specific times when profile photos are uploaded.</p><p>Answer: C</p><h2 id="An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application"><a href="#An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application" class="headerlink" title="An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?"></a>An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Amazon Glacier</p><p>Answer: C</p><ul><li>分析：没想到这道题原网站给出的答案是B，争议比较大，但是从题目描述需要Block Storage角度来看，选择C才是最合理的。这道题还需要进一步确认一下。</li></ul><h2 id="A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements"><a href="#A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements" class="headerlink" title="A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?"></a>A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?</h2><p>A. Create a Lambda function to move files older than 30 days to Amazon EBS and move files older than 60 days to Amazon Glacier.<br>B. Create a Lambda function to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon EBS.<br>C. Create lifecycle rules to move files older than 30 days to Amazon S3 Standard Infrequent Access and move files older than 60 days to Amazon Glacier.<br>D. Create lifecycle rules to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon S3 Standard Infrequent Access.</p><p>Answer: C</p><ul><li>分析：很明显的排除A和B，S3可以自定义规则，那么问题就是30天后和60天后需要哪种存储的问题了，根据题目C是明显正确的。</li></ul><h2 id="An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs"><a href="#An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs" class="headerlink" title="An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?"></a>An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?</h2><p>A. Amazon Redshift<br>B. Amazon RDS<br>C. Amazon DynamoDB<br>D. Amazon Aurora</p><p>Answer: C</p><h2 id="A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two"><a href="#A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two" class="headerlink" title="A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)"></a>A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)</h2><p>A. Create a VPC Endpoint for Amazon RDS.<br>B. Create the Lambda function within the Amazon RDS VPC.<br>C. Change the ingress rules of Lambda security group, allowing the Amazon RDS security group.<br>D. Change the ingress rules of the Amazon RDS security group, allowing the Lambda security group.<br>E. Add an Internet Gateway (IGW) to the VPC, route the private subnet to the IGW.</p><p>Answer: BD</p><ul><li>分析：又是原网站一道错题，原网站答案为AD。D选项是允许Lambda服务访问RDS，所以在进方向允许。</li><li>目前VPC支持Endpoint的服务：<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">接口终端节点是一个弹性网络接口，具有来自子网 IP 地址范围的私有 IP 地址，用作发送到受支持的服务的通信的入口点。支持以下服务：</span><br><span class="line">Amazon API Gateway</span><br><span class="line">Amazon AppStream 2.0</span><br><span class="line">AWS App Mesh</span><br><span class="line">Amazon Athena</span><br><span class="line">AWS CloudFormation</span><br><span class="line">AWS CloudTrail</span><br><span class="line">Amazon CloudWatch</span><br><span class="line">Amazon CloudWatch Events</span><br><span class="line">Amazon CloudWatch Logs</span><br><span class="line">AWS CodeBuild</span><br><span class="line">AWS CodeCommit</span><br><span class="line">AWS CodePipeline</span><br><span class="line">AWS Config</span><br><span class="line">AWS DataSync</span><br><span class="line">Amazon EC2 API</span><br><span class="line">Elastic Load Balancing</span><br><span class="line">Amazon Elastic Container Registry</span><br><span class="line">Amazon Elastic Container Service</span><br><span class="line">AWS Glue</span><br><span class="line">AWS Key Management Service</span><br><span class="line">Amazon Kinesis Data Firehose</span><br><span class="line">Amazon Kinesis Data Streams</span><br><span class="line">Amazon Rekognition</span><br><span class="line">Amazon SageMaker 和 Amazon SageMaker 运行时</span><br><span class="line">Amazon SageMaker 笔记本</span><br><span class="line">AWS Secrets Manager</span><br><span class="line">AWS Security Token Service</span><br><span class="line">AWS Service Catalog</span><br><span class="line">Amazon SNS</span><br><span class="line">Amazon SQS</span><br><span class="line">AWS Systems Manager</span><br><span class="line">AWS Storage Gateway</span><br><span class="line">AWS Transfer for SFTP</span><br><span class="line">其他 AWS 账户托管的终端节点服务</span><br><span class="line"></span><br><span class="line">网关终端节点是一个网关，作为您在路由表中指定的路由的目标，用于发往受支持的 AWS 服务的流量。支持以下 AWS 服务：</span><br><span class="line">Amazon S3</span><br><span class="line">DynamoDB</span><br></pre></td></tr></table></figure><h2 id="待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement"><a href="#待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement" class="headerlink" title="(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?"></a>(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?</h2><p>A. Modify the Redshift cluster and configure cross-region snapshots to the other region.<br>B. Modify the Redshift cluster to take snapshots of the Amazon EBS volumes each day, sharing those snapshots with the other region.<br>C. Modify the Redshift cluster and configure the backup and specify the Amazon S3 bucket in the other region.<br>D. Modify the Redshift cluster to use AWS Snowball in export mode with data delivered to the other region.</p><p>Answer: A</p><ul><li>分析：又是一道错题，Redhift备份是通过S3实现的, 所以不存在B的情况，我个人有点倾向于C，但是A确实是Redshift在快照时默认的格式，可能是更容易恢复吧，这道题需要在实际环境进行一下验证。另外国际版本的Redshift和国内的应该比国内的高很多。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">问：Amazon Redshift 如何备份数据？ 如何从备份中还原我的集群？</span><br><span class="line"></span><br><span class="line">在加载数据时，Amazon Redshift 会复制数据仓库集群内的所有数据并将其连续备份至 S3。Amazon Redshift 始终尝试维持至少三份数据（计算节点上的正本数据、副本数据和 Amazon S3 上的备份数据）。Redshift 还能将您的快照异步复制到另一个区域的 S3 中进行灾难恢复。</span><br><span class="line"></span><br><span class="line">默认情况下，Amazon Redshift 以一天的保留期启用数据仓库群集的自动化备份。您可将其配置为 35 天之久。</span><br><span class="line"></span><br><span class="line">免费备份存储受限于数据仓库群集中节点上的总存储大小，并仅适用于已激活的数据仓库群集。例如，如果您有 8TB 的数据仓库总存储大小，那么我们将提供最多 8TB 的备份存储而不另外收费。如果您想将备份保留期延长为超过一天，那么您可以使用 AWS 管理控制台或 Amazon Redshift API 来实现这一目的。有关自动快照的更多信息，请参阅《Amazon Redshift 管理指南》。Amazon Redshift 仅备份已更改的数据，因此大多数快照仅占用少量的免费备份存储。</span><br><span class="line"></span><br><span class="line">如果您需要还原备份，则可以在备份保留期内访问所有自动备份。在您选择某个要还原的备份后，我们将预置一个新的数据仓库集群并将数据还原至此集群中。</span><br></pre></td></tr></table></figure><h2 id="A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance"><a href="#A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance" class="headerlink" title="A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?"></a>A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?</h2><p>A. Convert the database to Amazon Redshift.<br>B. Create a CloudFront distribution.<br>C. Convert the database to use EBS Provisioned IOPS.<br>D. Create one or more read replicas.</p><p>Answer: C</p><ul><li>分析：这道题我最开始选择的是D，但是评论区的一种解释有一定的道理：这个网站应用类型为电商，原题中没有很清楚说明queris and load的压力有多大，很可能我们建立了read replicas只能临时性解决问题，并不是一劳永逸的方式。并且根据<a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">因此，所有 Aurora 副本均返回相同的查询结果数据，且副本滞后时间非常短 - 通常远远少于主实例写入更新后的 100 毫秒。副本滞后因数据库更改速率而异。也就是说，在对数据库执行大量写入操作期间，您可能发现副本滞后时间变长。</span><br></pre></td></tr></table></figure><p>如果读副本在这个延时上，很可能对业务系统造成很大的影响。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考链接：&lt;a href=&quot;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.me/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.me/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>使用阿里云函数计算构建小程序</title>
    <link href="http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/"/>
    <id>http://sunqi.me/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/</id>
    <published>2019-12-19T15:19:16.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a>1、需求</h1><p>在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。</p><a id="more"></a><img src="/images/blogs/2019-12-19/architecture.png" class=""><p>在原有设计中，使用了Python Flask提供WEB界面，后台使用Celery异步的将用户请求发送至钉钉中，之后采用轮询方式监控审批工单状态，当工单完成审批后，将生成好的License发送至客户提供的邮箱中。</p><p>实现的效果：</p><img src="/images/blogs/2019-12-19/UI.jpeg" class=""><p>这种方式虽然可以满足需求，但是在使用过程中也发现有如下痛点：<br>1、由于对于可用性要求比较高，所以将整套应用以容器化方式部署在云主机上，程序高可用性依赖于底层的平台，基于成本考虑并没有在多可用区进行部署。<br>2、当业务变化时，需要专人将容器从本地容器库上传后进行更新，更新速度慢，敏捷性低。<br>3、需要专人对操作系统层进行维护，并且由于该云主机还运行了其他程序，所以管控上也存在安全风险。</p><p>基于以上出现的问题，决定对原有二维码程序进行重构，并重新部署在阿里云函数计算服务上。<br>1、第一阶段的改造主要是将二维码扫描程序移植到函数计算服务中。<br>2、第二阶段的改造主要是将发送二维码程序改造为函数计算服务，使用钉钉流程接口中的Callback方法调用该接口，在审批结束后触发发送License流程。</p><h1 id="2、函数计算服务——无服务，零运维"><a href="#2、函数计算服务——无服务，零运维" class="headerlink" title="2、函数计算服务——无服务，零运维"></a>2、函数计算服务——无服务，零运维</h1><p>最早接触Serverless的雏形是在2011年开发Cloud Foundry项目时，当时留下一个非常深的印象就是把写好的应用直接上传就完成了部署、扩展等。但是当时Cloud Foundry有一个非常大的局限性，受限于几种开发语言和框架。记得当时的Cloud Foundry只支持Node.js、Python、Java、PHP、Ruby on Rails等，脱离了这个范围则就无法支持，所以当时我其实对这种形态的应用场景存在很大的疑问。<br>这种困惑直到2013年Docker的出现而逐步解开，Docker的出现让开发语言、框架不再是问题，巧妙的解决了Cloud Foundry上述局限性。但是Docker毕竟只是一种工具形态，还不能称得上是平台，紧接着k8s的出现弥补了这一空白，使得Docker从游击队变成了正规军。<br>在这个发展过程中我们不难看出，软件领域发展出现了重大变革，从服务器为王逐渐演进到应用为王的阶段。如果说虚拟化改变了整个物理机的格局，那么无服务化的出现则改变了整个软件开发行业。<br>由于网上各种文档太多了，这里就不对Serverless基本概念进行介绍了，借用一张图说明下。另外还有一点，我们从这里面看到IT行业里的某些岗位，注定要消失的，比如传统运维。</p><img src="/images/blogs/2019-12-19/compare.png" class=""><h1 id="3、应用架构"><a href="#3、应用架构" class="headerlink" title="3、应用架构"></a>3、应用架构</h1><p>整个架构上，分为两个函数计算服务完成：</p><ul><li>二维码前端：主要用于显示页面，并承担HTTP请求转发代理的角色，将请求转发至二维码后端，发给钉钉，采用HTTP触发器，允许公网访问。</li><li>二维码后端：用于将用户请求发送给钉钉，该部分服务仍然采用HTTP触发器，不同于前端，该服务是不允许公网直接访问的，但是需要配置NAT网关，通过网关访问钉钉，实现固定IP访问钉钉的效果。</li></ul><img src="/images/blogs/2019-12-19/new_architecture.png" class=""><p>从逻辑上讲，整个应用并不复杂，但是在实际使用时遇到最大的问题来自钉钉白名单。由于函数服务对外连接的IP并不固定，所以无法在钉钉中添加，那么就要求函数服务对外连接的IP地址一定要固定。社区中提供的方法主要分为：</p><ul><li>ECI（运行Nginx充当Proxy），优势是便宜，劣势是高可用性需要自己维护</li><li>NAT网关，优势是高可用性，劣势是比ECI贵</li></ul><h1 id="4、构建过程"><a href="#4、构建过程" class="headerlink" title="4、构建过程"></a>4、构建过程</h1><p>由于篇幅原因，这里只介绍关键步骤。</p><h2 id="4-1-构建模板"><a href="#4-1-构建模板" class="headerlink" title="4.1 构建模板"></a>4.1 构建模板</h2><p>为了后续管理和扩展方便，选用了阿里云函数计算中使用flask-web模板进行构建，同时可以将前端静态文件模板存放于项目下（出于统一管理的需要，也可以存放于阿里云的OSS中，作为静态网站发布）。</p><p>前端我们使用flask-web作为模板创建函数，后端我们直接采用最简单的HTTP函数。</p><img src="/images/blogs/2019-12-19/create_function_template.png" class=""><p>函数入口配置，及触发器配置：</p><img src="/images/blogs/2019-12-19/create_function.png" class=""><p>服务配置，包含公网访问权限，专有网络配置，日志配置，权限配置。</p><ul><li>前端服务需要公网访问权限，不需要专有网络配置，需要的权限为：AliyunLogFullAccess。</li><li>后端服务不需要公网访问权限，但是需要配置好的NAT映射的专有网络，由于函数服务在北京2区中在cn-beijing-c和cn-beijing-f，所以在新建交换机时需要使用这两个区。还需要选择安全组，由于出方向并没有明确禁止，所以不需要特别的安全组规则设定。需要的权限为：AliyunLogFullAccess/AliyunECSNetworkInterfaceManagementAccess。</li></ul><p>配置好后，通过导出功能，分别下载前端和后端代码和配置，在本地进行开发调试。</p><img src="/images/blogs/2019-12-19/export_function.png" class=""><h2 id="4-2-前端开发"><a href="#4-2-前端开发" class="headerlink" title="4.2 前端开发"></a>4.2 前端开发</h2><p>我们的前端采用Vue.js进行开发，在main.py同级新建templates目录。Vue编译好的静态文件可以放入该目录中，后续Flask会加载该文件作为入口文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── templates</span><br><span class="line">│   ├── index.html</span><br><span class="line">│   ├── static</span><br><span class="line">├── main.py</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># main.py sample</span><br><span class="line">from flask import render_template</span><br><span class="line"></span><br><span class="line">LICENSE_URL &#x3D; &quot;https:&#x2F;&#x2F;[x](https:&#x2F;&#x2F;.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license).cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license&quot;</span><br><span class="line"></span><br><span class="line">@app.route(&#39;&#x2F;qr_code&#39;, methods&#x3D;[&#39;GET&#39;])</span><br><span class="line">def index():</span><br><span class="line">      return render_template(&#39;index.html&#39;)</span><br><span class="line"></span><br><span class="line">      @app.route(&#39;&#x2F;qr_code&#x2F;license&#39;, methods&#x3D;[&#39;POST&#39;])</span><br><span class="line">      def create():</span><br><span class="line">            payload &#x3D; request.json</span><br><span class="line">                resp &#x3D; requests.post(LICENSE_URL,</span><br><span class="line">                                                 json&#x3D;payload,</span><br><span class="line">                                                                              headers&#x3D;DEFAULT_HEADERS)</span><br><span class="line">                return make_response(resp.text, resp.status_code)</span><br></pre></td></tr></table></figure><h2 id="4-3-后端开发"><a href="#4-3-后端开发" class="headerlink" title="4.3 后端开发"></a>4.3 后端开发</h2><p>后端的开发较为简单，实现一个函数支持POST请求，将转发的结果发送至钉钉即可。</p><h2 id="4-4-本地调试"><a href="#4-4-本地调试" class="headerlink" title="4.4 本地调试"></a>4.4 本地调试</h2><p>阿里云在本地开发时提供了fun应用部署和开发工具，详细使用方法见：<a href="https://help.aliyun.com/document_detail/64204.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/64204.html</a>。</p><h3 id="安装fun"><a href="#安装fun" class="headerlink" title="安装fun"></a>安装fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry [https:&#x2F;&#x2F;registry.npm.taobao.org](https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;) --global</span><br><span class="line">npm config set disturl [https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist](https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist) --global</span><br><span class="line"></span><br><span class="line">npm install @alicloud&#x2F;fun -g</span><br></pre></td></tr></table></figure><h3 id="配置fun"><a href="#配置fun" class="headerlink" title="配置fun"></a>配置fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fun config</span><br><span class="line"></span><br><span class="line">(venv) [root@ray-dev test_func]# fun config</span><br><span class="line">? Aliyun Account ID xxxxxxxx</span><br><span class="line">? Aliyun Access Key ID ***********r5Qd</span><br><span class="line">? Aliyun Access Key Secret ***********kCCi</span><br><span class="line">? Default region name cn-beijing</span><br><span class="line">? The timeout in seconds for each SDK client invoking 10</span><br><span class="line">? The maximum number of retries for each SDK client 3</span><br><span class="line">? Allow to anonymously report usage statistics to improve the tool over time? Yes</span><br></pre></td></tr></table></figure><h3 id="Http-Trigger本地运行"><a href="#Http-Trigger本地运行" class="headerlink" title="Http Trigger本地运行"></a>Http Trigger本地运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun local start</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun deploy</span><br></pre></td></tr></table></figure><h2 id="4-5-配置域名解析"><a href="#4-5-配置域名解析" class="headerlink" title="4.5 配置域名解析"></a>4.5 配置域名解析</h2><p>部署完成后有一点需要特别注意，必须要绑定域名，并且设定必要的路由。如果在没有绑定域名的情况下，服务端会为 response header中强制添加 content-disposition: attachment字段，此字段会使得返回结果在浏览器中以附件的方式打开。（<a href="https://www.alibabacloud.com/help/zh/doc-detail/56103.htm" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/56103.htm</a>）</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><ul><li>灵活使用函数计算对开发成本和运行成本具有“双降”的效果</li><li>函数计算除了Http Trigger外，还包含了Event Trigger。Event Trigger中包含了连接各个服务之间的作用，在一些服务衔接上的作用越来越明显</li><li>函数计算在线开发时比较麻烦，并且查看日志不方便，所以尽量在本地开发好在上传的方式</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1、需求&quot;&gt;&lt;a href=&quot;#1、需求&quot; class=&quot;headerlink&quot; title=&quot;1、需求&quot;&gt;&lt;/a&gt;1、需求&lt;/h1&gt;&lt;p&gt;在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="阿里云" scheme="http://sunqi.me/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/tags/Cloud-Computing/"/>
    
      <category term="Serverless" scheme="http://sunqi.me/tags/Serverless/"/>
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Newton国内代码贡献</title>
    <link href="http://sunqi.me/2016/09/30/contricution-in-newton/"/>
    <id>http://sunqi.me/2016/09/30/contricution-in-newton/</id>
    <published>2016-09-30T17:00:49.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。</p><p>这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。</p><p>最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。</p><p>还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><p>OpenStack Mitaka深度解读请见：<a href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>下面是最近三个版本Release的详细对比：</p><img src="/images/blogs/contribution-in-newton-projects.png" class="center"><p>让我们来关注这次Release中的三个新项目：</p><h3 id="Panko-计量服务事件消息存储"><a href="#Panko-计量服务事件消息存储" class="headerlink" title="Panko(计量服务事件消息存储)"></a>Panko(计量服务事件消息存储)</h3><p>Panko是计量模块中的一部分，主要是为了计量模块提供事件消息存储，众所周知，在上一个OpenStack Release中，Ceilometer被一分为三，分别为aodh(告警服务)/Gnocchi(基于时间的数据库服务)/Ceilometer，为了解决当前Ceilometer中存在的性能问题，提高更好的扩展性。</p><p>现在Panko的文档并不是很丰富，如果有需要了解更多详细内容的，可以关注Developer的文档：<a href="http://docs.openstack.org/developer/panko/" target="_blank" rel="noopener">http://docs.openstack.org/developer/panko/</a></p><h3 id="Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务"><a href="#Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务" class="headerlink" title="Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)"></a>Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)</h3><p>Vitrage是一个OpenStack RCA(Root Cause Analysis)服务，用于组织、分析和扩展OpenStack的告警和事件，在真正的问题发生前找到根本原因。</p><p>众所周知，OpenStack平台最大的优势来自于架构的可扩展性，这也是OpenStack能够在基础架构曾一枝独秀的重要原因。分布式架构最大的优势在于扩展，但是过于灵活的扩展性为运维带来的极大的困难，所以Vitrage的出现在一定程度上缓解了OpenStack运维上的痛点。</p><p>我们来简单看一下他的架构，更多详细的介绍请查看WIKI：<a href="https://wiki.openstack.org/wiki/Vitrage" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Vitrage</a></p><img src="/images/blogs/contribution-in-newton-vitrage-architecture.png" class="center"><h3 id="Watcher-OpenStack平台优化服务"><a href="#Watcher-OpenStack平台优化服务" class="headerlink" title="Watcher(OpenStack平台优化服务)"></a>Watcher(OpenStack平台优化服务)</h3><p>从名字上看，我们并不能理解这个模块的具体左右，我们通过文档中用户应用场景来了解一下Watcher的作用：</p><p>作为一名云平台的管理员在云平台使用一段时间后，想根据一些物理特性对云平台虚拟机的分布进行重新平衡，例如服务器的温度、电源的状态等信息，那么这时候就可以通过watcher，利用Nova虚拟机的在线迁移对整个数据中心云平台的虚拟机进行一些优化处理，从而达到某种平衡。我认为这其实类似于VMWare的DRS功能。</p><p>当然Watcher还有更多的应用场景，更多详细的介绍请查看：<a href="https://wiki.openstack.org/wiki/Watcher" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Watcher</a></p><p>我们来简单看一下他的架构，更多架构方面的详细的介绍请查看：<a href="http://docs.openstack.org/developer/watcher/architecture.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/watcher/architecture.html</a></p><img src="/images/blogs/contribution-in-newton-watcher-architecture.svg" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits和blueprints的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司和贡献者来说，都有所上升，这也不难理解，随着OpenStack模块增加，势必涉及更多的领域，所以更多的公司加入了这个生态圈。</p><img src="/images/blogs/contribution-in-newton-companies-contributors.png" class="center"><p>从commits角度进行分析，传统几大好强几乎没有变化，日本的Fujitsu在commits上挤掉了华为，进入了前十名的位置。模块方面，核心模块的贡献仍然位于前十名，也说明是应用最多的模块，所以才会不断的发现问题。本次统计的总项目数量为629个，可能stackalytics在统计策略上有所调整。</p><img src="/images/blogs/contribution-in-newton-companies-modules-commits.png" class="center"><p>单从commits角度统计其实有失偏颇，真正能够体现公司在OpenStack实力的指标应该是Blueprints。我认为完成Blueprints至少具备三个必要条件：英语要好、在社区有一定的影响力、架构设计能力。这些都是需要不断在社区进行积累和沉淀的。</p><p>本次release周期内，能够完成Blueprints的公司为64个，国内的华为和九州云均进入前10名，排名比较靠前的国内企业还包括：Easystack、中兴。</p><p>完成Blueprints最多的仍然是核心模块，排在第二名的是kolla，看来在上一个周期中，kolla项目的活跃程度是较高的。</p><img src="/images/blogs/contribution-in-newton-companies-modules-blueprints.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完总体的状况，再来关注一些国内的贡献情况，与去年相比，今年上榜的国内企业达到了21家，创历年之最，比去年的15家企业整整多了7家，并且我们发现在这些新增企业中大部分都是提供企业服务的公司，说明OpenStack在国内的企业级市场开始站稳脚跟。下面我们来做一个详细的分析：</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在最近的三个版本连续对社区有贡献的企业包括：华为，Easystack，九州云，海云捷迅，华三，Unitedstack，乐视，中国移动和北京休伦科技(Huron)。</p><p>本次爬升最快的企业：中兴，从108位攀升至13位。</p><p>本次统计新增的7家企业：云途腾(t2cloud)，大唐高鸿数据(GohighSec)，华云数据，烽火通信，爱数，北京国电通，云英，中国银联，赛特斯信息。</p><p>本次排名中OpenStack的直接用户：中国移动和中国银联。中国移动更是参选了OpenStack SuperAward的评比，预祝他们能顺利当选。</p><img src="/images/blogs/contribution-in-newton-china-companies.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>我们再来从人员投入来分析贡献情况一下：</p><ul><li>投入人数最多的仍然是华为，有65名工程师贡献了本次的commits</li><li>中兴无疑是本次人员投入增长最快的，从6名工程师一下子扩张到61名，也是唯一能和华为抗衡的</li><li>超过2位数人员投入的包括，Easystack，九州云和Unitedstack，另外海云捷迅有9人，华三有8人，中国移动有7人参与社区贡献</li></ul><img src="/images/blogs/contribution-in-newton-companies-effort.png" class="center"><h3 id="模块贡献分析"><a href="#模块贡献分析" class="headerlink" title="模块贡献分析"></a>模块贡献分析</h3><p>从模块贡献角度来分析，国内企业的贡献仍然没有出现一个统一的趋势，与Mitaka Release相比，贡献涉及模块的总量从192个增加至Newton Release的246个，一方面说明OpenStack本身模块的增加，也说明国内企业使用或开发OpenStack在方向上的多元化。</p><p>从贡献的模块来看，华为主导的dargonflow高居榜首，紧随其后的是手册和clients两个项目，随后的贡献集中在OpenStack的核心模块，与Docker相关的几个模块中。Kolla项目无疑是最近关注的热点，随着Docker的快速发展，OpenStack和Docker不断碰撞出新的火花。</p><img src="/images/blogs/contribution-in-newton-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这个问题仍然是比较敏感的问题，只有每个公司的CEO能够回答这个问题，这里面我从融资的角度来回顾一下2015至2016年之间在OpenStack领域发生过什么。</p><ul><li>2015年9月17日，英特尔投资部门披露了此前投资的中国8家公司名单。投资总额达6700万美元，领域覆盖了新材料、智能设备、物联网、云服务等领域。其中包含九州云和海云捷迅两家OpenStack企业。(<a href="http://tech.qq.com/a/20150917/038604.htm" target="_blank" rel="noopener">http://tech.qq.com/a/20150917/038604.htm</a>)</li><li>2015年10月17日，中国最大的独立公有云提供商UCloud和全球领先的OpenStack厂商Mirantis在东京的OpenStack峰会上正式宣布成立合资公司UMCloud，以求更好的在中国做OpenStack。(<a href="http://www.doit.com.cn/article/1027290510.html" target="_blank" rel="noopener">http://www.doit.com.cn/article/1027290510.html</a>)</li><li>2015年12月16日，UnitedStack有云宣布完成C轮融资，该轮融资由思科和红杉资本投资，具体数额未公布(<a href="http://www.infoq.com/cn/news/2015/12/unitedstack-financing" target="_blank" rel="noopener">http://www.infoq.com/cn/news/2015/12/unitedstack-financing</a>)</li><li>2016年5月20日，云途腾(T2Cloud)完成A轮3650万融资(<a href="http://iimedia.cn/42262.html" target="_blank" rel="noopener">http://iimedia.cn/42262.html</a>)</li><li>2016年9月21日，腾讯与海云捷迅昨日下午在京共同宣布达成战略投资合作关系，海云捷迅接受腾讯的战略投资(<a href="http://www.36dsj.com/archives/62353" target="_blank" rel="noopener">http://www.36dsj.com/archives/62353</a>)</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回到开篇的那句话，OpenStack贡献量只能反应中国企业对于开源项目的重视程度，无法反应真实的用户需求。VMWare花了将近10年的时间教育用户，说服用户把应用从物理机迁移至虚拟机。OpenStack从2011年出生到现在也仅仅短短的5年，可见OpenStack还有很长的路要走。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。&lt;/p&gt;
&lt;p&gt;这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。&lt;/p&gt;
&lt;p&gt;最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。&lt;/p&gt;
&lt;p&gt;还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Mitaka深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>使用国内源部署Ceph</title>
    <link href="http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/"/>
    <id>http://sunqi.me/2016/06/19/deploy-ceph-using-china-mirror/</id>
    <published>2016-06-19T01:25:37.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。</p><a id="more"></a><h2 id="关于国内源"><a href="#关于国内源" class="headerlink" title="关于国内源"></a>关于国内源</h2><p>根据Ceph中国社区的<a href="http://bbs.ceph.org.cn/?/page/image" target="_blank" rel="noopener">统计</a>，国内已经有四家网站开始同步Ceph源，分别是：</p><ul><li>网易镜像源<a href="http://mirrors.163.com/ceph" target="_blank" rel="noopener">http://mirrors.163.com/ceph</a></li><li>阿里镜像源<a href="http://mirrors.aliyun.com/ceph" target="_blank" rel="noopener">http://mirrors.aliyun.com/ceph</a></li><li>中科大镜像源<a href="http://mirrors.ustc.edu.cn/ceph" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ceph</a></li><li>宝德镜像源 <a href="http://mirrors.plcloud.com/ceph" target="_blank" rel="noopener">http://mirrors.plcloud.com/ceph</a></li></ul><h2 id="国内源分析"><a href="#国内源分析" class="headerlink" title="国内源分析"></a>国内源分析</h2><p>以163为例，是以天为单位向回同步Ceph源，完全可以满足大多数场景的需求，同步的源也非常全，包含了calamari，debian和rpm的全部源，最近几个版本的源也能从中找到。</p><h2 id="安装指定版本的Ceph"><a href="#安装指定版本的Ceph" class="headerlink" title="安装指定版本的Ceph"></a>安装指定版本的Ceph</h2><p>这里以安装最新版本的Jewel为例，由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-jewel&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br></pre></td></tr></table></figure><p>之后的过程就没有任何区别了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create monitor node</span><br><span class="line">ceph-deploy new node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Software Installation</span><br><span class="line">ceph-deploy install deploy node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Gather keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"># Ceph deploy parepare and activate</span><br><span class="line">ceph-deploy osd prepare node1:&#x2F;dev&#x2F;sdb node2:&#x2F;dev&#x2F;sdb node3:&#x2F;dev&#x2F;sdb</span><br><span class="line">ceph-deploy osd activate node1:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 node2:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1 node3:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-2</span><br><span class="line"></span><br><span class="line"># Make 3 copies by default</span><br><span class="line">echo &quot;osd pool default size &#x3D; 3&quot; | tee -a $HOME&#x2F;ceph.conf</span><br><span class="line"></span><br><span class="line"># Copy admin keys and configuration files</span><br><span class="line">ceph-deploy --overwrite-conf admin deploy node1 node2 node3</span><br></pre></td></tr></table></figure><p>这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Docker部署Ceph</title>
    <link href="http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/"/>
    <id>http://sunqi.me/2016/06/12/bootstrap-your-ceph-cluster-in-docker/</id>
    <published>2016-06-12T23:20:50.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是根据Sébastien Han的<a href="https://www.youtube.com/watch?v=FUSTjTBA8f8&feature=youtu.be" target="_blank" rel="noopener">演示视频</a>进行整理的，对过程中有问题的部分进行了修复。</p><p>Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。</p><a id="more"></a><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><ul><li>至少需要三台虚拟机或者物理机，每台虚拟机或者物理机至少有两块硬盘，这里我是在一台物理机上用vagrant模拟出三台CentOS 6.6虚拟机进行的实验</li><li>三台虚拟机需要安装docker，本文附带Docker加速方案</li><li>获取ceph/daemon镜像</li></ul><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><img src="/images/blogs/bootstrap-ceph-docker-flow.png" class="center"><h2 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h2><p>主机名和集群的对应关系如下：</p><ul><li>node1 -&gt; 192.168.33.11</li><li>node2 -&gt; 192.168.33.12</li><li>node3 -&gt; 192.168.33.13</li></ul><img src="/images/blogs/bootstrap-ceph-docker-architecture.png" class="center"><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="安装Docker，下载镜像"><a href="#安装Docker，下载镜像" class="headerlink" title="安装Docker，下载镜像"></a>安装Docker，下载镜像</h3><p>国内安装Dcoker还是速度很慢的，这里推荐使用daocloud的加速方案。不但docker安装速度提高了，pull镜像的速度也大幅度提高。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh</span><br></pre></td></tr></table></figure><p>我是在CentOS系统上进行的测试，将docker加入自动启动，并启动docker，接下来pull ceph daemon镜像，该镜像包含了所有的ceph服务和entrypoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chkconfig docker</span><br><span class="line">service docker start</span><br><span class="line">docker pull ceph&#x2F;daemon</span><br></pre></td></tr></table></figure><h3 id="启动第一个Monitor"><a href="#启动第一个Monitor" class="headerlink" title="启动第一个Monitor"></a>启动第一个Monitor</h3><p>在node1上启动第一个Monitor，注意，如果你的环境中IP和我不同，请修改MON_IP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.11 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>验证一下效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES</span><br><span class="line">7babea544ef1        ceph&#x2F;daemon         &quot;&#x2F;entrypoint.sh mon&quot;   3 seconds ago       Up 2 seconds                            backstabbing_brattain</span><br></pre></td></tr></table></figure><p>查看一下集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec 7babea544ef1 ceph -s</span><br></pre></td></tr></table></figure><p>当前集群状态，能看到当前已经有一个mon启动起来了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e1: 1 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 2, quorum 0 node1.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="复制配置文件"><a href="#复制配置文件" class="headerlink" title="复制配置文件"></a>复制配置文件</h3><p>接下来需要将node1的配置文件复制到node2和node3上，复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。这些配置文件非常重要，如果没有这些配置文件的存在，我们在其他节点启动新的docker ceph daemon的时候会被认为是一个新的集群。<br>我们在node1执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssh root@node2 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node2:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node2:&#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line"></span><br><span class="line">ssh root@node3 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node3:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node3:&#x2F;var&#x2F;lib&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="启动第二个和第三个Monitor"><a href="#启动第二个和第三个Monitor" class="headerlink" title="启动第二个和第三个Monitor"></a>启动第二个和第三个Monitor</h3><p>在node2上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.12 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node3上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.13 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node1上查看集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="启动OSD的遇到的问题"><a href="#启动OSD的遇到的问题" class="headerlink" title="启动OSD的遇到的问题"></a>启动OSD的遇到的问题</h3><p>按照原视频的介绍的方法，启动OSD可以直接指定某个分区，然后用osd_ceph_disk作为启动ceph/daemon的参数，之后docker镜像会自动的进行分区等动作。但是经过实际验证却发现在mkjournal创建错误，OSD无法启动。</p><p>经过和社区确认，发现这个Bug在之前版本中得到过修复，但是之后的版本又出现了。根据社区的建议使用jewel版本的ceph daemon进行了再次验证，发现问题依旧，所以这里介绍的方法只能退而求其次，采用手动方式分区、格式化，之后用osd_directory启动ceph/daemon。</p><p>这是github上的相关讨论：<a href="https://github.com/ceph/ceph-docker/issues/171" target="_blank" rel="noopener">https://github.com/ceph/ceph-docker/issues/171</a></p><p>这是用osd_ceph_disk方式启动后的错误日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd --cluster ceph --mkfs --mkkey -i 4 --monmap &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;activate.monmap --osd-data &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG --osd-journal &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal --osd-uuid 89e240e1-17e9-4d6c-8d4f-f1a3e0278b91 --keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;keyring --setuser ceph --setgroup disk</span><br><span class="line">2016-06-12 23:37:26.180610 7f8889654800 -1 filestore(&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG) mkjournal error creating journal on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal: (2) No such file or directory</span><br><span class="line">2016-06-12 23:37:26.180752 7f8889654800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -2</span><br><span class="line">2016-06-12 23:37:26.180918 7f8889654800 -1 ** ERROR: error creating empty object store in &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG: (2) No such file or directory</span><br><span class="line">mount_activate: Failed to activate</span><br><span class="line">unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br><span class="line">command_check_call: Running command: &#x2F;bin&#x2F;umount -- &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br></pre></td></tr></table></figure><h3 id="启动OSD"><a href="#启动OSD" class="headerlink" title="启动OSD"></a>启动OSD</h3><p>第一步先进行分区和格式化，这里只给出node1的操作方式，其他两个节点的方式类似。</p><p>先来安装必要的工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y parted xfsprogs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# parted &#x2F;dev&#x2F;sdb</span><br><span class="line">GNU Parted 2.1</span><br><span class="line">Using &#x2F;dev&#x2F;sdb</span><br><span class="line">(parted) mklabel</span><br><span class="line">New disk label type? gpt</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition name?  []? &quot;Linux filesystem&quot;</span><br><span class="line">File system type?  [ext2]? xfs</span><br><span class="line">Start? 0G</span><br><span class="line">End? 107GB</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name              Flags</span><br><span class="line"> 1      1049kB  107GB  107GB               Linux filesystem</span><br><span class="line"></span><br><span class="line">(parted) quit</span><br></pre></td></tr></table></figure><p>格式化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# mkfs.xfs &#x2F;dev&#x2F;sdb1</span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdb1              isize&#x3D;256    agcount&#x3D;4, agsize&#x3D;6553472 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;0        finobt&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;0</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br></pre></td></tr></table></figure><p>我们把目录在node1上进行挂载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;ceph&#x2F;sdb</span><br><span class="line">mount &#x2F;dev&#x2F;sdb1 &#x2F;ceph&#x2F;sdb</span><br></pre></td></tr></table></figure><p>最后启动OSD，这里最重要的就是把我们刚刚挂载好的OSD的实际路径透传给Docker内部的/var/lib/ceph/osd，如果每个节点有多个OSD的情况下，只需要在Host上映射到不同的目录，启动Docker的时候变更和/var/lib/ceph/osd的映射关系即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -v &#x2F;dev&#x2F;:&#x2F;dev&#x2F; \</span><br><span class="line">    -v &#x2F;ceph&#x2F;sdb:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd \</span><br><span class="line">    --privileged&#x3D;true \</span><br><span class="line">    ceph&#x2F;daemon osd_directory</span><br></pre></td></tr></table></figure><p>按照同样的方法，将node2和node3的OSD也加入到集群，最终的效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        64 pgs degraded</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        64 pgs undersized</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e13: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        4551 MB used, 11306 MB &#x2F; 16720 MB avail</span><br><span class="line">              64 active+undersized+degraded</span><br></pre></td></tr></table></figure><h3 id="创建MDS"><a href="#创建MDS" class="headerlink" title="创建MDS"></a>创建MDS</h3><p>创建好基本的环境，其他的就容易了很多，下面来启动MDS。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -e CEPHFS_CREATE&#x3D;1 \</span><br><span class="line">    ceph&#x2F;daemon mds</span><br></pre></td></tr></table></figure><h3 id="启动RGW，并且映射80端口"><a href="#启动RGW，并且映射80端口" class="headerlink" title="启动RGW，并且映射80端口"></a>启动RGW，并且映射80端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    -p 80:80 \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    ceph&#x2F;daemon rgw</span><br></pre></td></tr></table></figure><h3 id="最终的集群状态"><a href="#最终的集群状态" class="headerlink" title="最终的集群状态"></a>最终的集群状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        48 pgs stuck inactive</span><br><span class="line">        48 pgs stuck unclean</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> mdsmap e5: 1&#x2F;1&#x2F;1 up &#123;0&#x3D;mds-node1.docker.com&#x3D;up:active&#125;</span><br><span class="line"> osdmap e25: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v38: 128 pgs, 9 pools, 588 bytes data, 11 objects</span><br><span class="line">        6791 MB used, 16996 MB &#x2F; 25081 MB avail</span><br><span class="line">              80 active+clean</span><br><span class="line">              45 creating</span><br><span class="line">               3 creating+activating</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Docker中部署Ceph并没有想象中的那么顺利，社区的版本中仍然有Bug需要解决。</p><p>Docker作为一种快捷的部署方式，的确可以大幅度提高Ceph的部署效率，提高扩展的速度。但是从另一个角度我们应该注意到，随着Docker的引入也改变了Ceph的运维方式，比如在OSD增减的时候，需要到容器中对Ceph集群进行维护。再比如配置文件变更后的重启问题等。</p><p>但是无论如何，我相信这些问题都会得到完美的解决，用Docker部署Ceph作为一种新的尝试，值得推广。<br>之后还会为大家带来，如何使用Ansible结合Docker更快速的部署Ceph集群，敬请期待。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是根据Sébastien Han的&lt;a href=&quot;https://www.youtube.com/watch?v=FUSTjTBA8f8&amp;feature=youtu.be&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;演示视频&lt;/a&gt;进行整理的，对过程中有问题的部分进行了修复。&lt;/p&gt;
&lt;p&gt;Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.me/categories/Ceph/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Ceph/Docker/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Mitaka国内代码贡献</title>
    <link href="http://sunqi.me/2016/04/07/contribution-in-mitaka/"/>
    <id>http://sunqi.me/2016/04/07/contribution-in-mitaka/</id>
    <published>2016-04-07T07:19:39.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。</p><p>去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。</p><p>这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。</p><p>对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>在Big Tent公布之后，OpenStack的项目被分为Core Projects和Big Tent Projects。</p><img src="/images/blogs/contribution-in-mitaka-big-tent.jpg" class="center"><p>让我们来看一下在Mitaka版本中，多了哪些新项目。</p><ul><li>几个与Docker相关的项目被发布出来，magnum, senlin, solum</li><li>数据备份容灾的项目：freezer</li><li>计费的项目：cloudkitty</li><li>NFV相关的项目：tracker</li><li>监控相关的项目：monasca</li></ul><p>关于这些新项目的一些介绍，我将放在另外一篇博客里，敬请关注。</p><img src="/images/blogs/contribution-in-mitaka-projects.png" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司数量来看，Mitaka版本略有下降，但是参与的人数多了100多人。</p><img src="/images/blogs/contribution-in-mitaka-companies-contributors.png" class="center"><p>整个社区的公司贡献排名上没有明显的变化，传统的几大豪强仍然霸占公司排名的前十位，华为表现依然强劲，是中国区唯一能进入前十名的公司。</p><p>在模块方面，整体统计的绝大部分比例已经被others所占据，说明在Big Tent计划下，OpenStack正在朝更多元化的方向演进。在Mitaka排名前十位的项目中，fuel相关的两个项目都进入了前十，说明fuel在OpenStack部署的地位已经越来越重要了。同时，核心项目中的nova，neutron，cinder项目仍然在前十名的范围内，贡献量基本保持不变。值得一提的是，在Mitaka统计的项目数量已经从Liberty的708个增长到了829个，可见在短短的6个月内，OpenStack社区的蓬勃发展。</p><img src="/images/blogs/contribution-in-mitaka-companies-modules.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完了整体统计，我们再回到国内，因为已经有文章做了我在Liberty时候的分析，所以这里我换个角度来看国内的社区贡献，首先是统计排名的变化。</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在Liberty中，有13家国内企业为社区做了贡献，在Mitaka中这个数量增加到了15家企业，这里简单的将这些企业做了一下分类：</p><ul><li>互联网用户：乐视、新浪、网易</li><li>电信用户：中国移动</li><li>传统IT服务商：华为、中兴、华三</li><li>私有云服务商：Easystack、九州云、海云捷迅、北京有云、麒麟云、UMCloud、象云、Huron(休伦科技)</li></ul><img src="/images/blogs/contribution-in-mitaka-china-companies.png" class="center"><h3 id="行业分析"><a href="#行业分析" class="headerlink" title="行业分析"></a>行业分析</h3><p>通过行业的分析我们可以看出，国内的主要贡献仍然来自私有云服务商和传统IT服务商，换言之来自于以OpenStack提供产品或者服务的公司。厂商们贡献的目的很明确，主要为了展示自身在开源项目中的积累和专家形象。而用户的贡献主要来自平时在使用OpenStack时候遇到Bug，就是在实际应用过程中出现的问题。</p><img src="/images/blogs/contribution-in-mitaka-china-by-industry.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>单纯的社区贡献排名的比较仅仅是一个维度，下面我们来看一下各个公司的人员投入情况：</p><ul><li>排名前几位的公司对社区投入的人力基本都是两位数，相对于Liberty版本，人员均有所增加</li><li>在人均贡献投入上，99cloud是国内最高的，平均达到了59天，甚至超过了华为，这个统计不仅仅包含了代码贡献，还包含了邮件、Review、Blueprint的时间，基本可以衡量每个公司在OpenStack社区贡献方面的投入力量</li><li>人员投入来看，Easystack和中国移动无疑是最下本的两家，Easystack从Liberty的3人，增长到了23人，一下子增加了20人；中国移动也从最初的4个人，增加到了13个人，可见中国移动未来对OpenStack的野心</li></ul><img src="/images/blogs/contribution-in-mitaka-companies-effort.png" class="center"><h3 id="贡献模块分析"><a href="#贡献模块分析" class="headerlink" title="贡献模块分析"></a>贡献模块分析</h3><p>从模块的角度进行统计，国内企业的贡献情况并未出现一个统一的趋势，总体的贡献项目为193个，项目几乎涉及OpenStack所有最活跃的项目，从排名前十的项目来看：</p><ul><li>得益于华为的主导，dargonflow项目的贡献量超高</li><li>紧随其后的，也是当下的热点，容器相关的两个项目</li><li>几大OpenStack老模块贡献量也高居前十位，说明这些模块是在解决方案中使用频率较高的</li></ul><img src="/images/blogs/contribution-in-mitaka-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这是一个很敏感的话题，每个公司对社区的投入到底换来多少项目上的回报呢？可能这只有每个公司的CEO能够回答的问题了。我在这里就不多做过多的分析，留给大家充分讨论的空间吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>刚刚结束在南京的OpenStack开发培训，也了解到5G的通信网络上已经确定引入了OpenStack，虽然我说不清楚他的具体用途，但是我相信这对OpenStack这个项目、社区是一个重大的利好消息。我也相信，通过国内企业的集体努力，一定能让OpenStack在中国遍地开花结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。&lt;/p&gt;
&lt;p&gt;去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。&lt;/p&gt;
&lt;p&gt;这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。&lt;/p&gt;
&lt;p&gt;对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenStack培训的用户体验</title>
    <link href="http://sunqi.me/2016/03/27/openstack-training-user-experience/"/>
    <id>http://sunqi.me/2016/03/27/openstack-training-user-experience/</id>
    <published>2016-03-27T02:42:38.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？</p><p>这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。</p><a id="more"></a><h2 id="关于我"><a href="#关于我" class="headerlink" title="关于我"></a>关于我</h2><p>简单来说，我带过OpenStack产品的研发团队，谈过OpenStack的合作，做过OpenStack培训讲师，也卖过OpenStack的私有云产品，也和大量的用户聊过OpenStack，所以还算是对OpenStack这个行业整体上有个清晰认识。</p><h2 id="OpenStack培训的目标群体"><a href="#OpenStack培训的目标群体" class="headerlink" title="OpenStack培训的目标群体"></a>OpenStack培训的目标群体</h2><p>我做过的OpenStack培训大体上分为两类，内训和外训。</p><p>内训是面向公司内部，因为我曾经带过的两个团队都是以开发OpenStack私有云产品为主的，所以我的培训对象主要是研发、运维、售前和销售人员。</p><p>外训的对象很多，包括知名的国企、外企和民营企业以及学校，行业大部分以传统行业为主，涉及通讯、金融、系统集成等，面向的群体主要是研发、IT和售前，培训的内容以OpenStack的基础和研发为主。</p><p>所以我把OpenStack培训的目标群体定义为：研发人员、系统工程师和运维人员、售前、销售人员、学生。</p><h2 id="针对不同的群体，到底需要哪些培训？"><a href="#针对不同的群体，到底需要哪些培训？" class="headerlink" title="针对不同的群体，到底需要哪些培训？"></a>针对不同的群体，到底需要哪些培训？</h2><h3 id="销售人员"><a href="#销售人员" class="headerlink" title="销售人员"></a>销售人员</h3><p>现在做OpenStack生意的无外乎两种：产品和服务。无论是哪一种，对传统的销售人员都是一种极大的挑战。云平台并不像传统软件一样，能够一眼看明白他到底是做什么的，解决了用户的哪些痛点。并且在企业中，能够做决策的人往往并不全是技术出身，所以想和他们解释清楚OpenStack到底能做什么，又是难上艰难。</p><p>所以对于销售人员来讲，培训的重点应该有以下几点：</p><ul><li>使用培训：我觉得无论为哪一类群体培训，演示如何使用OpenStack，都是最有效的帮助人理解的方式。但是这里的演示，必须要设定场景，即传统的业务形态下我们的业务系统是什么样子的，迁移到云平台后该如何部署，从这种比较中，加深对OpenStack的理解。销售人员通过对OpenStack操作，加深对OpenStack或OpenStack产品的理解。毕竟图形是最高效的一种记忆方式。</li><li>理解什么是开源软件：开源软件一定是未来的发展趋势，如果无法对开源软件有一个清晰的认识，也就无法理解清楚OpenStack这个项目出现的价值和意义。</li><li>了解OpenStack的发展历史、OpenStack基金会以及OpenStack社区的运营方式：学习这些的目的是为了给用户讲故事，让用户了解为什么要选择OpenStack，为什么OpenStack项目有持续的生命力，让用户相信使用了OpenStack能够保证未来的基础架构灵活面对业务层面敏捷性的需求。</li><li>案例学习：案例最大的价值就是教育用户，VMWare花了十几年的时间教育了用户，OpenStack不可能在短短的几年时间内就改变这样的局面，所以“学会用别人的案例来教育自己的用户”，是在销售人员OpenStack培训中非常重要的一课。</li></ul><h3 id="售前人员"><a href="#售前人员" class="headerlink" title="售前人员"></a>售前人员</h3><p>售前人员不但要从技术层面让用户信服产品，而且还要结合用户的业务系统需求提供建设方案，外企中的很多售前工程师还要承担搭建POC环境的职责。售前人员沟通的主要对象是企业中有实际需求的业务部门，也是最有可能落地的部门，沟通的成败决定了是否能签单，所以需要更多的专业知识来满足和用户的沟通需要。 培训的重点应该是：</p><ul><li>使用培训：理由同上，但是我觉得售前人员还需要站在用户的角度来思考一下，我的用户到底会如何使用云平台？业务系统迁移到平台后，会有哪些问题？</li><li>如何部署：部署培训向来是各大OpenStack培训必讲的内容，而且90%的内容都是围绕部署展开的，例如某知名企业的OpenStack授证培训。对于售前人员，我认为OpenStack部署训练还是很有必要的。一方面，能够帮助培训对象快速理解OpenStack的架构；另一方面，也能在未来的方案设计上提供参考和依据。由于云平台在使用上与企业传统的IT环境有较大的区别，所以售前人员在学习过程中，应该更多的了解OpenStack部署的特点，服务和服务之间的关系，云平台高可靠等和生产环境部署息息相关的问题。另外还要关注，用户的业务系统迁移到云平台后，可能带来的变化以及应对方式。例如：OpenStack里的网络分为fixed ip和floating ip，但是用户原有的业务系统只会有一个IP，这时候就需要考虑如何为用户选择适当的部署方案。</li><li>OpenStack架构：掌握OpenStack模块的基本工作原理和模块的详细作用。学习这些内容，是为了帮助售前人员在和用户后续交流中，帮助用户选择适当的模块解决用户的需求。</li><li>OpenStack的发展趋势：这部分内容就是能够引导客户未来的项目需求。例如在分布式存储，NFV和SDN方面。</li></ul><h3 id="系统工程师和运维人员"><a href="#系统工程师和运维人员" class="headerlink" title="系统工程师和运维人员"></a>系统工程师和运维人员</h3><p>Iaas云平台不但是对传统的企业IT架构进行了变革，也从管理上对企业原有的流程形成了冲击。需要培训的用户往往集中在自用OpenStack云平台的企业。</p><ul><li>使用培训，不同于上面两种简单的使用，运维人员要求对OpenStack管理部分的使用也要有很深的理解，而且还需要掌握命令行方式的相关操作。</li><li>OpenStack架构，了解OpenStack内部的工作原理，有助于快速定位问题，对系统进行维护。这部分包含的内容比较多，从OpenStack自身的原理到虚拟机，存储，再到虚拟网络的实现都需要有一个系统的了解才可以。</li><li>部署培训，要求详细掌握安装的过程，了解全部配置文件的功能及常用选项和参数。</li><li>自动化部署培训，手动部署即耗费时间又不能保证准确，所以作为运维人员，必须要掌握至少一种自动化部署的方法。这方面的方案有很多，从TripleO、Fuel到Puppet，Salt，Ansible。个人还是推崇应该选择Salt或者Ansible的一种进行学习和掌握。</li><li>运维培训，要求就是在云平台出现问题之后快速定位问题。</li><li>自动化运维培训，DevOps作为未来运维的趋势，反复被提到。云平台自动化运维的内容很多，部署、监控、告警、自动巡检、健康检查等等，使用的工具无外乎上面提到的Salt或者Ansible这样的工具。自动化运维不仅仅是云平台未来培训的一大趋势，也是企业有需求的培训内容。</li></ul><h3 id="开发人员"><a href="#开发人员" class="headerlink" title="开发人员"></a>开发人员</h3><p>开发人员对OpenStack培训的需求主要和未来的工作有关（除了是公司强制或者兴趣之外），从我的经验来看：一种是基于OpenStack API开发，一种是开发OpenStack。所以针对两种不同的需求，培训内容需要单独进行设计，总体来说后一种包含前一种培训。</p><p>与之前几种培训不同，我认为部署培训对开发人员并不是必须的，因为在实际工作中，开发人员很难有机会真正接触到安装过程，这部分工作往往由公司的IT人员去完成，并且其中涉及到大量的Linux基础命令，很多研发人员其实对这部分并不是十分熟悉，所以即使学习了安装内容，也还是一知半解。与其在安装上浪费时间，不如多了解一些架构方面的细节。</p><ul><li>使用培训，帮助开发人员快速了解OpenStack。</li><li>了解社区的开发流程，OpenStack之所以发展到今天的程度，和社区的代码的管理流程密不可分，所以这部分是值得每一名开发人员学习的。</li><li>搭建研发环境，既然要开发OpenStack就应该按照开发的方式搭建研发环境，这样屏蔽了很多安装上的细节，并且让开发人员有个快速能使用和开发的环境。</li><li>基于OpenStack API开发，这部分应该是个重点，我通常会设定一个具体的用户需求，通过解决用户需求来了解API的使用。例如：作为一名用户，我想给我的虚拟机挂卷并自动分区，挂载到/mnt目录。这里的内容包含API文档的使用，通过浏览器REST Client插件详细了解OpenStack API的调用过程，学习使用OpenStack SDK。</li><li>OpenStack编排服务，将API开发中的场景，用编排服务加以实现，还可以包含Scaling和Auto Scaling的场景。这部分很可能是开发人员在未来开发中非常需要的一部分内容。</li><li>OpenStack发展方向，OpenStack的大帐篷展现了对未来的野心，所以了解OpenStack未来的发展方向是很有必要的。</li></ul><p>针对于以后开发OpenStack的研发人员，还需要根据实际的开发内容增加以下的培训内容：</p><ul><li>OpenStack通用技术，学习OpenStack的通用技术有助于理解OpenStack的所有模块，这部分内容主要包括：Eventlet，REST和WSGI，Taskflow，OSLO项目等诸多重要的类库。</li><li>典型模块的架构及开发入门，这里面推荐的模块包含：Nova/Neutron/Horizon/Ceilometer，这几种模块几乎涵盖了OpenStack大部分模块的架构，所以重点理解这些模块的架构和工作原理，对于理解整个OpenStack项目都非常有帮助。直接将代码其实真的很困难，我习惯于使用场景的方式追踪代码的运行轨迹，从而整理出时序图的方式讲解。</li></ul><h3 id="学生"><a href="#学生" class="headerlink" title="学生"></a>学生</h3><p>学生群体事实上是相当有潜力的市场，现在国内OpenStack人才紧缺，所以OpenStack一定要从大学抓起。学生对OpenStack的学习不能仅仅停留在OpenStack本身，与之相关的内容都要学习，但是又不建议完全理论化的学习，强调动手的能力是关键。例如：对Python的学习，虚拟化软件的学习，OpenStack的安装，OpenStack的开发进行循序渐进的学习。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>我认为培训中很重要的一环就是让学员动手，否则培训的效果不会很好。以上就是我对OpenStack培训的粗浅认识，还请各位多多指教。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;尽管在云计算领域仍然有很大的争议，但是OpenStack事实上已经成为Iaas云平台的事实标准和首选的平台。从培训市场的火热也证明了这一点，现在的OpenStack培训有很多，讲的内容也不尽相同，那么哪一种培训才是用户最需要的呢？&lt;/p&gt;
&lt;p&gt;这篇文章并不是要评价任何一个OpenStack培训，只是想从用户体验的角度分析一下，到底什么才是用户真正需要的。如果文章观点有任何不妥，还请各位前辈和大牛们多多海涵。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul主要使用场景</title>
    <link href="http://sunqi.me/2015/12/24/use-consul/"/>
    <id>http://sunqi.me/2015/12/24/use-consul/</id>
    <published>2015-12-24T02:07:24.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：<a href="http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/</a></p><p>这篇文章里主要介绍Consul的使用场景，服务和健康检查。</p><a id="more"></a><h2 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h2><p>服务注册有点像OpenStack Keystone的Endpoints，可以通过API方式查询到所有服务的端点信息。</p><p>在Agent的节点上添加一个service，之后重启服务。</p><ul><li>添加一个服务</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80&#125;&#125;&#39; \</span><br><span class="line">    &gt;&#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><p>重新加载新的服务并不需要杀死进程重启服务，只需要给进程直接发送一个SIGHUP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:01:11 [INFO] agent: Synced service &#39;web&#39;</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>我们在任意节点上利用REST API查看服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;catalog&#x2F;service&#x2F;web</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;Address&quot;:&quot;200.21.1.101&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;,&quot;ServiceTags&quot;:[&quot;rails&quot;],&quot;ServiceAddress&quot;:&quot;&quot;,&quot;ServicePort&quot;:80&#125;]</span><br></pre></td></tr></table></figure><h2 id="Health-Check"><a href="#Health-Check" class="headerlink" title="Health Check"></a>Health Check</h2><p>健康检查的方法主要是通过运行一小段脚本的方式，根据运行的结果判断检查对象的健康状况。所以可以通过任意语言定义这个脚本，脚本运行将通过和consul执行的相同用户执行。</p><ul><li>添加一个健康检查</li></ul><p>每30秒ping google.com</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;check&quot;: &#123;&quot;name&quot;: &quot;ping&quot;,</span><br><span class="line">  &quot;script&quot;: &quot;ping -c1 google.com &gt;&#x2F;dev&#x2F;null&quot;, &quot;interval&quot;: &quot;30s&quot;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;ping.json</span><br></pre></td></tr></table></figure><p>为刚才的服务添加健康检查</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ echo &#39;&#123;&quot;service&quot;: &#123;&quot;name&quot;: &quot;web&quot;, &quot;tags&quot;: [&quot;rails&quot;], &quot;port&quot;: 80,</span><br><span class="line">  &quot;check&quot;: &#123;&quot;script&quot;: &quot;curl localhost &gt;&#x2F;dev&#x2F;null 2&gt;&amp;1&quot;, &quot;interval&quot;: &quot;10s&quot;&#125;&#125;&#125;&#39; \</span><br><span class="line">    &gt; &#x2F;etc&#x2F;consul.d&#x2F;web.json</span><br></pre></td></tr></table></figure><ul><li>重启agent</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ kill -HUP $(ps -ef | grep agent | grep -v grep | awk &#39;&#123;print $2&#125;&#39;)</span><br></pre></td></tr></table></figure><ul><li>日志输出</li></ul><p>从输出的日志上都可以看到加载了新的服务web。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; Caught signal: hangup</span><br><span class="line">&#x3D;&#x3D;&gt; Reloading configuration...</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced service &#39;web&#39;</span><br><span class="line">    2015&#x2F;12&#x2F;24 12:43:56 [INFO] agent: Synced check &#39;ping&#39;</span><br></pre></td></tr></table></figure><p>经过一段时间后出现了critical和warning日志</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 12:43:58 [WARN] agent: Check &#39;service:web&#39; is now critical</span><br><span class="line">2015&#x2F;12&#x2F;24 12:44:08 [WARN] agent: Check &#39;ping&#39; is now warning</span><br></pre></td></tr></table></figure><ul><li>利用API查询</li></ul><p>Health check的状态包含了很多种，有any, unkown, passing, warning, critical。any包含了所有状态。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl http:&#x2F;&#x2F;localhost:8500&#x2F;v1&#x2F;health&#x2F;state&#x2F;critical</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&#123;&quot;Node&quot;:&quot;server1.consul.com&quot;,&quot;CheckID&quot;:&quot;service:web&quot;,&quot;Name&quot;:&quot;Service &#39;web&#39; check&quot;,&quot;Status&quot;:&quot;critical&quot;,&quot;Notes&quot;:&quot;&quot;,&quot;Output&quot;:&quot;&quot;,&quot;ServiceID&quot;:&quot;web&quot;,&quot;ServiceName&quot;:&quot;web&quot;&#125;]</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://www.consul.io/docs/agent/http/catalog.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/catalog.html</a></li><li><a href="http://www.consul.io/docs/agent/http/health.html" target="_blank" rel="noopener">http://www.consul.io/docs/agent/http/health.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;假设你已经按照之前的Consul安装方法部署了一套具备环境，具体方法可以参考：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/12/07/consul-installation/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;这篇文章里主要介绍Consul的使用场景，服务和健康检查。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>Consul的安装方法</title>
    <link href="http://sunqi.me/2015/12/06/consul-installation/"/>
    <id>http://sunqi.me/2015/12/06/consul-installation/</id>
    <published>2015-12-06T18:00:13.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="什么是Consul"><a href="#什么是Consul" class="headerlink" title="什么是Consul?"></a>什么是Consul?</h2><p>Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。</p><a id="more"></a><h2 id="安装Consul"><a href="#安装Consul" class="headerlink" title="安装Consul"></a>安装Consul</h2><p>Consul的安装比较简单，下载之后直接解压缩就可以了，下载地址：<a href="https://www.consul.io/downloads.html" target="_blank" rel="noopener">https://www.consul.io/downloads.html</a></p><p>我们把consul直接放在/usr/local/bin目录中。</p><h2 id="Consul-Server"><a href="#Consul-Server" class="headerlink" title="Consul Server"></a>Consul Server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -server -bootstrap-expect 3 -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;server1 -bind&#x3D;10.10.10.10</span><br></pre></td></tr></table></figure><h3 id="参数说明"><a href="#参数说明" class="headerlink" title="参数说明"></a>参数说明</h3><ul><li>-server - Serve模式</li><li>-bootstrap-expect - Server数量</li><li>-data-dir - 数据目录</li><li>-node - Node名称</li><li>-bind - 集群通讯地址</li></ul><h3 id="输出"><a href="#输出" class="headerlink" title="输出"></a>输出</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: Expect Mode enabled, expecting 3 servers</span><br><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;server1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: true (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.101 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] serf: EventMemberJoin: server1.consul.com.dc1 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] raft: Node at 200.21.1.101:8300 [Follower] entering Follower state</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [INFO] consul: adding server server1.consul.com.dc1 (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:36 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:37 [WARN] raft: EnableSingleNode disabled, and no known peers. Aborting election.</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:13:51 [ERR] agent: failed to sync remote state: No cluster leader</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;23 03:14:17 [ERR] agent: failed to sync remote state: No cluster leader</span><br></pre></td></tr></table></figure><h3 id="查看成员"><a href="#查看成员" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="Consul-Agent"><a href="#Consul-Agent" class="headerlink" title="Consul Agent"></a>Consul Agent</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ &#x2F;usr&#x2F;local&#x2F;bin&#x2F;consul agent -data-dir &#x2F;tmp&#x2F;consul -node&#x3D;agent1 -bind&#x3D;10.10.10.100 -config-dir &#x2F;etc&#x2F;consul.d</span><br></pre></td></tr></table></figure><ul><li>输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">&#x3D;&#x3D;&gt; WARNING: It is highly recommended to set GOMAXPROCS higher than 1</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent...</span><br><span class="line">&#x3D;&#x3D;&gt; Starting Consul agent RPC...</span><br><span class="line">&#x3D;&#x3D;&gt; Consul agent running!</span><br><span class="line">         Node name: &#39;agent1.consul.com&#39;</span><br><span class="line">        Datacenter: &#39;dc1&#39;</span><br><span class="line">            Server: false (bootstrap: false)</span><br><span class="line">       Client Addr: 127.0.0.1 (HTTP: 8500, HTTPS: -1, DNS: 8600, RPC: 8400)</span><br><span class="line">      Cluster Addr: 200.21.1.201 (LAN: 8301, WAN: 8302)</span><br><span class="line">    Gossip encrypt: false, RPC-TLS: false, TLS-Incoming: false</span><br><span class="line">             Atlas: &lt;disabled&gt;</span><br><span class="line"></span><br><span class="line">&#x3D;&#x3D;&gt; Log data will now stream in as it occurs:</span><br><span class="line"></span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [WARN] memberlist: Binding to public address without encryption!</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:51 [ERR] agent: failed to sync remote state: No known Consul servers</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent.rpc: Accepted client: 127.0.0.1:42794</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] agent: (LAN) joining: [200.21.1.101 200.21.1.102 200.21.1.103]</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] serf: EventMemberJoin: server1.consul.com 200.21.1.101</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:56 [INFO] consul: adding server server1.consul.com (Addr: 200.21.1.101:8300) (DC: dc1)</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:09:58 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent: (LAN) joined: 1 Err: &lt;nil&gt;</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:02 [INFO] agent.rpc: Accepted client: 127.0.0.1:42800</span><br><span class="line">&#x3D;&#x3D;&gt; Newer Consul version available: 0.6.0</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:22 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:10:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:02 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:23 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:41 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:11:43 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:12 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:21 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:12:36 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:01 [WARN] agent: Check &#39;ping&#39; is now warning</span><br><span class="line">    2015&#x2F;12&#x2F;24 08:13:03 [ERR] agent: failed to sync remote state: rpc error: No cluster leader</span><br></pre></td></tr></table></figure><ul><li>server日志输出</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">2015&#x2F;12&#x2F;24 08:09:58 [INFO] serf: EventMemberJoin: agent1.consul.com 200.21.1.201</span><br></pre></td></tr></table></figure><h3 id="查看成员-1"><a href="#查看成员-1" class="headerlink" title="查看成员"></a>查看成员</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="最终结果"><a href="#最终结果" class="headerlink" title="最终结果"></a>最终结果</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ consul members</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Node                Address            Status  Type    Build  Protocol  DC</span><br><span class="line">server1.consul.com  200.21.1.101:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent1.consul.com   200.21.1.201:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">agent2.consul.com   200.21.1.202:8301  alive   client  0.5.2  2         dc1</span><br><span class="line">server2.consul.com  200.21.1.102:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">server3.consul.com  200.21.1.103:8301  alive   server  0.5.2  2         dc1</span><br><span class="line">agent3.consul.com   200.21.1.203:8301  alive   client  0.5.2  2         dc1</span><br></pre></td></tr></table></figure><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://www.consul.io/intro/getting-started/install.html" target="_blank" rel="noopener">https://www.consul.io/intro/getting-started/install.html</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;什么是Consul&quot;&gt;&lt;a href=&quot;#什么是Consul&quot; class=&quot;headerlink&quot; title=&quot;什么是Consul?&quot;&gt;&lt;/a&gt;什么是Consul?&lt;/h2&gt;&lt;p&gt;Consul拥有众多的组件，简言之，就是一个用于在你的基础设施中，发现和配置服务的工具。包含以下关键功能：服务发现、健康检查、键值存储和多数据中心支持。再说的通俗一点，就是用于管理分布式系统的利器。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
      <category term="Docker" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/"/>
    
      <category term="Consul" scheme="http://sunqi.me/categories/Cloud-Computing/Docker/Consul/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Grafana+Diamond+Graphite构造完美监控面板</title>
    <link href="http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/"/>
    <id>http://sunqi.me/2015/11/30/use-grafana-to-monitor-your-cluster/</id>
    <published>2015-11-30T15:59:46.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。</p><p>本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：</p><img src="/images/blogs/grafana-screenshot.png" class="center" title="800x600"><a id="more"></a><h2 id="安装及配置Dimaond"><a href="#安装及配置Dimaond" class="headerlink" title="安装及配置Dimaond"></a>安装及配置Dimaond</h2><p>安装Diamond最直接和简单的方法就是自己编译RPM或者DEB的安装包, Diamond在这方面提供了比较好的支持。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cd /root</span></span><br><span class="line"><span class="comment"># yum install -y git rpm-build python-configobj python-setuptools</span></span><br><span class="line"><span class="comment"># git clone https://github.com/python-diamond/Diamond</span></span><br><span class="line"><span class="comment"># cd Diamond</span></span><br><span class="line"><span class="comment"># make rpm</span></span><br><span class="line"><span class="comment"># cd dist</span></span><br><span class="line"><span class="comment"># rpm -ivh diamond-*.noarch.rpm</span></span><br></pre></td></tr></table></figure><p>默认情况下，Diamond开启了基本的监控信息，包括CPU、内存、磁盘的性能数据。当然，我们可以通过配置启动相应的监控项，也能通过自定义的方式进行相应的扩展。这里，我们在/etc/diamond/collectors加载额外的插件，下面的例子中开启了网络的监控。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cp -f /etc/diamond/diamond.conf.example /etc/diamond/diamond.conf</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/diamond/diamond.conf</span></span><br><span class="line">[configs]</span><br><span class="line">path = <span class="string">"/etc/diamond/collectors/"</span></span><br><span class="line">extension = <span class="string">".conf"</span></span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/net.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[NetworkCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>那么到目前为止，Diamond的基本安装和配置已经完成，但是现在只是简单的采集数据，并没有指明数据要发送给谁，所以下一步我们来开始配置Graphite。</p><h2 id="安装及配置Graphite"><a href="#安装及配置Graphite" class="headerlink" title="安装及配置Graphite"></a>安装及配置Graphite</h2><p>Graphite主要做两件事情：按照时间存储数据、生成图表，在我们的场景里面，实质上就是把Graphite作为数据源给Grafana提供数据。另外还需要安装的是carbon，负责通过网络接受数据并保存到后端存储中；另外还需要whisper，负责生成Graphite样式的基于文件的时间序列的数据库。</p><h3 id="安装软件包"><a href="#安装软件包" class="headerlink" title="安装软件包"></a>安装软件包</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y graphite-web graphite-web-selinux</span></span><br><span class="line"><span class="comment"># yum install -y mysql mysql-server MySQL-python</span></span><br><span class="line"><span class="comment"># yum install -y python-carbon python-whisper</span></span><br></pre></td></tr></table></figure><h3 id="配置MySQL"><a href="#配置MySQL" class="headerlink" title="配置MySQL"></a>配置MySQL</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># /etc/init.d/mysqld start</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># mysql -e "CREATE DATABASE graphite;" -u root</span></span><br><span class="line"><span class="comment"># mysql -e "GRANT ALL PRIVILEGES ON graphite.* TO 'graphite'@'localhost' IDENTIFIED BY 'sysadmin';" -u root</span></span><br><span class="line"><span class="comment"># mysql -e 'FLUSH PRIVILEGES;' -u root</span></span><br></pre></td></tr></table></figure><h3 id="配置Graphite"><a href="#配置Graphite" class="headerlink" title="配置Graphite"></a>配置Graphite</h3><ul><li>local setting</li></ul><figure class="highlight bash"><figcaption><span>/etc/graphite-web/local_settings.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SECRET_KEY=$(md5sum /etc/passwd | awk &#123;'print $1'&#125;)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "SECRET_KEY = '$SECRET_KEY'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"><span class="comment"># echo "TIME_ZONE = 'Asia/Shanghai'" | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee -a /etc/graphite-web/local_settings.py</span></span><br><span class="line">DATABASES = &#123;</span><br><span class="line">    <span class="string">'default'</span>: &#123;</span><br><span class="line">        <span class="string">'NAME'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'ENGINE'</span>: <span class="string">'django.db.backends.mysql'</span>,</span><br><span class="line">        <span class="string">'USER'</span>: <span class="string">'graphite'</span>,</span><br><span class="line">        <span class="string">'PASSWORD'</span>: <span class="string">'sysadmin'</span>,</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"></span><br><span class="line"><span class="comment"># cd /usr/lib/python2.6/site-packages/graphite</span></span><br><span class="line"><span class="comment"># ./manage.py syncdb --noinput</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># echo "from django.contrib.auth.models import User; User.objects.create_superuser('admin', 'admin@hihuron.com', 'sysadmin')" | ./manage.py shell</span></span><br></pre></td></tr></table></figure><ul><li>Apache配置</li></ul><figure class="highlight bash"><figcaption><span>/etc/httpd/conf.d/graphite-web.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Listen 0.0.0.0:10000</span><br><span class="line">&lt;VirtualHost *:10000&gt;</span><br><span class="line">    ServerName graphite-web</span><br><span class="line">    DocumentRoot <span class="string">"/usr/share/graphite/webapp"</span></span><br><span class="line">    ErrorLog /var/<span class="built_in">log</span>/httpd/graphite-web-error.log</span><br><span class="line">    CustomLog /var/<span class="built_in">log</span>/httpd/graphite-web-access.log common</span><br><span class="line">    Alias /media/ <span class="string">"/usr/lib/python2.6/site-packages/django/contrib/admin/media/"</span></span><br><span class="line"></span><br><span class="line">    WSGIScriptAlias / /usr/share/graphite/graphite-web.wsgi</span><br><span class="line">    WSGIImportScript /usr/share/graphite/graphite-web.wsgi process-group=%&#123;GLOBAL&#125; application-group=%&#123;GLOBAL&#125;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/content/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line"></span><br><span class="line">    &lt;Location <span class="string">"/media/"</span>&gt;</span><br><span class="line">        SetHandler None</span><br><span class="line">    &lt;/Location&gt;</span><br><span class="line">&lt;/VirtualHost&gt;</span><br></pre></td></tr></table></figure><ul><li>Diamond配置</li></ul><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># HOST_IP=$(ifconfig | sed -En 's/127.0.0.1//;s/.*inet (addr:)?(([0-9]*\.)&#123;3&#125;[0-9]*).*/\2/p' | head -1)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphiteHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br><span class="line"><span class="comment"># sed  -i "/^\[\[GraphitePickleHandler\]\]$/,/^\[.*\]/s/^host = 127.0.0.1$/host = $HOST_IP/" /etc/diamond/diamond.conf</span></span><br></pre></td></tr></table></figure><h3 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service carbon-cache restart</span></span><br><span class="line"><span class="comment"># service httpd restart</span></span><br><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure><h2 id="安装和配置Grafana"><a href="#安装和配置Grafana" class="headerlink" title="安装和配置Grafana"></a>安装和配置Grafana</h2><p>Grafana最主要的功能就是对数据的呈现，基于一切可提供time series的后台服务。这里面我们使用Graphite为Grafana提供数据。</p><h3 id="安装及配置"><a href="#安装及配置" class="headerlink" title="安装及配置"></a>安装及配置</h3><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># yum install -y nodejs</span></span><br><span class="line"><span class="comment"># rpm -ivh https://grafanarel.s3.amazonaws.com/builds/grafana-2.5.0-1.x86_64.rpm</span></span><br><span class="line"><span class="comment"># sudo /sbin/chkconfig --add grafana-server</span></span><br><span class="line"><span class="comment"># sed -i 's/^;http_port = 3000$/http_port = 10001/g' /etc/grafana/grafana.ini</span></span><br><span class="line"><span class="comment"># sudo service grafana-server start</span></span><br></pre></td></tr></table></figure><h3 id="添加datasource"><a href="#添加datasource" class="headerlink" title="添加datasource"></a>添加datasource</h3><p>Grafana提供了非常丰富的REST API，我们不仅可以直接利用Grafana作为数据呈现层，还可以利用REST API直接将Grafana的Graph集成在我们的应用中。下面我们利用REST API为Grafana添加datasource。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># curl -i 'http://admin:admin@localhost:10001/api/datasources' -X POST -H "Accept: application/json" -H "Content-Type: application/json" -d '&#123;"name": "graphite", "type": "graphite", "url": "http://localhost:10000", "access": "proxy", "basicAuth": false&#125;'</span></span><br></pre></td></tr></table></figure><h2 id="Ceph监控"><a href="#Ceph监控" class="headerlink" title="Ceph监控"></a>Ceph监控</h2><h3 id="修改ceph脚本兼容性"><a href="#修改ceph脚本兼容性" class="headerlink" title="修改ceph脚本兼容性"></a>修改ceph脚本兼容性</h3><p>Diamond是基于Python开发的，但是由于CentOS 6.5的Python版本较低(2.6)，所以直接使用社区版本的Ceph监控时，会导致错误。可以通过简单的修改进行修复。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_get_stats_from_socket</span><span class="params">(self, name)</span>:</span></span><br><span class="line">    <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">    dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">    an empty result set is returned.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">        <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">        <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">        <span class="comment">#     name,</span></span><br><span class="line">        <span class="comment">#     'perf',</span></span><br><span class="line">        <span class="comment">#     'dump',</span></span><br><span class="line">        <span class="comment">#     ])</span></span><br><span class="line">        cmd = [</span><br><span class="line">             self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">             <span class="string">'--admin-daemon'</span>,</span><br><span class="line">             name,</span><br><span class="line">             <span class="string">'perf'</span>,</span><br><span class="line">             <span class="string">'dump'</span>,</span><br><span class="line">        ]</span><br><span class="line">        process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">        json_blob = process.communicate()[<span class="number">0</span>]</span><br></pre></td></tr></table></figure><h3 id="增加对ceph-osd-perf监控"><a href="#增加对ceph-osd-perf监控" class="headerlink" title="增加对ceph osd perf监控"></a>增加对ceph osd perf监控</h3><p>在实际运维Ceph过程中，ceph osd perf是一个非常重要的指令，能够观察出集群中磁盘的latency的信息，通过观察变化，可以辅助判断磁盘出现性能问题。Diamond的设计中，每个Diamond Agent只会采集自己本机的指标，所以我们在添加的时候，只需要在一个节点上增加这个监控就可以了。在ceph.py中结尾处新增加一个类。</p><figure class="highlight python"><figcaption><span>/usr/share/diamond/collectors/ceph/ceph.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CephOsdCollector</span><span class="params">(CephCollector)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_get_stats</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Return the parsed JSON data returned when ceph is told to</span></span><br><span class="line"><span class="string">        dump the stats from the named socket.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        In the event of an error error, the exception is logged, and</span></span><br><span class="line"><span class="string">        an empty result set is returned.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment">#json_blob = subprocess.check_output(</span></span><br><span class="line">            <span class="comment">#    [self.config['ceph_binary'],</span></span><br><span class="line">            <span class="comment">#     '--admin-daemon',</span></span><br><span class="line">            <span class="comment">#     name,</span></span><br><span class="line">            <span class="comment">#     'perf',</span></span><br><span class="line">            <span class="comment">#     'dump',</span></span><br><span class="line">            <span class="comment">#     ])</span></span><br><span class="line">            cmd = [</span><br><span class="line">                 self.config[<span class="string">'ceph_binary'</span>],</span><br><span class="line">                 <span class="string">'osd'</span>,</span><br><span class="line">                 <span class="string">'perf'</span>,</span><br><span class="line">                 <span class="string">'--format=json'</span>,</span><br><span class="line">            ]</span><br><span class="line">            process = subprocess.Popen(cmd, stdout=subprocess.PIPE)</span><br><span class="line">            json_blob = process.communicate()[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">except</span> subprocess.CalledProcessError, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not get stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not get stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            json_data = json.loads(json_blob)</span><br><span class="line">        <span class="keyword">except</span> Exception, err:</span><br><span class="line">            self.log.info(<span class="string">'Could not parse stats from %s: %s'</span>,</span><br><span class="line">                          name, err)</span><br><span class="line">            self.log.exception(<span class="string">'Could not parse stats from %s'</span> % name)</span><br><span class="line">            <span class="keyword">return</span> &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> json_data</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_publish_stats</span><span class="params">(self, stats)</span>:</span></span><br><span class="line">        <span class="string">"""Given a stats dictionary from _get_stats_from_socket,</span></span><br><span class="line"><span class="string">        publish the individual values.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">for</span> perf <span class="keyword">in</span> stats[<span class="string">'osd_perf_infos'</span>]:</span><br><span class="line">            counter_prefix = <span class="string">'osd.'</span> + str(perf[<span class="string">'id'</span>])</span><br><span class="line">            <span class="keyword">for</span> stat_name, stat_value <span class="keyword">in</span> flatten_dictionary(</span><br><span class="line">                perf[<span class="string">'perf_stats'</span>],</span><br><span class="line">                prefix=counter_prefix,</span><br><span class="line">            ):</span><br><span class="line">              self.log.info(<span class="string">'stat_name is %s'</span>, stat_name)</span><br><span class="line">              self.log.info(<span class="string">'stat_value is %s'</span>, stat_value)</span><br><span class="line">              self.publish_gauge(stat_name, stat_value)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">collect</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Collect stats</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.log.info(<span class="string">'in ceph osd collector'</span>)</span><br><span class="line">        stats = self._get_stats()</span><br><span class="line">        self._publish_stats(stats)</span><br></pre></td></tr></table></figure><h3 id="修改Diamond监控配置"><a href="#修改Diamond监控配置" class="headerlink" title="修改Diamond监控配置"></a>修改Diamond监控配置</h3><figure class="highlight bash"><figcaption><span>/etc/diamond/collectors/ceph.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cat &lt;&lt; EOF | tee /etc/diamond/collectors/ceph.conf</span></span><br><span class="line">[collectors]</span><br><span class="line"></span><br><span class="line">[[CephCollector]]</span><br><span class="line">enabled = True</span><br><span class="line"></span><br><span class="line">[[CephOsdCollector]]</span><br><span class="line">enabled = True</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># service diamond restart</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;服务器监控软件五花八门，没有一个是对的，但是总有一款是适合你的，本文中将使用Grafana+Dimaond+Graphite构造一款漂亮的监控面板，你可以独自欣赏，也可以让他们和你的应用勾勾搭搭。&lt;/p&gt;
&lt;p&gt;本文中的安装测试，主要在CentOS 6.5下完成。先来张Grafna效果图，左边是我们的数据源Graphite，右边是我们的Grafna的效果图：&lt;/p&gt;
&lt;img src=&quot;/images/blogs/grafana-screenshot.png&quot; class=&quot;center&quot; title=&quot;800x600&quot;&gt;
    
    </summary>
    
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Liberty国内代码贡献</title>
    <link href="http://sunqi.me/2015/10/29/contribution-in-liberty/"/>
    <id>http://sunqi.me/2015/10/29/contribution-in-liberty/</id>
    <published>2015-10-29T02:56:06.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。</p><p>OpenStack Liberty完整版本的翻译可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans</a></p><p>本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。</p><a id="more"></a><h2 id="社区贡献分析"><a href="#社区贡献分析" class="headerlink" title="社区贡献分析"></a>社区贡献分析</h2><p>本次统计，并没有采用Review的数量为依据，而直接采用commits的方式，也就是代码实际merge入库的数量。</p><p>我们仍然要先看一下模块的贡献情况：</p><img src="/images/blogs/contribution-in-liberty-contribution-by-modules.png" class="left" title="400x300"><p>与之前Release的特点相似，OpenStack早期的核心模块Nova, Keystone代码commits数量出现明显下滑状态，而Neutron, Heat, Trove, Ceilometer, Cinder等模块都保持着稳中有升的态势。值得关注的是，在排名前20名的项目中，出现了两个直接与Docker有关的项目Kolla和Magnum，一个与docker间接有关的项目Murano。可以预见，OpenStack下一步发展的热点就是在与Docker之间的勾勾搭搭。</p><p>特别需要注意的是，在stackalytics.com统计的模块中，在Kilo中是259个，而到了Liberty到了389个，当然有一些项目并非完全是OpenStack的项目，但是也从一个侧面反映出OpenStack以及周边项目的蓬勃发展。</p><p>从更新日志中我们也能看到，本次Release的正式项目中，变动较大的是Neutron和Heat两个模块。在经历不断锤炼后，Neutron逐渐走向成熟，但是从生产级别角度看，Neutron的确还有很长的路要走。</p><h2 id="国内社区贡献分析"><a href="#国内社区贡献分析" class="headerlink" title="国内社区贡献分析"></a>国内社区贡献分析</h2><img src="/images/blogs/contribution-in-liberty-contributor.png" class="center" title="400x300"><p>从全球企业的贡献排名来看，排名状况基本变化不大，仍然是HP, Redhat, Mirantis, IBM, Rackspace, Intel, Cisco，但是非常欣喜的，国内的IT的航空母舰华为已经成功杀入前十名，这无疑是振奋人心的事情，希望华为未来能多一些对OpenStack社区的主导力，提高中国在OpenStack社区的地位，当然最好也能扶植一下国内的OpenStack创业公司，实现共同发展、共同进步。华为的主要代码贡献集中在dragonflow，magnum，heat等模块，特别是在dragonflow上，几乎全部是华为贡献的，magnum上也将近有五分之一的代码。</p><p><strong><em>华为社区贡献统计</em></strong></p><img src="/images/blogs/contribution-in-liberty-huawei.png" class="center" title="800x600"><p>记得在OpenStack五周年的庆祝活动上，Intel的陈绪博士说过，国内OpenStack贡献企业，就是一朵大云，四朵小云，下面让我们来看看这几朵小云在这个版本的表现。</p><p><strong>* 99cloud社区贡献统计*</strong></p><img src="/images/blogs/contribution-in-liberty-99cloud.png" class="center" title="800x600"><p>排名第16位的是99cloud，99cloud自上一个版本排名四朵小云之首后，本次继续强劲来袭，排名创造历史新高，第16名。通过对贡献模块的分析，我们能看出99cloud最大的贡献来自于社区文档，而在项目方面的贡献则主要来自murano-dashboard，horizon，neutron等项目上，从中可以看出99cloud对murano这个applicaton catalog的项目关注程度比较高，可能会在将来的产品中有所体现。从贡献中，我们隐约看到了九州云的副总裁李开总的提交，由此可见九州云为社区贡献的积极程度。<br>更加难能可贵的是，Horizon的全球贡献99cloud是全球前十，Tempest全球前八，Murano项目更是进入全球前三，相当给力。</p><p><strong>* UnitedStack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-unitedstack.png" class="center" title="800x600"><p>排在第30位的是UnitedStack，经过了上一个版本的短暂沉寂后，这个版本卷土重来，杀回前30。从代码贡献来看，UnitedStack的主要贡献来自python-openstackclient以及部署用到的puppet相关代码，当然对neutron、trove、kolla、heat等也有一定数量的贡献。</p><p><strong>* Kylin Cloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-kylincloud.png" class="center" title="800x600"><p>排名第38位的是麒麟云，其实麒麟云每次Release中总是有她的身影，但好像总是被忽略的。麒麟云最大的贡献来自Horizon项目，其他模块也有一定数量的贡献。总之，我们想到OpenStack企业的时候，的确应该时常提起麒麟云。</p><p><strong>* Easystack社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-easystack.png" class="center" title="800x600"><p>排名第70位的是Easystack，Easystack也属于OpenStack早期创业的公司，对于OpenStack的贡献也是持续的。Easystack最大的贡献来自nova，虽然数量不是很多，但是在国内企业里应该算名列前茅的啦。Easystack对Nova的贡献主要来自对libvirt层的bug修复。</p><p><strong>* Awcloud社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-awcloud.png" class="center" title="800x600"><p>排名第75位的是海云捷迅，海云应该算是在国内发展比较迅猛的一家OpenStack早期创业公司。他们的贡献主要来自Neutron相关的项目，看起来应该是为了解决项目中出现的实际问题所做的努力。海云的马力应该是公司内部贡献排名第一的，尤其是前一段时间发布的两篇关于”Neutron &amp; OpenStack漫谈”，非常值得一读。</p><p><strong>* LeTV社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-letv.png" class="center" title="800x600"><p><strong>* Netease社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-netease.png" class="center" title="800x600"><p>排名第94和95位的分别是两家互联网企业，乐视和网易，乐视是最近互联网中使用OpenStack动静最大的一家了，应该能在大规模应用中发现OpenStack很多问题吧。</p><p><strong>* Huron社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-huron.png" class="center" title="800x600"><p>排名第122位的是我的公司——北京休伦科技有限公司，其实我们公司也算是国内最早一批从事OpenStack创业的公司，z早在2013年的时候就已经开始投入OpenStack私有云产品相关的研发。我们贡献的代码主要来自Nova和Murano两个模块中，都是我们在开发和项目使用中发现的问题，修复后回馈给社区的，我也希望我们能在下一个版本Release中贡献更多的力量。</p><p><strong>* China Mobile社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-chinamobile.png" class="center" title="800x600"><p>排名第133位的是中国移动，之前并没有在哪一个排名上看到过中国移动在OpenStack贡献，我也是第一次发现。中国移动应该算是国内运营商领域技术实力较强的一家，也是运营商里开始从事OpenStack预研较早的一家。中国移动有大量的IT资源和设备，理应像AT&amp;T一样在OpenStack领域大有所为。纵观中国移动的社区贡献，主要来自Neutron和Ceilometer两个项目，几个Bug修复都是与Volume相关。</p><p><strong>* Lenovo社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-lenovo.png" class="center" title="800x600"><p>排名第135位的是联想。不评论了。</p><p>排名第139位的是清华大学医学院附属医院，这个有点意思。但是stackalytics.com有Bug，他们的具体统计显示不出来。</p><p><strong>* H3C社区贡献统计 *</strong></p><img src="/images/blogs/contribution-in-liberty-h3c.png" class="center" title="800x600"><p>排名第143位的是H3C。贡献是Nova中的关于VMware的Bug Fix。</p><p>由于stackalytics并没有按照区域统计的功能，所以本次统计完全是全自动统计(全靠我自己手动)，所以难免遗漏了为OpenStack贡献的国内企业，如果发生该情况请及时告知。</p><h2 id="社区贡献内容分析"><a href="#社区贡献内容分析" class="headerlink" title="社区贡献内容分析"></a>社区贡献内容分析</h2><img src="/images/blogs/contribution-in-liberty-complete-blueprints.png" class="center" title="800x600"><p>从贡献的commits的类型来区分，国内贡献出的代码主要还是以bug为主，这可能也与我们使用的都是OpenStack较成熟的模块有关，本身这些模块成熟程度较高，所以想做blueprint很难。另外一个很重要的原因是和OpenStack管理流程有关的，现在像Nova, Cinder等项目都是需要先Review Specs的，其实就是所谓的设计文档，语言成为国内很多工程师贡献的最大障碍，所以这也导致了Blueprint的贡献度在国内并不高。</p><p><strong>* Huawei社区贡献——完成Blueprint *</strong></p><img src="/images/blogs/contribution-in-liberty-blueprint-huawei.png" class="center" title="800x600"><p>纵观整个Blueprint的完成统计情况，华为作为国内最有实力的企业，高居全球第五名，完成最多的模块为cinder和mistral。</p><p>之后能完成Blueprint的企业还包括UnitedStack、中国移动、麒麟云、海云捷迅和九州云，但是相比来说数量较少，都是个位数字。</p><p>OpenStack在国内发展已经超过了四年的时间，但是遗憾的一点，尽管我们拥有世界上最多的开发人员，但是我们对社区仍然没有话语权，国内的用户的需求无法对社区上游形成影响，导致很多本地化定制的需求无法真正的在社区版本代码得到体现。所以如何让中国的声音出现在社区，是我们所有OpenStack人需要思考的问题。欣喜的一点，本土的巨头华为已经身先士卒，投入很大的力量搞OpenStack的社区贡献，我们更希望越来越多的国内传统IT巨头能够意识到这个问题，投身于开源的事业中，否则我们又在起跑线上输给了别人。</p><p>以上仅代表个人观点，如有任何异议，欢迎批评指正。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;又到了OpenStack 新版本发布的季节，虽然秋意寒寒，但是仍然挡不住OpenStack再次掀起全球关注的热点。这是OpenStack第12个版本，与之前的沉稳低调相比，这次的Release中一口气多了5个新模块，也创下了OpenStack项目创建以来的最高纪录。由于天然的架构优势，让OpenStack在云计算横行天下的年代游刃有余，已经逐步成为了云平台的即成标准，从OpenStack对待AWS的API兼容的态度就能看出，OpenStack变得越来越自信。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty完整版本的翻译可见：&lt;a href=&quot;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://wiki.openstack.org/wiki/ReleaseNotes/Liberty/zh-hans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;本次OpenStack Liberty更新日志中文版本的翻译工作由我完成。由于时间仓促，难免有很多问题，欢迎各位批评指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 14.04 Server开发者安装指南</title>
    <link href="http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/"/>
    <id>http://sunqi.me/2015/09/08/ubuntu-14-dot-04-installation-guide-for-developer/</id>
    <published>2015-09-08T21:50:24.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="为什么会写这篇Blog"><a href="#为什么会写这篇Blog" class="headerlink" title="为什么会写这篇Blog"></a>为什么会写这篇Blog</h2><p>近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。</p><a id="more"></a><h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>这篇Blog没有提及的地方：</p><ul><li>网络，需要根据实际情况进行配置，我这里面使用的是DHCP自动获取，所以没有相关步骤</li><li>分区，这里面使用的是默认配置，但是生产环境的配置一般需要手动划分</li></ul><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><ul><li>一定要选择English，否则处理中文的时候太麻烦<img src="/images/blogs/install-ubuntu/1.png" class="center"></li><li>正式开始进入安装<img src="/images/blogs/install-ubuntu/2.png" class="center"></li><li>与上面的原则一致，一定要选择English<img src="/images/blogs/install-ubuntu/3.png" class="center"></li><li>Location一定要选择中国，否则默认不会使用中文的Ubuntu源，影响安装速度，这一步很多初学者不会在意<img src="/images/blogs/install-ubuntu/4.png" class="center"><img src="/images/blogs/install-ubuntu/5.png" class="center"><img src="/images/blogs/install-ubuntu/6.png" class="center"></li><li>这里面主要是字符集的问题，选择United States<img src="/images/blogs/install-ubuntu/7.png" class="center"></li><li>不需要检查键盘布局<img src="/images/blogs/install-ubuntu/8.png" class="center"></li><li>默认使用English布局就好了<img src="/images/blogs/install-ubuntu/9.png" class="center"></li><li>主机名设置，就是hostname<img src="/images/blogs/install-ubuntu/10.png" class="center"></li><li>用户设置，建议建立一个普通用户<img src="/images/blogs/install-ubuntu/11.png" class="center"><img src="/images/blogs/install-ubuntu/12.png" class="center"><img src="/images/blogs/install-ubuntu/13.png" class="center"><img src="/images/blogs/install-ubuntu/15.png" class="center"><img src="/images/blogs/install-ubuntu/16.png" class="center"></li><li>不加密Home目录<img src="/images/blogs/install-ubuntu/17.png" class="center"></li><li>设置时区，这一步也很重要，默认情况下会自动检测到，但是如果不对，一定要修改一下，否则你的系统时间与你实际不一致，你程序里的时间跟着不对，跟调试增加难度<img src="/images/blogs/install-ubuntu/18.png" class="center"></li><li>这里面分区用默认的就好啦，当然如果你知道该如何分区，可以采用Manual方式<img src="/images/blogs/install-ubuntu/19.png" class="center"><img src="/images/blogs/install-ubuntu/20.png" class="center"><img src="/images/blogs/install-ubuntu/21.png" class="center"><img src="/images/blogs/install-ubuntu/22.png" class="center"><img src="/images/blogs/install-ubuntu/23.png" class="center"></li><li>如果访问网络需要使用代理，可以设置一下<img src="/images/blogs/install-ubuntu/24.png" class="center"></li><li>不选择自动更新<img src="/images/blogs/install-ubuntu/25.png" class="center"></li><li>默认只需要选择SSH服务，保证我们在安装后能够SSH登陆服务器即可<img src="/images/blogs/install-ubuntu/26.png" class="center"></li><li>安装grub<img src="/images/blogs/install-ubuntu/27.png" class="center"></li><li>重启完成安装<img src="/images/blogs/install-ubuntu/28.png" class="center"></li></ul><h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><p>谨记此篇Blog送给我的小徒弟周小球小朋友，希望你能利用利用最后的一年的时间努力学习，找到称心如意的工作。</p>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;为什么会写这篇Blog&quot;&gt;&lt;a href=&quot;#为什么会写这篇Blog&quot; class=&quot;headerlink&quot; title=&quot;为什么会写这篇Blog&quot;&gt;&lt;/a&gt;为什么会写这篇Blog&lt;/h2&gt;&lt;p&gt;近期，接触了一些OpenStack的入门者，很多人对Linux系统并不是很熟悉，导致安装出来的系统五花八门，间接地影响了后面的开发与调试，所以这里给出我的安装流程，供初学者们参考。我使用的是Ubuntu 14.04 64bit Server版本的ISO进行安装，其他版本方法类似。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ubuntu" scheme="http://sunqi.me/categories/openstack/ubuntu/"/>
    
    
  </entry>
  
  <entry>
    <title>(Kilo)Devstack完全用户手册</title>
    <link href="http://sunqi.me/2015/09/03/devstack-guide/"/>
    <id>http://sunqi.me/2015/09/03/devstack-guide/</id>
    <published>2015-09-03T02:34:20.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。</p><p>本篇Blog主要介绍以下几个实用场景：</p><ul><li>如何利用Devstack构建一套完美的开发环境</li><li>提高Devstack安装成功率的方法</li><li>Devstack的实用技巧</li><li>各种场景下的配置和注意事项</li></ul><p>本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。</p><a id="more"></a><h2 id="运行环境的选择"><a href="#运行环境的选择" class="headerlink" title="运行环境的选择"></a>运行环境的选择</h2><p>对于刚刚接触OpenStack的开发者而言，没有太多闲置的资源，所以比较容易的上手方式就是使用虚拟机。对于桌面的虚拟机软件来说，主流的软件无外乎VMWare Workstation和Oracle Virtualbox，对于OpenStack开发而言，二者并无太大差异。以下几点可能会作为选择的主要依据：</p><ul><li>VMWare Workstation是收费软件，Virtualbox是免费软件</li><li>VMWare Workstation支持nested virtualization，就是安装完的devstack virt type是kvm，节省资源，Virtualbox安装以后只能使用qemu，虽然在Virtualbox 5以上版本号称支持，但是实际验证中仍然不能生效，还在研究中</li><li>VMWare Workstation使用NAT方式时，内部的IP可以在HOST主机直接访问到，Virtualbox还需要端口转发，所以建议单独增加一块Host-only的Apdaptor便于调试</li><li>使用Virtualbox时，为了让虚拟机能够访问外部网络，并且允许Host通过Floating IP对虚拟机进行访问，需要在Host层面设置NAT规则，转换到可以访问的物理网卡上，详情请见下文</li></ul><h2 id="Virtualbox网络设置"><a href="#Virtualbox网络设置" class="headerlink" title="Virtualbox网络设置"></a>Virtualbox网络设置</h2><img src="/images/blogs/devstack-guide-network-topology.jpg" class="center"><ul><li>Nova Network网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 172.16.0.101</span><br><span class="line">netmask 255.255.255.0</span><br></pre></td></tr></table></figure><ul><li>Neutron网卡配置</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">auto eth0</span><br><span class="line">iface eth0 inet dhcp</span><br><span class="line"></span><br><span class="line">auto eth1</span><br><span class="line">iface eth1 inet static</span><br><span class="line">address 192.168.56.101</span><br><span class="line">netmask 255.255.255.0</span><br><span class="line"></span><br><span class="line">auto eth2</span><br><span class="line">iface eth2 inet manual</span><br><span class="line">up ip link set dev $IFACE up</span><br><span class="line">down ip link set dev $IFACE down</span><br></pre></td></tr></table></figure><ul><li>MAC网卡NAT映射</li></ul><p>我们将第三块网卡作为提供外部网络的接口，采用系统层面的NAT方式让该网卡能够访问外部网络。</p><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo sysctl net.inet.ip.forwarding&#x3D;1</span><br></pre></td></tr></table></figure><p>在nat-anchor后面添加</p><figure class="highlight plain"><figcaption><span>/etc/pf.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nat on en0 from 172.16.0.0&#x2F;24 -&gt; (en0)</span><br></pre></td></tr></table></figure><p>之后加载</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pfctl -e -f /etc/pf.conf</span><br></pre></td></tr></table></figure><ul><li>Linux网卡NAT映射</li></ul><figure class="highlight plain"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;net&#x2F;ipv4&#x2F;ip_forward</span><br><span class="line">iptables -t nat -A POSTROUTING -o eth0 -j MASQUERADE</span><br></pre></td></tr></table></figure><h2 id="Devstack快速开始"><a href="#Devstack快速开始" class="headerlink" title="Devstack快速开始"></a>Devstack快速开始</h2><p>其实，Devstack本身并不需要很复杂的配置就可以成功运行，但是仍然有几个需要注意的地方：</p><ul><li>Ubuntu 14.04 64bit(LTS), 12.04已经逐渐退出历史舞台，所以这里推荐14.04</li><li>不能使用root用户，即使你使用root用户执行Devstack，默认也会为你建立一个stack用户，所以不如老老实实的直接使用普通用户运行Devstack，或者提前建立好stack用户，切换后再执行</li><li>默认获取Devstack进行安装，安装的是master分支的代码，但是在实际开发中(比如我们做产品的时候)，都是基于某个stable分支进行，所以一般情况在clone devstack的时候需要指定stable分支</li></ul><p>下面给出一个最简安装步骤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># adduser stack</span><br><span class="line"># apt-get install sudo -y</span><br><span class="line"># echo &quot;stack ALL&#x3D;(ALL) NOPASSWD: ALL&quot; &gt;&gt; &#x2F;etc&#x2F;sudoers</span><br><span class="line"># sudo su - stack</span><br><span class="line"></span><br><span class="line">(stack)$ git clone https:&#x2F;&#x2F;git.openstack.org&#x2F;openstack-dev&#x2F;devstack --branch&#x3D;stable&#x2F;kilo</span><br><span class="line">(stack)$ cd devstack &amp;&amp; .&#x2F;stack.sh</span><br></pre></td></tr></table></figure><h2 id="提高Devstack安装成功率"><a href="#提高Devstack安装成功率" class="headerlink" title="提高Devstack安装成功率"></a>提高Devstack安装成功率</h2><p>估计在国内使用Devstack的人基本都遇到过安装失败的状况，为了节约大家的时间，先分析一下Devstack为什么会失败，我们先从这张时序图看一下Devstack执行的过程：</p><img src="/images/blogs/devstack-guide-flow.png" class="center"><p>从上述流程图中可以很清楚的看到Devstack有以下几个地方需要访问网络：</p><ul><li>安装依赖时，需要访问Ubuntu的源</li><li>执行get_pip.sh时，地址是彻底被墙的，需要访问<a href="https://bootstrap.pypa.io/get-pip.py" target="_blank" rel="noopener">https://bootstrap.pypa.io/get-pip.py</a></li><li>从github clone源代码，github在国内访问速度并不很快而且间歇性被墙</li><li>安装过程中执行pip install requirements，需要访问pip repo</li><li>下载镜像，这一步骤取决于你需要安装的模块，如果默认安装只会下载cirros镜像，但是如果是安装类似Trove的模块，可能需要下载的更多</li></ul><hr><p>所以综上所述，为了提高devstack的安装成功率，需要从这几个方面着手优化：</p><ul><li>使用国内源</li></ul><figure class="highlight plain"><figcaption><span>/etc/apt/sources.list</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-security main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-updates main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-proposed main restricted universe multiverse</span><br><span class="line">deb http:&#x2F;&#x2F;mirrors.163.com&#x2F;ubuntu&#x2F; trusty-backports main restricted universe multiverse</span><br></pre></td></tr></table></figure><ul><li>从国内源获取get-pip.py，从源代码可以分析出，检测get-pip.py的方式，这里面有两种方式一种是手动下载get-pip.py之后，注释代码，还有一种就是修改PIP_GET_PIP_URL的地址，但是这里只能通过修改install_pip.sh的方式，暂时无法从环境变量里获取</li></ul><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">FILES=<span class="variable">$TOP_DIR</span>/files</span><br><span class="line"></span><br><span class="line">PIP_GET_PIP_URL=https://bootstrap.pypa.io/get-pip.py</span><br><span class="line">LOCAL_PIP=<span class="string">"<span class="variable">$FILES</span>/<span class="variable">$(basename $PIP_GET_PIP_URL)</span>"</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">function</span> install_get_pip &#123;</span><br><span class="line">    <span class="comment"># The OpenStack gate and others put a cached version of get-pip.py</span></span><br><span class="line">    <span class="comment"># for this to find, explicitly to avoid download issues.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># However, if DevStack *did* download the file, we want to check</span></span><br><span class="line">    <span class="comment"># for updates; people can leave their stacks around for a long</span></span><br><span class="line">    <span class="comment"># time and in the mean-time pip might get upgraded.</span></span><br><span class="line">    <span class="comment">#</span></span><br><span class="line">    <span class="comment"># Thus we use curl's "-z" feature to always check the modified</span></span><br><span class="line">    <span class="comment"># since and only download if a new version is out -- but only if</span></span><br><span class="line">    <span class="comment"># it seems we downloaded the file originally.</span></span><br><span class="line">    <span class="keyword">if</span> [[ ! -r <span class="variable">$LOCAL_PIP</span> || -r <span class="variable">$LOCAL_PIP</span>.downloaded ]]; <span class="keyword">then</span></span><br><span class="line">        curl --retry 6 --retry-delay 5 \</span><br><span class="line">            -z <span class="variable">$LOCAL_PIP</span> -o <span class="variable">$LOCAL_PIP</span> <span class="variable">$PIP_GET_PIP_URL</span> || \</span><br><span class="line">            die <span class="variable">$LINENO</span> <span class="string">"Download of get-pip.py failed"</span></span><br><span class="line">        touch <span class="variable">$LOCAL_PIP</span>.downloaded</span><br><span class="line">    <span class="keyword">fi</span></span><br><span class="line">    sudo -H -E python <span class="variable">$LOCAL_PIP</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>修改为我在coding.net上缓存的get-pip脚本</p><figure class="highlight bash"><figcaption><span>devstack/tools/install_pip.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">PIP_GET_PIP_URL=https://coding.net/u/xiaoquqi/p/pip/git/raw/master/contrib/get-pip.py</span><br></pre></td></tr></table></figure><ul><li>国内的代码托管服务器有从github上定期同步源代码的，但是经过实际测试都不是很理想，所以可能这是最不稳定的一部分，但是可以提前使用脚本，人工的下载所有代码，之后我会尝试在我自己的源中定时同步OpenStack源代码，敬请关注</li><li>现在pip的安装速度明显提升，原来还需要使用国内源，例如豆瓣，现在即使不修改也能很快的进行安装</li><li>镜像下载建议使用一些下载工具，然后放到指定的目录中，这样最有效</li></ul><h2 id="无网络状况下安装Devstack"><a href="#无网络状况下安装Devstack" class="headerlink" title="无网络状况下安装Devstack"></a>无网络状况下安装Devstack</h2><p>因为我们是做OpenStack的产品的公司，所以就要求我们的Devstack要能够满足无网络状况下的安装，之前也写过一篇详细介绍无网络安装Devstack博客,由于时间关系，可能一些内容已经过时了，这里面再进行一下更新，思路还是上面的思路，这里给出一些使用的工具，如果不清楚如何使用的话，可以参考我之前的博客。</p><ul><li>本地源的缓存使用apt-mirror，这是一个需要时间的工作，第一次同步的时间会非常长，准备好大约100G左右的空间吧</li><li>缓存get-pip.py，这个比较容易，搭建一个Apache服务器，但是需要把端口修改为10000，否则在安装好OpenStack后，会占用80端口，重新执行Devstack时候会出现错误</li><li>建立本地的Gerrit，并且上传所有代码</li><li>从requirements项目中，下载所有的pip，建立本地的pip缓存源，如果是搭建研发环境，可能还需要下载test-requirements的内容和tox</li><li>将镜像下载到刚刚创建的Apache服务器</li></ul><p>完成以上步骤，你可以尽情断掉外网，愉快的进行Devstack的安装了，稍后我会将以上步骤进行进一步完善。</p><h2 id="OFFLINE模式下安装Devstack"><a href="#OFFLINE模式下安装Devstack" class="headerlink" title="OFFLINE模式下安装Devstack"></a>OFFLINE模式下安装Devstack</h2><p>在Devstack中提供了一种OFFLINE的方式，这种方式的含义就是，当你第一次完成安装后，所有需要的内容已经下载到本地，再次运行就没有必要访问网络了(前提是你不想升级)，所以可以将安装模式设置为OFFLINE，避免网络的访问，方法为：</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">OFFLINE=True</span><br></pre></td></tr></table></figure><h2 id="虚拟机重启后，如何利用rejoin-stack-sh，免重新安装"><a href="#虚拟机重启后，如何利用rejoin-stack-sh，免重新安装" class="headerlink" title="虚拟机重启后，如何利用rejoin-stack.sh，免重新安装"></a>虚拟机重启后，如何利用rejoin-stack.sh，免重新安装</h2><p>其实使用OFFLINE模式，可以在离线状态下无数次重新运行devstack，但是如果不是为了重新配置，我们并没有需要每次重新运行stack.sh。在Devstack中提供了另外一个脚本叫做rejoin-stack.sh，原理很简单就是把所有的进程重新组合进screen，所以我们借助这个脚本完全可以不重新执行stack.sh，快速恢复环境。但是当虚拟机重启后，cinder使用的卷组并不会自动重建，所以在运行rejoin之前，需要将恢复卷组的工作，放入开机启动的脚本中。</p><figure class="highlight bash"><figcaption><span>/etc/init.d/cinder-setup-backing-file</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">losetup /dev/loop1 /opt/stack/data/stack-volumes-default-backing-file</span><br><span class="line">losetup /dev/loop2 /opt/stack/data/stack-volumes-lvmdriver-1-backing-file</span><br><span class="line"><span class="built_in">exit</span> 0</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as root</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">chmod 755 /etc/init.d/cinder-setup-backing-file</span><br><span class="line">ln -s /etc/init.d/cinder-setup-backing-file /etc/rc2.d/S10cinder-setup-backing-file</span><br></pre></td></tr></table></figure><figure class="highlight bash"><figcaption><span>Run as normal user</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> <span class="variable">$HOME</span>/devstack</span><br><span class="line">./rejoin-stack.sh</span><br></pre></td></tr></table></figure><h2 id="Scenario-0-公共部分"><a href="#Scenario-0-公共部分" class="headerlink" title="Scenario 0: 公共部分"></a>Scenario 0: 公共部分</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Misc</span></span><br><span class="line">ADMIN_PASSWORD=sysadmin</span><br><span class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line">SERVICE_TOKEN=<span class="variable">$ADMIN_PASSWORD</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Target Path</span></span><br><span class="line">DEST=/opt/stack.kilo</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable Logging</span></span><br><span class="line">LOGFILE=<span class="variable">$DEST</span>/logs/stack.sh.log</span><br><span class="line">VERBOSE=True</span><br><span class="line">LOG_COLOR=True</span><br><span class="line">SCREEN_LOGDIR=<span class="variable">$DEST</span>/logs</span><br></pre></td></tr></table></figure><h2 id="Scenario-1-单节点Nova-Network的安装"><a href="#Scenario-1-单节点Nova-Network的安装" class="headerlink" title="Scenario 1: 单节点Nova-Network的安装"></a>Scenario 1: 单节点Nova-Network的安装</h2><p>这应该就是Devstack默认的模式，有以下几点需要注意：</p><ul><li>根据上面的网卡配置</li></ul><blockquote><p>第一块网卡为NAT方式，用于访问外部网络</p><p>第二块为Host-only Adaptor，用于访问云平台</p><p>第三块为Host-only Adaptor，用于虚拟机桥接网路</p><p>需要注意的是：这种方式下并不能让虚拟机正常访问外部网络，可以通过将eth2设置为Bridge模式，但是这样会造成DHCP冲突(如果外部网络有DHCP)，所以暂时没有完美的解决方案</p></blockquote><ul><li>打开novnc和consoleauth，否则无法访问VNC</li></ul><p>这里给出的配置方案是第一种网络配置，即虚拟机无法网络外部网络的情况</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># eth1 address</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=172.24.17.0/24</span><br><span class="line">FIXED_NETWORK_SIZE=254</span><br><span class="line">FLOATING_RANGE=172.16.0.128/25</span><br></pre></td></tr></table></figure><h2 id="Scenario-2-双节点Nova-Network的安装"><a href="#Scenario-2-双节点Nova-Network的安装" class="headerlink" title="Scenario 2: 双节点Nova-Network的安装"></a>Scenario 2: 双节点Nova-Network的安装</h2><ul><li>控制节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line">MULTI_HOST=1</span><br></pre></td></tr></table></figure><ul><li>计算节点</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">ENABLED_SERVICES=n-cpu,n-net,n-api-meta,c-vol</span><br><span class="line"></span><br><span class="line"><span class="comment"># current host ip</span></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FLAT_INTERFACE=eth1</span><br><span class="line"><span class="comment"># needed by cinder-volume service</span></span><br><span class="line">DATABASE_TYPE=mysql</span><br><span class="line"></span><br><span class="line"><span class="comment"># controller ip</span></span><br><span class="line">SERVICE_HOST=192.168.56.101</span><br><span class="line">MYSQL_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">RABBIT_HOST=<span class="variable">$SERVICE_HOST</span></span><br><span class="line">GLANCE_HOSTPORT=<span class="variable">$SERVICE_HOST</span>:9292</span><br><span class="line">NOVA_VNC_ENABLED=True</span><br><span class="line">NOVNCPROXY_URL=<span class="string">"http://<span class="variable">$SERVICE_HOST</span>:6080/vnc_auto.html"</span></span><br><span class="line">VNCSERVER_LISTEN=<span class="variable">$HOST_IP</span></span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS=<span class="variable">$VNCSERVER_LISTEN</span></span><br></pre></td></tr></table></figure><h2 id="Scenario-3-单节点Neutron的安装"><a href="#Scenario-3-单节点Neutron的安装" class="headerlink" title="Scenario 3: 单节点Neutron的安装"></a>Scenario 3: 单节点Neutron的安装</h2><ul><li>基本配置</li></ul><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Nova</span></span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line"><span class="comment"># Neutron</span></span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+=,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line">ENABLED_SERVICES+=,q-lbaas,q-vpn,q-fwaas</span><br><span class="line"></span><br><span class="line">HOST_IP=192.168.56.101</span><br><span class="line">FIXED_RANGE=20.0.0.0/24</span><br><span class="line">NETWORK_GATEWAY=20.0.0.1</span><br><span class="line">FLOATING_RANGE=172.16.0.0/24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY=172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL=start=172.16.0.101,end=172.16.0.200</span><br></pre></td></tr></table></figure><ul><li>OVS设置</li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-4-多节点Neutron的安装-控制-网络-计算节点"><a href="#Scenario-4-多节点Neutron的安装-控制-网络-计算节点" class="headerlink" title="Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)"></a>Scenario 4: 多节点Neutron的安装(控制/网络+计算节点)</h2><ul><li><p>控制/网络节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line">HOST_IP&#x3D;192.168.56.101</span><br><span class="line">disable_service n-cpu n-net n-api-meta c-vol</span><br><span class="line"></span><br><span class="line"># Neutron</span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta</span><br><span class="line">FIXED_RANGE&#x3D;20.0.0.0&#x2F;24</span><br><span class="line">NETWORK_GATEWAY&#x3D;20.0.0.1</span><br><span class="line">FLOATING_RANGE&#x3D;172.16.0.0&#x2F;24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;172.16.0.1</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;172.16.0.101,end&#x3D;172.16.0.200</span><br></pre></td></tr></table></figure></li><li><p>计算节点</p><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"># Nova</span><br><span class="line">disable_all_services</span><br><span class="line">ENABLED_SERVICES&#x3D;n-cpu,rabbit,neutron,q-agt,c-vol</span><br><span class="line"></span><br><span class="line"># current host ip</span><br><span class="line">HOST_IP&#x3D;192.168.56.103</span><br><span class="line"># needed by cinder-volume service</span><br><span class="line">DATABASE_TYPE&#x3D;mysql</span><br><span class="line"></span><br><span class="line"># controller ip</span><br><span class="line">SERVICE_HOST&#x3D;192.168.56.101</span><br><span class="line">MYSQL_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">RABBIT_HOST&#x3D;$SERVICE_HOST</span><br><span class="line">GLANCE_HOSTPORT&#x3D;$SERVICE_HOST:9292</span><br><span class="line">NOVA_VNC_ENABLED&#x3D;True</span><br><span class="line">NOVNCPROXY_URL&#x3D;&quot;http:&#x2F;&#x2F;$SERVICE_HOST:6080&#x2F;vnc_auto.html&quot;</span><br><span class="line">VNCSERVER_LISTEN&#x3D;$HOST_IP</span><br><span class="line">VNCSERVER_PROXYCLIENT_ADDRESS&#x3D;$VNCSERVER_LISTEN</span><br><span class="line">Q_HOST&#x3D;$SERVICE_HOST</span><br></pre></td></tr></table></figure></li><li><p>OVS设置</p></li></ul><p>由于在Devstack安装过程中，将br-ex的地址也设置成了PUBLIC_NETWORK_GATEWAY的地址，但是实际使用过程中，我们建立的Host Apdator充当了gateway的角色，所以为了避免冲突，直接将br-ex地址清除掉。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ip addr flush dev br-ex</span><br></pre></td></tr></table></figure><p>之后将eth2作为br-ex的port，之后创建的虚拟机就可以通过eth2访问网络了，Host也可以通过floating ip访问虚拟机了。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth2</span><br></pre></td></tr></table></figure><h2 id="Scenario-5-从源代码安装客户端"><a href="#Scenario-5-从源代码安装客户端" class="headerlink" title="Scenario 5: 从源代码安装客户端"></a>Scenario 5: 从源代码安装客户端</h2><p>新的Devstack里面默认不再提供client的源代码的安装方式，需要使用localrc中的环境变量进行开启，否则将直接从master获取的client代码进行安装，当然这样会造成系统无法正常使用。那么如何才能确定client在当前Devstack可用的版本呢？最简单的方法可以先从pip中安装包，之后通过pip list | grep client的方式获取client的源代码。这里面提供我在Kilo中使用的版本依赖。</p><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">KEYSTONECLIENT_BRANCH=1.3.1</span><br><span class="line">CINDERCLIENT_BRANCH=1.1.1</span><br><span class="line">GLANCECLIENT_BRANCH=0.17.1</span><br><span class="line">HEATCLIENT_BRANCH=0.4.0</span><br><span class="line">NEUTRONCLIENT_BRANCH=2.4.0</span><br><span class="line">NOVACLIENT_BRANCH=2.23.0</span><br><span class="line">SWIFTCLIENT_BRANCH=2.4.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># client code</span></span><br><span class="line">LIBS_FROM_GIT=python-keystoneclient,python-glanceclient,python-novaclient,python-neutronclient,python-swiftclient,python-cinderclient</span><br></pre></td></tr></table></figure><h2 id="Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift"><a href="#Scenario-6-安装Ceilometer-Heat-Trove-Sahara-Swift" class="headerlink" title="Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift"></a>Scenario 6: 安装Ceilometer/Heat/Trove/Sahara/Swift</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceilometer</span></span><br><span class="line">enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api</span><br><span class="line">enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator</span><br><span class="line"></span><br><span class="line"><span class="comment"># Heat</span></span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"><span class="comment"># Trove</span></span><br><span class="line">enable_service trove tr-api tr-tmgr tr-cond</span><br><span class="line"></span><br><span class="line"><span class="comment"># Sahara</span></span><br><span class="line">enable_service sahara</span><br><span class="line"></span><br><span class="line"><span class="comment"># Swift</span></span><br><span class="line">enable_service s-proxy s-object s-container s-account</span><br><span class="line">SWIFT_REPLICAS=1</span><br><span class="line">SWIFT_HASH=011688b44136573e209e</span><br></pre></td></tr></table></figure><h2 id="Scenario-7-安装Ceph"><a href="#Scenario-7-安装Ceph" class="headerlink" title="Scenario 7: 安装Ceph"></a>Scenario 7: 安装Ceph</h2><figure class="highlight bash"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Ceph</span></span><br><span class="line">ENABLED_SERVICES+=,ceph</span><br><span class="line">CEPH_LOOPBACK_DISK_SIZE=200G</span><br><span class="line">CEPH_CONF=/etc/ceph/ceph.conf</span><br><span class="line">CEPH_REPLICAS=1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Glance - Image Service</span></span><br><span class="line">GLANCE_CEPH_USER=glance</span><br><span class="line">GLANCE_CEPH_POOL=glance-pool</span><br><span class="line"></span><br><span class="line"><span class="comment"># Cinder - Block Device Service</span></span><br><span class="line">CINDER_DRIVER=ceph</span><br><span class="line">CINDER_CEPH_USER=cinder</span><br><span class="line">CINDER_CEPH_POOL=cinder-pool</span><br><span class="line">CINDER_CEPH_UUID=1b1519e4-5ecd-11e5-8559-080027f18a73</span><br><span class="line">CINDER_BAK_CEPH_POOL=cinder-backups</span><br><span class="line">CINDER_BAK_CEPH_USER=cinder-backups</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line">CINDER_ENABLED_BACKENDS=ceph</span><br><span class="line"></span><br><span class="line"><span class="comment"># Nova - Compute Service</span></span><br><span class="line">NOVA_CEPH_POOL=nova-pool</span><br></pre></td></tr></table></figure><h2 id="Scenario-8-安装Murano"><a href="#Scenario-8-安装Murano" class="headerlink" title="Scenario 8: 安装Murano"></a>Scenario 8: 安装Murano</h2><p>想通过这个例子演示，对于一个新的OpenStack项目，如何使用Devstack尝鲜。</p><figure class="highlight bash"><figcaption><span>bash</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/stack.kilo</span><br><span class="line">git <span class="built_in">clone</span> https://github.com/openstack/murano --branch=stable/kilo</span><br><span class="line"><span class="built_in">cd</span> murano/contrib/devstack</span><br><span class="line">cp lib/murano <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp lib/murano-dashboard <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/lib</span><br><span class="line">cp extras.d/70-murano.sh <span class="variable">$&#123;DEVSTACK_DIR&#125;</span>/extras.d</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>devstack/localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># Enable Neutron</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line"></span><br><span class="line"># Enable Heat</span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"># Enable Murano</span><br><span class="line">enable_service murano murano-api murano-engine</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack作为开发OpenStack必不可少的辅助环境搭建工具，其重要性不言而喻，但是由于网络上的原因，在使用中总是出现各种各样的问题，而且也不是所有人对使用上的细节非常清晰，所以想用这篇Blog总结一下在三年多的使用过程中的心得，来帮助将要走进OpenStack开发的志愿者们。下一篇博客我将为大家介绍Devstack的源代码，以及扩展插件的开发方法。&lt;/p&gt;
&lt;p&gt;本篇Blog主要介绍以下几个实用场景：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;如何利用Devstack构建一套完美的开发环境&lt;/li&gt;
&lt;li&gt;提高Devstack安装成功率的方法&lt;/li&gt;
&lt;li&gt;Devstack的实用技巧&lt;/li&gt;
&lt;li&gt;各种场景下的配置和注意事项&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;本篇博客提到的所有方法均在2015年9月4日使用stable/kilo branch得到验证，后续版本请持续关注本博客。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Devstack" scheme="http://sunqi.me/categories/OpenStack/Devstack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Devstack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>为什么叫Monkey Patch？</title>
    <link href="http://sunqi.me/2015/08/18/about-monkey-patch/"/>
    <id>http://sunqi.me/2015/08/18/about-monkey-patch/</id>
    <published>2015-08-18T02:51:21.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？</p><p>估计刚刚看到这个表述的开发人员可能很难理解到底这是什么意思，其实Monkey Patch本与猴子无关，这个词原来为Guerrilla Patch，这样看着好像能明白一些了，游击队嘛，神出鬼没的，好像和运行状态打补丁这个功能贴近点了，但是为什么又变成猴子了。原来老外们都是很顽皮的，他们喜欢一些玩笑式的表述，就像很多技术的文档中一样。在英文里，Guerrilla和Gorilla读音是几乎一样的，Gorilla当什么讲呢？大猩猩。但是大猩猩有点吓人，所以干脆换成了大猩猩的近亲——猴子。就这样Monkey Patch形成了。</p><p>当然这并不是这个词的唯一解释，还有一种解释是说由于这种方式将原来的代码弄乱了(messing with it)，在英文里叫monkeying about(顽皮的)，所以叫做Monkey Patch。这种描述应该是和Monkey Test有异曲同工之妙。但是无论这个词从哪里来，我们只要正确理解Monkey Patch的含义就好了。</p><p>相同的表述还有Duck Typing，描述的是动态类型的一种风格。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;在程序运行时给代码加补丁的方法被称为Monkey Patch，这种方式多见于脚本类语言中(Dynamic Programming Languages)，例如: Ruby/Python等。国内很多人翻译为猴子补丁，但是为什么叫猴子补丁而不叫老虎补丁、狮子补丁呢？&lt;/p&gt;
&lt;p
      
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Ceph性能优化总结(v0.94)</title>
    <link href="http://sunqi.me/2015/06/27/ceph-performance-optimization-summary/"/>
    <id>http://sunqi.me/2015/06/27/ceph-performance-optimization-summary/</id>
    <published>2015-06-27T22:30:22.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。</p><h2 id="优化方法论"><a href="#优化方法论" class="headerlink" title="优化方法论"></a>优化方法论</h2><p>做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：</p><h3 id="1-硬件层面"><a href="#1-硬件层面" class="headerlink" title="1. 硬件层面"></a>1. 硬件层面</h3><ul><li>硬件规划</li><li>SSD选择</li><li>BIOS设置</li></ul><h3 id="2-软件层面"><a href="#2-软件层面" class="headerlink" title="2. 软件层面"></a>2. 软件层面</h3><ul><li>Linux OS</li><li>Ceph Configurations</li><li>PG Number调整</li><li>CRUSH Map</li><li>其他因素</li></ul><a id="more"></a><h2 id="硬件优化"><a href="#硬件优化" class="headerlink" title="硬件优化"></a>硬件优化</h2><h3 id="1-硬件规划"><a href="#1-硬件规划" class="headerlink" title="1. 硬件规划"></a>1. 硬件规划</h3><ul><li>Processor</li></ul><p>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</p><p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p><p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p><ul><li>内存</li></ul><p>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p><ul><li>网络规划</li></ul><p>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p><h3 id="2-SSD选择"><a href="#2-SSD选择" class="headerlink" title="2. SSD选择"></a>2. SSD选择</h3><p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="http://www.intel.com/content/www/us/en/solid-state-drives/solid-state-drives-dc-s3500-series.html" target="_blank" rel="noopener">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p><p>如何确定你的SSD是否适合作为SSD Journal，可以参考SÉBASTIEN HAN的<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/" target="_blank" rel="noopener">Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p><h3 id="3-BIOS设置"><a href="#3-BIOS设置" class="headerlink" title="3. BIOS设置"></a>3. BIOS设置</h3><ul><li>Hyper-Threading(HT)</li></ul><p>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p><ul><li>关闭节能</li></ul><p>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请参考<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/" target="_blank" rel="noopener">链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">for CPUFREQ in &#x2F;sys&#x2F;devices&#x2F;system&#x2F;cpu&#x2F;cpu*&#x2F;cpufreq&#x2F;scaling_governor; do [ -f $CPUFREQ ] || continue; echo -n performance &gt; $CPUFREQ; done</span><br></pre></td></tr></table></figure><ul><li><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/" target="_blank" rel="noopener">NUMA</a></li></ul><p>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html" target="_blank" rel="noopener">NUMA可能会在某些情况下影响ceph-osd</a>。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel &#x2F;vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root&#x3D;UUID&#x3D;870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG&#x3D;en_US.UTF-8 rd_NO_MD SYSFONT&#x3D;latarcyrheb-sun16 crashkernel&#x3D;auto  KEYBOARDTYPE&#x3D;pc KEYTABLE&#x3D;us rd_NO_DM   biosdevname&#x3D;0 numa&#x3D;off</span><br></pre></td></tr></table></figure><h2 id="软件优化"><a href="#软件优化" class="headerlink" title="软件优化"></a>软件优化</h2><h3 id="1-Linux-OS"><a href="#1-Linux-OS" class="headerlink" title="1. Linux OS"></a>1. Linux OS</h3><ul><li>Kernel pid max</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo 4194303 &gt; &#x2F;proc&#x2F;sys&#x2F;kernel&#x2F;pid_max</span><br></pre></td></tr></table></figure><ul><li>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 mtu 9000</span><br></pre></td></tr></table></figure><p>永久设置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;MTU&#x3D;9000&quot; | tee -a &#x2F;etc&#x2F;sysconfig&#x2F;network-script&#x2F;ifcfg-eth0</span><br><span class="line">&#x2F;etc&#x2F;init.d&#x2F;networking restart</span><br></pre></td></tr></table></figure><ul><li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;read_ahead_kb</span><br></pre></td></tr></table></figure><p>根据一些Ceph的公开分享，8192是比较理想的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;8192&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sda&#x2F;queue&#x2F;read_ahead_kb</span><br></pre></td></tr></table></figure><ul><li>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;vm.swappiness &#x3D; 0&quot; | tee -a &#x2F;etc&#x2F;sysctl.conf</span><br></pre></td></tr></table></figure><ul><li>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">echo &quot;deadline&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sd[x]&#x2F;queue&#x2F;scheduler</span><br><span class="line">echo &quot;noop&quot; &gt; &#x2F;sys&#x2F;block&#x2F;sd[x]&#x2F;queue&#x2F;scheduler</span><br></pre></td></tr></table></figure><ul><li>cgroup</li></ul><p>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html" target="_blank" rel="noopener">原因</a>。</p><blockquote><ul><li>不在process和thread在不同的core上移动(更好的缓存利用)</li><li>减少NUMA的影响</li><li>网络和存储控制器影响 - 较小</li><li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li><li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li></ul></blockquote><p>这一点具体实现待补充！！！</p><h3 id="2-Ceph-Configurations"><a href="#2-Ceph-Configurations" class="headerlink" title="2. Ceph Configurations"></a>2. Ceph Configurations</h3><h4 id="global"><a href="#global" class="headerlink" title="[global]"></a>[global]</h4><p>| 参数名 | 描述 | 默认值 | 建议值 |<br>| —– | — | —– | —– | ——— |<br>| public network | 客户端访问网络 | | 192.168.100.0/24 |<br>| cluster network | 集群网络 | | 192.168.1.0/24 |<br>| max open files | 如果设置了该选项，Ceph会设置系统的max open fds | 0 | 131072 |</p><hr><ul><li>查看系统最大文件打开数可以使用命令</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat &#x2F;proc&#x2F;sys&#x2F;fs&#x2F;file-max</span><br></pre></td></tr></table></figure><hr><h4 id="osd-filestore"><a href="#osd-filestore" class="headerlink" title="[osd] - filestore"></a>[osd] - filestore</h4><p>| 参数名 | 描述 | 默认值 | 建议值 |<br>| —– | — | —– | —– | ——— |<br>| filestore xattr use omap | 为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用 | false | true |<br>| filestore max sync interval | 从日志到数据盘最大同步间隔(seconds) | 5 | 15 |<br>| filestore min sync interval | 从日志到数据盘最小同步间隔(seconds) | 0.1 | 10 |<br>| filestore queue max ops | 数据盘最大接受的操作数 | 500 | 25000 |<br>| filestore queue max bytes | 数据盘一次操作最大字节数(bytes) | 100 &lt;&lt; 20 | 10485760 |<br>| filestore queue committing max ops | 数据盘能够commit的操作数 | 500 | 5000 |<br>| filestore queue committing max bytes | 数据盘能够commit的最大字节数(bytes) | 100 &lt;&lt; 20 | 10485760000 |<br>| filestore op threads | 并发文件系统操作数 | 2 | 32 |</p><hr><ul><li>调整omap的原因主要是EXT4文件系统默认仅有4K</li><li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li></ul><hr><h4 id="osd-journal"><a href="#osd-journal" class="headerlink" title="[osd] - journal"></a>[osd] - journal</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd journal size</td><td>OSD日志大小(MB)</td><td>5120</td><td>20000</td></tr><tr><td>journal max write bytes</td><td>journal一次性写入的最大字节数(bytes)</td><td>10 &lt;&lt; 20</td><td>1073714824</td></tr><tr><td>journal max write entries</td><td>journal一次性写入的最大记录数</td><td>100</td><td>10000</td></tr><tr><td>journal queue max ops</td><td>journal一次性最大在队列中的操作数</td><td>500</td><td>50000</td></tr><tr><td>journal queue max bytes</td><td>journal一次性最大在队列中的字节数(bytes)</td><td>10 &lt;&lt; 20</td><td>10485760000</td></tr></tbody></table><hr><ul><li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li><li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li></ul><hr><h4 id="osd-osd-config-tuning"><a href="#osd-osd-config-tuning" class="headerlink" title="[osd] - osd config tuning"></a>[osd] - osd config tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd max write size</td><td>OSD一次可写入的最大值(MB)</td><td>90</td><td>512</td></tr><tr><td>osd client message size cap</td><td>客户端允许在内存中的最大数据(bytes)</td><td>524288000</td><td>2147483648</td></tr><tr><td>osd deep scrub stride</td><td>在Deep Scrub时候允许读取的字节数(bytes)</td><td>524288</td><td>131072</td></tr><tr><td>osd op threads</td><td>OSD进程操作的线程数</td><td>2</td><td>8</td></tr><tr><td>osd disk threads</td><td>OSD密集型操作例如恢复和Scrubbing时的线程</td><td>1</td><td>4</td></tr><tr><td>osd map cache size</td><td>保留OSD Map的缓存(MB)</td><td>500</td><td>1024</td></tr><tr><td>osd map cache bl size</td><td>OSD进程在内存中的OSD Map缓存(MB)</td><td>50</td><td>128</td></tr><tr><td>osd mount options xfs</td><td>Ceph OSD xfs Mount选项</td><td>rw,noatime,inode64</td><td>rw,noexec,nodev,noatime,nodiratime,nobarrier</td></tr></tbody></table><hr><ul><li>增加osd op threads和disk threads会带来额外的CPU开销</li></ul><hr><h4 id="osd-recovery-tuning"><a href="#osd-recovery-tuning" class="headerlink" title="[osd] - recovery tuning"></a>[osd] - recovery tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>osd recovery op priority</td><td>恢复操作优先级，取值1-63，值越高占用资源越高</td><td>10</td><td>4</td></tr><tr><td>osd recovery max active</td><td>同一时间内活跃的恢复请求数</td><td>15</td><td>10</td></tr><tr><td>osd max backfills</td><td>一个OSD允许的最大backfills数</td><td>10</td><td>4</td></tr></tbody></table><h4 id="osd-client-tuning"><a href="#osd-client-tuning" class="headerlink" title="[osd] - client tuning"></a>[osd] - client tuning</h4><table><thead><tr><th>参数名</th><th>描述</th><th>默认值</th><th>建议值</th></tr></thead><tbody><tr><td>rbd cache</td><td>RBD缓存</td><td>true</td><td>true</td></tr><tr><td>rbd cache size</td><td>RBD缓存大小(bytes)</td><td>33554432</td><td>268435456</td></tr><tr><td>rbd cache max dirty</td><td>缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</td><td>25165824</td><td>134217728</td></tr><tr><td>rbd cache max dirty age</td><td>在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</td><td>1</td><td>5</td></tr></tbody></table><h4 id="关闭Debug"><a href="#关闭Debug" class="headerlink" title="关闭Debug"></a>关闭Debug</h4><h3 id="3-PG-Number"><a href="#3-PG-Number" class="headerlink" title="3. PG Number"></a>3. PG Number</h3><p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数。</p><pre><code>Total PGs = (Total_number_of_OSD * 100) / max_replication_count</code></pre><p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool set volumes pg_num 512</span><br><span class="line">ceph osd pool set volumes pgp_num 512</span><br></pre></td></tr></table></figure><h3 id="4-CRUSH-Map"><a href="#4-CRUSH-Map" class="headerlink" title="4. CRUSH Map"></a>4. CRUSH Map</h3><p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p><h3 id="5-其他因素的影响"><a href="#5-其他因素的影响" class="headerlink" title="5. 其他因素的影响"></a>5. 其他因素的影响</h3><p>在今年的(2015年)的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph osd perf</span><br></pre></td></tr></table></figure><pre><code>osd fs_commit_latency(ms) fs_apply_latency(ms)  0                    14                   17  1                    14                   16  2                    10                   11  3                     4                    5  4                    13                   15  5                    17                   20  6                    15                   18  7                    14                   16  8                   299                  329</code></pre><h2 id="ceph-conf"><a href="#ceph-conf" class="headerlink" title="ceph.conf"></a>ceph.conf</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">fsid &#x3D; 059f27e8-a23f-4587-9033-3e3679d03b31</span><br><span class="line">mon_host &#x3D; 10.10.20.102, 10.10.20.101, 10.10.20.100</span><br><span class="line">auth cluster required &#x3D; cephx</span><br><span class="line">auth service required &#x3D; cephx</span><br><span class="line">auth client required &#x3D; cephx</span><br><span class="line">osd pool default size &#x3D; 3</span><br><span class="line">osd pool default min size &#x3D; 1</span><br><span class="line"></span><br><span class="line">public network &#x3D; 10.10.20.0&#x2F;24</span><br><span class="line">cluster network &#x3D; 10.10.20.0&#x2F;24</span><br><span class="line"></span><br><span class="line">max open files &#x3D; 131072</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon data &#x3D; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;mon&#x2F;ceph-$id</span><br><span class="line"></span><br><span class="line">[osd]</span><br><span class="line">osd data &#x3D; &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-$id</span><br><span class="line">osd journal size &#x3D; 20000</span><br><span class="line">osd mkfs type &#x3D; xfs</span><br><span class="line">osd mkfs options xfs &#x3D; -f</span><br><span class="line"></span><br><span class="line">filestore xattr use omap &#x3D; true</span><br><span class="line">filestore min sync interval &#x3D; 10</span><br><span class="line">filestore max sync interval &#x3D; 15</span><br><span class="line">filestore queue max ops &#x3D; 25000</span><br><span class="line">filestore queue max bytes &#x3D; 10485760</span><br><span class="line">filestore queue committing max ops &#x3D; 5000</span><br><span class="line">filestore queue committing max bytes &#x3D; 10485760000</span><br><span class="line"></span><br><span class="line">journal max write bytes &#x3D; 1073714824</span><br><span class="line">journal max write entries &#x3D; 10000</span><br><span class="line">journal queue max ops &#x3D; 50000</span><br><span class="line">journal queue max bytes &#x3D; 10485760000</span><br><span class="line"></span><br><span class="line">osd max write size &#x3D; 512</span><br><span class="line">osd client message size cap &#x3D; 2147483648</span><br><span class="line">osd deep scrub stride &#x3D; 131072</span><br><span class="line">osd op threads &#x3D; 8</span><br><span class="line">osd disk threads &#x3D; 4</span><br><span class="line">osd map cache size &#x3D; 1024</span><br><span class="line">osd map cache bl size &#x3D; 128</span><br><span class="line">osd mount options xfs &#x3D; &quot;rw,noexec,nodev,noatime,nodiratime,nobarrier&quot;</span><br><span class="line">osd recovery op priority &#x3D; 4</span><br><span class="line">osd recovery max active &#x3D; 10</span><br><span class="line">osd max backfills &#x3D; 4</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">rbd cache &#x3D; true</span><br><span class="line">rbd cache size &#x3D; 268435456</span><br><span class="line">rbd cache max dirty &#x3D; 134217728</span><br><span class="line">rbd cache max dirty age &#x3D; 5</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>优化是一个长期迭代的过程，所有的方法都是别人的，只有在实践过程中才能发现自己的，本篇文章仅仅是一个开始，欢迎各位积极补充，共同完成一篇具有指导性的文章。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;最近一直在忙着搞Ceph存储的优化和测试，看了各种资料，但是好像没有一篇文章把其中的方法论交代清楚，所以呢想在这里进行一下总结，很多内容并不是我原创，只是做一个总结。如果其中有任何的问题，欢迎各位喷我，以便我提高。&lt;/p&gt;
&lt;h2 id=&quot;优化方法论&quot;&gt;&lt;a href=&quot;#优化方法论&quot; class=&quot;headerlink&quot; title=&quot;优化方法论&quot;&gt;&lt;/a&gt;优化方法论&lt;/h2&gt;&lt;p&gt;做任何事情还是要有个方法论的，“授人以鱼不如授人以渔”的道理吧，方法通了，所有的问题就有了解决的途径。通过对公开资料的分析进行总结，对分布式存储系统的优化离不开以下几点：&lt;/p&gt;
&lt;h3 id=&quot;1-硬件层面&quot;&gt;&lt;a href=&quot;#1-硬件层面&quot; class=&quot;headerlink&quot; title=&quot;1. 硬件层面&quot;&gt;&lt;/a&gt;1. 硬件层面&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;硬件规划&lt;/li&gt;
&lt;li&gt;SSD选择&lt;/li&gt;
&lt;li&gt;BIOS设置&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&quot;2-软件层面&quot;&gt;&lt;a href=&quot;#2-软件层面&quot; class=&quot;headerlink&quot; title=&quot;2. 软件层面&quot;&gt;&lt;/a&gt;2. 软件层面&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Linux OS&lt;/li&gt;
&lt;li&gt;Ceph Configurations&lt;/li&gt;
&lt;li&gt;PG Number调整&lt;/li&gt;
&lt;li&gt;CRUSH Map&lt;/li&gt;
&lt;li&gt;其他因素&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="ceph" scheme="http://sunqi.me/categories/ceph/"/>
    
      <category term="openstack" scheme="http://sunqi.me/categories/ceph/openstack/"/>
    
    
  </entry>
  
  <entry>
    <title>Ceph集群磁盘没有剩余空间的解决方法</title>
    <link href="http://sunqi.me/2015/05/11/ceph-osd-is-full/"/>
    <id>http://sunqi.me/2015/05/11/ceph-osd-is-full/</id>
    <published>2015-05-11T17:21:42.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="故障描述"><a href="#故障描述" class="headerlink" title="故障描述"></a>故障描述</h2><p>OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。</p><a id="more"></a><h2 id="故障现象"><a href="#故障现象" class="headerlink" title="故障现象"></a>故障现象</h2><ul><li>尝试使用OpenStack重启虚拟机无效</li><li>尝试直接用rbd命令直接删除块失败</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@controller ~]# rbd -p volumes rm volume-c55fd052-212d-4107-a2ac-cf53bfc049be</span><br><span class="line">2015-04-29 05:31:31.719478 7f5fb82f7760  0 client.4781741.objecter  FULL, paused modify 0xe9a9e0 tid 6</span><br></pre></td></tr></table></figure><ul><li>查看ceph健康状态</li></ul><figure class="highlight plain"><figcaption><span>ceph -s</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cluster 059f27e8-a23f-4587-9033-3e3679d03b31</span><br><span class="line"> health HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482&#x2F;129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)</span><br><span class="line"> monmap e6: 4 mons at &#123;node-5e40.cloud.com&#x3D;10.10.20.40:6789&#x2F;0,node-6670.cloud.com&#x3D;10.10.20.31:6789&#x2F;0,node-66c4.cloud.com&#x3D;10.10.20.36:6789&#x2F;0,node-fb27.cloud.com&#x3D;10.10.20.41:6789&#x2F;0&#125;, election epoch 886, quorum 0,1,2,3 node-6670.cloud.com,node-66c4.cloud.com,node-5e40.cloud.com,node-fb27.cloud.com</span><br><span class="line"> osdmap e2743: 3 osds: 3 up, 3 in</span><br><span class="line">        flags full</span><br><span class="line">  pgmap v6564199: 320 pgs, 4 pools, 262 GB data, 43027 objects</span><br><span class="line">        786 GB used, 47785 MB &#x2F; 833 GB avail</span><br><span class="line">        7482&#x2F;129081 objects degraded (5.796%)</span><br><span class="line">             300 active+clean</span><br><span class="line">              20 active+degraded+remapped+backfill_toofull</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>ceph health detail</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">HEALTH_ERR 20 pgs backfill_toofull; 20 pgs degraded; 20 pgs stuck unclean; recovery 7482&#x2F;129081 objects degraded (5.796%); 2 full osd(s); 1 near full osd(s)</span><br><span class="line">pg 3.8 is stuck unclean for 7067109.597691, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.7d is stuck unclean for 1852078.505139, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.21 is stuck unclean for 7072842.637848, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.22 is stuck unclean for 7070880.213397, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.a is stuck unclean for 7067057.863562, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.7f is stuck unclean for 7067122.493746, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.5 is stuck unclean for 7067088.369629, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1e is stuck unclean for 7073386.246281, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.19 is stuck unclean for 7068035.310269, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.5d is stuck unclean for 1852078.505949, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1a is stuck unclean for 7067088.429544, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.1b is stuck unclean for 7072773.771385, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.3 is stuck unclean for 7067057.864514, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.15 is stuck unclean for 7067088.825483, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.11 is stuck unclean for 7067057.862408, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.6d is stuck unclean for 7067083.634454, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.6e is stuck unclean for 7067098.452576, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.c is stuck unclean for 5658116.678331, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.e is stuck unclean for 7067078.646953, current state active+degraded+remapped+backfill_toofull, last acting [2,0]</span><br><span class="line">pg 3.20 is stuck unclean for 7067140.530849, current state active+degraded+remapped+backfill_toofull, last acting [0,2]</span><br><span class="line">pg 3.7d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.7f is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.6d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.6e is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.5d is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.20 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.21 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.22 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.1e is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.19 is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.1a is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.1b is active+degraded+remapped+backfill_toofull, acting [0,2]</span><br><span class="line">pg 3.15 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.11 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.c is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.e is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.8 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.a is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.5 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">pg 3.3 is active+degraded+remapped+backfill_toofull, acting [2,0]</span><br><span class="line">recovery 7482&#x2F;129081 objects degraded (5.796%)</span><br><span class="line">osd.0 is full at 95%</span><br><span class="line">osd.2 is full at 95%</span><br><span class="line">osd.1 is near full at 93%</span><br></pre></td></tr></table></figure><h2 id="解决方案一-已验证"><a href="#解决方案一-已验证" class="headerlink" title="解决方案一(已验证)"></a>解决方案一(已验证)</h2><p>增加OSD节点，这也是官方文档中推荐的做法，增加新的节点后，Ceph开始重新平衡数据，OSD使用空间开始下降</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2015-04-29 06:51:58.623262 osd.1 [WRN] OSD near full (91%)</span><br><span class="line">2015-04-29 06:52:01.500813 osd.2 [WRN] OSD near full (92%)</span><br></pre></td></tr></table></figure><h2 id="解决方案二-理论上，没有进行验证"><a href="#解决方案二-理论上，没有进行验证" class="headerlink" title="解决方案二(理论上，没有进行验证)"></a>解决方案二(理论上，没有进行验证)</h2><p>如果在没有新的硬盘的情况下，只能采用另外一种方式。在当前状态下，Ceph不允许任何的读写操作，所以此时任何的Ceph命令都不好使，解决的方案就是尝试降低Ceph对于full的比例定义，我们从上面的日志中可以看到Ceph的full的比例为95%，我们需要做的就是提高full的比例，之后尽快尝试删除数据，将比例下降。</p><ul><li>尝试直接用命令设置，但是失败了，Ceph集群并没有重新同步数据，怀疑可能仍然需要重启服务本身</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph mon tell \* injectargs &#39;--mon-osd-full-ratio 0.98&#39;</span><br></pre></td></tr></table></figure><ul><li>修改配置文件，之后重启monitor服务，但是担心出问题，所以没有敢尝试该方法，后续经过在邮件列表确认，该方法应该不会对数据产生影响，但是前提是在恢复期间，所有的虚拟机不要向Ceph再写入任何数据。</li></ul><p>默认情况下full的比例是95%，而near full的比例是85%，所以需要根据实际情况对该配置进行调整。</p><figure class="highlight plain"><figcaption><span>/etc/ceph/ceph.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[global]</span><br><span class="line">    mon osd full ratio &#x3D; .98</span><br><span class="line">    mon osd nearfull ratio &#x3D; .80</span><br></pre></td></tr></table></figure><h2 id="分析总结"><a href="#分析总结" class="headerlink" title="分析总结"></a>分析总结</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><p>根据Ceph官方文档中的描述，当一个OSD full比例达到95%时，集群将不接受任何Ceph Client端的读写数据的请求。所以导致虚拟机在重启时，无法启动的情况。</p><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><p>从官方的推荐来看，应该比较支持添加新的OSD的方式，当然临时的提高比例是一个解决方案，但是并不推荐，因为需要手动的删除数据去解决，而且一旦再有一个新的节点出现故障，仍然会出现比例变满的状况，所以解决之道最好是扩容。</p><h3 id="思考"><a href="#思考" class="headerlink" title="思考"></a>思考</h3><p>在这次故障过程中，有两点是值得思考的：</p><ul><li>监控：由于当时服务器在配置过程中DNS配置错误，导致监控邮件无法正常发出，从而没有收到Ceph WARN的提示信息</li><li>云平台本身： 由于Ceph的机制，在OpenStack平台中分配中，大多时候是超分的，从用户角度看，拷贝大量数据的行为并没有不妥之处，但是由于云平台并没有相应的预警机制，导致了该问题的发生</li></ul><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity" target="_blank" rel="noopener">http://ceph.com/docs/master/rados/configuration/mon-config-ref/#storage-capacity</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;故障描述&quot;&gt;&lt;a href=&quot;#故障描述&quot; class=&quot;headerlink&quot; title=&quot;故障描述&quot;&gt;&lt;/a&gt;故障描述&lt;/h2&gt;&lt;p&gt;OpenStack + Ceph集群在使用过程中，由于虚拟机拷入大量新的数据，导致集群的磁盘迅速消耗，没有空余空间，虚拟机无法操作，Ceph集群所有操作都无法执行。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ceph" scheme="http://sunqi.me/categories/openstack/ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>(Kilo)Devstack Kilo版本localrc推荐</title>
    <link href="http://sunqi.me/2015/05/10/best-localrc-for-devstack-kilo/"/>
    <id>http://sunqi.me/2015/05/10/best-localrc-for-devstack-kilo/</id>
    <published>2015-05-10T19:33:44.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。</p><a id="more"></a><p>该配置文件中开启了所有的OpenStack的核心模块，以下几点需要注意：</p><ul><li>为了运行Neutron，服务器必须是双网卡，否则外网不会通</li><li>我的实验网段为200.21.0.0/16，eth0的IP为200.21.1.61，eth1与eth0为同一网段</li><li>eth1为公网访问网络，floating网络范围200.21.50.1/24，配置的GATEWAY为200.21.50.2</li><li>保证eth1所处的网段能够连接外网，但是配置为manual模式，配置如下：</li></ul><figure class="highlight plain"><figcaption><span>/etc/network/interface</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">auto eth1</span><br><span class="line">iface eth1 inet manual</span><br><span class="line">up ifconfig $IFACE 0.0.0.0 up</span><br><span class="line">down ifconfig $IFACE 0.0.0.0 down</span><br></pre></td></tr></table></figure><ul><li>localrc的配置</li></ul><figure class="highlight plain"><figcaption><span>localrc</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># Misc</span><br><span class="line">ADMIN_PASSWORD&#x3D;sysadmin</span><br><span class="line">DATABASE_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">RABBIT_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">SERVICE_PASSWORD&#x3D;$ADMIN_PASSWORD</span><br><span class="line">SERVICE_TOKEN&#x3D;$ADMIN_PASSWORD</span><br><span class="line"></span><br><span class="line"># Target Path</span><br><span class="line">DEST&#x3D;&#x2F;opt&#x2F;stack.kilo</span><br><span class="line"></span><br><span class="line"># Enable Logging</span><br><span class="line">LOGFILE&#x3D;$DEST&#x2F;logs&#x2F;stack.sh.log</span><br><span class="line">VERBOSE&#x3D;True</span><br><span class="line">LOG_COLOR&#x3D;True</span><br><span class="line">SCREEN_LOGDIR&#x3D;$DEST&#x2F;logs</span><br><span class="line"></span><br><span class="line"># Nova</span><br><span class="line">enable_service n-novnc n-cauth</span><br><span class="line"></span><br><span class="line"># Neutron</span><br><span class="line">disable_service n-net</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-svc,q-agt,q-dhcp,q-l3,q-meta,neutron</span><br><span class="line">ENABLED_SERVICES+&#x3D;,q-lbaas,q-vpn,q-fwaas</span><br><span class="line"></span><br><span class="line"># Ceilometer</span><br><span class="line">enable_service ceilometer-acompute ceilometer-acentral ceilometer-anotification ceilometer-collector ceilometer-api</span><br><span class="line">enable_service ceilometer-alarm-notifier ceilometer-alarm-evaluator</span><br><span class="line"></span><br><span class="line"># Enable Heat</span><br><span class="line">enable_service heat h-api h-api-cfn h-api-cw h-eng</span><br><span class="line"></span><br><span class="line"># Trove</span><br><span class="line">enable_service trove tr-api tr-tmgr tr-cond</span><br><span class="line"></span><br><span class="line"># Sahara</span><br><span class="line">enable_service sahara</span><br><span class="line"></span><br><span class="line">#FIXED_RANGE&#x3D;10.0.0.0&#x2F;24</span><br><span class="line">HOST_IP&#x3D;200.21.1.61</span><br><span class="line">FLOATING_RANGE&#x3D;200.21.50.1&#x2F;24</span><br><span class="line">PUBLIC_NETWORK_GATEWAY&#x3D;200.21.50.2</span><br><span class="line">Q_FLOATING_ALLOCATION_POOL&#x3D;start&#x3D;200.21.50.100,end&#x3D;200.21.50.150</span><br></pre></td></tr></table></figure><ul><li><p>确认br-ex是否存在</p><figure class="highlight plain"><figcaption><span>sudo ovs-vsctl show</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Bridge br-ex</span><br><span class="line">    Port br-ex</span><br><span class="line">        Interface br-ex</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;qg-7ec5be02-69&quot;</span><br><span class="line">        Interface &quot;qg-7ec5be02-69&quot;</span><br><span class="line">            type: internal</span><br><span class="line">ovs_version: &quot;2.0.2&quot;</span><br></pre></td></tr></table></figure></li><li><p>将eth1作为br-ex的接口</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ovs-vsctl add-port br-ex eth1</span><br></pre></td></tr></table></figure></li></ul><figure class="highlight plain"><figcaption><span>sudo ovs-vsctl show</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Bridge br-ex</span><br><span class="line">    Port br-ex</span><br><span class="line">        Interface br-ex</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;qg-7ec5be02-69&quot;</span><br><span class="line">        Interface &quot;qg-7ec5be02-69&quot;</span><br><span class="line">            type: internal</span><br><span class="line">    Port &quot;eth1&quot;</span><br><span class="line">        Interface &quot;eth1&quot;</span><br><span class="line">ovs_version: &quot;2.0.2&quot;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Devstack在Kilo版本中发生了一些变化，其中一个commit(279cfe75198c723519f1fb361b2bff3c641c6cef)的就是优化默认启动的程序，尽量减小对硬件的要求。如果不修改默认的配置进行安装，会产生一些问题，例如VNC无法打开，Heat模块没有加载等。这里给出一个个人比较常用的localrc，供大家参考。该配置在Ubuntu 14.04 Server LTS进行了测试。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="cloud computing" scheme="http://sunqi.me/categories/openstack/cloud-computing/"/>
    
      <category term="devstack" scheme="http://sunqi.me/categories/openstack/cloud-computing/devstack/"/>
    
      <category term="kilo" scheme="http://sunqi.me/categories/openstack/cloud-computing/devstack/kilo/"/>
    
    
  </entry>
  
  <entry>
    <title>OpenStack Kilo版本新功能分析</title>
    <link href="http://sunqi.me/2015/05/03/what-is-new-in-kilo/"/>
    <id>http://sunqi.me/2015/05/03/what-is-new-in-kilo/</id>
    <published>2015-05-03T18:37:22.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。</p><p>OpenStack Kilo版本的完整翻译版本可见：<a href="https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans</a></p><p>OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。</p><a id="more"></a><h2 id="社区贡献分析"><a href="#社区贡献分析" class="headerlink" title="社区贡献分析"></a>社区贡献分析</h2><p>我们先来看一下OpenStack在最近的4个稳定版本发布中，每一个项目的贡献情况：</p><img src="/images/blogs/what-is-new-in-kilo-contribution-by-modules.jpg" class="left"><p>我们能够很明显的发现，OpenStack最早的几大核心模块(Nova, Cinder, Glance, Keystone, Horizon, Swift)的代码贡献所占比例呈明显下降趋势，这里强调一下，是比例而不是数量，从数量上来看，版本之间相差并不大，以Nova为例，从Havana版本的24%下降到如今的10%。这从一个侧面反映了OpenStack的核心模块日趋稳定，更多的关注集中到更高层次或者功能优化上。</p><p>Neutron模块则一直处于稳中有升的状态,从Havana版本的7%上升到10%，说明Neutron仍然处于需要进一步完善的状态。</p><p>对于Ceilometer，Heat，Sahara，Ironic, Trove等新晋的核心模块，都处于稳步增长的阶段。贡献的比例在四个版本中基本保持持平的态势。在Kilo版本中，Sahara和Heat进入了前十名。</p><p>从Kilo版本的比例来看，Others的比例过半，Others主要包括了OpenStack测试相关项目，例如Rally；开发相关项目，例如Devstack;以及一些新的模块，例如：Manila，Magnum等众多进入孵化器的项目;还包括所有的Client以及Spec等。可以预见，OpenStack的开发重心逐步从底层的核心模块，逐步向更高层次、提供更丰富功能的方向发展。</p><h2 id="国内社区贡献分析"><a href="#国内社区贡献分析" class="headerlink" title="国内社区贡献分析"></a>国内社区贡献分析</h2><img src="/images/blogs/what-is-new-in-kilo-contributor.png" class="center"><p>从企业贡献排名来看，几大巨头企业牢牢占据贡献榜的前几名，OpenStack最成功的公司-Mirantis排名紧追Redhat成为第二贡献大户。排名前几位的公司还包括：IBM, Rackspace, Cisco, Suse, VMware, Intel等。</p><p>国内方面，华为继续稳定在第13名，但Review的数量从Juno版本的1353提升到2548个，贡献的项目几乎涵盖所有的项目，主要贡献来自Heat，Ceilometer, Horizon，Neutron, Nova等项目。</p><p>国内排名第2的贡献企业是九州云，排名达到了21位，看来龚永生的到来为九州云添加了无限活力。九州云的贡献主要来自Horizon和Neutron两个项目，龚永生不愧为Neutron的Core，在网络方面的贡献，九州云的确很给力。</p><p>排名第3的企业是海云捷迅，排名为44位，海云是国内比较早的一批OpenStack创业企业，贡献方面以Sahara，Neutron，Nova，oslo.messaging以及Cinder为主，从之前了解的情况来看，海云的项目不少，可能提交的修改是与在实际项目中遇到的问题有关。</p><p>排名之后的企业还有Kylin Cloud，UnitedStack，EasyStack等。由于是手工统计，在统计过程中如有遗漏，希望大家多多指正。</p><h2 id="Horizon新功能"><a href="#Horizon新功能" class="headerlink" title="Horizon新功能"></a>Horizon新功能</h2><p>Horizon在K版本除了增强了对新增模块的支持，从UE的角度也为我们带来了很多新功能</p><ul><li>支持向导式的创建虚拟机，现在还处于beta版本，如果想在Horizon里激活，可以通过设置local_setting.py的配置实现：</li></ul><figure class="highlight plain"><figcaption><span>local_setting.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">LAUNCH_INSTANCE_NG_ENABLED &#x3D; True</span><br></pre></td></tr></table></figure><img src="/images/blogs/what-is-new-in-kilo-instance-guide1.png" class="left"><img src="/images/blogs/what-is-new-in-kilo-instance-guide2.png" class="left"><ul><li>支持简单的主题，主要通过修改_variables.scss和_style.scss完成对主题颜色和简单样式的修改，但是格局不能改变，修改local_settings.py</li></ul><figure class="highlight plain"><figcaption><span>local_setting.py</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CUSTOM_THEME_PATH &#x3D; &#39;static&#x2F;themes&#x2F;blue&#39;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>static/themes/blue/_variables.scss</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$gray:                   #2751DB !default;</span><br><span class="line">$gray-darker:            #94A5F2 !default;</span><br><span class="line">$gray-dark:              #0C0CED !default;</span><br><span class="line">$gray-light:             #C7CFF2 !default;</span><br><span class="line">$gray-lighter:           #DCE1F5 !default;</span><br><span class="line"></span><br><span class="line">$brand-primary:         #375A7F !default;</span><br><span class="line">$brand-success:         #00bc8c !default;</span><br><span class="line">$brand-info:            #34DB98 !default;</span><br><span class="line">$brand-warning:         #F39C12 !default;</span><br><span class="line">$brand-danger:          #E74C3C !default;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>static/themes/blue/_style.scss</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;&#x2F; Blue</span><br><span class="line">&#x2F;&#x2F; ----</span><br><span class="line"></span><br><span class="line">@mixin btn-shadow($color) &#123;</span><br><span class="line">  @include gradient-vertical-three-colors(lighten($color, 3%), $color, 6%, darken($color, 3%));</span><br><span class="line">  filter: none;</span><br><span class="line">  border: 1px solid darken($color, 10%);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">&#x2F;&#x2F; Buttons &#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line"></span><br><span class="line">.btn-default,</span><br><span class="line">.btn-default:hover &#123;</span><br><span class="line">  @include btn-shadow($btn-default-bg);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">.btn-primary,</span><br><span class="line">.btn-primary:hover &#123;</span><br><span class="line">  @include btn-shadow($btn-primary-bg);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><img src="/images/blogs/what-is-new-in-kilo-horizon-theme1.png" class="left"><img src="/images/blogs/what-is-new-in-kilo-horizon-theme2.png" class="left"><h2 id="Nova新功能"><a href="#Nova新功能" class="headerlink" title="Nova新功能"></a>Nova新功能</h2><h3 id="Nova-Scheduler"><a href="#Nova-Scheduler" class="headerlink" title="Nova Scheduler"></a>Nova Scheduler</h3><ul><li>标准化了conductor，compute与scheduler的接口，为之后的接口分离做好准备，对于部分直接访问nova数据库的filters进行了优化，不再允许直接访问，参考链接：<a href="https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst" target="_blank" rel="noopener">https://github.com/openstack/nova-specs/blob/master/specs/kilo/approved/isolate-scheduler-db-filters.rst</a></li><li>对Scheduler做了一些优化，例如：Scheduler对于每一个请求都会重新进行Filters/Weighers，为了优化这个问题，将filter/weighter的初始化从handler移到scheduler，这样每次请求的时候都可以重新使用了。</li></ul><h3 id="Libvirt-NFV相关功能"><a href="#Libvirt-NFV相关功能" class="headerlink" title="Libvirt NFV相关功能"></a>Libvirt NFV相关功能</h3><ul><li>NUMA(Non Uniform Memory Architecture)，在这个架构下，每个处理器都会访问“本地”的内存池，从而在CPU和存储之间有更小的延迟和更大的带宽。</li><li>在Kilo版本中针对此功能的实现包括：基于NUMA的调度的实现；可以将vCPU绑定在物理CPU上；超大页的支持。以上提到的三点都是通过Flavor的Extra Spces完成定义的。</li></ul><h3 id="EC2-API"><a href="#EC2-API" class="headerlink" title="EC2 API"></a>EC2 API</h3><ul><li>EC2 API被从Nova中踢出去了</li><li>取而代之的是在stackforge的EC2 API转换服务</li></ul><h3 id="API-Microversioning"><a href="#API-Microversioning" class="headerlink" title="API Microversioning"></a>API Microversioning</h3><p>先来解释一下为什么需要API的微版本：主要原因在于现在这种API扩展方式，对于API实现的代码的增加或减少管理非常不方便，容易导致不一致性。引入微版本主要目的就是让开发人员在修改API代码时能够向前兼容，而不是加入一个新的API扩展；用户通过指定API的版本，在请求时也能决定是使用的具体的动作。</p><p>包含版本的返回:</p><figure class="highlight plain"><figcaption><span>Result</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">GET &#x2F;</span><br><span class="line">&#123;</span><br><span class="line">     &quot;versions&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">            &quot;id&quot;: &quot;v2.1&quot;,</span><br><span class="line">            &quot;links&quot;: [</span><br><span class="line">                  &#123;</span><br><span class="line">                    &quot;href&quot;: &quot;http:&#x2F;&#x2F;localhost:8774&#x2F;v2&#x2F;&quot;,</span><br><span class="line">                    &quot;rel&quot;: &quot;self&quot;</span><br><span class="line">                &#125;</span><br><span class="line">            ],</span><br><span class="line">            &quot;status&quot;: &quot;CURRENT&quot;,</span><br><span class="line">            &quot;version&quot;: &quot;5.2&quot;</span><br><span class="line">            &quot;min_version&quot;: &quot;2.1&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">   ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>客户端的Header信息：</p><figure class="highlight plain"><figcaption><span>Header</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X-OpenStack-Nova-API-Version: 2.114</span><br></pre></td></tr></table></figure><h3 id="一个已知的问题：Evacuate"><a href="#一个已知的问题：Evacuate" class="headerlink" title="一个已知的问题：Evacuate"></a>一个已知的问题：Evacuate</h3><p>这个问题的产生主要是因为Evacuate的清理机制，主机名的变化会导致nova-compute重启过程中误删所有虚拟机，所以一个变通的方法是设置</p><figure class="highlight plain"><figcaption><span>nova.conf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">destroy_after_evacuate&#x3D;False</span><br></pre></td></tr></table></figure><p>这个问题会在Liberty中得到修复，相关的Spec：<a href="https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst" target="_blank" rel="noopener">https://review.openstack.org/#/c/161444/3/specs/liberty/approved/robustify_evacuate.rst</a></p><h2 id="Glance新功能"><a href="#Glance新功能" class="headerlink" title="Glance新功能"></a>Glance新功能</h2><ul><li>自动进行镜像格式转化，例如，Ceph是使用RAW格式的，假如我们上传的是QCOW2，创建虚拟机时，就会经历一番上传下载的过程，速度异常缓慢。而且RAW格式通常都是原始大小，上传时候非常慢，完全可以通过上传小镜像自动转换为指定格式。</li><li>Glance支持多字段排序<figure class="highlight plain"><figcaption><span>API</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#x2F;images?sort_key&#x3D;status&amp;sort_dir&#x3D;asc&amp;sort_key&#x3D;name&amp;sort_dir&#x3D;asc&amp;sort_key&#x3D;created_at&amp;sort_dir&#x3D;desc</span><br></pre></td></tr></table></figure></li><li>临时将镜像设置为非活跃状态，假如一个镜像里有病毒，管理员就会将该镜像设置为非活跃状态，在清理后重新发布该镜像，在这个过程中，所有非管理员用户都无法使用或者下载这个镜像</li><li>免重启动态加载配置文件，配置文件改动后重启服务，现在可以给glance服务发送SIGHUP触发，这样升级就可以零当机时间。</li><li>使用多个Swift容器存储镜像，减少大规模部署时的请求瓶颈</li></ul><h2 id="Cinder新功能"><a href="#Cinder新功能" class="headerlink" title="Cinder新功能"></a>Cinder新功能</h2><ul><li>实现服务逻辑代码与数据库结构之间的解耦，支持Rolling更新</li><li>一致性组是指具备公共操作的卷，逻辑上化为一组。在K版本中对增强一致性组的功能：可以添加、删除卷，从已经存在的快照创建新的组，关于一致性组的详细操作可以参考：<a href="http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html" target="_blank" rel="noopener">http://docs.openstack.org/admin-guide-cloud/content/consistency-groups.html</a></li></ul><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">cinder consisgroup-update</span><br><span class="line">[--name NAME]</span><br><span class="line">[--description DESCRIPTION]</span><br><span class="line">[--add-volumes UUID1,UUID2,......]</span><br><span class="line">[--remove-volumes UUID3,UUID4,......]</span><br><span class="line">CG</span><br></pre></td></tr></table></figure><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cinder consisgroup-create-from-src</span><br><span class="line">[--cgsnapshot CGSNAPSHOT]</span><br><span class="line">[--name NAME]</span><br><span class="line">[--description DESCRIPTION]</span><br></pre></td></tr></table></figure><ul><li>卷类型的增强功能主要包含两个：为某一项目创建私有的卷类型和为卷类型增加描述信息</li></ul><figure class="highlight plain"><figcaption><span>cinder</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cinder type-create &lt;name&gt; --is-public</span><br><span class="line">cinder type-create &lt;name&gt; &lt;description&gt;</span><br></pre></td></tr></table></figure><h2 id="Neutron新功能"><a href="#Neutron新功能" class="headerlink" title="Neutron新功能"></a>Neutron新功能</h2><ul><li>DVR支持OVS中的VLANs</li><li>新的V2版本的LBaas的API</li><li>新的插件的更新，详情请见更新日志中</li><li>一些高级服务的分离，例如：L3, ML2, VPNaaS, LBaaS</li></ul><p>网络方面我不是权威，希望有高人能出来讲讲Kilo中的Neutron新功能。</p><h2 id="Keystone新功能"><a href="#Keystone新功能" class="headerlink" title="Keystone新功能"></a>Keystone新功能</h2><ul><li>项目嵌套，创建一个新的Project时候，可以指定parent的Project</li></ul><figure class="highlight plain"><figcaption><span>keystone</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST &#x2F;projects</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;project&quot;: &#123;</span><br><span class="line">        &quot;description&quot;: &quot;Project space for Test Group&quot;,</span><br><span class="line">        &quot;domain_id&quot;: &quot;1789d1&quot;,</span><br><span class="line">        &quot;enabled&quot;: true,</span><br><span class="line">        &quot;name&quot;: &quot;Test Group&quot;,</span><br><span class="line">        &quot;parent_id&quot;: &quot;7fa612&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>Keystone与Keystone的联盟，有了这个功能两个或者更多的云服务提供者就可以共享资源，这个功能在J版本引入，在K版本中主要针对该功能的进一步增强，具体的使用方法可参考这篇博文：<a href="http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/" target="_blank" rel="noopener">http://blog.rodrigods.com/playing-with-keystone-to-keystone-federation/</a></li><li>针对新人授权的一些增强功能</li><li>keystone的配置中有部分配置发生了变化，例如：keystone.token.backends.memcache被keystone.token.persistence.backends.memcache取代，更多详细内容请参考更新日志</li></ul><h2 id="Swift新功能"><a href="#Swift新功能" class="headerlink" title="Swift新功能"></a>Swift新功能</h2><ul><li>纠删码的加入应该是这个版本最大的亮点，但是纠删码作为beta版本发布，并不推荐应用于生产环境，关于纠删码的详细介绍可以参考：<a href="http://docs.openstack.org/developer/swift/overview_erasure_code.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/swift/overview_erasure_code.html</a></li><li>复合型令牌，简而言之就是需要用户加上服务的Token才能对Swfit存放的内容进行操作，如下图所示：</li></ul><figure class="highlight plain"><figcaption><span>swift</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">client</span><br><span class="line">   \</span><br><span class="line">    \   &lt;request&gt;: &lt;path-specific-to-the-service&gt;</span><br><span class="line">     \  x-auth-token: &lt;user-token&gt;</span><br><span class="line">      \</span><br><span class="line">    SERVICE</span><br><span class="line">       \</span><br><span class="line">        \    PUT: &#x2F;v1&#x2F;SERVICE_1234&#x2F;&lt;container&gt;&#x2F;&lt;object&gt;</span><br><span class="line">         \   x-auth-token: &lt;user-token&gt;</span><br><span class="line">          \  x-service-token: &lt;service-token&gt;</span><br><span class="line">           \</span><br><span class="line">          Swift</span><br></pre></td></tr></table></figure><p>具体的设计文档：<a href="http://docs.openstack.org/developer/swift/overview_backing_store.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/swift/overview_backing_store.html</a></p><ul><li>全局性的集群复制优化，大幅提高效率，避免经过广域网传播的数据</li></ul><h2 id="Ceilometer新功能"><a href="#Ceilometer新功能" class="headerlink" title="Ceilometer新功能"></a>Ceilometer新功能</h2><ul><li>支持Ceph对象存储监控，当对象存储为Ceph而不是Swfit的时候，使用Polling机制，使用Ceph的Rados Gateway的API接口获取数据，具体的设计文档：<a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst" target="_blank" rel="noopener">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer_ceph_integration.rst</a></li><li>Ceilometer API RBAC - 更细粒度的权限控制: <a href="https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst" target="_blank" rel="noopener">https://github.com/openstack/ceilometer-specs/blob/master/specs/kilo/ceilometer-rbac.rst</a><figure class="highlight plain"><figcaption><span>Ceilometer</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;context_is_admin&quot;: [[&quot;role:admin&quot;]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>更细粒度的控制<figure class="highlight plain"><figcaption><span>Ceilometer</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">     &quot;context_is_admin&quot;: [[&quot;role:admin&quot;]],</span><br><span class="line">     &quot;admin_or_cloud_admin&quot;: [[&quot;rule:context_is_admin&quot;],</span><br><span class="line">              [&quot;rule:admin_and_matching_project_domain_id&quot;]],</span><br><span class="line">     &quot;telemetry:alarm_delete&quot;: [[&quot;rule:admin_or_cloud_admin&quot;]]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li><li>接口中的模糊查询，增加了一个新的查询符号=~</li><li>支持更多的测量，包括Hyper-V，IPMI相关的</li></ul><h2 id="Ironic新功能"><a href="#Ironic新功能" class="headerlink" title="Ironic新功能"></a>Ironic新功能</h2><ul><li>iLO的优化</li><li>使用Config Drive替代Metadata服务</li><li>全盘镜像支持，可以跳过raddisk和kernel，这样就可以部署Windows的镜像了</li><li>使用本地盘启动，替代PXE方式，可以通过设置flavor的capabilities:boot_option实现</li></ul><h2 id="Oslo"><a href="#Oslo" class="headerlink" title="Oslo"></a>Oslo</h2><p>解决了很多之前遗留的技术债，还有一些命名规范的问题。olso.messaging实现了心跳，olso.log在所有项目中使用，优化了oslo.db的代码。</p><h2 id="OpenStack文档"><a href="#OpenStack文档" class="headerlink" title="OpenStack文档"></a>OpenStack文档</h2><p>优化了docs.openstack.org页面，也可以从中选择相应的语言。有专门的团队负责安装、网络和高可靠的文档。</p><h2 id="其他模块"><a href="#其他模块" class="headerlink" title="其他模块"></a>其他模块</h2><p>对于Sahara, Heat, Trove等模块的更新没有在这里Highlight出来，大家可以参考更新日志里的内容，或者查看specs中的具体描述。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>通过Kilo的一些更新可以看到，Kilo版本在不断优化代码结构的基础上，增加了一些新功能，也偿还了一些技术债，总体来说是一种稳中有升的态势，但是总体感觉并没有太多的惊喜和出人意料。相信随着更多的孵化项目进入正式版本中，OpenStack一定会向更多元化的方向发展。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OpenStack Kilo版本已经于2015年4月30日正式Release，这是OpenStack第11个版本，距离OpenStack项目推出已经整整过去了5年多的时间。在这个阶段OpenStack得到不断的增强，同时OpenStack社区也成为即Linux之后的第二大开源社区，参与的人数、厂商众多，也成就了OpenStack今天盛世的局面。虽然OpenStack在今年经历了Nebula的倒闭，但是随着国内的传统行业用户对OpenStack越来越重视，我们坚信OpenStack明天会更好。&lt;/p&gt;
&lt;p&gt;OpenStack Kilo版本的完整翻译版本可见：&lt;a href=&quot;https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://wiki.openstack.org/wiki/ReleaseNotes/Kilo/zh-hans&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Kilo版本的翻译工作由我和我的同事裴莹莹(Wendy)共同完成，翻译校对工作由裴莹莹完成。如果翻译有任何问题，请各位多多指正。&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.me/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.me/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>(Havana)将OpenStack Havana源代码编译为DEB包</title>
    <link href="http://sunqi.me/2015/03/05/build-openstack-source-code-to-deb-package/"/>
    <id>http://sunqi.me/2015/03/05/build-openstack-source-code-to-deb-package/</id>
    <published>2015-03-05T05:57:16.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Why"><a href="#Why" class="headerlink" title="Why"></a>Why</h2><p>我想有以下有几个原因促使我写这篇Blog：</p><ul><li>很多人开始OpenStack之旅是从Ubuntu开始，但是却没有一篇文章系统的介绍如何将修改后的代码重新编译回DEB包。</li><li>如果我们采用源代码直接安装的方式对OpenStack模块进行管理，一致性很难保证，难以维护。</li><li>Debian类系统的打包看起来比RPM包复杂很多。</li></ul><h2 id="Who"><a href="#Who" class="headerlink" title="Who"></a>Who</h2><p>谁需要看这篇文章呢？</p><ul><li>不了解如何编译DEB包</li><li>想把修改过的OpenStack源代码重新发布，供内部使用</li><li>希望改变直接维护源代码</li></ul><p>当然，如果您已经是这方面的高手，欢迎给我指正我Blog中的不足，十分感谢。</p><a id="more"></a><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><p>我已经将整个的编译过程集成在Vagrant脚本中，你可以直接安装Vagrant后，下载我的源代码，启动后就能看到整个的编译过程。</p><p>Vagrant 版本要求为1.3.5，Virtualbox版本要求为4.1或者4.2均可。</p><h2 id="Let’s-play-some-magic"><a href="#Let’s-play-some-magic" class="headerlink" title="Let’s play some magic"></a>Let’s play some magic</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;vagrant-build-openstack-deb</span><br><span class="line">cd vagrant-build-openstack-deb</span><br><span class="line">vagrant up</span><br></pre></td></tr></table></figure><p>虚拟机启动后，将会自动从github(这里使用的是csdn code的镜像代码)同步最新代码，然后使用编译脚本，执行打包操作。如果不考虑下载的时间，整个过程大概持续5分钟左右的时间，编译好的Deb包将会存放在/root/build目录下。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vagrant ssh</span><br></pre></td></tr></table></figure><p>即可登陆到虚拟机，切换到root目录就可以查看到所有打包好的DEB的情况了，当然你也可以直接使用dpkg -i命令进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo -s</span><br><span class="line">cd &#x2F;root&#x2F;build</span><br><span class="line">ls -lrt *.deb</span><br><span class="line">dpkg -i python-glance_2013.2.2.dev1.g5cd7a22~precise-0ubuntu1_all.deb</span><br></pre></td></tr></table></figure><h2 id="Step-by-Step"><a href="#Step-by-Step" class="headerlink" title="Step by Step"></a>Step by Step</h2><p>看过了整个的编译过程，下面来介绍一点点细节。</p><p>全部的编译部分代码都在这个文件中：<a href="https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh" target="_blank" rel="noopener">https://github.com/xiaoquqi/vagrant-build-openstack-deb/blob/master/scripts/build.sh</a></p><p>下面让我们来仔细分析一下整个编译过程。</p><ul><li>添加必要的源</li></ul><p>这里面我们用的源包含sohu的Ubuntu 12.04源以及Ubuntu的Havana源</p><pre><code>deb http://mirrors.sohu.com/ubuntu/ precise main restricteddeb http://mirrors.sohu.com/ubuntu/ precise-updates main restricteddeb http://mirrors.sohu.com/ubuntu/ precise universedeb http://mirrors.sohu.com/ubuntu/ precise-updates universedeb http://mirrors.sohu.com/ubuntu/ precise multiversedeb http://mirrors.sohu.com/ubuntu/ precise-updates multiversedeb http://mirrors.sohu.com/ubuntu/ precise-backports main restricted universe multiversedeb http://mirrors.sohu.com/ubuntu/ precise-security main restricteddeb http://mirrors.sohu.com/ubuntu/ precise-security universedeb http://mirrors.sohu.com/ubuntu/ precise-security multiversedeb http://ubuntu-cloud.archive.canonical.com/ubuntu precise-updates/havana main</code></pre><ul><li>安装必要的编译软件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apt-get install -y debootstrap equivs schroot</span><br><span class="line">apt-get install -y devscripts</span><br><span class="line">apt-get install -y build-essential checkinstall sbuild</span><br><span class="line">apt-get install -y dh-make</span><br><span class="line">apt-get install -y bzr bzr-builddeb</span><br><span class="line">apt-get install -y git</span><br><span class="line">apt-get install -y python-setuptools</span><br></pre></td></tr></table></figure><ul><li>编译脚本的源代码仓库</li></ul><p>Ubuntu维护源代码编译脚本是使用叫做bzr的工具，常使用Launchpad的朋友应该比较熟悉，这是一套类似于git的分布式管理工具，不同的是这是一套完全用python语言实现的管理工具，不仅具有代码版本控制功能并且与Launchpad高度整合，作为Ubuntu维护不可缺少的重要工具之一。例如，这里面用到的glance编译脚本就可以在这里找到：</p><p><a href="https://code.launchpad.net/~ubuntu-server-dev/glance/havana" target="_blank" rel="noopener">https://code.launchpad.net/~ubuntu-server-dev/glance/havana</a></p><p>页面上方有下载代码的方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bzr branch lp:~ubuntu-server-dev&#x2F;glance&#x2F;havana</span><br><span class="line">git clone https:&#x2F;&#x2F;code.csdn.net&#x2F;openstack&#x2F;glance.git --branch &quot;stable&#x2F;havana&quot; glance_source</span><br></pre></td></tr></table></figure><ul><li>准备环境</li></ul><p>在Vagrant启动一台新虚拟机之后，并没有pip，如果不安装pip，则会在python setup.py sdist过程中，把pip安装到源代码目录中，引起Build失败。将//vagrant/pip/pip-1.4.1.tar.gz解压缩并安装，之后安装pbr：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar zxvf pip-1.4.1.tar.gz</span><br><span class="line">cd pip-1.4.1</span><br><span class="line">sudo python setup.py install</span><br><span class="line">sudo pip install pbr</span><br></pre></td></tr></table></figure><ul><li>生成source文件</li></ul><p>进入glance_source目录，执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python setup.py sdist</span><br></pre></td></tr></table></figure><p>生成的tar.gz文件会在glance_source/dist下，注意此时该文件的名称为：</p><pre><code>glance-2013.2.2.dev1.g5cd7a22.tar.gz</code></pre><p>接下来我们需要将该文件重命名为：</p><pre><code>glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz</code></pre><p>特别注意：glance后面已经变为下划线！！！</p><p>把文件移动到与glance和glance_source同一级别的目录，这样在编译的时候，才能找到source文件。此时的目录结构为：</p><pre><code>├── glance│   ├── debian├── glance_source├── glance_2013.2.2.dev1.g5cd7a22~precise.orig.tar.gz</code></pre><ul><li>安装依赖包</li></ul><p>为了保证顺利的完成编译，我们需要安装要编译包的所有依赖包，简单来说就是glance/debian/control文件中定义的Depends部分的内容。当然在编译的时候我们也可以完全忽略依赖，但是并不推荐。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mk-build-deps -i -t &#39;apt-get -y&#39; debian&#x2F;control</span><br></pre></td></tr></table></figure><p>这样系统就会自动安装所有依赖包，并且生成一个glance-build-deps_1.0_all.deb文件。</p><ul><li>生成日志信息</li></ul><p>开始编译前，我们还需要告诉编译器我们要编译的版本，还记得刚才生成的dist包吗，把那个版本拿出来作为我们commit的版本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd glance</span><br><span class="line">dch -b -D precise --newversion &quot;1:2013.2.2.dev1.g5cd7a22~precise-0ubuntu1&quot; &#39;This is a build test.&#39;</span><br><span class="line">debcommit</span><br></pre></td></tr></table></figure><p>这样在glance/debian/changelog中就会增加一条新的日志。</p><ul><li>开始编译</li></ul><p>万事俱备，只欠东风。我们利用bzr提供的builddeb开始编译，这里我们忽略签名问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd glance</span><br><span class="line">bzr builddeb -- -sa -us -uc</span><br></pre></td></tr></table></figure><p>大功告成啦。快去/root/build/glance下看看你的deb包吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Debian包的编译的确涉及很多知识点，而且可使用的编译工具很多，关系很复杂。这篇博文，只为了帮助大家对DEB包的编译有一个快速的认识，如果想了解更多关于编译的知识，请关注后续的博文。</p><p>最后，我们仍然希望有更多的热爱OpenStack的朋友们加入我们公司，如果有意向的请与我联系</p><ul><li>邮箱：<a href="mailto:xiaoquqi@gmail.com">xiaoquqi@gmail.com</a></li><li>新浪微博：@RaySun(<a href="http://weibo.com/xiaoquqi" target="_blank" rel="noopener">http://weibo.com/xiaoquqi</a>)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;Why&quot;&gt;&lt;a href=&quot;#Why&quot; class=&quot;headerlink&quot; title=&quot;Why&quot;&gt;&lt;/a&gt;Why&lt;/h2&gt;&lt;p&gt;我想有以下有几个原因促使我写这篇Blog：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;很多人开始OpenStack之旅是从Ubuntu开始，但是却没有一篇文章系统的介绍如何将修改后的代码重新编译回DEB包。&lt;/li&gt;
&lt;li&gt;如果我们采用源代码直接安装的方式对OpenStack模块进行管理，一致性很难保证，难以维护。&lt;/li&gt;
&lt;li&gt;Debian类系统的打包看起来比RPM包复杂很多。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;Who&quot;&gt;&lt;a href=&quot;#Who&quot; class=&quot;headerlink&quot; title=&quot;Who&quot;&gt;&lt;/a&gt;Who&lt;/h2&gt;&lt;p&gt;谁需要看这篇文章呢？&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;不了解如何编译DEB包&lt;/li&gt;
&lt;li&gt;想把修改过的OpenStack源代码重新发布，供内部使用&lt;/li&gt;
&lt;li&gt;希望改变直接维护源代码&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;当然，如果您已经是这方面的高手，欢迎给我指正我Blog中的不足，十分感谢。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="ubuntu" scheme="http://sunqi.me/categories/openstack/ubuntu/"/>
    
      <category term="deb" scheme="http://sunqi.me/categories/openstack/ubuntu/deb/"/>
    
    
  </entry>
  
  <entry>
    <title>(Juno)OpenStack Neutron L3高可靠</title>
    <link href="http://sunqi.me/2015/03/05/openstack-neutron-l3-high-availability/"/>
    <id>http://sunqi.me/2015/03/05/openstack-neutron-l3-high-availability/</id>
    <published>2015-03-05T04:53:57.000Z</published>
    <updated>2020-01-03T03:14:17.843Z</updated>
    
    <content type="html"><![CDATA[<h2 id="L3层Agent的低可靠解决方案"><a href="#L3层Agent的低可靠解决方案" class="headerlink" title="L3层Agent的低可靠解决方案"></a>L3层Agent的低可靠解决方案</h2><p>当前，你可以通过多网络节点的方式解决负载分担，但是这并非高可靠和冗余的解决方案。假设你有三个网络节点，当你创建新的路由时，会自动的将新路由规划和分布在这三个网络节点中的一个上。但是，如果一个节点坏掉，上面运行的所有路由将无法提供服务，路由转发也无法正常进行。在Neutron的IceHouse版本中，没有提供任何内置的解决方案。</p><a id="more"></a><h2 id="DHCP-Agent的高可靠的变通之道"><a href="#DHCP-Agent的高可靠的变通之道" class="headerlink" title="DHCP Agent的高可靠的变通之道"></a>DHCP Agent的高可靠的变通之道</h2><p>DHCP的Agent是一个另类——DHCP协议本身就支持在同一个资源池内同时使用多个DHCP提供服务。</p><p>在neutron.conf中仅仅需要改变：</p><pre><code>dhcp_agents_per_network = X</code></pre><p>这样DHCP的调度程序会为同一网络启动X个DHCP Agents。所以，对于三个网络节点，并设置dhcp_agents_per_network = 2，每个Neutron网络会在三个节点中启动两个DHCP Agents。这个是如何工作的呢？</p>{% img right /images/blogs/neutron-l3-ha-dhcp-ha.png 200 300 %}<p>首先，让我们来看一下物理层面的实现。当一台主机连接到子网10.0.0.0/24，会发出DHCP Discover广播包。两个DHCP服务进程dnsmasq1和dnsmasq2(或者其他的DHCP服务)收到广播包，并回复10.0.0.2。假设第一个DHCP服务响应了服务器请求，并将10.0.0.2的请求广播出去，并且指明提供IP的是dnsmasq1-10.0.0.253。所有服务都会接收到广播，但是只有dnsmasq1会回复ACK。由于所有DHCP通讯都是基于广播，第二个DHCP服务也会收到ACK，于是将10.0.0.2标记已经被AA:BB:CC:11:22:33获取，而不会提供给其他的主机。总结一下，所有客户端与服务端的通讯都是基于广播，因此状态(IP地址什么时候被分配，被分配给谁)可以被所有分布的节点正确获知。</p><p>在Neutron中，分配MAC地址与IP地址的关系，是在每个dnsmasq服务之前完成的，也就是当Neutron创建Port时。因此，在DHCP请求广播之前，所有两个dnsmasq服务已经在leases文件中获知了，AA:BB:CC:11:22:33应该分配10.0.0.2的映射关系。</p><h2 id="回到L3-Agent的低可用"><a href="#回到L3-Agent的低可用" class="headerlink" title="回到L3 Agent的低可用"></a>回到L3 Agent的低可用</h2><p>L3 Agent，没有(至少现在没有)提供任何DHCP所能提供的高可靠解决方案，但是用户的确很需要高可靠。怎么办呢？</p><p>Pacemaker/Corosync - 使用外部的集群管理技术，为Active节点指定一个Standby的网络节点。Standby节点在正常情况下等呀、等呀等，一旦Active节点发生故障，L3 Agent立即在Standby节点启动。这两个节点配置相同的主机名，当Standby的Agent出台并和服务之前开始同步后，它自己的ID不会改变，因此就像管理同一个router一样。</p><p>另外一个方案是采用定时同步的方式(cron job)。用Python SDK开发一段脚本，使用API获取所有已经故去的Agent们，之后获取所有上面承载的路由，并且把他们重新分配给其他的Agent。</p><p>在Juno开发过程中，查看Kevin Benton的这个Patch，让Neutron自己具备重新分配路由的功能: <a href="https://review.openstack.org/#/c/110893/" target="_blank" rel="noopener">https://review.openstack.org/#/c/110893/</a></p><h2 id="重新分配路由——路漫漫兮"><a href="#重新分配路由——路漫漫兮" class="headerlink" title="重新分配路由——路漫漫兮"></a>重新分配路由——路漫漫兮</h2><p>以上列出的解决方案，实质上都有从失败到恢复的时间，如果在简单的应用场景下，恢复一定数量的路由到新节点并不算慢。但是想象一下，如果有上千个路由就需要话费数个小时完成，重新分配和配置的过程。人们非常需要快速的故障恢复！</p><h2 id="分布式虚拟路由-Distributed-Virtual-Router"><a href="#分布式虚拟路由-Distributed-Virtual-Router" class="headerlink" title="分布式虚拟路由(Distributed Virtual Router)"></a>分布式虚拟路由(Distributed Virtual Router)</h2><p>这里有一些文档描述DVR是如何工作的：</p><ul><li><a href="http://specs.openstack.org/openstack/neutron-specs/specs/juno/neutron-ovs-dvr.html" target="_blank" rel="noopener">http://specs.openstack.org/openstack/neutron-specs/specs/juno/neutron-ovs-dvr.html</a></li><li><a href="https://docs.google.com/document/d/1jCmraZGirmXq5V1MtRqhjdZCbUfiwBhRkUjDXGt5QUQ/" target="_blank" rel="noopener">https://docs.google.com/document/d/1jCmraZGirmXq5V1MtRqhjdZCbUfiwBhRkUjDXGt5QUQ/</a></li><li><a href="https://docs.google.com/document/d/1depasJSnGZPOnRLxEC_PYsVLcGVFXZLqP52RFTe21BE/" target="_blank" rel="noopener">https://docs.google.com/document/d/1depasJSnGZPOnRLxEC_PYsVLcGVFXZLqP52RFTe21BE/</a></li></ul><p>这里的要点是将路由放到计算节点(compute nodes)，这让网络节点的L3 Agent变得没用了。是不是这样呢？</p><ul><li>DVR主要处理Floating IPs，把SNAT留给网络节点的L3 Agent</li><li>不和VLANs一起工作，仅仅支持tunnes和L2pop开启</li><li>每个计算节点需要连接外网</li><li>L3 HA是对部署的一种简化，这个基于Havana和Icehouse版本部署的云平台所不具备的</li></ul><p>理想情况下，你想把DVR和L3 HA一起使用。Floating IP的流量会从你的计算节点直接路由出去，而SNAT的流量还是会从你的计算节点HA集群的L3 Agent进行转发。</p><h2 id="3层高可靠"><a href="#3层高可靠" class="headerlink" title="3层高可靠"></a>3层高可靠</h2><p>Juno版本的L3的HA解决方案应用了Linux上流行的keepalived工具，在内部使用了VRRP。首先，我们先来讨论一下VRRP。</p><h2 id="什么是VRRP，如何在真实世界里工作"><a href="#什么是VRRP，如何在真实世界里工作" class="headerlink" title="什么是VRRP，如何在真实世界里工作"></a>什么是VRRP，如何在真实世界里工作</h2><p>虚拟路由冗余协议(Virtual Router Redundancy Protocol)是一个第一条冗余协议——目的是为了提供一个网络默认网关的高可靠，或者是路由的下一跳的高可靠。那它解决了什么问题呢？在一个网络拓扑中，有两个路由提供网络连接，你可以将网络的默认路由配置为第一个路由地址，另外一个配置成第二个。</p>{% img left /images/blogs/neutron-l3-ha-router_ha-topology-before-vrrp.png 200 300 %}{% img left /images/blogs/neutron-l3-ha-switch-moves-mac.png 200 300 %}<p>这样将提供负载分担，但是如果其中一个路由失去了连接，会发生什么呢？这里一个想法就是虚拟IP地址，或者一个浮动的地址，配置为网络默认的网管地址。当发生错误时，Standby的路由并不会收到从Master节点发出的VRRP Hello信息，并且这将触发选举程序，获胜的成为Active网管，另外一个仍然作为Standby。Active路由配置虚拟IP地址(简写VIP)，内部的局域网接口，回复ARP请求时会附加虚拟的MAC地址。由于在该网络内的计算机已经拥有了ARP缓存(VIP+虚拟机MAC地址)，也就没有必要重新发送ARP请求了。依据选举机制，有效的Standby路由变为Active，并且发送一个非必要的ARP请求——来向网络中声明当前的VIP+MAC对已经属于它了。这个切换，包含了网络中将虚拟机MAC地址从旧的迁移到新的。</p><p>为了实现这一点，指向默认网关的流量会从当前的(新的)Active路由经过。注意这个解决方案中并没有实现负载分担，这种情况下，所有的流量都是从Active路由转发的。(注意：在Neutron的用户使用场景中，负载分担并没有在单独的路由级别完成，而是在节点(Node)级别，也就是要有一定数量的路由)。那么如何在路由层面实现负载分担呢？VRRP组：VRRP的投中包含虚拟机路由识别码(Virtual Router Identifier)，也就是VRID。网络中一半的主机配置为第一个VIP，另外一个使用第二个。在失败的场景下，VIP会从失败的路由转移到另外一个。</p>{% img left /images/blogs/neutron-l3-ha-router-ha-two-vrrp-groups.png 200 300 %}{% img left /images/blogs/neutron-l3-ha-router-ha-external-trap.png 200 300 %}<p>善于观察的读者已经发现了一个明显的问题——如果一个Active路由失去了与Internet的连接怎么办？那它还会作为Active路由，但是不能转发？VRRP增加了监控外部连接的能力，并且当发生失败后交出Active路由的地位。</p><p>注意：一旦地址发生改变，可能会出发两种模式：</p><ul><li>每一个路由得到一个新的IP地址，不管VRRP的状态。Master路由将VIP配置为附加的或第二地址。</li><li>仅仅VIP配置了。例如：Master路由配置了VIP，同时Slave上没有IP配置。</li></ul><h2 id="VRRP-——-一些事实"><a href="#VRRP-——-一些事实" class="headerlink" title="VRRP —— 一些事实"></a>VRRP —— 一些事实</h2><ul><li>直接在IP协议中封装</li><li>Active实例使用多播地址224.0.0.18, MAC 01-00-5E-00-00-12来给Standby路由发送hello消息</li><li>虚拟MAC地址格式：00-00-5E-00-01-{VRID}，因此只有256个不同的VRIDs(0到255)在一个广播域中</li><li>选举机制使用用户配置优先级，从1到255，越高优先级越高</li><li>优先选举策略(Preemptive elections)，和其他网络协议一样，意味着一个Standby被配置为较高优先级，或者从连接中断回复后(之前是Active实例)，它始终会恢复为Active路由</li><li>非优先选举策略(Non-preemptive elections)意外着当Active路由回复后，仍然作为Standby角色</li><li>发送Hello间隔是可以设置的(假定为每T秒)，如果Standby路由在3T秒后仍然没有收到master的hello消息，就会触发选举机制</li></ul><h2 id="回到Neutron的领地"><a href="#回到Neutron的领地" class="headerlink" title="回到Neutron的领地"></a>回到Neutron的领地</h2><p>L3 HA在每一个路由空间上启动一个keepalived实例。不同路由实例间的通讯，通过制定的HA网络，每一个tenant一个。这个网络是由一个空白的(blank) tenant下创建的，并且无法通过CLI或者GUI操作。这个HA网络也是一个Neutron tenant网络，和所有其他的一样，也是用了默认的分区技术。keepalived流量被转发到HA设备(在keepalived.conf中指定，在路由的命名空间中keepalived实例会使用)。这是路由中命名空间’ip address’的输出：</p><pre><code>[stack@vpn-6-88 ~]$ sudo ip netns exec qrouter-b30064f9-414e-4c98-ab42-646197c74020 ip address1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default    ...2794: &lt;/span&gt;&lt;span style=&quot;color:#ff0000;&quot;&gt;&lt;strong&gt;ha-45249562-ec&lt;/strong&gt;&lt;/span&gt;&lt;span style=&quot;color:#333333;&quot;&gt;: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default    link/ether 12:34:56:78:2b:5d brd ff:ff:ff:ff:ff:ff    inet 169.254.0.2/24 brd 169.254.0.255 scope global ha-54b92d86-4f      valid_lft forever preferred_lft forever    inet6 fe80::1034:56ff:fe78:2b5d/64 scope link      valid_lft forever preferred_lft forever2795: qr-dc9d93c6-e2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default    link/ether ca:fe:de:ad:be:ef brd ff:ff:ff:ff:ff:ff    inet 10.0.0.1/24 scope global qr-0d51eced-0f      valid_lft forever preferred_lft forever    inet6 fe80::c8fe:deff:fead:beef/64 scope link      valid_lft forever preferred_lft forever2796: qg-843de7e6-8f: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default    link/ether ca:fe:de:ad:be:ef brd ff:ff:ff:ff:ff:ff    inet 19.4.4.4/24 scope global qg-75688938-8d      valid_lft forever preferred_lft forever    inet6 fe80::c8fe:deff:fead:beef/64 scope link      valid_lft forever preferred_lft forever&lt;/span&gt;</code></pre><p>这是在master实例中的输出。在另外一个节点的同一个路由上，在ha, hr或者qg设备上没有IP地址。也没有Floating Ip或者是路由记录。这些是被配置在keepalived.conf中的，当keepalived检测到Master实例的失败后，这些地址(或者:VIPs)会被keepalived在适当的设备中重新配置。这是对于同一个路由的keepalived.conf的实例：</p><pre><code>vrrp_sync_group VG_1 {    group {        VR_1    }    notify_backup &quot;/path/to/notify_backup.sh&quot;    notify_master &quot;/path/to/notify_master.sh&quot;    notify_fault &quot;/path/to/notify_fault.sh&quot;}vrrp_instance VR_1 {    state BACKUP    interface ha-45249562-ec    virtual_router_id 1    priority 50    nopreempt    advert_int 2    track_interface {        ha-45249562-ec    }    virtual_ipaddress {        19.4.4.4/24 dev qg-843de7e6-8f    }    virtual_ipaddress_excluded {        10.0.0.1/24 dev qr-dc9d93c6-e2    }    virtual_routes {        0.0.0.0/0 via 19.4.4.1 dev qg-843de7e6-8f    }}</code></pre><p>这些notify脚本是干虾米用的呢？这些脚本是被keepalived执行，转换为Master，备份或者失败。这些Master脚本内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/usr/bin/env bash</span></span><br><span class="line">neutron-ns-metadata-proxy --pid_file=/tmp/tmpp_6Lcx/tmpllLzNs/external/pids/b30064f9-414e-4c98-ab42-646197c74020/pid --metadata_proxy_socket=/tmp/tmpp_6Lcx/tmpllLzNs/metadata_proxy --router_id=b30064f9-414e-4c98-ab42-646197c74020 --state_path=/opt/openstack/neutron --metadata_port=9697 --debug --verbose</span><br><span class="line"><span class="built_in">echo</span> -n master &gt; /tmp/tmpp_6Lcx/tmpllLzNs/ha_confs/b30064f9-414e-4c98-ab42-646197c74020/state</span><br></pre></td></tr></table></figure><p>这个Master脚本简单的打开了metadata代理，将状态写入文件，状态文件之后会被L3 Agent读取。备份和出错脚本杀死代理服务并且将状态写入刚才的提到的状态文件。这就意味着metadata服务职能在master路由节点存在。</p><h2 id="我们是不是忘了Metadata-Agent呢？"><a href="#我们是不是忘了Metadata-Agent呢？" class="headerlink" title="我们是不是忘了Metadata Agent呢？"></a>我们是不是忘了Metadata Agent呢？</h2><p>在每个网络节点启动metadata agent就好啦。</p><h2 id="未来的工作和局限性"><a href="#未来的工作和局限性" class="headerlink" title="未来的工作和局限性"></a>未来的工作和局限性</h2><ul><li>TCP连接跟踪——在当前的实现中，TCP连接的session在失败恢复后中断。一种解决方案是使用conntrackd复制HA路由间的session状态，这样当故障恢复后，TCP的session会恢复到失败前的状态。</li><li>Master节点在哪？当前，还没有办法让管理员知道哪个网络节点是HA路由的Master实例。计划由Agent提供这些信息，并可以通过API进行查询。</li><li>Agent大逃亡——理想状态下，将一个节点变为维护模式后，应该触发所有HA路由所在节点回收他们的Master状态，加速恢复速度。</li><li>通知L2pop VIP的改变——考虑在一个tenant网络中路由IP/MAC，只有Master配置真正的有IP地址，但是同一个Neutron Port和同样的MAC会在相关的网络出现。这可能对配置了L2pop驱动产生不利，因为它只希望在一个网络中MAC地址只存在一处。解决的计划是一旦检测到VRRP状态改变，从Agent发送一个RPC消息，这样当路由变为Master，控制节点被通知到了，这样就能改变L2pop的状态了。</li><li>内置的防火墙、VPN和负载均衡即服务。在DVR和L3 HA与这些服务整合时还有问题，可能会在kilo中解决。</li><li>每一个Tenant一个HA网络。这就意味着每个Tenant只能有255个HA路由，因为每个路由需要一个VRID，根据VRRP协议每个广播域只允许255个不同的VRID值。</li></ul><h2 id="使用和配置"><a href="#使用和配置" class="headerlink" title="使用和配置"></a>使用和配置</h2><p>neutron.conf</p><pre><code>l3_ha = Truemax_l3_agents_per_router = 2min_l3_agents_per_router = 2</code></pre><ul><li>l3_ha 表示所有的路由默认使用HA模式(与之前不同)。默认是关闭的。</li><li>你可以根据网络节点的数量设置最大最小值。如果你部署了4个网络节点，但是设置最大值为2，只有两个L3 Agent会被用于HA路由(一个是Master，一个是Slave)。</li><li>min被用来稳健性(sanity)的检查：如果你有两个网络节点，其中一个坏掉了，任何新建的路由在这段时间都会失败，因为你至少需要min个L3 Agent启动来建立HA路由。</li></ul><p>l3_ha控制默认的开关，当管理员(仅管理员)通过CLI方式可以覆盖这个选项，为每个路由创建单独的配置：</p><pre><code>neutron router-create --ha=&lt;True | False&gt; router1</code></pre><h2 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h2><ul><li><a href="https://blueprints.launchpad.net/neutron/+spec/l3-high-availability" target="_blank" rel="noopener">Blueprint</a></li><li><a href="http://specs.openstack.org/openstack/neutron-specs/specs/juno/l3-high-availability.html" target="_blank" rel="noopener">Spec</a></li><li><a href="https://docs.google.com/document/d/1P2OnlKAGMeSZTbGENNAKOse6B2TRXJ8keUMVvtUCUSM/" target="_blank" rel="noopener">How to test</a></li><li><a href="https://review.openstack.org/#/q/topic:bp/l3-high-availability,n,z" target="_blank" rel="noopener">Code</a></li><li><a href="https://wiki.openstack.org/wiki/Neutron/L3_High_Availability_VRRP" target="_blank" rel="noopener">Dedicated wiki page</a></li><li><a href="https://wiki.openstack.org/wiki/Meetings/Neutron-L3-Subteam#Blueprint:_l3-high-availability_.28safchain.2C_amuller.29" target="_blank" rel="noopener">Section in Neutron L3 sub team wiki (Including overview of patch dependencies and future work)</a></li><li><a href="http://assafmuller.com/2014/08/16/layer-3-high-availability/" target="_blank" rel="noopener">http://assafmuller.com/2014/08/16/layer-3-high-availability/</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h2 id=&quot;L3层Agent的低可靠解决方案&quot;&gt;&lt;a href=&quot;#L3层Agent的低可靠解决方案&quot; class=&quot;headerlink&quot; title=&quot;L3层Agent的低可靠解决方案&quot;&gt;&lt;/a&gt;L3层Agent的低可靠解决方案&lt;/h2&gt;&lt;p&gt;当前，你可以通过多网络节点的方式解决负载分担，但是这并非高可靠和冗余的解决方案。假设你有三个网络节点，当你创建新的路由时，会自动的将新路由规划和分布在这三个网络节点中的一个上。但是，如果一个节点坏掉，上面运行的所有路由将无法提供服务，路由转发也无法正常进行。在Neutron的IceHouse版本中，没有提供任何内置的解决方案。&lt;/p&gt;
    
    </summary>
    
    
      <category term="openstack" scheme="http://sunqi.me/categories/openstack/"/>
    
      <category term="neutron" scheme="http://sunqi.me/categories/openstack/neutron/"/>
    
      <category term="network" scheme="http://sunqi.me/categories/openstack/neutron/network/"/>
    
      <category term="sdn" scheme="http://sunqi.me/categories/openstack/neutron/network/sdn/"/>
    
    
  </entry>
  
</feed>
