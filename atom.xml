<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>孙琦的技术人生</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://sunqi.site/"/>
  <updated>2020-11-01T07:20:36.404Z</updated>
  <id>http://sunqi.site/</id>
  
  <author>
    <name>孙琦(Ray)</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>利用钉钉通讯录同步构建本地LDAP服务</title>
    <link href="http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/"/>
    <id>http://sunqi.site/2020/10/31/%E5%88%A9%E7%94%A8%E9%92%89%E9%92%89%E9%80%9A%E8%AE%AF%E5%BD%95%E5%90%8C%E6%AD%A5%E6%9E%84%E5%BB%BA%E6%9C%AC%E5%9C%B0LDAP%E6%9C%8D%E5%8A%A1/</id>
    <published>2020-10-31T10:24:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？</p><a id="more"></a><p>目前，在我们公司使用OpenLDAP服务作为各个服务统一鉴权的入口，使用的应用系统包括：Gerrit/Jenkins/Yapi/Wiki/进度跟踪等，目前所有的系统都支持LDAP鉴权，所以如果能将钉钉的通讯录定期同步至LDAP中就可以实现统一鉴权的需求。但是由于钉钉的密码无法同步回本地，所以密码层面仍然是独立的。</p><p>本文章的实现思路参考了<a href="https://xujiwei.com/blog/2020/02/internal-authorize-based-on-dingtalk-virtual-ldap-keyclaok/" target="_blank" rel="noopener">《基于钉钉 + Virtual-LDAP + KeyCloak 的内网统一认证系统<br>》</a>，感谢原作者的思路及贡献的virtual-ldap模块，本文所有的优化都是基于此文章基础上进行的优化。</p><h1 id="实现思路"><a href="#实现思路" class="headerlink" title="实现思路"></a>实现思路</h1><p>简单来说，我们希望能通过钉钉提供的LDAP作为统一鉴权方式，但是由于钉钉没有开放这个能力，那么我们需要将钉钉模拟一个LDAP服务。模拟出的LDAP环境，在内网环境中，我们对于LDAP信息的使用基本上围绕着用户名和密码，其他的信息以钉钉为准。所以，除了开放LDAP接口外，我们还需要提供用户界面，允许用户在内网修改密码。</p><p>整体的实现思路如下：</p><ul><li>钉钉开发者平台：需要在钉钉开发者平台新建一个程序，获取鉴权信息后，赋予通讯录同步权限，提供给VirtualLDAP进行数据同步</li><li>VirtualLDAP：该组件是上面提到的作者开发的虚拟LDAP组件，主要功能为同步钉钉通讯录，并以LDAP协议对外提供服务</li><li>KeyCloak：对于这个场景过重，但是暂时没有发现更好的方案，可以触发自动同步并且可以让内网用户进行密码修改</li><li>本地的全部系统按照LDAP配置方式即可实现鉴权</li></ul><p><img src="/images/pasted-90.png" alt="upload successful"></p><h2 id="钉钉开发者平台"><a href="#钉钉开发者平台" class="headerlink" title="钉钉开发者平台"></a>钉钉开发者平台</h2><p>这里我创建的是H5微应用，配置时有几点需要注意：</p><ul><li>IP地址白名单：需要为你未来运行VirtualLDAP配置访问IP白名单，目前钉钉开发者平台对于同一个IP只能给一个应用使用，但是可以通过通配符进行配置，例如：192.168.10.*的方式，那么192.168.10网段所有地址均可以访问</li><li>权限：需要为该应用开放所有通讯录只读权限即可</li></ul><p><img src="/images/pasted-92.png" alt="upload successful"></p><h2 id="VirtualLDAP"><a href="#VirtualLDAP" class="headerlink" title="VirtualLDAP"></a>VirtualLDAP</h2><p>这是基于Node.js开发的一款组件，主要用于同步钉钉通讯录和模拟LDAP协议。基于原作者版本，为了满足自身应用场景，进行了如下修改：</p><ul><li>由于作者没有提供Docker运行方式，所以在github的pull request中有人进行了改造</li><li>仍然是在同一个pull request中，增加了pinyin组件，在LDAP中增加了一个pinyin属性，方便业务系统使用</li><li>登录名和密码：为了防止公司人员重复，所以特别采用了全拼名称+电话号码后四位方式，作为唯一的用户名，而初始密码为全名名称+电话号码前四位，例如：张三的电话号码为13812345678，则登录名称为zhangsan5678，密码为zhangsan1381,</li><li>在使用VirtualLDAP时，需要使用MySQL存储持久化数据，例如用户修改后的密码，所以对鉴权规则进行了修改，先检查数据库密码是否匹配，再检查LDAP</li><li>实现了整体组件的编排，增加了docker-compose.yaml，方便用户使用，该编排文件中包含了KeyCloak、VirtualLDAP和MySQL</li></ul><h2 id="KeyCloak"><a href="#KeyCloak" class="headerlink" title="KeyCloak"></a>KeyCloak</h2><p>KeyCloak两部分需要进行配置：</p><ul><li>管理员在第一次使用时配置VirtualLDAP的信息，用于用户同步，方便新用户修改密码</li><li>新用户自行修改密码</li></ul><p>正如之前所说，KeyCloak功能过于强大，这里用到的功能非常有限，如果有新的应用场景，欢迎留言。</p><p><img src="/images/pasted-93.png" alt="upload successful"></p><h1 id="搭建方式"><a href="#搭建方式" class="headerlink" title="搭建方式"></a>搭建方式</h1><p>这里提供了完整的编排文件，直接使用即可完成整套环境的快速建立。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;virtual-ldap</span><br><span class="line">cd virtual-ldap&#x2F;docker-compose</span><br><span class="line">docker-compose up -d0</span><br></pre></td></tr></table></figure><h2 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h2><ul><li>VirtualLDAP配置文件修改。所有配置在virtual-ldap/docker-compose/config.js中进行修改，需要修改钉钉的appKey和appSecret，以及root DN的信息，配置文件有比较详细的介绍，所以这里不再赘述。</li><li>KeyCloak的默认密码修改在docker-compose.yaml中</li></ul><h2 id="登录相关信息"><a href="#登录相关信息" class="headerlink" title="登录相关信息"></a>登录相关信息</h2><ul><li>URL: ldap://ip:1389</li><li>ManageDN: cn=admin,dc=oneprocloud,dc=com</li><li>ManagePassword: password</li><li>User Search Base: ou=People,o=department,dc=oneprocloud,dc=com</li><li>User Search Filter: uid={0}</li><li>Display Name LDAP attribute: cn</li><li>Email Address LDAP attribute: mail</li></ul><h1 id="待优化"><a href="#待优化" class="headerlink" title="待优化"></a>待优化</h1><ul><li>目前对于LDAP的组没有充分利用，配置文件中允许创建特定组，并且通过用户email进行匹配，如果需要可以进行配置</li><li>如果有外部用户，暂时无方法进行创建，例如：如果需要在LDAP中增加一个非钉钉用户暂时无法实现，需要进行开发实现</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;目前钉钉已经成为很多企业日常处理流程的必备工具，但是由于钉钉并没有开放鉴权接口，无法让钉钉作为本地系统的统一鉴权系统使用，每次有同事加入或者离开时，都需要人为的对本地系统进行维护，非常繁琐。那么有没有一种方法可以让钉钉作为本地的统一鉴权系统使用呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="Node.js" scheme="http://sunqi.site/tags/Node-js/"/>
    
  </entry>
  
  <entry>
    <title>使用Kolla部署OpenStack Stein版本</title>
    <link href="http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/10/30/%E4%BD%BF%E7%94%A8Kolla%E9%83%A8%E7%BD%B2OpenStack-Stein%E7%89%88%E6%9C%AC/</id>
    <published>2020-10-30T14:15:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。</p><a id="more"></a><h1 id="1、云平台规划"><a href="#1、云平台规划" class="headerlink" title="1、云平台规划"></a>1、云平台规划</h1><p>在实际环境中，我们在一台2U的超微四子星服务器上进行了部署。由于是内部使用的研发环境，为了节约成本，我们并没有部署高可靠方案，而是采用了一台作为控制节点+计算节点+存储节点，另外三台作为计算节点+存储节点的方式进行部署。</p><p>由于OpenStack最新的Ussari在使用Kolla部署时，不再支持CentOS 7版本，所以这里我们选定了上一个稳定版本Stein版本进行部署。</p><h2 id="硬件配置"><a href="#硬件配置" class="headerlink" title="硬件配置"></a>硬件配置</h2><table><thead><tr><th>硬件名称</th><th>配置规格</th><th>备注</th></tr></thead><tbody><tr><td>CPU</td><td>Intel(R) Xeon(R) CPU E5-2630 v4 @ 2.20GHz    x 2</td><td>共40线程</td></tr><tr><td>内存</td><td>DDR4 2400 MHz 64GB</td><td></td></tr><tr><td>硬盘</td><td>板载64 GB x 1 <br/> 240 GB Intel SSD x 1 <br/> 1.2 TB SAS x 5</td><td>经过测试，由于板载64GB空间过小，在控制节点需要损失一块SAS盘空间用于root分区挂载</td></tr><tr><td>网卡</td><td>千兆 x 4 <br/> 万兆 x 4 <br/> IPMI x 1</td><td></td></tr></tbody></table><h3 id="分区规划"><a href="#分区规划" class="headerlink" title="分区规划"></a>分区规划</h3><table><thead><tr><th>磁盘</th><th>规划</th><th>备注</th></tr></thead><tbody><tr><td>64G</td><td>系统盘</td><td>不要使用LVM分区</td></tr><tr><td>SSD 240G</td><td>Ceph Journal<br></td><td>1块盘</td></tr><tr><td>SAS 1.2 T</td><td>Ceph OSD</td><td>5块盘</td></tr></tbody></table><h2 id="网络规划"><a href="#网络规划" class="headerlink" title="网络规划"></a>网络规划</h2><h3 id="交换机配置"><a href="#交换机配置" class="headerlink" title="交换机配置"></a>交换机配置</h3><ul><li>我们默认采用了VLAN模式，所以无须在交换机上进行Trunk配置</li></ul><h3 id="网络规划-1"><a href="#网络规划-1" class="headerlink" title="网络规划"></a>网络规划</h3><table><thead><tr><th>网卡</th><th>网络类型</th><th>VLAN ID</th><th>网段</th><th>说明</th><th>网关</th><th>备注</th></tr></thead><tbody><tr><td></td><td>管理网络</td><td>3</td><td>192.168.10.0/24</td><td>OpenStack管理</td><td>192.168.10.1</td><td>192.168.10.201 - 204</td></tr><tr><td></td><td>存储网络</td><td></td><td>10.0.100.0/24</td><td>Ceph网络</td><td>无需网关</td><td>10.0.100.201 -&nbsp;204</td></tr><tr><td></td><td>External网络</td><td>3</td><td>192.168.10.0/24</td><td>External网络</td><td>192.168.10.1</td><td>可分配地址192.168.10.100 - 192.168.10.200</td></tr><tr><td></td><td>Tunnel网络</td><td></td><td>172.16.100.0/24</td><td>VxLAN通讯网络</td><td><br data-mce-bogus="1"></td><td>172.16.100.201 - 204</td></tr><tr><td>console</td><td>IPMI</td><td>4</td><td>192.168.10.0/24</td><td></td><td></td><td>与管理网地址一一对应, 192.168.10.201</td></tr></tbody></table><h3 id="网卡配置"><a href="#网卡配置" class="headerlink" title="网卡配置"></a>网卡配置</h3><table><thead><tr><th>主机名</th><th>em1(管理网地址)</th><th>em2(存储网)</th><th>em3(External网络)</th><th>em4(Tunnel网络)</th><th>备注</th></tr></thead><tbody><tr><td>control201</td><td>192.168.10.201</td><td>10.0.100.201</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.201</td><td></td></tr><tr><td>compute202</td><td>192.168.10.202</td><td>10.0.100.202</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.202</td><td></td></tr><tr><td>compute203</td><td>192.168.10.203</td><td>10.0.100.203</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr><tr><td>compute204</td><td>192.168.10.204</td><td>10.10.20.204</td><td><pre style="line-height: 1.42857;"><span class="na">DEVICE</span><span class="o">=</span><span class="s">INTERFACE_NAME</span><br><span class="na">TYPE</span><span class="o">=</span><span class="s">Ethernet</span><br><span class="na">ONBOOT</span><span class="o">=</span><span class="s">"yes"</span><br><span class="na">BOOTPROTO</span><span class="o">=</span><span class="s">"none"</span></pre></td><td>172.16.100.203</td><td></td></tr></tbody></table><h2 id="OpenStack规划"><a href="#OpenStack规划" class="headerlink" title="OpenStack规划"></a>OpenStack规划</h2><h3 id="安装组件"><a href="#安装组件" class="headerlink" title="安装组件"></a>安装组件</h3><p>Ceph采用单独安装方式，这目前也是Kolla项目主推的方式，在U版本中已经彻底不支持通过Kolla安装Ceph了。我们主要安装OpenStack核心模块，另外安装的是日志收集ELK的相关模块，便于运维。</p><ul><li>Horizon</li><li>Nova</li><li>Keystone</li><li>Cinder</li><li>Glance</li><li>Neutron</li><li>Heat</li></ul><h1 id="2、部署准备"><a href="#2、部署准备" class="headerlink" title="2、部署准备"></a>2、部署准备</h1><h2 id="部署架构图"><a href="#部署架构图" class="headerlink" title="部署架构图"></a>部署架构图</h2><p><img src="/images/pasted-60.png" alt="upload successful"></p><h2 id="服务器前期准备"><a href="#服务器前期准备" class="headerlink" title="服务器前期准备"></a>服务器前期准备</h2><ul><li>BIOS配置：在BIOS中打开VT，并且正确配置IPMI地址，方便远程管理</li><li>RAID配置：所有磁盘需要配置成NON-RAID模式</li><li>操作系统安装：<ul><li>使用CentOS 7光盘进行最小化安装</li><li>不要使用LVM分区</li><li>配置主机名</li><li>配置第一块网卡，并配置自动启动</li></ul></li></ul><h2 id="网卡配置-1"><a href="#网卡配置-1" class="headerlink" title="网卡配置"></a>网卡配置</h2><h3 id="em1"><a href="#em1" class="headerlink" title="em1"></a>em1</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em1</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em1</span><br><span class="line">DEVICE&#x3D;em1</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;192.168.10.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">GATEWAY&#x3D;192.168.10.1</span><br><span class="line">DNS1&#x3D;114.114.114.114</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em2"><a href="#em2" class="headerlink" title="em2"></a>em2</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em2</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">DEFROUTE&#x3D;yes</span><br><span class="line">NAME&#x3D;em2</span><br><span class="line">DEVICE&#x3D;em2</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;10.0.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em3"><a href="#em3" class="headerlink" title="em3"></a>em3</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em3</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;static</span><br><span class="line">NAME&#x3D;em3</span><br><span class="line">DEVICE&#x3D;em3</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">IPADDR&#x3D;172.16.100.201</span><br><span class="line">NETMASK&#x3D;255.255.255.0</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h3 id="em4"><a href="#em4" class="headerlink" title="em4"></a>em4</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-em4</span><br><span class="line">TYPE&#x3D;Ethernet</span><br><span class="line">BOOTPROTO&#x3D;none</span><br><span class="line">NAME&#x3D;em4</span><br><span class="line">DEVICE&#x3D;em4</span><br><span class="line">ONBOOT&#x3D;yes</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><h1 id="3、安装步骤"><a href="#3、安装步骤" class="headerlink" title="3、安装步骤"></a>3、安装步骤</h1><h2 id="3-1-准备部署节点"><a href="#3-1-准备部署节点" class="headerlink" title="3.1 准备部署节点"></a>3.1 准备部署节点</h2><p>该节点承担了后续所有的部署流程，该节点可以作为OpenStack控制节点复用，包括运行OpenStack Kolla和Ceph Deploy。</p><p>注意：节点之间可以通过密码或者密钥方式进行访问，附录中提供了自动上传密钥的方式，建议在正式安装前配置完成，这里不提供自动化配置方法。</p><h3 id="下载初始化脚本"><a href="#下载初始化脚本" class="headerlink" title="下载初始化脚本"></a>下载初始化脚本</h3><p>目前已经将常用的操作写成了Ansible脚本。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install -y git</span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;my_ansible_playbooks</span><br><span class="line"></span><br><span class="line">cd my_ansible_playbooks</span><br><span class="line">prepare_on_centos7.sh</span><br></pre></td></tr></table></figure><h3 id="修改hosts-ini文件"><a href="#修改hosts-ini文件" class="headerlink" title="修改hosts.ini文件"></a>修改hosts.ini文件</h3><p>修改hosts.ini文件来初始化所有节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># my_ansible_playbooks&#x2F;hosts.ini</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.201 ip&#x3D;192.168.10.201 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.202 ansible_user&#x3D;root</span><br><span class="line">compute203 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.203 ansible_user&#x3D;root</span><br><span class="line">compute204 ansible_host&#x3D;192.168.10.202 ip&#x3D;192.168.10.204 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="初始化节点"><a href="#初始化节点" class="headerlink" title="初始化节点"></a>初始化节点</h3><p>该步骤主要包含了，更新软件，修改主机名，增加/etc/hosts等操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;bootstrap_centos7.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;change_hostname.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_etc_hosts.yml</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_docker.yml</span><br><span class="line"></span><br><span class="line"># 安装pip和系统环境下的python docker模块，否则在precheck的时候会发现没有安装docker模块</span><br><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;install_pip2_package.yml</span><br></pre></td></tr></table></figure><h3 id="安装Ceph-Deploy"><a href="#安装Ceph-Deploy" class="headerlink" title="安装Ceph Deploy"></a>安装Ceph Deploy</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h3 id="安装Kolla"><a href="#安装Kolla" class="headerlink" title="安装Kolla"></a>安装Kolla</h3><p>由于Python Warning的提示信息导致在安装时出现如下错误，需要增加忽略Python Warning的环境变量，具体修复信息如下：<a href="https://bugs.launchpad.net/kolla-ansible/+bug/1888657" target="_blank" rel="noopener">https://bugs.launchpad.net/kolla-ansible/+bug/1888657</a></p><p>目前通过pip方式还没有8.2.1这个release，所以kolla的安装从源代码中进行。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Ansible 2.2.0.0 used in Stein kolla-toolbox requires paramiko (no version</span><br><span class="line">constraints), which installs latest cryptography package. It results in</span><br><span class="line">Python deprecation warning about Python 2:</span><br><span class="line"></span><br><span class="line">&#x2F;usr&#x2F;lib64&#x2F;python2.7&#x2F;site-packages&#x2F;cryptography&#x2F;__init__.py:39: CryptographyDeprecationWarning: Python 2 is no longer supported by the Python core team. Support for it is now deprecated in cryptography, and will be removed in a future release.</span><br><span class="line"></span><br><span class="line">This warning breaks kolla_toolbox module.</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sudo yum -y install python-devel libffi-devel gcc openssl-devel libselinux-python</span><br><span class="line"></span><br><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;openstack&#x2F;kolla-ansible --branch stable&#x2F;stein</span><br><span class="line">cd kolla-ansible</span><br><span class="line">pip install . --ignore-installed PyYAML</span><br><span class="line"></span><br><span class="line"># 虚拟环境还需要再安装一次ansible，否则kolla-ansible会提示</span><br><span class="line"># ERROR: kolla_ansible has to be available in the Ansible PYTHONPATH.</span><br><span class="line"># Please install both in the same (virtual) environment.</span><br><span class="line">pip install &#39;ansible&lt;2.10&#39;</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla</span><br><span class="line">cp -r $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;etc_examples&#x2F;kolla&#x2F;* &#x2F;etc&#x2F;kolla</span><br><span class="line"></span><br><span class="line">mkdir -p &#x2F;root&#x2F;kolla</span><br><span class="line">cp $VENV_HOME&#x2F;share&#x2F;kolla-ansible&#x2F;ansible&#x2F;inventory&#x2F;* &#x2F;root&#x2F;kolla</span><br></pre></td></tr></table></figure><p>生成密码，如果需要指定密码，可以到/etc/kolla/password.yml中修改。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-genpwd</span><br></pre></td></tr></table></figure><h2 id="3-2-部署Ceph"><a href="#3-2-部署Ceph" class="headerlink" title="3.2 部署Ceph"></a>3.2 部署Ceph</h2><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><p>之前有一篇软文详细介绍了使用Ceph Deploy部署Ceph的方法，这里不再赘述，下面直接给出部署命令，这里我们只部署块服务。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;root&#x2F;ceph</span><br><span class="line">cd &#x2F;root&#x2F;ceph</span><br><span class="line"></span><br><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">#</span><br><span class="line"># 如果cluster-network和public-network需要分开，可以这样定义：</span><br><span class="line"># ceph-deploy new --cluster-network 172.31.6.0&#x2F;24 --public-network 192.168.4.0&#x2F;24 node1 node2 node3</span><br><span class="line"></span><br><span class="line">ceph-deploy new --public-network 10.0.100.0&#x2F;24 compute201</span><br><span class="line">ceph-deploy install compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin compute201 compute202 compute203 compute204</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create compute201</span><br><span class="line"></span><br><span class="line"># 需要根据实际情况修改，这里模拟的是将RocksDB存放至单独的SSD磁盘，相当于之前的Journal，所以没必要分别指定block-db和block-wal，默认在一起即可，一起指定报错</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdc --block-db &#x2F;dev&#x2F;vdb compute201</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdc --block-db &#x2F;dev&#x2F;vdb compute202</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdc --block-db &#x2F;dev&#x2F;vdb compute203</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdc --block-db &#x2F;dev&#x2F;vdb compute204</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><h3 id="生成配置文件"><a href="#生成配置文件" class="headerlink" title="生成配置文件"></a>生成配置文件</h3><p>为Glance/Nova/Cinder创建资源池并生成鉴权文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create images 128</span><br><span class="line">ceph auth get-or-create client.glance mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rdb_children, allow rwx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create volumes 128</span><br><span class="line">ceph auth get-or-create client.cinder mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;volumes, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create backups 128</span><br><span class="line">ceph auth get-or-create client.cinder-backup mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;backups&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring</span><br><span class="line"></span><br><span class="line">ceph osd pool create vms 128</span><br><span class="line">ceph auth get-or-create client.nova mon &#39;allow r&#39; osd &#39;allow class-read object_prefix rbd_children, allow rwx pool&#x3D;vms, allow rx pool&#x3D;images&#39; -o &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring</span><br></pre></td></tr></table></figure><h2 id="3-3-OpenStack部署"><a href="#3-3-OpenStack部署" class="headerlink" title="3.3 OpenStack部署"></a>3.3 OpenStack部署</h2><h3 id="kolla配置文件"><a href="#kolla配置文件" class="headerlink" title="kolla配置文件"></a>kolla配置文件</h3><h4 id="etc-kolla-globals-yml"><a href="#etc-kolla-globals-yml" class="headerlink" title="/etc/kolla/globals.yml"></a>/etc/kolla/globals.yml</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">kolla_base_distro: &quot;centos&quot;</span><br><span class="line">kolla_install_type: &quot;source&quot;</span><br><span class="line">openstack_release: &quot;stein&quot;</span><br><span class="line">kolla_internal_vip_address: &quot;192.168.10.123&quot;</span><br><span class="line"></span><br><span class="line">docker_registry: registry.cn-beijing.aliyuncs.com</span><br><span class="line">docker_namespace: &quot;openstack-dockers&quot;</span><br><span class="line"></span><br><span class="line">network_interface: &quot;eth0&quot;</span><br><span class="line">storage_interface: &quot;eth1&quot;</span><br><span class="line">tunnel_interface: &quot;eth3&quot;</span><br><span class="line">neutron_external_interface: &quot;eth2&quot;</span><br><span class="line"></span><br><span class="line">openstack_logging_debug: &quot;True&quot;</span><br><span class="line">enable_haproxy: &quot;no&quot;</span><br><span class="line">enable_ceph: &quot;no&quot;</span><br><span class="line">enable_cinder: &quot;yes&quot;</span><br><span class="line">enable_cinder_backup: &quot;yes&quot;</span><br><span class="line">enable_fluentd: &quot;no&quot;</span><br><span class="line">enable_openstack_core: &quot;yes&quot;</span><br><span class="line">glance_backend_ceph: &quot;yes&quot;</span><br><span class="line">glance_backend_file: &quot;no&quot;</span><br><span class="line">glance_enable_rolling_upgrade: &quot;no&quot;</span><br><span class="line">cinder_backend_ceph: &quot;yes&quot;</span><br><span class="line">nova_backend_ceph: &quot;yes&quot;</span><br></pre></td></tr></table></figure><h4 id="multinode"><a href="#multinode" class="headerlink" title="multinode"></a>multinode</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[control]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[network]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[compute]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[monitoring]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line"></span><br><span class="line">[storage]</span><br><span class="line">compute201 ansible_host&#x3D;192.168.10.123 ip&#x3D;192.168.10.123 ansible_user&#x3D;root</span><br><span class="line">compute202 ansible_host&#x3D;192.168.10.160 ip&#x3D;192.168.10.160 ansible_user&#x3D;root</span><br></pre></td></tr></table></figure><h3 id="定制服务配置文件"><a href="#定制服务配置文件" class="headerlink" title="定制服务配置文件"></a>定制服务配置文件</h3><h4 id="Ceph-Glance"><a href="#Ceph-Glance" class="headerlink" title="Ceph Glance"></a>Ceph Glance</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance</span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;glance-api.conf &lt;&lt; EOF</span><br><span class="line">[glance_store]</span><br><span class="line">stores &#x3D; rbd</span><br><span class="line">default_store &#x3D; rbd</span><br><span class="line">rbd_store_pool &#x3D; images</span><br><span class="line">rbd_store_user &#x3D; glance</span><br><span class="line">rbd_store_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.glance.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;glance&#x2F;ceph.client.glance.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Cinder"><a href="#Ceph-Cinder" class="headerlink" title="Ceph Cinder"></a>Ceph Cinder</h4><p>cinder_rbd_secret_uuid是在passwords.yml中生成的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder-volume</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder-backup</span><br><span class="line"></span><br><span class="line">export cinder_rbd_secret_uuid&#x3D;$(grep cinder_rbd_secret_uuid &#x2F;etc&#x2F;kolla&#x2F;passwords.yml | awk &#39;&#123;print $2&#125;&#39;)</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">enabled_backends&#x3D;rbd-1</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;cinder</span><br><span class="line">backend_host&#x3D;rbd:volumes</span><br><span class="line">rbd_pool&#x3D;volumes</span><br><span class="line">volume_backend_name&#x3D;rbd-1</span><br><span class="line">volume_driver&#x3D;cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">rbd_secret_uuid &#x3D; $cinder_rbd_secret_uuid</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup.conf &lt;&lt; EOF</span><br><span class="line">[DEFAULT]</span><br><span class="line">backup_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">backup_ceph_user&#x3D;cinder-backup</span><br><span class="line">backup_ceph_chunk_size &#x3D; 134217728</span><br><span class="line">backup_ceph_pool&#x3D;backups</span><br><span class="line">backup_driver &#x3D; cinder.backup.drivers.ceph.CephBackupDriver</span><br><span class="line">backup_ceph_stripe_unit &#x3D; 0</span><br><span class="line">backup_ceph_stripe_count &#x3D; 0</span><br><span class="line">restore_discard_excess_bytes &#x3D; true</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><p>所有文件必须命名为ceph.client*</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-volume&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder-backup.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;cinder&#x2F;cinder-backup&#x2F;ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure><h4 id="Ceph-Nova"><a href="#Ceph-Nova" class="headerlink" title="Ceph Nova"></a>Ceph Nova</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova</span><br><span class="line"></span><br><span class="line">tee &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;nova-compute.conf &lt;&lt; EOF</span><br><span class="line">[libvirt]</span><br><span class="line">images_rbd_pool&#x3D;vms</span><br><span class="line">images_type&#x3D;rbd</span><br><span class="line">images_rbd_ceph_conf&#x3D;&#x2F;etc&#x2F;ceph&#x2F;ceph.conf</span><br><span class="line">rbd_user&#x3D;nova</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.conf &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.conf</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.nova.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.nova.keyring</span><br><span class="line">cp &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder.keyring &#x2F;etc&#x2F;kolla&#x2F;config&#x2F;nova&#x2F;ceph.client.cinder.keyring</span><br></pre></td></tr></table></figure><h3 id="检查"><a href="#检查" class="headerlink" title="检查"></a>检查</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 初始化节点，与上述我们自己的初始化有重复之处</span><br><span class="line">kolla-ansible -i multinode bootstrap-servers</span><br><span class="line"></span><br><span class="line">kolla-ansible -i multinode prechecks</span><br><span class="line"></span><br><span class="line"># 拉取所有镜像</span><br><span class="line">kolla-ansible -i multinode pull</span><br></pre></td></tr></table></figure><h3 id="部署-1"><a href="#部署-1" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -i multinode deploy</span><br><span class="line">kolla-ansible -i multinode post-deploy</span><br></pre></td></tr></table></figure><h1 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h1><h2 id="节点互信"><a href="#节点互信" class="headerlink" title="节点互信"></a>节点互信</h2><p>节点之间互信建议采用key方式，这里并没有实现完全自动化手段，需要首先在控制节点上生成公钥和私钥。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><p>然后将~/.ssh/id_rsa.pub文件拷贝至可以正常访问两台节点的环境中的playbooks/keys目录下，再更新所有节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.&#x2F;run_ansible.sh playbooks&#x2F;update_authorized_keys.yml</span><br></pre></td></tr></table></figure><h2 id="部署出错如何调试"><a href="#部署出错如何调试" class="headerlink" title="部署出错如何调试"></a>部署出错如何调试</h2><p>如果在部署中出现任何错误，可以添加更多的Verbose来判断具体问题，有可能是kolla自身bug，也有可能是配置的问题，具体可以根据详细输出进行判断。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kolla-ansible -vvv -i multinode deploy</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;开源版本的OpenStack+Ceph的组合已经日趋稳定，所以搭建一朵私有云环境的难度在逐步降低。当然OpenStack安装问题其实一直没有得到有效的解决，学习曲线非常陡峭。本文主要介绍基于Kolla项目使用容器化快速部署OpenStack方法，该部署方法已经在内部环境得到了多次验证，安装简便容易维护。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>云原生趋势下的迁移与容灾思考</title>
    <link href="http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/"/>
    <id>http://sunqi.site/2020/10/18/%E4%BA%91%E5%8E%9F%E7%94%9F%E8%B6%8B%E5%8A%BF%E4%B8%8B%E7%9A%84%E4%BA%91%E5%AE%B9%E7%81%BE%E6%80%9D%E8%80%83/</id>
    <published>2020-10-18T12:05:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="趋势"><a href="#趋势" class="headerlink" title="趋势"></a>趋势</h1><h2 id="云原生发展趋势"><a href="#云原生发展趋势" class="headerlink" title="云原生发展趋势"></a>云原生发展趋势</h2><p>云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。</p><p>2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？</p><a id="more"></a><h2 id="为什么云上需要全新的迁移和容灾"><a href="#为什么云上需要全新的迁移和容灾" class="headerlink" title="为什么云上需要全新的迁移和容灾"></a>为什么云上需要全新的迁移和容灾</h2><h3 id="1、传统方案的局限性"><a href="#1、传统方案的局限性" class="headerlink" title="1、传统方案的局限性"></a>1、传统方案的局限性</h3><p>在这种大的趋势下，传统的迁移和容灾仍然停留在数据搬运的层次上，而忽略了面向云的特性和用户业务重新思考和构建。云计算的愿景是让云资源像水、电一样按需使用，所以基于云上的迁移和容灾也理应顺应这样的历史潮流。Snowflake也是通过这种商业模式的创新，成功打破旧的竞争格局。</p><p>为什么传统容灾的手段无法满足云原生需求呢？简单来说，二者关注的核心不同。传统的容灾往往以存储为核心，拥有对存储的至高无上的控制权。并且在物理时代，对于计算、存储和网络等基础架构层也没有有效的调度方法，无法实现高度自动化的编排。而基于云原生构建的应用，核心变成了云原生服务本身。当用户业务系统全面上云后，用户不再享有对底层存储的绝对控制权，所以传统的容灾手段，就风光不在了。</p><p><img src="/images/pasted-88.png" alt="upload successful"></p><p>我认为在构建云原生容灾的解决方案上，要以业务为核心去思考构建方法，利用云原生服务的编排能力实现业务系统的连续性。</p><h3 id="2、数据安全性"><a href="#2、数据安全性" class="headerlink" title="2、数据安全性"></a>2、数据安全性</h3><p>AWS CTO Werner Vogels曾经说过：Everything fails, all the time。通过AWS的责任共担模型，我们不难发现云商对底层基础架构负责，用户仍然要对自身自身数据安全性和业务连续性负责。</p><p><img src="/images/pasted-74.png" alt="upload successful"></p><p>我认为在云原生趋势下，用户最直接诉求的来自数据安全性即备份，而迁移、恢复、高可靠等都是基于备份表现出的业务形态，而备份能力可能是由云原生能力提供的，也有可能是第三方能力提供的，但最终实现业务形态，是由编排产生的。</p><p>用户上云并不等于高枕无忧，相反用户要学习云的正确打开方式，才能最大程度来保证业务的连续性。虽然云在底层设计上上是高可靠的，但是仍然避免不了外力造成的影响，例如：光缆被挖断、断电、人为误操作导致的云平台可用区无法使用，所以才有了类似“蓝翔决定了中国云计算稳定性”的调侃。我认为用户决定将业务迁移到云上的那一刻开始，备份、迁移、恢复、高可靠是一个连续的过程，如何合理利用云原生服务的特性实现业务连续性，同时进行成本优化，降低总体拥有成本（TCO）。</p><h3 id="3、防止厂商锁定"><a href="#3、防止厂商锁定" class="headerlink" title="3、防止厂商锁定"></a>3、防止厂商锁定</h3><p>某种意义上说，云原生的方向是新一轮厂商锁定，就像当年盛极一时的IOE架构一样，只不过现在换成了云厂商作为底座承载应用。在IOE时代，用户很难找到完美的替代品，但是在云时代，这种差异并不那么明显。所以大部分的客户通常选用混合云作为云建设策略，为了让应用在不同云之间能够平滑移动，利用容灾技术的迁移一定是作为一个常态化需求存在的。Gartnar也在多云管平台定义中，将迁移和DR作为单独的一项能力。充分说明迁移与容灾在多云环境的的常态化趋势。</p><p><img src="/images/pasted-82.png" alt="upload successful"></p><h1 id="云迁移与云容灾的关系"><a href="#云迁移与云容灾的关系" class="headerlink" title="云迁移与云容灾的关系"></a>云迁移与云容灾的关系</h1><h2 id="云迁移需求的产生"><a href="#云迁移需求的产生" class="headerlink" title="云迁移需求的产生"></a>云迁移需求的产生</h2><p>在传统环境下，迁移的需求并不十分突出，除非是遇到机房搬迁或者硬件升级，才会想到迁移，但这里的迁移更像是搬铁，迁移工具化与自动化的需求并不明显。当VMware出现后，从物理环境到虚拟化的迁移需求被放大，但由于是单一的虚拟化平台，基本上虚拟化厂商自身的工具就完全能够满足需求了。在虚拟化平台上，大家突然发现原来只能人工操作的物理环境一下子轻盈起来，简单来说，我们的传统服务器从一堆铁变成了一个文件，并且这个文件还能够被来回移动、复制。再后来，进入云时代，各家云平台风生水起，国内云计算市场更是百家争鸣，上云更是成为了一种刚性需求。随着时间的推移，出于对成本、厂商锁定等诸多因素的影响，在不同云之间的互相迁移更是会成为一种常态化的需求。</p><h2 id="底层技术一致"><a href="#底层技术一致" class="headerlink" title="底层技术一致"></a>底层技术一致</h2><p>这里提到的云迁移和容灾，并不是堆人提供的迁移服务，而是强调的高度自动化的手段。目标就是在迁移过程中保证业务连续性，缩短停机时间甚至不停机的效果。这里就借助了容灾的存储级别同步技术来实现在异构环境下的的“热迁移”。现有解决方案里，既有传统物理机搬迁时代的迁移软件，也有基于云原生开发的工具。但无论何种形式，都在不同程度上都解决了用户上云的基本诉求。最大的区别在于人效比，这一点与你的利益直接相关。</p><p>从另外一个角度也不难发现，所谓的迁移在正式切换之前实质上就是容灾的中间过程。同时，业务系统迁移到云平台后，灾备是一个连续的动作，这里既包含了传统的备份和容灾，还应该包含云上高可靠的概念。这样，用户业务系统在上云后，才能摆脱传统基础架构的负担，做到“零运维”，真正享受到云所带来的的红利。所以，我认为在云原生状态下，云迁移、云容灾、云备份本质上就是一种业务形态，底层采用的技术手段可以是完全一致的。</p><h2 id="发展方向"><a href="#发展方向" class="headerlink" title="发展方向"></a>发展方向</h2><p>在上述的痛点和趋势下，必然会出现一种全新的平台来帮助客户解决数据的安全性和业务连续性问题，今天就从这个角度来分析一下，在云原生的趋势下如何构建应用系统的迁移与容灾方案。</p><h1 id="云迁移发展趋势"><a href="#云迁移发展趋势" class="headerlink" title="云迁移发展趋势"></a>云迁移发展趋势</h1><h2 id="云迁移方式"><a href="#云迁移方式" class="headerlink" title="云迁移方式"></a>云迁移方式</h2><p>迁移是一项重度的咨询业务，网上各家云商、MSP都有自己的方法论，其实看下来差别都不大，之前也有很多人在分享相关话题，本文就不再赘述。这里我们重点讨论，在实际落地过程中到底该采用哪种工具，哪种方式的效率最高。所谓云迁移工具，就是将源端迁移至目标端，保证源端在目标端正确运行。常见的方式包括：物理机到虚拟化、虚拟化到虚拟化、物理机到云平台、虚拟化到云平台等。</p><p><img src="/images/pasted-62.png" alt="upload successful"></p><p>这是经典的6R迁移理论（现在已经升级为了7R，多了VMware出来搅局），在这个图中与真正迁移相关的其实只有Rehosting, Replatforming, Repurchasing和Refactoring，但是在这4R中，Refactoring明显是一个长期的迭代过程，需要用户和软件开发商共同参与解决，Repurchasing基本上与人为重新部署没有太大的区别。所以真正由用户或MSP在短期完成的只剩下Rehosting和Replatofrming。</p><p>与上面这张经典的迁移理论相比，我更喜欢下面这张图，这张图更能反应一个传统应用到云原生成长的全过程。与上述的结论相似，我们在真正拥抱云的时候，路径基本为上述的三条</p><ul><li>Lift &amp; Shift是Rehost方式的另一种称呼，这种方式路面最宽，寓意这条路是上云的最短路径，应用不需要任何改造直接上云使用</li><li>Evolve和Go Native都属于较窄的路径，寓意为相对于Rehost方式，这两条路径所消耗的时间更久，难度更高</li><li>在图的最右侧，三种形态是存在互相转换的可能，最终演进为彻底的云原生，寓意为迁移并不是一蹴而就，需要循序渐进完成</li></ul><p><img src="/images/pasted-61.png" alt="upload successful"></p><h2 id="重新托管（Rehost）方式"><a href="#重新托管（Rehost）方式" class="headerlink" title="重新托管（Rehost）方式"></a>重新托管（Rehost）方式</h2><p>常用的重新托管方式为冷迁移和热迁移，冷迁移往往涉及到步骤比较繁琐，需要大量人力投入，并且容易出错效率低，对业务连续性有较大的影响，不适合生产系统迁移。而热迁移方案基本都是商用化的解决方案，这里又分为块级别和文件级别，再细分为传统方案与云原生方案。</p><h3 id="冷迁移"><a href="#冷迁移" class="headerlink" title="冷迁移"></a>冷迁移</h3><p>我们先来看一下冷迁移的手动方案，以VMware到OpenStack为例，最简单的方式就是将VMware虚拟机文件(VMDK)通过qemu-img工具进行格式转换，转换为QCOW2或者RAW格式，上传至OpenStack Glance服务，再重新在云平台上进行启动。当然这里面需要进行virtio驱动注入，否则主机无法正常在云平台启动。这个过程中最耗时的应该是虚拟机文件上传至OpenStack Glance服务的过程，在我们最早期的实践中，一台主机从开始迁移到启动完成足足花了24小时。同时，在你迁移这段时间的数据是有增量产生的，除非你将源端关机等待迁移完成，否则，你还要将上述步骤重新来一遍。所以说这种方式真的不适合有业务连续性的生产系统进行迁移。</p><p>那如果是物理机的冷迁移方案怎么做呢？经过我们的最佳实践，这里为大家推荐的是老牌的备份工具CloneZilla，中文名为再生龙。是一款非常老牌的备份软件，常用于进行整机备份与恢复，与我们常见的Norton Ghost原理非常相似。CloneZilla从底层的块级别进行复制，可以进行整盘的备份，并且支持多种目标端，例如我们将磁盘保存至移动硬盘，实际格式就是RAW，你只需要重复上述的方案即可完成迁移。但是在使用CloneZilla过程中，需要使用Live CD方式进行引导，同样会面临长时间业务系统中断的问题，这也是上面我们提到的冷迁移并不适合生产环境迁移的原因。</p><p><img src="/images/pasted-63.png" alt="upload successful"></p><p><img src="/images/pasted-64.png" alt="upload successful"></p><h3 id="传统热迁移方案"><a href="#传统热迁移方案" class="headerlink" title="传统热迁移方案"></a>传统热迁移方案</h3><p>传统的热迁移方案基本分为块级别和文件级别，两者相似之处都是利用差量同步技术进行实现，即全量和增量交叉同步方式。</p><p>文件级别的热迁移方案往往局限性较大，并不能算真正的ReHost方式，因为前期需要准备于源端完全一样的操作系统，无法实现整机搬迁，从操作的复杂性更大和迁移的稳定性来说都不高。我们在Linux上常用的Rsync其实可以作为文件级别热迁移的一种解决方案。</p><p>真正可以实现热迁移的方案，还要使用块级别同步，降低对底层操作系统依赖，实现整机的搬迁效果。传统的块级别热迁移方案基本上来自于传统容灾方案的变种，利用内存操作系统WIN PE或其他Live CD实现，基本原理和过程如下图所示。从过程中我们不难发现这种方式虽然在一定程度解决了迁移的目标，但是作为未来混合云常态化迁移需求来说，仍然有以下几点不足：</p><ul><li>由于传统热迁移方案是基于物理环境构建的，所以我们发现在整个过程中人为介入非常多，对于使用者的技能要求比较高</li><li>无法满足云原生时代多租户、自服务的需求</li><li>安装代理是用户心中永远的芥蒂</li><li>一比一同步方式，从成本角度来说不够经济</li><li>最好的迁移验证方式，就是将业务系统集群在云端完全恢复，但是手动验证的方式，对迁移人力成本是再一次增加</li></ul><p><img src="/images/pasted-67.png" alt="upload successful"></p><h3 id="云原生热迁移方案"><a href="#云原生热迁移方案" class="headerlink" title="云原生热迁移方案"></a>云原生热迁移方案</h3><p>正是由于传统迁移方案的弊端，应运而生了云原生的热迁移方案，这一方面的代表厂商当属AWS在2019年以2.5亿美金击败Google Cloud收购的以色列云原生容灾、迁移厂商CloudEndure。</p><p>云原生热迁移方案是指利用块级别差量同步技术结合云原生API接口和资源实现高度自动化迁移效果，同时提供多租户、API接口满足混合云租户自服务的需求。我们先从原理角度分析一下，为什么相对于传统方案，云原生的方式能够满足高度自动化、用户自服务的用户体验。通过两个方案对比，我们不难发现云原生方式的几个优势：</p><ul><li>利用云原生API接口和资源，操作简便，完全取代了传统方案大量繁琐的人为操作，对使用者技术要求降低，学习陡峭程度大幅度降低</li><li>由于操作简便，迁移效率提高，有效提高迁移实施的人效比</li><li>一对多的同步方式，大幅度降低计算资源使用，计算资源只在验证和最终切换时使用</li><li>能够满足多租户、自服务的要求</li><li>源端也可以支持无代理方式，打消用户疑虑，并且适合大规模批量迁移</li><li>高度自动化的验证手段，在完成迁移切换前，能够反复进行验证</li></ul><p><img src="/images/pasted-69.png" alt="upload successful"></p><p>这是CloudEndure的架构图，当然你也可以利用CloudEndure实现跨区域的容灾。</p><p><img src="/images/pasted-70.png" alt="upload successful"></p><p>不过可惜的一点是由于被AWS收购，CloudEndure目前只能支持迁移至AWS，无法满足国内各种云迁移的需求。所以这里为大家推荐一款纯国产化的迁移平台——万博智云的HyperMotion( <a href="https://hypermotion.oneprocloud.com/" target="_blank" rel="noopener">https://hypermotion.oneprocloud.com/</a> )，从原理上与CloudEndure非常相似，同时支持了VMware及OpenStack无代理的迁移，更重要的是覆盖了国内主流的公有云、专有云和私有云的迁移。</p><p><img src="/images/pasted-71.png" alt="upload successful"></p><h2 id="平台重建（Replatforming）方式"><a href="#平台重建（Replatforming）方式" class="headerlink" title="平台重建（Replatforming）方式"></a>平台重建（Replatforming）方式</h2><p>随着云原生提供越来越多的服务，降低了应用架构的复杂度，使得企业能够更专注自己的业务本身开发。但是研发侧工作量的减少意味着这部分成本被转嫁到部署及运维环节，所以DevOps成为在云原生运用中比不可少的一个缓解，也让企业能够更敏捷的应对业务上的复杂变化。</p><p>正如上面所提到的，用户通过少量的改造可以优先使用一部分云原生服务，这种迁移方式我们成为平台重建（Replatforming），目前选择平台重建方式的迁移，多以与用户数据相关的服务为主。常见的包括：数据库服务RDS、对象存储服务、消息队列服务、容器服务等。这些云原生服务的引入，降低了用户运维成本。但是由于云原生服务自身封装非常严密，底层的基础架构层对于用户完全不可见，所以无法用上述Rehost方式进行迁移，必须采用其他的辅助手段完成。</p><p>以关系型数据库为例，每一种云几乎都提供了迁移工具，像AWS DMS，阿里云的DTS，腾讯云的数据传输服务DTS，这些云原生工具都可以支持 MySQL、MariaDB、PostgreSQL、Redis、MongoDB 等多种关系型数据库及 NoSQL 数据库迁移。以MySQL为例，这些服务都巧妙的利用了binlog复制的方式，实现了数据库的在线迁移。</p><p>再以对象存储为例，几乎每一种云都提供了自己的迁移工具，像阿里云的ossimport，腾讯云COS Migration工具，都可以实现本地到云端对象存储的增量迁移。但是在实际迁移时，还应考虑成本问题，公有云的对象存储在存储数据上比较便宜，但是在读出数据时是要根据网络流量和请求次数进行收费的，这就要求我们在设计迁移方案时，充分考虑成本因素。如果数据量过大，还可以考虑采用离线设备方式，例如：AWS的Snowball，阿里云的闪电立方等。这部分就不展开介绍，以后有机会再单独为大家介绍。</p><p><img src="/images/pasted-72.png" alt="upload successful"></p><p>如果选择平台重建方式上云，除了要进行必要的应用改造，还需要选择一款适合你的迁移工具，保证数据能够平滑上云。结合上面的Rehost方式迁移，能够实现业务系统的整体上云效果。由于涉及的服务较多，这里为大家提供一张迁移工具表格供大家参考。</p><p><img src="/images/pasted-89.png" alt="upload successful"></p><h1 id="云原生下的容灾发展趋势"><a href="#云原生下的容灾发展趋势" class="headerlink" title="云原生下的容灾发展趋势"></a>云原生下的容灾发展趋势</h1><p>目前为止，还没有一套平台能够完全满足云原生状态下的统一容灾需求，我们通过以下场景来分析一下，如何才能构建一套统一的容灾平台满足云原生的需求。</p><h2 id="传统架构"><a href="#传统架构" class="headerlink" title="传统架构"></a>传统架构</h2><p>我们以一个简单的Wordpress + MySQL环境为例，传统下的部署环境一般是这样架构的：</p><p><img src="/images/pasted-58.png" alt="upload successful"></p><p>如果为这套应用架构设计一套容灾方案，可以采用以下的方式：</p><ul><li>负载均衡节点容灾：负载均衡分为硬件和软件层面，硬件负载均衡高可靠和容灾往往通过自身的解决方案实现。如果是软件负载均衡，往往需要安装在基础操作系统上，而同城的容灾可以使用软件高可靠的方式实现，而异地的容灾往往是通过提前建立对等节点，或者干脆采用容灾软件的块或者文件级别容灾实现。是容灾切换（Failover）很重要的一个环节。</li><li>Web Server的容灾：Wordpress的运行环境无非是Apache + PHP，由于分离了用于存放用户上传的文件系统，所以该节点几乎是无状态的，通过扩展节点即可实现高可靠，而异地容灾也比较简单，传统的块级别和文件级别都可以满足容灾的需求</li><li>共享文件系统的容灾，图中采用了Gluster的文件系统，由于分布式系统的一致性通常由内部维护，单纯使用块级别很难保证节点的一致性，所以这里面使用文件级别容灾更为精确</li><li>数据库的容灾，单纯依靠存储层面是无法根本实现数据库0丢失数据的，所以一般采用从数据库层面实现，当然如果为了降低成本，数据库的容灾可以简单的使用周期Dump数据库的方式实现，当然如果对可靠性要求较高，还可以使用CDP方式实现</li></ul><p>从以上的案例分析不难看出，传统基础架构下的容灾往往以存储为核心，无论是磁盘阵列的存储镜像，还是基于I/O数据块、字节级的捕获技术，结合网络、数据库和集群的应用级别技术完成高可靠和容灾体系的构建。在整个容灾过程的参与者主要为：主机、存储、网络和应用软件，相对来说比较单一。所以在传统容灾方案中，如何正确解决存储的容灾也就成为了解决问题的关键。</p><h2 id="混合云容灾"><a href="#混合云容灾" class="headerlink" title="混合云容灾"></a>混合云容灾</h2><p>这应该是目前最常见的混合云的方案，也是各大容灾厂商主推的一种方式。这里我们相当于将云平台当成了一套虚拟化平台，几乎没有利用云平台任何特性。在恢复过程中，需要大量人为的接入才能将业务系统恢复到可用状态。这样的架构并不符合云上的最佳实践，但的确是很多业务系统备份或迁移上云后真实的写照。</p><p><img src="/images/pasted-83.png" alt="upload successful"></p><p>这样的架构确实能解决容灾的问题，但是从成本上来说很高，现在我们来换一种方式。我们利用了对象存储和数据库进行一次优化。我们将原有存储服务存放至对象存储中，而使用数据传输服务来进行实时的数据库复制。云主机仍然采用传统的块级别进行同步。一旦出现故障，则需要自动化编排能力，重新将备份进行恢复，在最短时间内根据我们预设的方案进行恢复，完成容灾。</p><p><img src="/images/pasted-84.png" alt="upload successful"></p><h2 id="云上同城容灾架构"><a href="#云上同城容灾架构" class="headerlink" title="云上同城容灾架构"></a>云上同城容灾架构</h2><p>上述的备份方式，实质上就是利用平台重建的方式进行的迁移，既然已经利用迁移进行了备份，那完全可以对架构进行如下改造，形成同城的容灾架构。我们根据云平台的最佳实践，对架构进行了如下调整：</p><p><img src="/images/pasted-85.png" alt="upload successful"></p><p>这个架构不仅实现了应用级高可靠，还能够支撑一定的高并发性，用户在最少改造代价下就能够在同城实现双活的效果。我们来分析一下在云上利用了多少云原生的服务：</p><ul><li>域名解析服务</li><li>VPC服务</li><li>负载均衡服务</li><li>自动伸缩服务</li><li>云主机服务</li><li>对象存储服务</li><li>关系型数据库RDS服务</li></ul><p>除了云主机外，其他服务均是天然就支持跨可用区的高可用特性，对于云主机我们可以制作镜像方式，由自动伸缩服务负责实例的状态。由于云上可用区就是同城容灾的概念，这里我们就实现了同城的业务系统容灾。</p><p>经过调整的架构在一定程度上满足了业务连续性的要求，但是对于数据的安全性仍然缺乏保障。近几年，勒索病毒横行，大量企业为此蒙受巨大损失，所以数据备份是上云后必须实施的。云原生服务本身提供了备份方案，例如云主机的定期快照等，但往往服务比较分散，不容易统一进行管理。同时，在恢复时往往也是只能每一个服务进行恢复，如果业务系统规模较大，也会增加大量的恢复成本。虽然云原生服务解决了自身备份问题，但是将备份重新组织成应用是需要利用自动化的编排能力实现。</p><h2 id="同云异地容灾架构"><a href="#同云异地容灾架构" class="headerlink" title="同云异地容灾架构"></a>同云异地容灾架构</h2><p>大部分的云原生服务都在可用区内，提供了高可靠能力，但是对于跨区域上通常提供的是备份能力。例如：可以将云主机变为镜像，将镜像复制到其他区域内；关系型数据库和对象存储也具备跨域的备份能力。利用这些组件自身的备份能力，外加上云自身资源的编排能力，我们可以实现在容灾可用域将系统恢复至可用状态。那如何触发切换呢？</p><p>这里我们根据业务系统的特点，在云原生的监控上定制告警，利用告警平台的触发能力触发函数计算，完成业务系统的跨域切换，形成异地容灾的效果。</p><p><img src="/images/pasted-86.png" alt="upload successful"></p><h2 id="跨云容灾"><a href="#跨云容灾" class="headerlink" title="跨云容灾"></a>跨云容灾</h2><p>但跨云容灾不像同云容灾时，在不同的可用区之间至少服务是一致的，那么此时，在同云上使用的方法基本失效，完全需要目标云平台的能力或者中立的第三方的解决方案。这里除了数据的备份，还有一点是服务配置的互相匹配。才能完全满足跨云容灾恢复的需求。另外需要考虑的一点就是成本为例，以对象存储为例，是典型的的“上云容易下云难”。所以如何利用云原生资源特性合理设计容灾方案是对成本的极大考验。</p><p><img src="/images/pasted-87.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>云原生容灾还处于早期阶段，目前尚没有完整的平台能够支持以上各种场景的容灾需求，是值得持续探索的话题。云原生容灾以备份为核心，以迁移、恢复和高可靠为业务场景，实现多云之间的自由流转，最终满足用户的业务需求。</p><p>所以，作为面向云原生的容灾平台要解决好三方面的能力：</p><p>一、以数据为核心，让数据在多云之间互相流转。数据是用户核心价值，所以无论底层基础架构如何变化，数据备份一定是用户的刚醒需求。对于不同云原生服务如何解决好数据备份，是数据流转的必要基础。</p><p>二、利用云原生编排能力，实现高度自动化，在数据基础上构建业务场景。利用自动化编排能力实现更多的基于数据层的应用，帮助用户完成更多的业务创新。</p><p>三、灵活运用云原生资源特点，降低总体拥有成本。解决传统容灾投入巨大的问题，让用户的成本真的能像水、电一样按需付费。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;趋势&quot;&gt;&lt;a href=&quot;#趋势&quot; class=&quot;headerlink&quot; title=&quot;趋势&quot;&gt;&lt;/a&gt;趋势&lt;/h1&gt;&lt;h2 id=&quot;云原生发展趋势&quot;&gt;&lt;a href=&quot;#云原生发展趋势&quot; class=&quot;headerlink&quot; title=&quot;云原生发展趋势&quot;&gt;&lt;/a&gt;云原生发展趋势&lt;/h2&gt;&lt;p&gt;云原生（Cloud Native）是最近几年非常火爆的话题，在2020年7月由信通院发布的《云原生发展白皮书（2020）年》明确指出：云计算的拐点已到，云原生成为驱动业务增长的重要引擎。我们不难发现云原生带给IT产业一次重新洗牌，从应用开发过程到IT从业者的技术能力，都是一次颠覆性的革命。在此基础上，出现了基于云原生平台的Open Application Model定义，在云原生平台基础上进一步抽象，更加关注应用而非基础架构。同时，越来越多的公有云开始支持Serverless服务，更加说明了未来的发展趋势：应用为核心，轻量化基础架构层在系统建设过程中的角色。但是无论如何变化，IT整体发展方向，一定是向着更有利于业务快速迭代、满足业务需求方向演进的。&lt;/p&gt;
&lt;p&gt;2020年9月，Snowflake以每股120美金IPO，创造了今年规模最大的IPO，也是有史以来最大的软件IPO。Snowflake利用云原生方式重构了数据仓库，成功颠覆了行业竞争格局。这正是市场对云原生发展趋势的最佳认可，所以下一个云原生颠覆的领域会不会是在传统的容灾领域呢？&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
      <category term="云原生" scheme="http://sunqi.site/tags/%E4%BA%91%E5%8E%9F%E7%94%9F/"/>
    
      <category term="云迁移" scheme="http://sunqi.site/tags/%E4%BA%91%E8%BF%81%E7%A7%BB/"/>
    
      <category term="云容灾" scheme="http://sunqi.site/tags/%E4%BA%91%E5%AE%B9%E7%81%BE/"/>
    
      <category term="Cloud Native" scheme="http://sunqi.site/tags/Cloud-Native/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack对接多Ceph资源池</title>
    <link href="http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/"/>
    <id>http://sunqi.site/2020/09/14/OpenStack%E5%AF%B9%E6%8E%A5%E5%A4%9ACeph%E8%B5%84%E6%BA%90%E6%B1%A0/</id>
    <published>2020-09-14T08:52:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：</p><ul><li>Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系</li><li>Libvirt配置：配置与Ceph之间的鉴权关系</li></ul><a id="more"></a><h1 id="Cinder配置"><a href="#Cinder配置" class="headerlink" title="Cinder配置"></a>Cinder配置</h1><p>其中rbd_secret_uuid可以使用uuidgen命令生成</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"># &#x2F;etc&#x2F;cinder&#x2F;cinder.conf</span><br><span class="line">[DEFAULT]</span><br><span class="line">......</span><br><span class="line"># 与下面的段落对应</span><br><span class="line">enabled_backends &#x3D; rbd-1, rbd-2</span><br><span class="line">......</span><br><span class="line"></span><br><span class="line">[rbd-1]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line"></span><br><span class="line"># 与上面的enabled_backends对应</span><br><span class="line">volume_backend_name &#x3D; rbd-1</span><br><span class="line"></span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line"></span><br><span class="line"># 需要从Ceph集群拷贝这两个配置文件到相应目录</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-1.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder1.keyring</span><br><span class="line"></span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 5774b929-0690-4513-a1f7-41aac49cbb31</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br><span class="line"> </span><br><span class="line">[rbd-2]</span><br><span class="line">volume_driver &#x3D; cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name &#x3D; rbd-2</span><br><span class="line">rbd_pool &#x3D; volumes</span><br><span class="line">rbd_ceph_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph-2.conf</span><br><span class="line">rbd_keyring_conf &#x3D; &#x2F;etc&#x2F;ceph&#x2F;ceph.client.cinder2.keyring</span><br><span class="line">rbd_flatten_volume_from_snapshot &#x3D; false</span><br><span class="line">rbd_max_clone_depth &#x3D; 5</span><br><span class="line">rbd_store_chunk_size &#x3D; 4</span><br><span class="line">rados_connect_timeout &#x3D; 4</span><br><span class="line">rbd_user &#x3D; admin</span><br><span class="line">rbd_secret_uuid &#x3D; 0563c419-bc4c-4794-972a-685498248869</span><br><span class="line">report_discard_supported &#x3D; True</span><br><span class="line">image_upload_use_cinder_backend &#x3D; True</span><br></pre></td></tr></table></figure><h2 id="建立与Volume-Type对应关系"><a href="#建立与Volume-Type对应关系" class="headerlink" title="建立与Volume Type对应关系"></a>建立与Volume Type对应关系</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cinder type-create rbd-1</span><br><span class="line">cinder type-key rbd-1 set volume_backend_name&#x3D;rbd-1</span><br><span class="line">cinder extra-specs-list</span><br><span class="line"></span><br><span class="line">cinder type-create rbd-2</span><br><span class="line">cinder type-key rbd-2 set volume_backend_name&#x3D;rbd-2</span><br><span class="line">cinder extra-specs-list</span><br></pre></td></tr></table></figure><h1 id="Libvirt配置"><a href="#Libvirt配置" class="headerlink" title="Libvirt配置"></a>Libvirt配置</h1><p>在/etc/libvirt/secretes建立与上述rbd_secret_uuid同名的两个文件，后缀为.xml和.base64，两个文件的内容为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">&lt;secret ephemeral&#x3D;&#39;no&#39; private&#x3D;&#39;no&#39;&gt;</span><br><span class="line">  &lt;uuid&gt;5774b929-0690-4513-a1f7-41aac49cbb31&lt;&#x2F;uuid&gt;</span><br><span class="line">  &lt;usage type&#x3D;&#39;ceph&#39;&gt;</span><br><span class="line">    &lt;name&gt;client.cinder1 secret&lt;&#x2F;name&gt;</span><br><span class="line">  &lt;&#x2F;usage&gt;</span><br><span class="line">&lt;&#x2F;secret&gt;</span><br></pre></td></tr></table></figure><p>其中base64文件的内容就是keyring文件中key的值</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[client.admin]</span><br><span class="line">key &#x3D; AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">AQB&#x2F;E15f42WdABAAR32oTiidCbVGpwhYbWcKAw&#x3D;&#x3D;</span><br></pre></td></tr></table></figure><p>最后执行如下命令完成配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">virsh secret-define --file 5774b929-0690-4513-a1f7-41aac49cbb31.xml</span><br><span class="line">virsh secret-set-value --secret 5774b929-0690-4513-a1f7-41aac49cbb31 --base64 $(cat 5774b929-0690-4513-a1f7-41aac49cbb31.base64)</span><br><span class="line">systemctl restart libvirtd</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;OpenStack支持与多个不同的Ceph资源池进行对接，通过cinder的volume type与backend进行对应，创建时只需要选择不同的volume type就可以实现指定资源池创建。配置OpenStack对接分为两个部分：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Cinder配置：主要配置存储资源池与Volume Type和Backend对应关系&lt;/li&gt;
&lt;li&gt;Libvirt配置：配置与Ceph之间的鉴权关系&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
    
      <category term="OpenStack" scheme="http://sunqi.site/tags/OpenStack/"/>
    
      <category term="Ceph" scheme="http://sunqi.site/tags/Ceph/"/>
    
      <category term="云计算" scheme="http://sunqi.site/tags/%E4%BA%91%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>快速构建Ceph集群</title>
    <link href="http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/"/>
    <id>http://sunqi.site/2020/09/12/%E5%BF%AB%E9%80%9F%E6%9E%84%E5%BB%BACeph%E9%9B%86%E7%BE%A4/</id>
    <published>2020-09-12T09:05:53.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。</p><p>本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。</p><a id="more"></a><h1 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h1><p>我们准备四台服务器，其中一台作为部署发起节点和后续Client节点使用。另外三台作为Ceph节点使用，其中第一台节点node01上，除了monitor和osd外，还将运行Manager, MDS和RGW服务，用于提供文件及对象存储服务。每一台Ceph节点都另外挂载了一块单独的磁盘，由于我使用的是虚拟机环境，所以挂载节点为/dev/vdb，如果使用是其他环境需要注意挂载点名称。</p><p><img src="/images/pasted-58-1.png" alt="upload successful"></p><h1 id="部署时序图"><a href="#部署时序图" class="headerlink" title="部署时序图"></a>部署时序图</h1><p>使用Ceph Deploy将大幅度简化安装过程，大体上分为以下安装步骤：</p><ul><li>节点初始化配置</li><li>Ceph Deploy节点安装</li><li>Ceph集群初始化</li><li>ODS节点安装、安装Mgr服务及添加ODS磁盘，完成Ceph基本安装</li><li>CephFS安装，部署Metadata服务</li><li>Ceph RGW安装，部署RGW服务</li></ul><p><img src="/images/pasted-59-1.png" alt="upload successful"></p><h1 id="（全部节点）环境准备"><a href="#（全部节点）环境准备" class="headerlink" title="（全部节点）环境准备"></a>（全部节点）环境准备</h1><p>这是我非常常用的针对CentOS 7的设置，为了测试方便，关闭了防火墙、SELINUX，同时更新了系统和EPEL源为阿里源，最后进行系统更新，保证系统软件包更新到最新版本后，再进行环境安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><p>如果按照正常流程安装后，执行ceph -s，会出现restful模块无法找到，缺少pecan的安装包，所以在初始化阶段直接将缺少的包进行安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum install -y python3-pip</span><br><span class="line">pip3 install pecan werkzeug</span><br></pre></td></tr></table></figure><h1 id="Ceph-Deploy节点安装"><a href="#Ceph-Deploy节点安装" class="headerlink" title="Ceph-Deploy节点安装"></a>Ceph-Deploy节点安装</h1><h2 id="（Ceph-Deploy节点）安装Ceph-Deploy"><a href="#（Ceph-Deploy节点）安装Ceph-Deploy" class="headerlink" title="（Ceph Deploy节点）安装Ceph-Deploy"></a>（Ceph Deploy节点）安装Ceph-Deploy</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cat &lt;&lt; EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;ceph.repo</span><br><span class="line">[ceph-noarch]</span><br><span class="line">name&#x3D;Ceph noarch packages</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7&#x2F;noarch&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">type&#x3D;rpm-md</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">yum makecache</span><br><span class="line">yum install -y ceph-deploy</span><br></pre></td></tr></table></figure><h2 id="（全部节点）设置时间同步服务"><a href="#（全部节点）设置时间同步服务" class="headerlink" title="（全部节点）设置时间同步服务"></a>（全部节点）设置时间同步服务</h2><p>时间同步服务是分布式系统的生命线，所以安装时候先要安装NTP或者Chrony。在RHEL 7中，默认的时间同步被替换为Chrony，很多新的安装文档中也开始使用Chrony作为时间同步服务，但是NTP也被同时保留。我的环境中Chrony已经被安装并启动，如果没有请自行安装。</p><h2 id="（Ceph-Deploy节点）无密码登录"><a href="#（Ceph-Deploy节点）无密码登录" class="headerlink" title="（Ceph Deploy节点）无密码登录"></a>（Ceph Deploy节点）无密码登录</h2><p>这里为了简便，使用了root用户进行安装。配置完成后，需要让Ceph Deploy能够无密码的方式访问全部Ceph节点。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-keygen</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id root@node1</span><br><span class="line">ssh-copy-id root@node2</span><br><span class="line">ssh-copy-id root@node3</span><br></pre></td></tr></table></figure><h1 id="Ceph集群安装"><a href="#Ceph集群安装" class="headerlink" title="Ceph集群安装"></a>Ceph集群安装</h1><h2 id="（Ceph-Deploy节点）Ceph块存储服务安装"><a href="#（Ceph-Deploy节点）Ceph块存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph块存储服务安装"></a>（Ceph Deploy节点）Ceph块存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;rpm-octopus&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br><span class="line"></span><br><span class="line"># 集群初始化，这一步会生成初始化的ceph.conf，可以配置网络等信息</span><br><span class="line">ceph-deploy new node01</span><br><span class="line">ceph-deploy install node01 node02 node03</span><br><span class="line"></span><br><span class="line"># 初始化monitor，并收集keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy admin node01 node02 node03</span><br><span class="line"></span><br><span class="line">ceph-deploy mgr create node01</span><br><span class="line"></span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node01</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node02</span><br><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb node03</span><br><span class="line"></span><br><span class="line"># 检查集群状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>由于默认采用了Bluestore安装方式，如果想使用SSD作为block.db和block.wal，可以这样创建OSD</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy osd create --data &#x2F;dev&#x2F;vdb --block-db &#x2F;dev&#x2F;vdc --block-wal &#x2F;dev&#x2F;vdc node01</span><br></pre></td></tr></table></figure><p>在Ceph Deploy节点，将Ceph相关配置文件拷贝至系统的/etc/ceph目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.conf &#x2F;etc&#x2F;ceph</span><br><span class="line">cp ceph.client.admin.keyring &#x2F;etc&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="（Ceph-Deploy节点）增加多个Monitor节点"><a href="#（Ceph-Deploy节点）增加多个Monitor节点" class="headerlink" title="（Ceph Deploy节点）增加多个Monitor节点"></a>（Ceph Deploy节点）增加多个Monitor节点</h3><p>添加多个Monitor节点，可以实现高可靠，但是一定为奇数。先更新配置文件，在刚才初始化集群目录下的ceph.conf中的mon_host添加所有节点IP，之后设定public network，这里我们使用了Ceph节点的网段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># ceph.conf</span><br><span class="line"># ...</span><br><span class="line">mon_host &#x3D; 192.168.10.105,192.168.10.176,192.168.10.139</span><br><span class="line">public network &#x3D; 192.168.10.1&#x2F;24</span><br><span class="line"># ...</span><br></pre></td></tr></table></figure><p>分发配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy --overwrite-conf admin node01 node02 node03</span><br><span class="line">ceph-deploy mon add node02</span><br><span class="line">ceph-deploy mon add node03</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 检查quorum状态</span><br><span class="line">ceph quorum_status --format json-pretty</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph文件系统服务安装"><a href="#（Ceph-Deploy节点）Ceph文件系统服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph文件系统服务安装"></a>（Ceph Deploy节点）Ceph文件系统服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mds create node01</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"># 添加多个Manager服务，Manager采用的是主从模式</span><br><span class="line">ceph-deploy mgr create node02 node03</span><br><span class="line"></span><br><span class="line"># 可以看到Manager主从节点状态</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure><p>如果发现Ceph Monitor节点启动失败，需要到相应的节点上查看失败原因，比如我的Monitor使用Start启动，返回这样的提示</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Job for ceph-mon@node02.service failed because start of the service was attempted too often. See &quot;systemctl status ceph-mon@node02.service&quot; and &quot;journalctl -xe&quot; for details.</span><br><span class="line">To force a start use &quot;systemctl reset-failed ceph-mon@node02.service&quot; followed by &quot;systemctl start ceph-mon@node02.service&quot; again.</span><br></pre></td></tr></table></figure><p>按照提示重新启动即可</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">systemctl reset-failed ceph-mon@node02.service</span><br><span class="line">systemctl start ceph-mon@node02.service</span><br></pre></td></tr></table></figure><h2 id="（Ceph-Deploy节点）Ceph对象存储服务安装"><a href="#（Ceph-Deploy节点）Ceph对象存储服务安装" class="headerlink" title="（Ceph Deploy节点）Ceph对象存储服务安装"></a>（Ceph Deploy节点）Ceph对象存储服务安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy rgw create node01</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="块存储测试"><a href="#块存储测试" class="headerlink" title="块存储测试"></a>块存储测试</h2><h3 id="建立存储空间"><a href="#建立存储空间" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="挂载使用"><a href="#挂载使用" class="headerlink" title="挂载使用"></a>挂载使用</h3><h2 id="文件系统测试"><a href="#文件系统测试" class="headerlink" title="文件系统测试"></a>文件系统测试</h2><h3 id="建立存储空间-1"><a href="#建立存储空间-1" class="headerlink" title="建立存储空间"></a>建立存储空间</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph osd pool create cephfs_data 16</span><br><span class="line">ceph osd pool create cephfs_metadata 16</span><br><span class="line"></span><br><span class="line"># ceph fs new &lt;fs_name&gt; &lt;metadata&gt; &lt;data&gt;</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line">ceph fs ls</span><br></pre></td></tr></table></figure><h3 id="内核方式挂载"><a href="#内核方式挂载" class="headerlink" title="内核方式挂载"></a>内核方式挂载</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">mount -t ceph 192.168.10.11:6789:&#x2F; &#x2F;mnt&#x2F;mycephfs -o name&#x3D;admin,secretfile&#x3D;&#x2F;etc&#x2F;ceph&#x2F;admin.secret</span><br></pre></td></tr></table></figure><h3 id="Fuse方式挂载"><a href="#Fuse方式挂载" class="headerlink" title="Fuse方式挂载"></a>Fuse方式挂载</h3><p>确保/etc/ceph下面已经拷贝了ceph.conf和keyring文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;mnt&#x2F;mycephfs</span><br><span class="line">ceph-fuse -m 192.168.10.11:6789 &#x2F;mnt&#x2F;mycephfs</span><br></pre></td></tr></table></figure><h2 id="对象存储测试"><a href="#对象存储测试" class="headerlink" title="对象存储测试"></a>对象存储测试</h2>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;虽然安装环境并不是属于研发人员的本质工作，甚至有些研发人员抵触一些环境的搭建工作。在一些大型企业中，由于分工明确，造成了一些研发人员在这一方面能力的严重缺失。其实环境安装对于开发人员从整体上掌握软件架构师非常有益的，同时随着云计算、云原生的发展，对于DevOps的软件开发模式也越来越被企业接受，可以预见的是，未来DevOps将是所有研发人员必备的技能之一。&lt;/p&gt;
&lt;p&gt;本文主要目标是帮助研发人员用最小成本搭建一套Ceph环境，为了降低搭建成本，使用了Ceph Deploy及国内源加速安装速度。我们选择目前Ceph Octopus最新的稳定版本进行安装。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>一款云迁移产品的成长史</title>
    <link href="http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/"/>
    <id>http://sunqi.site/2020/08/11/%E4%B8%80%E6%AC%BE%E4%BA%91%E8%BF%81%E7%A7%BB%E4%BA%A7%E5%93%81%E7%9A%84%E6%88%90%E9%95%BF%E5%8F%B2/</id>
    <published>2020-08-11T06:04:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于作者"><a href="#关于作者" class="headerlink" title="关于作者"></a>关于作者</h1><p>孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。</p><h1 id="关于万博智云"><a href="#关于万博智云" class="headerlink" title="关于万博智云"></a>关于万博智云</h1><p>万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。</p><p>万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。</p><a id="more"></a><h1 id="结缘云迁移"><a href="#结缘云迁移" class="headerlink" title="结缘云迁移"></a>结缘云迁移</h1><p>2011年开始，我一直从事OpenStack在企业私有云应用的研发工作。从2011年一直到2018年，是开源社区最为活跃的时间段，各个公司将自己的主要精力全部投入到OpenStack各个模块的优化中。当时建设私有云平台所提供的服务往往是全方位的，从系统集成、安装实施再到后面的运行维护和定制化开发，基本上就是一整套全栈式解决方案，甚至有时候云平台之上的业务系统出问题，客户也会来找你。这对于任何尚处于初创型规模的OpenStack公司往往是个巨大的挑战。</p><p>2016年的时候，我们为一家农商行客户建设私有云，经过反复的前期验证，最终在2016年底拿下了该项目。当时除了建设云平台的需求外，还有一项作为验收标准的需求是将用户原有运行在各种物理机的业务系统平稳的迁移到新的云平台上，迁移过程不能对现有业务产生任何影响。最后还要将旧的硬件进行必要升级后，重新加入到新的云平台。</p><p>回想起当时云平台的建设过程，架构上并不复杂，就是一个典型的OpenStack使用硬件存储再加上VLAN的简单模式。在实际的项目实施中，从硬件到货到上架安装，再到云平台部署完成，前前后后的时间大约在三周左右。但是由于用户对于热迁移和资源回收的需求，整个项目实际耗时竟然长达半年之久。由于客户所处的位置不直通高铁，我们的工程师从北京出发，要不就是坐一夜的绿皮火车，要不就先高铁到徐州再转长途车的方式。无论哪种方式，路上的时间至少要8个小时以上。从方案验证到最终实施完毕，团队内全体成员总共出差次数超过50次以上，最终的实施成本极高。当我们尝试复盘整个过程时，耗时最久的其实就是解决各种迁移过程中产生的问题。</p><h1 id="挫折中前行"><a href="#挫折中前行" class="headerlink" title="挫折中前行"></a>挫折中前行</h1><p>这个客户的业务系统属于典型的老旧型业务系统，运行在物理机加上硬件存储阵列上，有少量的虚拟化环境，操作系统也是五花八门，最多的是SUSE 11，还有Windows 2003，CentOS等，数据库有DB2，Oracle，还有少量的MySQL。</p><p>由于是银行系统，所以对于业务连续性有非常强烈的诉求，在迁移上对我们提出了以下几点要求：</p><p>第一，风险控制。在任何行业中，稳定、可靠是当仁不让的第一原则，对于关乎民生的金融行业更是如此。所以在实际云平台建设过程中，原有业务系统上云时往往受到的阻力最大。究其原因就是在上云过程中没有一套完整的、科学的方法论及工具让用户打消对上云的顾虑。所以在向云迁移过程中，系统必须是可验证、可回退的。在正式切换到云平台之前，需要让业务系统在云平台之上得到充分的验证；在切换到云平台后，如果一旦发生失败，要马上能够回退到原有系统，继续提供服务。保障在云迁移过程中，风险降到最低。</p><p>第二、保障业务连续性。农商行不同于传统的四大行或者城商行，在IT建设上往往有很大的自主权，除了核心交易系统外，其他的业务系统均运行在本地系统上，所以对本地运维能力提出比较高的要求。在迁移过程中，本地业务系统的连续性非常重要，一旦中断银行就无法开门做生意了。同时，根据银监会印发的相关规定：在业务服务时段导致业务无法正常开展达半个小时(含)以上，属于重大运营中断事件。所以基本上迁移的切换时间窗口，只能在晚间进行，但是晚上银行又会有数据下发、跑批等程序的运行，所以留给迁移的时间窗口非常有限，所以必须采用一种近似于热迁移的效果来满足客户的需求。</p><p>第三，减少人为干预，保障迁移的可靠性。由于很多系统属于服务厂商开发，部分应用时间久远，甚至很多服务厂商已经不存在了，所以迁移过程中尽量减少对应用厂商的依赖很关键，比如重装、重新配置都会导致应用无法运行。同时，在迁移过程中，由于步骤非常复杂，人为操作过多非常容易产生错误。</p><p>在这个过程中，我们走了非常多的弯路，比如从最早采用冷迁移方式的Clonezilla，耗时24个小时才能迁移完一台主机；再比如调研了各种开源的p2v和v2v工具，没有一个好用的；再比如为了解决UEFI启动的问题，修改nova代码，但是加载后发现一台服务器启动过程黑屏了半个小时之久，为了这一个系统我们往返于北京和客户多达五次。这些困难促使我们不得不停下来思考，为什么一个看似简单的迁移，最终却成为影响项目进度和成本的关键因素呢？</p><h1 id="从项目中来，在项目中成长"><a href="#从项目中来，在项目中成长" class="headerlink" title="从项目中来，在项目中成长"></a>从项目中来，在项目中成长</h1><p>为了解决在项目中遇到的问题，我们尝试了各种手段，最终我们发现灾备领域的数据读取技术加上云原生的方式是最佳的组合方案。使用灾备的块级别差量复制技术能够充分保障业务连续性，而最大程度利用云平台原生接口和资源能够实现”两点之间直线最短“的效果，保障迁移的可靠性，大幅度降低人为介入而带来的不确定性，最后二者叠加的效果最终满足了风险可控的终极目标。</p><p>通过2016和2017年近两年的磨练，一个面向OpenStack的热迁移产品具备了初步产品雏形。在紧接着到来的2018年我们迎来了又一次大考，这一次我们面对着是江苏省农信的专有云平台的大规模迁移，我们需要将该省内全部62家二级法人的业务系统迁移上云。很快我们中标的兴奋就淹没在新的困难面前。在之前的项目中，我们的所有迁移行为都是在本地数据中心完成的，至少所有的网络基本都是千兆的。但是在这个项目中，省端和各个二级法人之间的连接变成了以10Mbps的专线，并且这还是最好的情况，还有更糟糕的只有2Mbps。省端与二级法人的专线连接主要用于省端的数据下发，所以用于迁移的数据传输只能在特定时间段进行，同时不能将全部的带宽占满，以防影响业务。但是，每个二级法人的用户数据量很大，大约在30TB - 50TB左右，如果完全依赖网络传输，理论上需要传上一年多的时间。所以完全依赖于网络传输是不可能的，我们需要的是一种硬件加网络的组合方案，由硬件保存全量数据，通过运输方式到省端，将全量数据切换至云端后，再通过网络传输增量，这样形成的效果仍然是热迁移，但是迁移的速度明显提高。</p><p>在解决了大规模数据传输后，我们紧接着遇到的问题就是先迁哪个，后迁哪个？我们都知道应用系统是存在一定的依赖关系的，所以在迁移前必须要梳理清楚应用系统的拓扑结构，同时还要对迁移后的网络、应用配置等变更做出预先分析，保障万无一失。这个过程其实就是在众多迁移方法论中提到的调研分析阶段。在这个过程中，我们也在实践中积累了自己的迁移调研方法和实施方案，对我们后来的项目起到了很大的帮助作用。同时我们也意识到，迁移绝对不是一个工具就解决的问题，而是一个重度的咨询过程，迁移工具只不过解决了最后一公里的问题。</p><p>从2018年初开始，我们和用户方组成的江苏省农信业务专家组，深入每个地市，严格遵照调研、评审、实施、切换进行科学的上云。从基本的系统信息采集、整理到业务系统上下联分析，绘制拓扑图，安全性等进行全面评估，之后根据调研的结论整理实施方案、进度，实施方案中要将一切在迁移后的变更提前进行整理，确保迁移过程中万无一失。通过辅助物理设备进行全量数据拷贝，运输到省端后进行切换上云，最终在合适的时间点完成增量及业务切换过程。在2018年下半年，平均一周就可以有三家农商行的业务系统实现全面上云。</p><p>在这个项目中，我们的产品得到了极大的锤炼，经受了大规模迁移的考验。通过专有云的建设和业务系统迁移，3年共为江苏农信节省IT投资5.6亿元。截止2018年9月30日，总共完成54家二级法人共1200多套系统迁移。同时，云平台的从最初的15个节点增长到了130多个节点，存储从0.2PB增长至3PB。</p><h1 id="从一朵云到一片云"><a href="#从一朵云到一片云" class="headerlink" title="从一朵云到一片云"></a>从一朵云到一片云</h1><p>时间到了2019年，我们产品的云原生的理念逐步得到了更多客户的认可，同时这种基于云原生构建的高度自动化的效果正好填补了云迁移这个市场空白。甚至某些老牌的灾备厂商把我们当成迁移竞争对手，直接在软文中进行”诋毁“，不过这一切恰好证明了我们产品所蕴含的巨大价值。</p><p>但是只能支持单云的迁移已经无法满足市场上越来越多的云迁移需求，所以在2019年上半年，我们准备全面支持更多的公有云和专有云平台。我们首先选择了国内的最大的公有云提供商——阿里云。阿里云在最近10年已经成长为中国云计算领域的标杆，拥有极高的市场占有率，同时提供了最广泛的API接口支持，为合作伙伴提供最大程度的赋能。由于阿里云与OpenStack在一些机制上存在差异，我们通过近3个月的调研和开发，终于突破了阿里云的热迁移。接下来，我们对云平台的支持范围不断扩大，又用了四个月左右时间，覆盖了国内绝大多数的公有云、专有云和私有云平台，成为了名副其实的多云迁移。</p><h1 id="打造极致的用户体验"><a href="#打造极致的用户体验" class="headerlink" title="打造极致的用户体验"></a>打造极致的用户体验</h1><p>很多企业级产品留给人的第一印象就是专业且复杂，不培训你两天你都不会用。在云迁移领域也是如此，很多云迁移产品都是由传统灾备厂商对原有灾备软件进行简单改造后的产物，界面复杂不说，操作还极其繁琐，迁移一台主机下来，十几个、二十几个步骤那是基本配置。所以在我们对产品进行迭代时，希望用To C的思维打造To B的产品。</p><p>在初始阶段，用户只要根据向导配置源端和目标端的信息后，就可以进入迁移流程。我们将迁移流程分成了三个简单的步骤：选择主机、同步数据和开始迁移。通过高度自动化的流程和对云原生API及资源的巧妙利用，初级的Linux工程师基本上几分钟就能完全上手。同时由于自动化程度高，在批量迁移时优势非常明显。</p><p>全新UI.png<br><img src="/images/pasted-52.png" alt="upload successful"></p><p>由于之前一直从事的是私有云领域的产品研发，导致我们的研发团队在产品开发中存在一种惯性。为了满足私有化部署的需要，我们往往需要将安装包做成无网络依赖的ISO格式。这直接导致的后果就是用户在试用我们的产品时往往需要先花很长一段时间去下载我们的安装介质，之后是安装，最后才能试用。这个一来一回的过程，往往就是一天的时间被浪费了。这一点在公有云迁移时，会让人觉得更加繁琐，所以在2019年下半年，我们决定将我们的产品SaaS化，让用户更快速的体验我们的产品而非将时间浪费在安装的环节上。由于人力资源的限制，研发团队和运维团队都受到了极大的挑战。研发团队需要开发新的模块以支持运营、多租户等SaaS需求，同时还要对原有的通讯模式进行改造，避免双向通讯的发生；而实施团队需要兼顾私有项目和线上运维，这就要求平台稳定、高可靠、易运维，所以对云原生的应用就变得尤为关键。我们利用阿里云的Kubernetes容器服务和各种云原生组件完成了SaaS化的改造，在没有增加任何人力的情况下，在2020年初完成SaaS的全面上线。</p><h1 id="在巨人肩膀上一起成长"><a href="#在巨人肩膀上一起成长" class="headerlink" title="在巨人肩膀上一起成长"></a>在巨人肩膀上一起成长</h1><p>2019年初，AWS斥资2.5亿美金收购了以色列灾备初创公司CloudEndure，虽然这家公司以灾备公司名义被收购，但主要业务却是提供向AWS的迁移服务。我们的产品在设计理念和用户体验上与CloudEndure非常相似，同时我们的产品可以支持国内众多的不同的云厂商。</p><p>AWS对CloudEndure的收购给了我们非常大的信心，让我们坚定了走云原生迁移、灾备产品的思路。我们发现这个市场在国内基本上属于空白阶段，虽然传统灾备厂商的工具可以靠堆人解决项目上的问题，但是真正让用户自助式的迁移平台才能让用户自主分配在云端的负载，让云资源得到更快速的消耗，最终让云厂商获益。</p><p>于是一个大胆的想法在脑海中形成，能不能把我们的迁移软件以云原生服务的方式集成在公有云平台中呢？经过几番周折，我们开始与阿里云进行接触。非常感谢阿里云的陈绪博士帮我打开了和阿里云团队的合作大门，在2019年与阿里云对接完成后，我们首先迎来了就是阿里云ECS团队的考验，在对产品充分测试后，我们在杭州与阿里云生态合作伙伴团队、投资部门进行了会面，这次会面彻底打开了我们与阿里云的合作大门。</p><p>2019年底，我被评为阿里云解决方案领域MVP，进一步促进了我们与阿里云之间的合作。2020年初，阿里云控制台上的应用工具市场吸引了我的目光。这种与阿里云深度整合的方式，对于云原生迁移、灾备是绝佳的栖息之地。通过阿里云MVP运营团队的引荐，我们成功的和阿里云应用工具市场团队进行了对接，同时在2月底决定上架阿里云应用工具市场。</p><p><img src="/images/pasted-53.png" alt="upload successful"></p><p>上架阿里云应用工具市场的过程绝非一帆顺利，阿里云对此有严格的安全性要求，上线前必须要通过阿里云安全部门的严格审查。为此，我们做了一些架构上的调整和安全性的加固。最终经过近3个月的努力，终于将我们的平台与2020年7月10日晚8点正式上线。上架后的迁移平台，与阿里云的用户体验保持完全一致。用户使用时毫无违和感。</p><p><img src="/images/pasted-54.png" alt="upload successful"></p><p>紧接着通过MVP运营团队与阿里云Apsara Stack团队取得了联系，开始对接Apsara Stack专有云，截止到8月初已经彻底实现了对Apsara Stack自动化迁移的全面支持。</p><h1 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h1><p>2020年4月，国家提出了新基建的发展目标，首当其冲的就是信息基础设施，而云计算作为新基建的底座，重要性不言而喻。2020年初的疫情，让全社会意识到”云上社会“的重要性，可以预见的一点是，全面云化的时代正在到来。</p><p>通过与阿里云的全面合作，为我们的产品带来了顶级流量入口，获取客户信任的时间更短。未来，我们也会将我们的产品打造成基于云原生的备份、容灾产品，为更多的云客户提供完美的用户体验。欢迎各位有志之士加入我们的团队，也欢迎有需求的客户加入我们的迁移群参与讨论（关注微信公众号后回复”支持“）。</p><img src="/images/pasted-55.png" alt="万博智云" title="万博智云" width="300" />]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;关于作者&quot;&gt;&lt;a href=&quot;#关于作者&quot; class=&quot;headerlink&quot; title=&quot;关于作者&quot;&gt;&lt;/a&gt;关于作者&lt;/h1&gt;&lt;p&gt;孙琦，万博智云CTO(万国数据(NASDAQ:GDS)合资子公司)，阿里云解决方案领域MVP，Ceph中国社区联合创始人，AWS Certified DevOps Professional。曾先后就职亿阳信通、摩托罗拉、瞬联软件等国内外知名企业。2013年开始创业，从事私有云领域研发工作，2016年带领团队开发云原生迁移产品HyperMotion，该产品在江苏农信、国家电网、海通证券等诸多项目得到广泛应用。2018年成功组织Ceph全球首次峰会，并帮助多家国内知名企业加入Linux Foundation旗下的Ceph基金会。&lt;/p&gt;
&lt;h1 id=&quot;关于万博智云&quot;&gt;&lt;a href=&quot;#关于万博智云&quot; class=&quot;headerlink&quot; title=&quot;关于万博智云&quot;&gt;&lt;/a&gt;关于万博智云&lt;/h1&gt;&lt;p&gt;万博智云信息科技（上海）有限公司成立于上海，是国内领先的云技术和数字化架构服务商。万博智云专注于为企业提供中立/专业的云咨询、云产品、云服务；致力成为企业 IT运营、数字化发展可信耐的云服务商。公司秉持以产品驱动服务，以科技提升企业商业价值的理念，持续提供丰富的云化产品、解决方案、专业咨询服务，并联合生态体系助力企业在数字化时代全速发展。&lt;/p&gt;
&lt;p&gt;万博智云核心研发团队组建于2013年5月，2013年到2016年期间团队致力于开发基于OpenStack私有云产品，2016年后团队转型全力开发云市场细分领域产品——云迁移。2017年完成了沭阳农商行私有云平台建设及业务系统上云项目，该项目获得银监会四类科技成果奖，第二届优秀云计算开源案例二等奖；2018年完成江苏农信省联社专有云平台建设，同时利用云迁移产品完成1200多套业务系统批量上云，该项目获得银监会二类科技成果奖，第三届优秀云计算开源案例二等奖；同年，完成国家电网27个省近20000台VMware虚拟机批量上云迁移；2019年完成海通证券云管平台与云迁移产品整合，该项目也是国内首个将云管平台整合到云管平台提供自助式迁移服务的项目；2020年完成前海股权VMware虚拟机批量迁移至阿里云项目。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>Kubernetes All-in-One环境安装</title>
    <link href="http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/"/>
    <id>http://sunqi.site/2020/07/31/Kubernetes%E5%9F%BA%E6%9C%AC%E5%AE%89%E8%A3%85/</id>
    <published>2020-07-31T01:38:24.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Kubernetes安装及初始化"><a href="#Kubernetes安装及初始化" class="headerlink" title="Kubernetes安装及初始化"></a>Kubernetes安装及初始化</h1><p>研发环境搭建Kubernetes All-in-One环境搭建。</p><a id="more"></a><h2 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h2 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure><h2 id="安装Kubernetes软件"><a href="#安装Kubernetes软件" class="headerlink" title="安装Kubernetes软件"></a>安装Kubernetes软件</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"># step1 Installation Process</span><br><span class="line">cat &lt;&lt;EOF &gt; &#x2F;etc&#x2F;yum.repos.d&#x2F;kubernetes.repo</span><br><span class="line">[kubernetes]</span><br><span class="line">name&#x3D;Kubernetes</span><br><span class="line">baseurl&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;repos&#x2F;kubernetes-el7-x86_64&#x2F;</span><br><span class="line">enabled&#x3D;1</span><br><span class="line">gpgcheck&#x3D;1</span><br><span class="line">repo_gpgcheck&#x3D;1</span><br><span class="line">gpgkey&#x3D;https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;yum-key.gpg https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;yum&#x2F;doc&#x2F;rpm-package-key.gpg</span><br><span class="line">exclude&#x3D;kube*</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">yum install -y kubelet kubeadm kubectl --disableexcludes&#x3D;kubernetes</span><br><span class="line"> </span><br><span class="line">systemctl enable kubelet</span><br><span class="line"> </span><br><span class="line"># for CentOS 7</span><br><span class="line">cat &lt;&lt;EOF &gt;  &#x2F;etc&#x2F;sysctl.d&#x2F;k8s.conf</span><br><span class="line">net.bridge.bridge-nf-call-ip6tables &#x3D; 1</span><br><span class="line">net.bridge.bridge-nf-call-iptables &#x3D; 1</span><br><span class="line">vm.swappiness&#x3D;0</span><br><span class="line">EOF</span><br><span class="line">sysctl --system</span><br><span class="line"> </span><br><span class="line"># from k8s 1.8, swap need to be cloased, otherwise k8s could not be started</span><br><span class="line"># swapoff -a</span><br><span class="line"># Modify &#x2F;etc&#x2F;fstab, comment swap mount</span><br></pre></td></tr></table></figure><h2 id="初始化Kubernetes集群"><a href="#初始化Kubernetes集群" class="headerlink" title="初始化Kubernetes集群"></a>初始化Kubernetes集群</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"># Step2 initialization</span><br><span class="line"># Specify kubernetes-version if mirror do not contain latest kubernetes container. ex: if kubeadm is version 1.18.5, you can only</span><br><span class="line"># install kubernetes &#x3D; 1.18.x</span><br><span class="line">kubeadm init --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version&#x3D;1.18.0</span><br><span class="line"> </span><br><span class="line"># Response from output</span><br><span class="line"># You should now deploy a pod network to the cluster.</span><br><span class="line"># Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">#   https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"># Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"># kubeadm join 192.168.10.111:6443 --token 1odaru.0by05advhbu7edgt \</span><br><span class="line">#     --discovery-token-ca-cert-hash sha256:3efb71c40cce36c5ed90fc8b5831233aba06eec26576088e8e7a7a892d272776</span><br><span class="line"> </span><br><span class="line"># Step3 flannel Network</span><br><span class="line">sysctl net.bridge.bridge-nf-call-iptables&#x3D;1</span><br><span class="line">kubectl apply -f https:&#x2F;&#x2F;gitee.com&#x2F;xiaoquqi&#x2F;flannel&#x2F;raw&#x2F;master&#x2F;Documentation&#x2F;kube-flannel.yml</span><br><span class="line"> </span><br><span class="line"># Step4 To use cluster</span><br><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure><h1 id="允许Master节点运行Pods"><a href="#允许Master节点运行Pods" class="headerlink" title="允许Master节点运行Pods"></a>允许Master节点运行Pods</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl taint nodes --all node-role.kubernetes.io&#x2F;master-</span><br></pre></td></tr></table></figure><h1 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h1><h2 id="安装Wordpress和MySQL"><a href="#安装Wordpress和MySQL" class="headerlink" title="安装Wordpress和MySQL"></a>安装Wordpress和MySQL</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;mysql-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;wordpress-deployment.yaml</span><br><span class="line">curl -LO https:&#x2F;&#x2F;raw.githubusercontent.com&#x2F;xiaoquqi&#x2F;k8s_demo&#x2F;master&#x2F;wordpress&#x2F;kustomization.yaml</span><br></pre></td></tr></table></figure><h2 id="执行安装"><a href="#执行安装" class="headerlink" title="执行安装"></a>执行安装</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl apply -k .&#x2F;</span><br></pre></td></tr></table></figure><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kubectl get secrets</span><br><span class="line">kubectl get pvc</span><br><span class="line">kubectl get pods</span><br><span class="line">kubectl get services wordpress</span><br></pre></td></tr></table></figure><h2 id="资源清理"><a href="#资源清理" class="headerlink" title="资源清理"></a>资源清理</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -k .&#x2F;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Kubernetes安装及初始化&quot;&gt;&lt;a href=&quot;#Kubernetes安装及初始化&quot; class=&quot;headerlink&quot; title=&quot;Kubernetes安装及初始化&quot;&gt;&lt;/a&gt;Kubernetes安装及初始化&lt;/h1&gt;&lt;p&gt;研发环境搭建Kubernetes All-in-One环境搭建。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>CentOS 7和Docker初始化安装</title>
    <link href="http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/"/>
    <id>http://sunqi.site/2020/07/31/CentOS-7%E5%88%9D%E5%A7%8B%E5%8C%96%E8%84%9A%E6%9C%AC/</id>
    <published>2020-07-31T01:34:49.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="CentOS-7初始化"><a href="#CentOS-7初始化" class="headerlink" title="CentOS 7初始化"></a>CentOS 7初始化</h1><p>该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Set SELinux in permissive mode (effectively disabling it)</span><br><span class="line">setenforce 0</span><br><span class="line">#sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;permissive&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line">sed -i &#39;s&#x2F;^SELINUX&#x3D;enforcing$&#x2F;SELINUX&#x3D;disabled&#x2F;&#39; &#x2F;etc&#x2F;selinux&#x2F;config</span><br><span class="line"> </span><br><span class="line">systemctl stop NetworkManager</span><br><span class="line">systemctl disable NetworkManager</span><br><span class="line"> </span><br><span class="line">systemctl status firewalld</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line">systemctl status firewalld</span><br><span class="line">firewall-cmd --state</span><br><span class="line"> </span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;CentOS-Base.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;Centos-7.repo</span><br><span class="line">curl -o &#x2F;etc&#x2F;yum.repos.d&#x2F;epel.repo http:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;repo&#x2F;epel-7.repo</span><br><span class="line">yum clean all &amp;&amp; yum makecache</span><br><span class="line">yum update -y</span><br></pre></td></tr></table></figure><h1 id="Docker国内源安装"><a href="#Docker国内源安装" class="headerlink" title="Docker国内源安装"></a>Docker国内源安装</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"># Install Docker</span><br><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh -s -- &quot;--mirror&quot; &quot;Aliyun&quot;</span><br><span class="line"> </span><br><span class="line"># Replace docker repo</span><br><span class="line">mkdir -p &#x2F;etc&#x2F;docker</span><br><span class="line">cat &gt; &#x2F;etc&#x2F;docker&#x2F;daemon.json &lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">  &quot;registry-mirrors&quot;: [&quot;https:&#x2F;&#x2F;6m7d428u.mirror.aliyuncs.com&quot;],</span><br><span class="line">  &quot;dns&quot;: [&quot;114.114.114.114&quot;],</span><br><span class="line">  &quot;exec-opts&quot;: [&quot;native.cgroupdriver&#x3D;systemd&quot;],</span><br><span class="line">  &quot;log-driver&quot;: &quot;json-file&quot;,</span><br><span class="line">  &quot;log-opts&quot;: &#123;</span><br><span class="line">    &quot;max-size&quot;: &quot;100m&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;storage-driver&quot;: &quot;overlay2&quot;,</span><br><span class="line">  &quot;storage-opts&quot;: [</span><br><span class="line">    &quot;overlay2.override_kernel_check&#x3D;true&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line"> </span><br><span class="line">systemctl enable docker &amp;&amp; systemctl daemon-reload &amp;&amp; systemctl restart docker</span><br><span class="line"> </span><br><span class="line">curl -L https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker&#x2F;compose&#x2F;releases&#x2F;download&#x2F;1.25.4&#x2F;docker-compose-&#96;uname -s&#96;-&#96;uname -m&#96; &gt; &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br><span class="line">chmod +x &#x2F;usr&#x2F;local&#x2F;bin&#x2F;docker-compose</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;CentOS-7初始化&quot;&gt;&lt;a href=&quot;#CentOS-7初始化&quot; class=&quot;headerlink&quot; title=&quot;CentOS 7初始化&quot;&gt;&lt;/a&gt;CentOS 7初始化&lt;/h1&gt;&lt;p&gt;该安装脚本为搭建研发环境常用的脚本，记录在Blog中便于查阅。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>OpenStack没落了吗？</title>
    <link href="http://sunqi.site/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/"/>
    <id>http://sunqi.site/2020/04/29/OpenStack%E8%BF%98%E7%83%AD%E5%90%97%EF%BC%9F/</id>
    <published>2020-04-29T13:20:37.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="OpenStack回顾"><a href="#OpenStack回顾" class="headerlink" title="OpenStack回顾"></a>OpenStack回顾</h1><p>OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。</p><p><img src="/images/pasted-42.png" alt="upload successful"></p><h1 id="OpenStack没落了吗？"><a href="#OpenStack没落了吗？" class="headerlink" title="OpenStack没落了吗？"></a>OpenStack没落了吗？</h1><p>我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。</p><a id="more"></a><p>随着容器、K8S等新兴技术的崛起，OpenStack无疑受到了很大的冲击，在之前两年经常看到一些唱衰OpenStack的文章。但是不可否认，目前OpenStack已经进入到了一个稳定阶段，很多私有云、专有云项目都是基于OpenStack提供解决方案。所以我认为并不存在OpenStack没落一说，只是技术发展的必经阶段：当底层逐渐稳定后，关注度往上发展。</p><p>同时，我们也看到，OpenStack基金会也在通过吸纳更多的项目来维持自身的影响力，比如：安全容器项目Kata Container，边缘计算项目StarlingX。</p><p>这是我对目前国内云计算市场的一张不完全总结，从这张图中我们应该可以很清晰的看到OpenStack对国内云计算市场深远的影响。同时，大家也能看出来谁才是真正的OpenStack这个开源项目的既得利益者。</p><p><img src="/images/pasted-44.png" alt="upload successful"></p><h1 id="OpenStack社区大数据"><a href="#OpenStack社区大数据" class="headerlink" title="OpenStack社区大数据"></a>OpenStack社区大数据</h1><p>从A版本开始到今天（2020年4月29日），总共有442家公司为OpenStack社区贡献过代码。排名前三位的分别是：Red Hat, Rackspace和Mirantis。中国唯一入选前十的是华为。</p><p><img src="/images/pasted-45.png" alt="upload successful"></p><p>OpenStack总共出现了706个Official项目，提交代码次数最多的是nova, neutron和cinder项目。</p><p><img src="/images/pasted-46.png" alt="upload successful"></p><p>总共有8523名开发者成功提交过1个以上的commits。从名字分析，前十名中有两位中国人：Zhong Shengping（麒麟云，主要贡献在自动化安装OpenStack相关项目puppet和ansible）和Qiming Teng（IBM 滕启明博士，主要贡献在senlin项目）。当然，我知道国内为OpenStack项目贡献的人很多，在这就不一一列举了。</p><p><img src="/images/pasted-47.png" alt="upload successful"></p><h1 id="OpenStack社区贡献变化趋势"><a href="#OpenStack社区贡献变化趋势" class="headerlink" title="OpenStack社区贡献变化趋势"></a>OpenStack社区贡献变化趋势</h1><p>参与贡献的公司已经呈现明显下降趋势，从国内情况来看，很多OpenStack初创公司也在积极投身K8S相关项目的研究，产品上提供基于容器的PaaS平台，丰富自己的解决方案。<br>从图中可以看到，OpenStack参与公司最多的是在Ocata Release中，参与公司达到了210家，从时间看是在2016年到2017年之间的时间点。这也是国内客户对OpenStack普遍接受的时间点。<br>另外从C版本开始一直到O版本（2011年到2017年）基本每个版本迭代维持20%以上的增长，可见在这个阶段绝大多数公司都看好OpenStack的未来。国内开源领域在这个阶段感觉也是最活跃的，毕竟只有当商业利益和开源目标相吻合时，这个开源项目才能得到最大的支持力度。<br>从O版本之后，参与的公司呈现小幅度下降趋势，不是很明显，大概在10%以内，下跌最明显的阶段是在S版本到T版本，也就是2019年。S版本有161家公司提交代码，而T版本只有126家，而目前U版本已经下降到了119家公司。</p><p><img src="/images/pasted-48.png" alt="upload successful"></p><p>从开发者数量看也呈现出相同的趋势，参与人数最多的是N版本，有2422人提交了commit。而到了S版本开发者仅为1189人，下降了一半还多。</p><p><img src="/images/pasted-49.png" alt="upload successful"></p><p>最后一张图，我们来看一下OpenStack模块数量。在早期OpenStack中一个新的项目获得批准是需要技术委员会批准的，也就是TC Approved，这样的项目到今天为止一共只有20个，主要是OpenStack基础的计算、存储和网络服务，包括：Nova, Neutron, Cinder, Heat, Horizon, Keystone, Ironic, Swift, Ceilometer, Glance, Sahara, Trove, Designate, neutron-lib, sahara的各种插件。<br>但是在2015年，社区决定采用Big Tent模式。Big Tent模式本意是基于OpenStack底层的计算、存储和网络等基础组件，构建更庞大的云原生应用场景，类似AWS。但是由于OpenStack自身部署、升级的复杂性，是社区力量更加分散，这样的设计并没有带来意料之中的效果。我个人理解，这样的生态建设更适合K8S。<br>在A版本中仅有8个模块，到了最新的T版本中，模块数量变为609个，还没有Release的U版本中，模块数量增长为627个。</p><p><img src="/images/pasted-50.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>由于OpenStack提供的服务属于基础架构层，从生态角度看，团结了各个层面的公司。从硬件的服务器、处理器、网络、存储厂商，到操作系统厂商，再到OpenStack创业公司，应用厂商，直到最终用户。<br>之前我们总说OpenStack是仅次于Linux的世界上第二大开源社区，不知道现在这种说法是否还准确。但是不可否认，OpenStack出现给了原来默默耕耘的开发者们走到前台充分展现的机会，也将国内开源的热潮推向了一个新的高度。</p>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;OpenStack回顾&quot;&gt;&lt;a href=&quot;#OpenStack回顾&quot; class=&quot;headerlink&quot; title=&quot;OpenStack回顾&quot;&gt;&lt;/a&gt;OpenStack回顾&lt;/h1&gt;&lt;p&gt;OpenStack在2010年7月由NASA和Rackspace宣布启动，2010年10月Austin Release后，除了Bexar、Cactus、Diablo版本外，后续版本都遵循6个月发布周期，如今正在开发的是Ussuri版本，最新的稳定版本是去年10月份发布的Train版本。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/images/pasted-42.png&quot; alt=&quot;upload successful&quot;&gt;&lt;/p&gt;
&lt;h1 id=&quot;OpenStack没落了吗？&quot;&gt;&lt;a href=&quot;#OpenStack没落了吗？&quot; class=&quot;headerlink&quot; title=&quot;OpenStack没落了吗？&quot;&gt;&lt;/a&gt;OpenStack没落了吗？&lt;/h1&gt;&lt;p&gt;我是从2012年初开始参与到OpenStack社区，这几年见证了OpenStack从一个开源项目逐渐成为开源产品的全过程。大概在两三年前每次发布前都会写一些关于OpenStack新版本功能和社区分析的文章，但是随着我的工作重心转移，对OpenStack社区关注逐渐减少。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[阿里云]使用DataQuotient 画像分析筛选优质基金</title>
    <link href="http://sunqi.site/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/"/>
    <id>http://sunqi.site/2020/04/14/%E9%98%BF%E9%87%8C%E4%BA%91-%E4%BD%BF%E7%94%A8DataQuotient-%E7%94%BB%E5%83%8F%E5%88%86%E6%9E%90%E7%AD%9B%E9%80%89%E4%BC%98%E8%B4%A8%E5%9F%BA%E9%87%91/</id>
    <published>2020-04-14T03:10:17.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>该教程是阿里云帮助文档一部分，这里做了进一步完善：<a href="https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK</a></p><h1 id="需求描述"><a href="#需求描述" class="headerlink" title="需求描述"></a>需求描述</h1><p>4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。</p><pre>4433法则如下：4：代表近一年收益率排名前1/4的基金。4：代表近两年、三年、五年以来，收益率排名前1/4的基金。3：指近六个月收益率排名前1/3的基金。3：指近三个月以来收益率排名前1/3的基金。</pre><p>本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。</p><a id="more"></a><h1 id="实现流程"><a href="#实现流程" class="headerlink" title="实现流程"></a>实现流程</h1><p><img src="/images/pasted-0.png" alt="upload successful"></p><h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul><li>获取基金数据，从天天基金网接口获取股票型基金的不同时间区间的收益率数据</li><li>数据输入MaxCompute</li></ul><h2 id="创建基金信息标签系统"><a href="#创建基金信息标签系统" class="headerlink" title="创建基金信息标签系统"></a>创建基金信息标签系统</h2><ul><li>新建实体，用于将数源和待分析的对象绑定在一起</li><li>绑定标签，将数据源与标签进行绑定</li></ul><h2 id="筛选并导出优质基金群体"><a href="#筛选并导出优质基金群体" class="headerlink" title="筛选并导出优质基金群体"></a>筛选并导出优质基金群体</h2><ul><li>同步标签至RDS，这个例子中因为只是从MaxCompute到RDS，真实环境中有可能从多个数据源同步至目标RDS中，该服务支持数据的合并等</li><li>根据上述预先建立好的模型，新建长期和短期表现较好群体</li><li>计算群体</li><li>导出优质基金列表</li></ul><h1 id="实现过程"><a href="#实现过程" class="headerlink" title="实现过程"></a>实现过程</h1><h2 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h2><p>本教程基于DataQuotient 画像分析、MaxCompute和RDS产品，请确保您已购买该产品。</p><h2 id="获取基础数据"><a href="#获取基础数据" class="headerlink" title="获取基础数据"></a>获取基础数据</h2><p>根据教程提供的线索，从天天基金网获取了全部股票型基金的数据，脚本已经提交到Github上，有需要的可以直接拿过去用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git clone https:&#x2F;&#x2F;github.com&#x2F;xiaoquqi&#x2F;fund</span><br><span class="line">cd fund</span><br><span class="line">python fund-cli.py -d -v</span><br></pre></td></tr></table></figure><h2 id="在MaxCompute导入数据"><a href="#在MaxCompute导入数据" class="headerlink" title="在MaxCompute导入数据"></a>在MaxCompute导入数据</h2><h3 id="开通MaxCompute服务"><a href="#开通MaxCompute服务" class="headerlink" title="开通MaxCompute服务"></a>开通MaxCompute服务</h3><p>之前我并没有开通过MaxCompute服务，所以需要开通一下DataWorks，才能使用MaxCompute服务。</p><p><img src="/images/pasted-1.png" alt="upload successful"></p><p>这个就是DataWorks控制台，从操作上看局限性很高（比如不支持删除表<br><img src="/images/pasted-2.png" alt="upload successful"></p><p>），所以建议采用IntelliJ IDEA中的MaxCompute Studio插件，安装方式见：<a href="https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/50889.htm?spm=a2c63.p38356.b99.275.5e652ea3SZgau1</a><br><img src="/images/pasted-41.png" alt="upload successful"></p><h3 id="创建MaxCompute表"><a href="#创建MaxCompute表" class="headerlink" title="创建MaxCompute表"></a>创建MaxCompute表</h3><p>由于做的时候才发现数据源并没有近五年的数据，所以修改了一下创建语句，先把表名建立起来，之后进入DDL模式进行表创建：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">create table if not exists fund_profit_stocktype</span><br><span class="line">( &#96;fundid&#96; bigint comment &#39;基金编号&#39;,</span><br><span class="line"> &#96;fundname&#96; string comment &#39;基金名称&#39;,</span><br><span class="line"> &#96;latest3months&#96; double comment &#39;近三月&#39;,</span><br><span class="line"> &#96;latest6months&#96; double comment &#39;近六月&#39;,</span><br><span class="line"> &#96;latest1year&#96; double comment &#39;近一年&#39;,</span><br><span class="line"> &#96;latest2years&#96; double comment &#39;近两年&#39;,</span><br><span class="line"> &#96;latest3years&#96; double comment &#39;近三年&#39;,</span><br><span class="line"> &#96;currentyear&#96; double comment &#39;今年来&#39;,  </span><br><span class="line"> &#96;fromcreated&#96; double comment &#39;成立来&#39;,</span><br><span class="line">) comment &#39;基金信息&#39; ;</span><br></pre></td></tr></table></figure><p><img src="/images/pasted-3.png" alt="upload successful"></p><h3 id="导入数据"><a href="#导入数据" class="headerlink" title="导入数据"></a>导入数据</h3><p>原有文档是通过DDL去创建的数据，由于我们已经将数据保存在CSV文件中，所以我们选择导入方式试一下。</p><p><img src="/images/pasted-4.png" alt="upload successful"></p><p><img src="/images/pasted-5.png" alt="upload successful"></p><p><img src="/images/pasted-7.png" alt="upload successful"></p><p>不知道什么原因，下拉菜单选择并不生效，于是我决定将原有CSV无用字段全部删除，便于后续测试。删除字段的时候，发现我获取的数据并不包含近五年的项，但是包含成立以来的数据，所以对字段进行一下修改。上面的SQL是我修改过的。页面好像并没有删除表的功能，所以为了不影响测试，我重新建了一张表来导入数据。</p><p><img src="/images/pasted-8.png" alt="upload successful"></p><h2 id="关联云计算资源"><a href="#关联云计算资源" class="headerlink" title="关联云计算资源"></a>关联云计算资源</h2><p>回到画像服务，继续进行配置</p><p><img src="/images/pasted-13.png" alt="upload successful"></p><p><img src="/images/pasted-14.png" alt="upload successful"></p><p>我用的主账号的AK/KS竟然提示我权限不足，于是我去查了半天RAM文档，以为MaxCompute需要更精确的授权，结果发现是提示信息误导了我，只是我的project写错了，最后project信息还是回到DataWorks控制台才找到，如下图所示。</p><p>需要添加这么多信息感觉不是特别方便，都是阿里云的资源感觉没有必要使用AK/KS去通讯吧？</p><p><img src="/images/pasted-15.png" alt="upload successful"></p><p><img src="/images/pasted-16.png" alt="upload successful"></p><p>配置好后的云计算资源</p><p><img src="/images/pasted-17.png" alt="upload successful"></p><h2 id="标签管理"><a href="#标签管理" class="headerlink" title="标签管理"></a>标签管理</h2><p>配置好后，可以继续进行标签配置。</p><p><img src="/images/pasted-9.png" alt="upload successful"></p><p><img src="/images/pasted-10.png" alt="upload successful"></p><h3 id="关联MaxCompute表"><a href="#关联MaxCompute表" class="headerlink" title="关联MaxCompute表"></a>关联MaxCompute表</h3><p><img src="/images/pasted-11.png" alt="upload successful"></p><p>如果你没有正确配置云计算资源，是无法看到MaxCompute下的表。<br><img src="/images/pasted-12.png" alt="upload successful"></p><p>绑定表和字段<br><img src="/images/pasted-18.png" alt="upload successful"></p><p><img src="/images/pasted-19.png" alt="upload successful"></p><p><img src="/images/pasted-20.png" alt="upload successful"></p><p>字段绑定后无法删除，需要到详情中删除<br><img src="/images/pasted-21.png" alt="upload successful"></p><p><img src="/images/pasted-22.png" alt="upload successful"></p><p>最开始显示为0，后来又显示出数据总量，但是根据上方提示，MaxCompute是不支持显示的。<br><img src="/images/pasted-23.png" alt="upload successful"></p><p>过了一会又出现了1185的数据总量，不知道为什么<br><img src="/images/pasted-33.png" alt="upload successful"></p><p><img src="/images/pasted-25.png" alt="upload successful"></p><h2 id="筛选出优质基金群体"><a href="#筛选出优质基金群体" class="headerlink" title="筛选出优质基金群体"></a>筛选出优质基金群体</h2><h3 id="同步标签至RDS"><a href="#同步标签至RDS" class="headerlink" title="同步标签至RDS"></a>同步标签至RDS</h3><p><img src="/images/pasted-24.png" alt="upload successful"></p><p><img src="/images/pasted-26.png" alt="upload successful"></p><p><img src="/images/pasted-27.png" alt="upload successful"></p><p><img src="/images/pasted-28.png" alt="upload successful"></p><p><img src="/images/pasted-29.png" alt="upload successful"></p><p>显示同步成功，但是发现输出的信息里有红色的信息，以为是错误，但是仔细一看又是INFO级别的日志，而且显示的太长了。<br><img src="/images/pasted-31.png" alt="upload successful"></p><p><img src="/images/pasted-32.png" alt="upload successful"></p><h3 id="群体画像"><a href="#群体画像" class="headerlink" title="群体画像"></a>群体画像</h3><p>需要切换至群体画像的工作空间。<br><img src="/images/pasted-30.png" alt="upload successful"></p><p>新建短期表现优质的基金。<br><img src="/images/pasted-34.png" alt="upload successful"></p><p>如果配置正确，可以从结果中看出筛选出的结果。<br><img src="/images/pasted-35.png" alt="upload successful"></p><p><img src="/images/pasted-36.png" alt="upload successful"></p><p>新建长期表现优质的基金。<br><img src="/images/pasted-37.png" alt="upload successful"></p><p>使用群体计算，找出二者的交集。这个就是我们希望得到的结果。<br><img src="/images/pasted-38.png" alt="upload successful"></p><p><img src="/images/pasted-39.png" alt="upload successful"></p><p>利用下载功能，可以下载我们筛选出来的基金。<br><img src="/images/pasted-40.png" alt="upload successful"></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>使用阿里云DataQuotient服务可以快速帮助用户构建用户画像，能够满足多种应用场景，利用函数计算进行数据爬取定期导入到MaxCompute，可以很容易定制出优质基金筛选功能的API接口，供上层业务场景使用。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该教程是阿里云帮助文档一部分，这里做了进一步完善：&lt;a href=&quot;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://help.aliyun.com/document_detail/160711.html?spm=a2c4g.11174283.6.603.682fa00bJ7AHYK&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;需求描述&quot;&gt;&lt;a href=&quot;#需求描述&quot; class=&quot;headerlink&quot; title=&quot;需求描述&quot;&gt;&lt;/a&gt;需求描述&lt;/h1&gt;&lt;p&gt;4433法则通过对同类基金（例如股票类基金）长期和短期的表现进行分析，为您在众多基金中筛选少数优质基金。&lt;/p&gt;
&lt;pre&gt;
4433法则如下：
4：代表近一年收益率排名前1/4的基金。
4：代表近两年、三年、五年以来，收益率排名前1/4的基金。
3：指近六个月收益率排名前1/3的基金。
3：指近三个月以来收益率排名前1/3的基金。
&lt;/pre&gt;

&lt;p&gt;本教程为您演示如何从当日的1126个股票类的基金产品中，筛选出符合4433法则的69条优质基金。&lt;/p&gt;
    
    </summary>
    
    
    
  </entry>
  
  <entry>
    <title>[Python]利用ZooKeeper构建分布式定时任务</title>
    <link href="http://sunqi.site/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/"/>
    <id>http://sunqi.site/2020/03/24/Python-How-to-use-zookeeper-to-build-distribution-system/</id>
    <published>2020-03-24T12:39:06.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>本文涉及的源代码路径：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz</a></p><h1 id="一、目前现状及存在的问题"><a href="#一、目前现状及存在的问题" class="headerlink" title="一、目前现状及存在的问题"></a>一、目前现状及存在的问题</h1><p>在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。</p><img src="/images/blogs/2020-03-24/1.png" class=""><a id="more"></a><h2 id="软件架构设计"><a href="#软件架构设计" class="headerlink" title="软件架构设计"></a>软件架构设计</h2><p>目前云迁移平台的各个服务模块在设计上使用了OpenStack方式，即大部分模块复用了类似Nova的实现框架。即API层直接集成oslo.service中定义好的WSGI Service基类，Worker采用了olso.service中定义好的Service基类，即Eventlet协程方式，API与Worker通讯使用RabbitMQ，API南向接口除少量直接更新数据库操作采用同步接口外，其余所有接口全部使用异步方式。API发送请求后，得到202 Accepted回复，后续通过GET接口不断轮询任务接口等到任务完成。</p><h2 id="高可靠部署"><a href="#高可靠部署" class="headerlink" title="高可靠部署"></a>高可靠部署</h2><p>根据OpenStack官方的HA部署文档（<a href="https://docs.openstack.org/ha-guide/" target="_blank" rel="noopener">https://docs.openstack.org/ha-guide/</a>），将服务分为无状态和有状态两种。无服务状态只需要直接部署多份即可，有状态服务往往需要通过Pacemaker控制副本数量，来保证高可靠。在云迁移平台部署中，我们将全部服务部署于K8S集群中，所以并不需要Pacemaker+Corosync这样的组件（Pacemaker节点上线为16）。但是，由于需要保持定时任务在单一节点被触发（避免任务被重复执行），所以承载定时快照的模块只能同时存在一个容器在运行，无法构成Active-Active（简称AA方式）模式。这样的部署方式，也造成了上述提到的AP模式对扩展性的瓶颈。</p><h1 id="二、问题思路及解决方案"><a href="#二、问题思路及解决方案" class="headerlink" title="二、问题思路及解决方案"></a>二、问题思路及解决方案</h1><h2 id="思路一、利用消息队列解耦任务分配与任务执行"><a href="#思路一、利用消息队列解耦任务分配与任务执行" class="headerlink" title="思路一、利用消息队列解耦任务分配与任务执行"></a>思路一、利用消息队列解耦任务分配与任务执行</h2><p>从上述对现状的描述，我们不难看出，现有任务分配与任务执行是在同一个任务中执行的，当存在大量任务时，任务执行会对任务产生产生很大的影响。同时，由于任务执行唯一性的需要，在部署上只能采用上述的AP模式，导致任务无法由多个任务同时执行。<br />所以，我们可以将任务分解为分配和执行两个阶段。任务分配上，单纯的进行任务生成，由于任务生成相对较快，生成后的任务发送至消息队列，由无状态性的Worker接收后执行。这样就解决了单点执行的效率低下问题。<br />但是这样的解决方案仍然存在缺陷，我们在任务生成的模块仍然必须需要采用AP模式部署，来保证任务的唯一性。如果在任务数量非常庞大时，该部分仍然是一个瓶颈；另外一方面这样的实现方式，我们需要将任务生成部分单独拆分出一个模块，同时增加了开发和部署上的复杂度，所以我们来看一下第二种解决思路。</p><img src="/images/blogs/2020-03-24/2.png" class=""><h2 id="思路二、利用Zookeeper构建可扩展的分布式定时任务"><a href="#思路二、利用Zookeeper构建可扩展的分布式定时任务" class="headerlink" title="思路二、利用Zookeeper构建可扩展的分布式定时任务"></a>思路二、利用Zookeeper构建可扩展的分布式定时任务</h2><p>为了解决思路一的局限性，我们本质上要解决的是任务执行的分布式问题，即如何让Worker不重复的判定任务的归属后再执行，由被动改为主动。</p><p>我们来看以下几种场景：<br />1、假定我们现在有3个Worker可以用于任务生成，在某一个时间点，将同时产生100个任务。如何由这3个Worker主动产生属于自身负责的任务？<br />2、我们知道大部分云平台目前都有云原生的弹性扩展服务，如果我们结合云平台的弹性扩展服务自动将我们用于任务生成的Worker动态进行调整时，例如变为6个时，还能保证这100个任务能够被自动的由6个节点不重复的产生呢？<br />3、当负载降低后，节点数量由6个变为3个后，如何恢复场景1的状态呢？保证任务不漏生成呢？</p><img src="/images/blogs/2020-03-24/3.png" class=""><p>如果想达到以上场景需求，需要以下几个条件：<br />1、节点之间能够准确知道其他节点的存在——利用Zookeeper进行服务发现<br />2、尽量合理的进行任务（对象）分布，同时兼顾节点增加和减少时，降低对象分配时的位移——利用一致性哈希环</p><h1 id="三、技术要点"><a href="#三、技术要点" class="headerlink" title="三、技术要点"></a>三、技术要点</h1><h2 id="1、Zookeeper"><a href="#1、Zookeeper" class="headerlink" title="1、Zookeeper"></a>1、Zookeeper</h2><p>对于Zookeeper的解释网络上有各种各样的详细集成，这里就不再赘述了，这里我直接引用了这篇文章（<a href="https://www.jianshu.com/p/50becf121c66）中开头的内容：" target="_blank" rel="noopener">https://www.jianshu.com/p/50becf121c66）中开头的内容：</a></p><blockquote><p>官方文档上这么解释zookeeper，它是一个分布式服务框架，是Apache Hadoop 的一个子项目，它主要是用来解决分布式应用中经常遇到的一些数据管理问题，如：统一命名服务、状态同步服务、集群管理、分布式应用配置项的管理等。<br>上面的解释有点抽象，简单来说zookeeper=文件系统+监听通知机制。</p></blockquote><img src="/images/blogs/2020-03-24/4.png" class=""><p>从我们应用场景的角度看，Zookeeper帮我们解决了Worker之间相互认识的过程，及时、准确的告诉我们：到底现在有多少个和我相同的活跃节点存在。至于底层是如何实现的，感兴趣的同学可以查看具体的Zookeeper实现原理文档，这里只介绍与我们实现相关的内容。</p><h2 id="2、一致性Hash"><a href="#2、一致性Hash" class="headerlink" title="2、一致性Hash"></a>2、一致性Hash</h2><p>又是一个经典的算法，相关的文章也很多，这里推荐大家几篇，这里摘抄出对理解我们实现有价值的内容。 参考文档：</p><ul><li>《面试必备：什么是一致性Hash算法？》<a href="https://zhuanlan.zhihu.com/p/34985026" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/34985026</a></li><li>《五分钟看懂一致性哈希算法》<a href="https://juejin.im/post/5ae1476ef265da0b8d419ef2" target="_blank" rel="noopener">https://juejin.im/post/5ae1476ef265da0b8d419ef2</a></li><li>《一致性hash在分布式系统中的应用》<a href="http://www.firefoxbug.com/index.php/archives/2791/" target="_blank" rel="noopener">http://www.firefoxbug.com/index.php/archives/2791/</a></li></ul><h3 id="2-1-关于一致性哈希算法"><a href="#2-1-关于一致性哈希算法" class="headerlink" title="2.1 关于一致性哈希算法"></a>2.1 关于一致性哈希算法</h3><blockquote><p>一致性哈希算法在1997年由麻省理工学院的Karger等人在解决分布式Cache中提出的，设计目标是为了解决因特网中的热点(Hot spot)问题，初衷和CARP十分类似。一致性哈希修正了CARP使用的简单哈希算法带来的问题，使得DHT可以在P2P环境中真正得到应用。但现在一致性hash算法在分布式系统中也得到了广泛应用。</p></blockquote><h3 id="2-2-一致性哈希算法在缓存技术中的应用"><a href="#2-2-一致性哈希算法在缓存技术中的应用" class="headerlink" title="2.2 一致性哈希算法在缓存技术中的应用"></a>2.2 一致性哈希算法在缓存技术中的应用</h3><img src="/images/blogs/2020-03-24/5.png" class=""><blockquote><p>上述的方式虽然提升了性能，我们不再需要对整个Redis服务器进行遍历！但是，使用上述Hash算法进行缓存时，会出现一些缺陷，主要体现在服务器数量变动的时候，所有缓存的位置都要发生改变！<br>试想一下，如果4台缓存服务器已经不能满足我们的缓存需求，那么我们应该怎么做呢？很简单，多增加几台缓存服务器不就行了！假设：我们增加了一台缓存服务器，那么缓存服务器的数量就由4台变成了5台。那么原本hash(a.png) % 4 = 2 的公式就变成了hash(a.png) % 5 = ？ ， 可想而知这个结果肯定不是2的，这种情况带来的结果就是当服务器数量变动时，所有缓存的位置都要发生改变！换句话说，当服务器数量发生改变时，所有缓存在一定时间内是失效的，当应用无法从缓存中获取数据时，则会向后端数据库请求数据（还记得上一篇的《缓存雪崩》吗？）！<br>同样的，假设4台缓存中突然有一台缓存服务器出现了故障，无法进行缓存，那么我们则需要将故障机器移除，但是如果移除了一台缓存服务器，那么缓存服务器数量从4台变为3台，也是会出现上述的问题！<br>所以，我们应该想办法不让这种情况发生，但是由于上述Hash算法本身的缘故，使用取模法进行缓存时，这种情况是无法避免的，为了解决这些问题，Hash一致性算法（一致性Hash算法）诞生了！</p></blockquote><h3 id="2-3-一致性哈希在缓存中的应用"><a href="#2-3-一致性哈希在缓存中的应用" class="headerlink" title="2.3 一致性哈希在缓存中的应用"></a>2.3 一致性哈希在缓存中的应用</h3><p>初始状态，将节点映射到哈希环中</p><img src="/images/blogs/2020-03-24/6.png" class=""><p>将对象映射到换后，找到负责处理的Node节点。</p><img src="/images/blogs/2020-03-24/7.png" class=""><p>容错性，Node C出现故障后，只需要将Object C迁移到Node D上。</p><img src="/images/blogs/2020-03-24/8.png" class=""><p>增加节点，此时增加了Node X，在Node C右侧，那么此时只有Object C需要移动到Node X节点。</p><img src="/images/blogs/2020-03-24/9.png" class=""><h2 id="3、tooz和kazoo"><a href="#3、tooz和kazoo" class="headerlink" title="3、tooz和kazoo"></a>3、tooz和kazoo</h2><p>Python中操作zookeeper的项目叫kazoo（<a href="https://kazoo.readthedocs.io/en/latest/" target="_blank" rel="noopener">https://kazoo.readthedocs.io/en/latest/</a>）。<br />tooz是OpenStack中为简化开发人员操作分布式系统一致性所开发的组件，利用底层组件抽象出一致性组成员管理、分布式锁、选举、构建哈希环等。tooz除支持zookeeper作为后端，还可以支持Memcached、Redis、IPC、File、PostgreSQL、MySQL、Etcd、Consul等。<br />有关于tooz的发展历史可以参考：<a href="https://julien.danjou.info/python-distributed-membership-lock-with-tooz/" target="_blank" rel="noopener">https://julien.danjou.info/python-distributed-membership-lock-with-tooz/</a><br />这里我们主要使用tooz操作zookeeper实现我们的一致性组及一致性哈希。</p><h2 id="4、oslo相关项目"><a href="#4、oslo相关项目" class="headerlink" title="4、oslo相关项目"></a>4、oslo相关项目</h2><p>这几年一直在做OpenStack项目，从OpenStack项目中学习到很多设计、架构、研发管理等各种新知识、新理念。oslo项目就是在OpenStack不断的迭代中产生的公共项目库，这些库可以让你非常轻松的构建基于Python的构建近似于OpenStack的分布式、可扩展的微服务系统。<br />之前在从事OpenStack开发培训过程中，有专门的一节课去讲解OpenStack中用到的公共库，其中oslo相关项目就是非常重要的一部分内容。olso项目设计的库非常多，在这个内容中会涉及到oslo.config、oslo.log、oslo.service、oslo.utils和oslo.messaging项目。严格意义上来说，为了更精准控制任务，我们还应该引入oslo.db项目由数据库持久化的维护任务运行状态，包括任务回收等工作，但是本次内容主要讲解的是zookeeper，所以这部分的内容需要开发者在实际项目中去实现。<br />关于olso开发的内容，我会以视频课程的形式为大家讲解，敬请期待。</p><h1 id="四、实现过程"><a href="#四、实现过程" class="headerlink" title="四、实现过程"></a>四、实现过程</h1><h2 id="1、Zookeeper部署"><a href="#1、Zookeeper部署" class="headerlink" title="1、Zookeeper部署"></a>1、Zookeeper部署</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker-compose -f zookeeper.yml -d up</span><br></pre></td></tr></table></figure><p>启动完成后，将使用本地的三个容器作为zookeeper的三个节点和三个不同的端口（2181/2182/2183）便于zookeeper连接。如果在生产环境中部署时，可以使用云原生服务或部署在多个可用区的方式，保证高可靠。</p><img src="/images/blogs/2020-03-24/10.png" class=""><h3 id="Zookeeper常用命令行"><a href="#Zookeeper常用命令行" class="headerlink" title="Zookeeper常用命令行"></a>Zookeeper常用命令行</h3><p>进入容器，就可以使用zkCli.sh进入zookeeper的CLI模式。如果是初次接触zookeeper，可以把zookeeper理解成一个文件系统，这里我们常用的命令就是ls。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker <span class="built_in">exec</span> -it zookeeper_zoo1_1 bash</span><br><span class="line"><span class="built_in">cd</span> bin</span><br><span class="line">zkCli.sh</span><br></pre></td></tr></table></figure><p>看到这样的提示，就表示连接成功了。</p><img src="/images/blogs/2020-03-24/11.png" class=""><p>如上面提到的zookeeper的存储结构所示，我们先从根节点（/）进行获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/12.png" class=""><p>这里zookeeper目录属于保留的目录，我们来看一下tooz的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz</span><br></pre></td></tr></table></figure><p>此时返回</p><img src="/images/blogs/2020-03-24/13.png" class=""><p>如果我们想继续查看distribution_tasks的内容，可以继续使用ls命令获取。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ls /tooz/distribution_tasks</span><br></pre></td></tr></table></figure><p>通常我们会为每一个加入的节点取一个唯一的标识，当节点加入后我们使用ls命令就可以看到，如果离开了，则返回为空。</p><img src="/images/blogs/2020-03-24/14.png" class=""><p>zookeeper常用的命令还包括get，stat等获取value和更详细的信息，还包含更新节点操作set和删除节点rm。这里面就不做一一介绍了，我们直接操作zookeeper主要是为了帮助大家更好的理解程序逻辑。<br />具体的命令行信息可以参考：<a href="https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm" target="_blank" rel="noopener">https://www.tutorialspoint.com/zookeeper/zookeeper_cli.htm</a></p><h2 id="2、tooz基本使用方法"><a href="#2、tooz基本使用方法" class="headerlink" title="2、tooz基本使用方法"></a>2、tooz基本使用方法</h2><p>关于tooz的两个示例主要来自于这篇博客：<a href="https://dzone.com/articles/scaling-a-polling-python-application-with-tooz" target="_blank" rel="noopener">https://dzone.com/articles/scaling-a-polling-python-application-with-tooz</a><br />原文中的例子是有些Bug的，这里面进行重新进行了优化和整理，并且使用zookeeper替代etcd3驱动。</p><h3 id="2-1-组成员（tooz-test-tooz-test-group-members-py）"><a href="#2-1-组成员（tooz-test-tooz-test-group-members-py）" class="headerlink" title="2.1 组成员（tooz/test_tooz/test_group_members.py）"></a>2.1 组成员（tooz/test_tooz/test_group_members.py）</h3><p>在这个例子中，我们主要为大家演示tooz如何进行组成员的管理。结合我们自身的需求，这里的成员就是每一个Worker。通过这个列子我们将观察三种不同场景的变化：<br />1、初始状态下，我们只能看到一个成员；<br />2、当启动了一个新的进程时，第一个成员马上会发现有第二个成员的加入；<br />3、同时，当我们用CTRL + C结束某一个进程时，另外一个活着的进程会立即发现组成员的变化。</p><h4 id="时序图"><a href="#时序图" class="headerlink" title="时序图"></a>时序图</h4><p>这里为了更直观表达，用时序图来说明程序的运行逻辑。</p><img src="/images/blogs/2020-03-24/15.png" class=""><h4 id="完整的代码"><a href="#完整的代码" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create the group</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    c.create_group(group).get()</span><br><span class="line"><span class="keyword">except</span> coordination.GroupAlreadyExist:</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the group</span></span><br><span class="line">c.join_group(group).get()</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># Print the members list</span></span><br><span class="line">        <span class="comment">#c.run_watchers()</span></span><br><span class="line">        members = c.get_members(group).get()</span><br><span class="line">        print(<span class="string">"[%s]Current nodes in cluster: %s"</span> % (</span><br><span class="line">            current_time(), members))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line">    print(<span class="string">"[%s]After leave cluster nodes: %s"</span> % (</span><br><span class="line">        current_time(), c.get_members(group).get()))</span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果"><a href="#执行效果" class="headerlink" title="执行效果"></a>执行效果</h4><p>第一个成员</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_group_members.py client1 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/16.png" class=""><p>第二个成员加入，观察第一个成员的标准输出，为了观察加入集群的时间，我们加入了date</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">date &amp;&amp; python test_group_members.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/17.png" class=""><p>第一个脚本的标准输出，在16:07:27秒的时候加入了集群:</p><img src="/images/blogs/2020-03-24/18.png" class=""><p>将第二个成员关闭，直接在第二个成员脚本按CTRL + C，首先观察第二个成员的输出：</p><img src="/images/blogs/2020-03-24/19.png" class=""><p>第一个成员的输出，在16:08:51分时，集群中已经没有了第二个成员了：</p><img src="/images/blogs/2020-03-24/20.png" class=""><h3 id="2-2-一致性哈希（tooz-test-tooz-test-ping-py）"><a href="#2-2-一致性哈希（tooz-test-tooz-test-ping-py）" class="headerlink" title="2.2 一致性哈希（tooz/test_tooz/test_ping.py）"></a>2.2 一致性哈希（tooz/test_tooz/test_ping.py）</h3><p>这个模拟测试中，使用分布式任务去ping某一个C类网段(255个IP地址)中的全部IP地址，如果由一个任务去完成，那么只能顺序执行，无法满足并发需求，这里采用一致性哈希算法，让任务分布在各个Worker上。为了节省时间，我们将原有程序中的实际ping换成了time.sleep等待方式。<br />另外在程序启动后，我们默认等待10秒等待其他成员(member)加入，在实际开发过程中，还需要对任务的状态进行严格控制，防止同一任务重复被执行，在演示代码中主要偏重演示分布式，所以并没有在任务状态上增加过多处理。</p><h4 id="时序图-1"><a href="#时序图-1" class="headerlink" title="时序图"></a>时序图</h4><img src="/images/blogs/2020-03-24/21.png" class=""><p>代码需要说明的几点：<br />0、在程序开始时，我们默认等待了10秒，等待其他节点加入，如果在循环开始后，再有新加入的节点时，由于并不知道第一个节点已经处理过的任务，所以在第二个Worker加入后根据当时哈希环对之前的任务重新分配并执行，造成了重复执行，这个问题需要通过额外的手段（例如数据库记录先前执行的任务状态）监控任务状态来防止任务重新执行。<br />1、代码中使用了tooz内置的Hash环，但是也可以在外部自己构建哈希环，我们在后续最终的例子中还是采用了外部构建哈希环的方法。<br />2、Tooz partitioner依赖于watchers，所以在每次循环的时候必须要调用run_watchers即使获取成员的加入和离开。<br />3、无论是group还是member在变量传递时都要变成bytes类型，这样可以确保对象的唯一性，所以在代码处理上都用到了encode()方法。<br />4、<strong>tooz_hash</strong>方法需要在使用Partition时自己实现，能够唯一标识出对象的方法，例如ID、名称等信息。</p><h4 id="完整的代码-1"><a href="#完整的代码-1" class="headerlink" title="完整的代码"></a>完整的代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> datetime <span class="keyword">import</span> datetime</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> subprocess</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> tooz <span class="keyword">import</span> coordination</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">current_time</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">return</span> datetime.now().strftime(<span class="string">"%Y-%m-%d %H:%M:%S"</span>)</span><br><span class="line"></span><br><span class="line">ZOOKEEPER_URL = <span class="string">"zookeeper://localhost:2181"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Check that a client and group ids are passed as arguments</span></span><br><span class="line"><span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">    print(<span class="string">"Usage: %s &lt;client id&gt; &lt;group id&gt;"</span> % sys.argv[<span class="number">0</span>])</span><br><span class="line">    sys.exit(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the Coordinator object</span></span><br><span class="line">c = coordination.get_coordinator(ZOOKEEPER_URL, sys.argv[<span class="number">1</span>].encode())</span><br><span class="line"></span><br><span class="line"><span class="comment"># Start it (initiate connection).</span></span><br><span class="line">c.start(start_heart=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">group = sys.argv[<span class="number">2</span>].encode()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Join the partitioned group</span></span><br><span class="line">p = c.join_partitioned_group(group)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Host</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hostname)</span>:</span></span><br><span class="line">        self.hostname = hostname</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__tooz_hash__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Returns a unique byte identifier so Tooz</span></span><br><span class="line"><span class="string">           can distribute this object."""</span></span><br><span class="line">        <span class="keyword">return</span> self.hostname.encode()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"&lt;%s: %s&gt;"</span> % (self.__class__.__name__, self.hostname)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">ping</span><span class="params">(self)</span>:</span></span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">hosts_to_ping = [Host(<span class="string">"192.168.10.%d"</span> % i) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20</span>)]</span><br><span class="line"></span><br><span class="line">print(<span class="string">"[%s]Waiting 10 seconds for other members..."</span> % current_time())</span><br><span class="line">time.sleep(<span class="number">10</span>)</span><br><span class="line">print(<span class="string">"[%s]Current members: %s"</span> % (</span><br><span class="line">    current_time(), c.get_members(group).get()))</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">for</span> host <span class="keyword">in</span> hosts_to_ping:</span><br><span class="line">            c.run_watchers()</span><br><span class="line">            <span class="keyword">if</span> p.belongs_to_self(host):</span><br><span class="line">                print(<span class="string">"[%s]%s belongs to %s"</span> % (</span><br><span class="line">                    current_time(), host, p.members_for_object(host)))</span><br><span class="line">                <span class="keyword">if</span> host.ping():</span><br><span class="line">                    <span class="keyword">pass</span></span><br><span class="line">        print(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">        print(<span class="string">"Waiting for next loop..."</span>)</span><br><span class="line">        time.sleep(<span class="number">10</span>)</span><br><span class="line"><span class="keyword">except</span> KeyboardInterrupt <span class="keyword">as</span> e:</span><br><span class="line">    print(<span class="string">"CTRL C is pressed!"</span>)</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">    <span class="comment"># Leave the group</span></span><br><span class="line">    c.leave_group(group).get()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Stop when we're done</span></span><br><span class="line">    c.stop()</span><br></pre></td></tr></table></figure><h4 id="执行效果-1"><a href="#执行效果-1" class="headerlink" title="执行效果"></a>执行效果</h4><p>我们分别使用两个不同的窗口，同时启动两个Worker，我们可以很明显的看到主机被分配到两个不同的Worker中。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client1 group1</span><br><span class="line">python test_pring.py client2 group1</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/22.png" class=""><p>加入第三个Worker，可以看到一部分任务又被分配给了第三个Worker上</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_ping.py client3 group2</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/23.png" class=""><h2 id=""><a href="#" class="headerlink" title=""></a></h2><p>暂停第二个Worker，我们看到第二个Worker被停止后，任务重新被平衡到Worker1和Worker2上。</p><img src="/images/blogs/2020-03-24/24.png" class=""><h2 id="3、构建分布式定时任务"><a href="#3、构建分布式定时任务" class="headerlink" title="3、构建分布式定时任务"></a>3、构建分布式定时任务</h2><p>为了保持代码的兼容性，所以这里的实现是基于目前OpenStack体系的实现。另外，将任务发送给消息的部分在这个例子中并没有体现。示例代码仍然重复实现上述ping的例子，部分代码参考于Sahara项目的实现。</p><p>由于代码量较大，这里不贴出全部代码，仅仅对核心实现进行分析，完整代码请参考：<a href="https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks" target="_blank" rel="noopener">https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz/distribute_periodic_tasks</a></p><h3 id="代码结构"><a href="#代码结构" class="headerlink" title="代码结构"></a>代码结构</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">.</span><br><span class="line">├── coordinator.py -&gt; 一致性哈希的实现，该类中并没有直接使用上述tooz的partition，而是自己重新实现了HashRing</span><br><span class="line">├── periodic.py -&gt; 定时任务，基于oslo_service的PeriodicTasks基类</span><br><span class="line">├── service.py -&gt; Service类，继承于oslo.service的Service基类</span><br><span class="line">└── test_periodic_task.py -&gt; 程序入口</span><br></pre></td></tr></table></figure><h3 id="coordinator-py"><a href="#coordinator-py" class="headerlink" title="coordinator.py"></a>coordinator.py</h3><p>Coordinator是关键实现，所以这里重点对该类进行解释，在period task中需要调用coordinator即可实现分布式触发定时任务。</p><p>在coordinator.py中共实现了两个类，Coordinator和HashRing。<br />1、Coordinator类主要是针对tooz中对group members相关操作的封装，类似我们在tooz中的第一个例子；<br />2、HashRing是继承于Coordinator类，在功能上接近于tooz中Hash和Partition的实现，但是更简洁，tooz构建HashRing的用的PartitionNumber是32(2^5)，而我们用的是40，更大的数字会带来更均匀的分布但是会导致构建成本增加<br />3、HashRing中最重要的方法就是get_subset，通过映射到HashRing上的ID来判断Object的归属Worker</p><img src="/images/blogs/2020-03-24/25.png" class=""><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HashRing</span><span class="params">(Coordinator)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, backend_url, group_id)</span>:</span></span><br><span class="line">        self.group_id = group_id</span><br><span class="line">        self.replicas = CONF.hash_ring_replicas_count</span><br><span class="line">        super(HashRing, self).__init__(backend_url)</span><br><span class="line">        self.join_group(group_id)</span><br><span class="line"></span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_hash</span><span class="params">(key)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> int(</span><br><span class="line">            hashlib.md5(str(key).encode(<span class="string">'utf-8'</span>)).hexdigest(), <span class="number">16</span>)  <span class="comment"># nosec</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_build_ring</span><span class="params">(self)</span>:</span></span><br><span class="line">        ring = &#123;&#125;</span><br><span class="line">        members = self.get_members(self.group_id)</span><br><span class="line">        LOG.info(<span class="string">"Coordinator members: %s"</span> % members)</span><br><span class="line">        <span class="keyword">for</span> member <span class="keyword">in</span> members:</span><br><span class="line">            <span class="keyword">for</span> r <span class="keyword">in</span> range(self.replicas):</span><br><span class="line">                hashed_key = self._hash(<span class="string">'%s:%s'</span> % (member, r))</span><br><span class="line">                ring[hashed_key] = member</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> ring, sorted(ring.keys())</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_check_object</span><span class="params">(self, object, ring, sorted_keys)</span>:</span></span><br><span class="line">        <span class="string">"""Checks if this object belongs to this member or not"""</span></span><br><span class="line">        hashed_key = self._hash(object.id)</span><br><span class="line">        position = bisect.bisect(sorted_keys, hashed_key)</span><br><span class="line">        position = position <span class="keyword">if</span> position &lt; len(sorted_keys) <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ring[sorted_keys[position]] == self.member_id</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_subset</span><span class="params">(self, objects)</span>:</span></span><br><span class="line">        <span class="string">"""Returns subset that belongs to this member"""</span></span><br><span class="line">        <span class="keyword">if</span> self.coordinator:</span><br><span class="line">            ring, keys = self._build_ring()</span><br><span class="line">            <span class="keyword">if</span> ring:</span><br><span class="line">                <span class="keyword">return</span> [obj <span class="keyword">for</span> obj <span class="keyword">in</span> objects <span class="keyword">if</span> self._check_object(</span><br><span class="line">                    obj, ring, keys)]</span><br><span class="line">            <span class="keyword">return</span> []</span><br><span class="line">        <span class="keyword">return</span> objects</span><br></pre></td></tr></table></figure><h3 id="运行效果"><a href="#运行效果" class="headerlink" title="运行效果"></a>运行效果</h3><p>分别在两个Terminal中运行脚本，可以看到Host被均匀的分布在两个Worker中执行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python test_periodic_task.py</span><br></pre></td></tr></table></figure><img src="/images/blogs/2020-03-24/26.png" class=""><h1 id="五、总结"><a href="#五、总结" class="headerlink" title="五、总结"></a>五、总结</h1><p>通过以上实例，我们了解了如何通过zookeeper构建分布式系统并进行任务调度，当然zookeeper在分布式系统还有更多的应用场景值得我们去学习。另外，OpenStack中很多抽象出来的模块对快速构建Python分布式系统是非常有帮助的，值得我们学习。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本文涉及的源代码路径：&lt;a href=&quot;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/xiaoquqi/openstackclient-demo/tree/master/tooz&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&quot;一、目前现状及存在的问题&quot;&gt;&lt;a href=&quot;#一、目前现状及存在的问题&quot; class=&quot;headerlink&quot; title=&quot;一、目前现状及存在的问题&quot;&gt;&lt;/a&gt;一、目前现状及存在的问题&lt;/h1&gt;&lt;p&gt;在实际业务系统中，经常有需要定时执行的任务，例如任务状态的定时更新、定时发送状态信息等。在我们的云迁移产品中，允许用户可以设定周期同步规则，定期执行数据同步并调用云平台接口执行快照操作。在单机版本中，通常在同一时间点并发任务量较少的情况下，问题并不是很突出，但是随着我们将云迁移服务从单机版本改造为平台版本后，当多个用户的多台主机同时触发快照任务时，一方面传统的设计方式就成为了瓶颈，无法保证用户的同步任务在同一时间点被触发（需要排队）；另外一方面，目前Active-Passive（简称AP方式）的高可靠部署方式无法利用集群横向扩展能力，无法满足高并发的要求。&lt;/p&gt;
&lt;img src=&quot;/images/blogs/2020-03-24/1.png&quot; class=&quot;&quot;&gt;
    
    </summary>
    
    
    
      <category term="Zookeeper" scheme="http://sunqi.site/tags/Zookeeper/"/>
    
      <category term="微服务" scheme="http://sunqi.site/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/"/>
    
      <category term="分布式" scheme="http://sunqi.site/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"/>
    
      <category term="Python" scheme="http://sunqi.site/tags/Python/"/>
    
      <category term="tooz" scheme="http://sunqi.site/tags/tooz/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architecture Associate Practice</title>
    <link href="http://sunqi.site/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/"/>
    <id>http://sunqi.site/2020/01/30/AWS-Certified-Solutions-Architecture-Associate-Practice/</id>
    <published>2020-01-30T13:35:00.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。</p><p>总得分: 76%<br>主题得分:<br>1.0. Design Resilient Architectures 89%<br>2.0. Define Performant Architectures 71%<br>3.0. Specify Secure Applications and Architectures 50%<br>4.0. Design Cost-Optimized Architectures 100%<br>5.0. Define Operationally Excellent Architectures 100%</p><p>个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。</p><a id="more"></a><h1 id="您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"><a href="#您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。" class="headerlink" title="(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。"></a>(*)您在us-west-2中运行一个应用程序，它需要始终运行6个EC2实例。</h1><p>该区域有三个可用区（us-west-2a，us-west-2b和us-west-2c)可以使用，如果us-west-2中的任何可用区变得不可用，以下哪种部署可以提供容错功能？（选择两顶◊)<br>A. 在us-west-2a中有2个EC2实例，在us-west-2b中有2个EC2实例，在us-west-2c中有2个EC2实例<br>B. 在 us- west- 2 a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中没有EC2实例<br>C. 在us-west-2a中有4个 EC2实例，在us-west-2b中有2个 EC2实例，在us-west-2c中有2个 EC2实例<br>D. 在 us- west- 2 a中有6个 EC2实例，在us-west-2b中有6个 EC2实例，在us-west-2c中没有EC2实例<br>E. 在us-west-2a中有3个 EC2实例，在us-west-2b中有3个 EC2实例，在us-west-2c中有3个 EC2实例</p><p>Answer: DE</p><p>该道题的重点是始终运行6个EC2实例，所以当一个区Down掉，仍然能保证有6台实例的答案为正确答案。</p><h1 id="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处"><a href="#一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon-EC2-Amazon-RDS，DynamoDB和Amazon-VPC-的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。-哪种服务会立即为组织带来好处" class="headerlink" title="一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?"></a>一家咨询公司反复使用来自很多AMS服务（包括IAM，Amazon EC2, Amazon RDS，DynamoDB和Amazon VPC)的AMS资源为客户构建大型标准化架构。顾问们有每个架构的架构图，但让他们感到沮丧的是，无法使用这些架构图自动创建资源。 哪种服务会立即为组织带来好处?</h1><p>A. Elastic Beanstalk<br>B. CloudFormation<br>C. AMS CodeBuild<br>D. AMS CodeDeploy</p><p>Answer: B</p><h1 id="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"><a href="#解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon-Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。" class="headerlink" title="解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。"></a>解决方案架构师正在设计一种解决方案以存储和存档公司文档，并确定Amazon Glacier是正确的解决方案。必须在发出检索请求后的10分钟内提供数据。</h1><p>Amazon Glacier中的哪种功能可以帮助满足该要求？<br>A. 文件库锁定<br>B. 加速检索<br>c. 批量检索<br>D. 标准检索</p><p>Answer: B</p><blockquote><p>问：如何从该服务检索数据？</p><p>当您请求从 S3 Glacier 检索数据时，即表示您启动了一个存档检索作业。当检索作业完成后，您的数据将在 24 小时内可供下载或通过 Amazon Elastic Compute Cloud (Amazon EC2) 访问。有三种方式可以检索数据，每种具有不同的访问时间和成本：加急、标准和批量检索。</p><p>问：什么是加急检索？</p><p>当您偶尔需要加急请求档案子集时，可以使用加急检索来快速访问您的数据。除了最大的存档 (250MB+) 以外，对于使用加急检索方式访问的所有数据，通常在 1-5 分钟内即可使用。有两种加急检索：按需和预置。当我们可以在 1-5 分钟内完成检索时，就可以实施按需检索。所提供的请求将确保在您需要时能够获得加急检索的能力。</p></blockquote><h1 id="一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"><a href="#一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。" class="headerlink" title="(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。"></a>(*)一个组织的安全策略要求应用程序在写入到磁盘之前加密数据。</h1><p>该组织应使用哪种解决方案以满足该要求？<br>A. AMS KMS API<br>B. AMS Certificate Manager<br>C. 具有 STS 的 API Gateway<br>D. IAM访问密钥</p><p>Answer: A</p><blockquote><p>问：什么是 Amazon EBS 加密？</p><p>Amazon EBS 加密提供 EBS 数据卷、引导卷和快照的无缝加密，无需构建和维护安全密钥管理基础设施。EBS 加密可使用 Amazon 托管的密钥或您使用 AWS Key Management Service (KMS) 创建和管理的密钥来给您的数据加密，从而保障静态数据的安全性。加密还发生在托管 EC2 实例的服务器上，当数据在 EC2 实例和 EBS 存储之间移动时提供数据加密。有关详细信息，请参阅 Amazon EC2 用户指南中的“Amazon EBS”加密。</p><p>问：什么是 AWS Key Management Service (KMS)？</p><p>AWS KMS 是一项托管服务，可让您轻松创建和控制加密数据所用的加密密钥。AWS Key Management Service 可与其他 AWS 服务集成，包括 Amazon EBS、Amazon S3 和 Amazon Redshift，可让您轻松使用您管理的加密密钥加密您的数据。AWS Key Management Service 还能与 AWS CloudTrail 集成，从而为您提供所有密钥的使用记录，帮助您满足监管和合规性要求。要了解有关 KMS 的更多信息，请访问 AWS Key Management Service 产品页面。</p></blockquote><h1 id="一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"><a href="#一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon-Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。" class="headerlink" title="(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。"></a>(*)一家零售商每天将其交易数据库中的数据导出到S3存储捅中。该零售商的数据仓库团队希望将这些数据导入到VPC中的现有Amazon Redshift群集◊公司安全策略规定只能在VPC中传输这些数据。</h1><p>以下哪种步骤组合满足安全策略要求？（选择两顶)<br>A. 启用 Amazon Redshift 增强 VPC 路由。<br>B. 创建集群安全组以允许Amazon Redshift集群访问Amazon S3<br>C. 在公有子网中创建NAT网关以允许Amazon Redshift集群访问Amazon S3<br>D. 创建并配置Amazon S3 VPC终端节点<br>E. 在私有子网中设置NAT网关以允许Amazon Redshift集群访问AmazonS3</p><p>Answer: AD</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/redshift/latest/mgmt/enhanced-vpc-working-with-endpoints.html</a></p><p>使用 VPC 终端节点<br>可以使用 VPC 终端节点创建 VPC 中的 Amazon Redshift 集群与 Amazon Simple Storage Service (Amazon S3) 之间的托管连接。在执行此操作时，您的集群与 Amazon S3 数据之间的 COPY 和 UNLOAD 流量将保留在您的 Amazon VPC 中。可以将终端节点策略附加到您的终端节点，以便更严格地管理对数据的访问。例如，可以向 VPC 终端节点添加策略以仅允许将数据上传到您账户中的特定 Amazon S3 存储桶。</p><p>重要<br>目前，Amazon Redshift 仅支持连接到 Amazon S3 的 VPC 终端节点。当 Amazon VPC 添加对其他 AWS 服务的支持以使用 VPC 终端节点时，Amazon Redshift 也将支持这些 VPC 终端节点连接。要使用 VPC 终端节点连接到 Amazon S3 存储桶，Amazon Redshift 集群与其连接到的 Amazon S3 存储桶必须在同一个 AWS 区域中。</p><p>要使用 VPC 终端节点，请为集群所在的 VPC 创建 VPC 终端节点，然后为集群启用增强型 VPC 路由。可以在 VPC 中创建集群时启用增强型 VPC 路由，也可以修改 VPC 中的集群以使用增强型 VPC 路由。</p><p>VPC 终端节点使用路由表来控制 VPC 中的集群和 Amazon S3 之间的流量路由。与指定路由表关联的子网中的所有集群会自动使用该终端节点来访问服务。</p><p>您的 VPC 使用与集群流量匹配的最具体的/最严格的路由来决定路由流量的方式。例如，假设路由表中有一条路由用于所有指向 Internet 网关和 Amazon S3 终端节点的 Internet 流量 (0.0.0.0/0)。在这种情况下，对所有传送到 Amazon S3 的流量优先使用终端节点路由。这是因为 Amazon S3 服务的 IP 地址范围比 0.0.0.0/0 更具体。在此示例中，所有其他 Internet 流量（包括定位到其他 AWS 区域内的 Amazon S3 存储桶的流量）将流向 Internet 网关。</p><p>有关创建终端节点的更多信息，请参阅 Amazon VPC 用户指南 中的 VPC 终端节点。</p><p>您使用终端节点策略控制从集群到包含数据文件的 Amazon S3 存储桶的访问。默认情况下，创建终端节点向导会附加一个终端节点策略，该策略不会进一步限制来自 VPC 中的任何用户或服务的访问。要实现更具体的控制，您可以选择附加一个自定义终端节点策略。有关更多信息，请参阅 Amazon VPC 用户指南 中的使用终端节点策略。</p><p>使用终端节点不收取任何额外费用。采用标准的数据传输和资源使用计费方式。有关定价的更多信息，请参阅 Amazon EC2 定价。</p></blockquote><blockquote><p><a href="https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html" target="_blank" rel="noopener">https://docs.amazonaws.cn/redshift/latest/mgmt/enhanced-vpc-routing.html</a></p><p>Amazon Redshift 增强型 VPC 路由</p><p>在使用 Amazon Redshift 增强型 VPC 路由时，Amazon Redshift 会强制通过您的 Amazon VPC 路由集群和数据存储库之间的所有 COPY 和 UNLOAD 流量。通过使用增强型 VPC 路由，您可以使用标准 VPC 功能，例如 VPC 安全组、网络访问控制列表 (ACL)、VPC 终端节点、VPC 终端节点策略、Internet 网关和域名系统 (DNS) 服务器，如 Amazon VPC 用户指南 中所述。 您可以使用这些功能来严格管理 Amazon Redshift 集群与其他资源之间的数据流。在使用增强型 VPC 路由通过您的 VPC 路由流量时，也可以使用 VPC 流日志来监视 COPY 和 UNLOAD 流量。</p><p>如果未启用增强型 VPC 路由，则 Amazon Redshift 会通过 Internet 路由流量，包括至 AWS 网络中的其他服务的流量。</p><p>重要<br>由于增强型 VPC 路由影响了 Amazon Redshift 访问其他资源的方式，因此，除非您正确配置 VPC，否则 COPY 和 UNLOAD 命令可能会失败。您必须专门在集群的 VPC 和数据资源之间创建网络路径，如下所述。</p><p>在对已启用增强型 VPC 路由的集群执行 COPY 或 UNLOAD 命令时，您的 VPC 会使用最严格 或最具体的可用网络路径来将流量路由到指定资源。</p></blockquote><h1 id="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"><a href="#一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。" class="headerlink" title="一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。"></a>一家公司正在生成包含数百万行的大型数据集，必须能按列对这些数据集进行汇总◊将使用现有的商业智能工具生成日常报告。</h1><p>哪种存储服务可满足这些要求？<br>A. Amazon Redshift<br>B. Amazon RDS<br>C. ElastiCache<br>D. DynamoDB</p><p>Answer: A</p><h1 id="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"><a href="#解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。" class="headerlink" title="解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。"></a>解决方案架构师正在设计一个活动注册网页；每次用户注册活动时，需要使用一个托管服务向用户发送文本消息。</h1><p>架构师应使用哪种AWS服务来实现该目的？<br>A. Amazon STS<br>B. Amazon SQS<br>C. Lambda<br>D. Amazon SNS</p><p>Answer: D</p><blockquote><p>Amazon Simple Notification Service (SNS) 是一种高度可用、持久、安全、完全托管的发布/订阅消息收发服务，可以轻松分离微服务、分布式系统和无服务器应用程序。Amazon SNS 提供了面向高吞吐量、多对多推送式消息收发的主题。借助 Amazon SNS 主题，发布系统可以向大量订阅终端节点（包括 Amazon SQS 队列、AWS Lambda 函数和 HTTP/S Webhook 等）扇出消息，从而实现并行处理。此外，SNS 可用于使用移动推送、短信和电子邮件向最终用户扇出通知。</p></blockquote><h1 id="解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"><a href="#解决方案架构师正在设计一个共享服务，以便在Amazon-ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。" class="headerlink" title="(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。"></a>(*)解决方案架构师正在设计一个共享服务，以便在Amazon ECS上托管来自很多客户的容器。这些容器将使用很多AWS服务。一个客户的容器无法访问其他客户的数据。</h1><p>架构师应使用哪种解决方案以满足这些要求？<br>A. 任务的IAM角色<br>B. EC2实例的 IAM角色<br>C. EC2实例的 IAM实例配置文件<br>D. 安全组规则</p><p>Answer: A</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonECS/latest/developerguide/task-iam-roles.html</a></p><p>借助 Amazon ECS 任务的 IAM 角色，您可以指定一个可由任务中的容器使用的 IAM 角色。应用程序必须使用 AWS 凭证签署其 AWS API 请求，并且此功能提供了一个管理凭证的策略以供应用程序使用，类似于 Amazon EC2 实例配置文件为 EC2 实例提供凭证的方式。您可以将 IAM 角色与 ECS 任务定义或 RunTask API 操作关联，而不是为容器创建和分配 AWS 凭证或使用 EC2 实例的角色。之后，任务的容器中的应用程序可以使用 AWS 开发工具包或 CLI 向授权的 AWS 服务发出 API 请求。</p></blockquote><h1 id="一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"><a href="#一家公司正在将本地10-TB-MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。" class="headerlink" title="一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。"></a>一家公司正在将本地10 TB MySQL数据库迁移到AWS，该公司预计数据库大小将增加3倍，业务要求是副本的滯后时间必须保持在100毫秒以内。</h1><p>哪种Amazon RDS引擎满足这些要求？<br>A. MySQL<br>B. Microsoft SQL Server<br>C. Oracle<br>D. Amazon Aurora</p><p>Answer: D</p><blockquote><p><a href="https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6" target="_blank" rel="noopener">https://amazonaws-china.com/cn/rds/aurora/faqs/?nc=sn&amp;loc=6</a></p><p>Amazon Aurora 副本复制是毫秒级别，而MySQL是秒级别</p></blockquote><h1 id="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"><a href="#管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。" class="headerlink" title="管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。"></a>管理员在AWS中运行一个高可用应用程序。管理员需要使用一个文件存储层，它可以在实例之间共享并能更轻松地扩展该应用平台。</h1><p>哪种AMS服务可以执行该操作？<br>A. Amazon EBS<br>B. Amazon EFS<br>C. Amazon S3<br>D. Amazon EC2实例存储</p><p>Answer: B</p><h1 id="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"><a href="#一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon-RDS-MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。" class="headerlink" title="一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。"></a>一家公司托管一个流行的Web应用程序，它连接到在私有VPC子网中运行的Amazon RDS MySQL数据库实例，该子网是使用默认ACL设置创建的。仅允许使用SSL连接的客户访问Web服务器◊仅公有子网中的Web服务器可以访问该数据库。</h1><p>哪种解决方案可满足这些要求而不会影响其他运行的应用程序？（选择两顶)<br>A. 在Web服务器的子网上创建一个网络ACL，允许HTTPS端口 443入站流量，并将源指定为0.0.0.0/0。<br>B. 创建一个允许来自Anywhere (0.0.0.0/0)的 HTTPS端口 443入站流量的Web服务器安全组，并将其应用于Web服务器。<br>C. 创建一个允许MySQL端口 3306入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。<br>D. 在数据库子网上创建一个网络ACL，允许Web服务器的MySQL端口 3306入站流量，并拒绝所有出站流量。<br>E. 创建一个允许HTTPS端口 443入站流量的数据库服务器安全组，并将源指定为一个Web服务器安全组。</p><p>Answer: BC</p><h1 id="一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"><a href="#一个应用程序当前在Amazon-EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。" class="headerlink" title="一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。"></a>一个应用程序当前在Amazon EBS卷上存储所有数据。必须在多个可用区中永久备份所有EBS卷。</h1><p>备份这些卷的最灵活方法是什么？<br>A. 定期创建EBS快照。<br>B. 启用EBS卷加密。<br>C. 创建脚本以将数据复制到EC2实例存储。<br>D. 在两个EBS卷之间镜像数据。</p><p>Answer: A</p><h1 id="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"><a href="#解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。" class="headerlink" title="解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。"></a>解决方案架构师正在开发一个文档共享应用程序，并需要使用一个存储层。该存储应提供自动版本控制支持，以便用户可以轻松回滚到以前的版本或恢复删除的文档。</h1><p>哪种AMS服务可满足这些要求？<br>A. Amazon S3<br>B. Amazon EBS<br>C. Amazon EFS<br>D. Amazon Storage Gateway VTL</p><p>Answer: A</p><h1 id="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"><a href="#AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。" class="headerlink" title="AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。"></a>AWS中的一个数据处理应用程序必须从Internet服务中提取数据。解决方案架构师必须设计一种高可用解决方案以访问数据，并且不会对应用程序流量施加带宽限制。</h1><p>哪种解决方案能满足这些要求？<br>A. 启动一个NAT网关并为0.0.0.0/0添加路由<br>B. 附加一个VPC终端节点并为0.0.0.0/0添加路由<br>C. 附加一个Internet网关并为0.0.0.0/0添加路由<br>D. 在公有子网中部署NAT实例并为0.0.0.0/0添加路由</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/VPC_Internet_Gateway.html</a></p><p>Internet 网关是一种横向扩展、支持冗余且高度可用的 VPC 组件，可实现 VPC 中的实例与 Internet 之间的通信。因此它不会对网络流量造成可用性风险或带宽限制。</p><p>NAT 网关<br>您可以使用网络地址转换 (NAT) 网关允许私有子网中的实例连接到 Internet 或其他 AWS 服务，但阻止 Internet 发起与这些实例的连接。有关 NAT 的更多信息，请参阅NAT。<br>您在账户中创建和使用 NAT 网关会产生费用。NAT 网关小时使用费率和数据处理费率适用于此。Amazon EC2 数据传输费同样适用。有关更多信息，请参阅 Amazon VPC 定价。</p></blockquote><h1 id="待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"><a href="#待确认-在审查您的应用程序的Auto-Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。" class="headerlink" title="(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。"></a>(待确认)在审查您的应用程序的Auto Scaling事件时，您注意到应用程序在同一小时内扩展和缩减多次。</h1><p>您可以选择哪种设计选顶以在保持弹性的同时优化成本？（选择两顶)<br>A. 修改Auto Scaling组终止策略以先终止最老的实例。<br>B. 修改Auto Scaling组终止策略以先终止最新的实例。<br>C. 修改Auto Scaling组冷却计时器。<br>D. 修改Auto Scaling策略以使用计划的缩放操作。<br>E. 修改触发Auto Scaling缩减策略的CloudWatch警报周期。</p><p>Answer: BC</p><p>这道题目前纠结点在于AB两个选项，从文档中可知有一种策略结束类型叫ClosestToNextInstanceHour类型更适合该题目。如果从这个角度说，新的实例好像更靠近最近计费时间点这个选项。</p><h1 id="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶"><a href="#对于以下哪种工作负载，解决方案架构师应考虑使用Elastic-Beanstalk-选择两顶" class="headerlink" title="对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)"></a>对于以下哪种工作负载，解决方案架构师应考虑使用Elastic Beanstalk?(选择两顶)</h1><p>A. 使用Amazon RDS的Web应用程序<br>B. 企业数据仓库<br>C. 长时间运行的工作进程<br>D. 静态网站<br>E. 每晚运行一次的管理任务</p><p>Answer: AD</p><h1 id="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"><a href="#一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。" class="headerlink" title="一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。"></a>一家公司在AMS上运行一个服务，以便为笔记本电脑和手机上的图像提供异地备份。该解决方案必须支持数百万个客户，每个客户有数千张图像，很少会检索这些图像，但必须可以立即检索这些图像。</h1><p>哪种是满足这些要求的最经济高效的存储选顶？<br>A. 具有加速检索的Amazon Glacier<br>B. Amazon S3标准-低频率访问<br>C. Amazon EFS<br>D. Amazon S3 标准</p><p>Answer: B</p><h1 id="一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"><a href="#一个带有150-GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。" class="headerlink" title="一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。"></a>一个带有150 GB大小的关系数据库的应用程序在EC2实例上运行。该应用程序很少使用，但在早上和晚上会出现很小的高峰。</h1><p>最经济高效的存储类型是什么？<br>A. Amazon EBS 预置 IOPS SSD<br>B. Amazon EBS吞吐量优化HDD<br>C. Amazon EBS 通用型 SSD<br>D. Amazon EFS</p><p>Answer: C</p><h1 id="一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"><a href="#一个应用程序允许生产站点上传文件。然后，处理每个3-GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。" class="headerlink" title="一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。"></a>一个应用程序允许生产站点上传文件。然后，处理每个3 GB大小的文件以提取元数据，处理每个文件需要几秒钟的时间◊更新频率是无法预铡的-可能几小时内没有更新，然后同时上传几个文件。</h1><p>哪种架构能以最经济高效的方式处理该工作负载？<br>A. 使用Kinesis数据传输流存储文件，并使用Uirtoda进行处理。<br>B. 使用SQS队列存储文件，然后，一组EC2实例访问该文件。<br>C. 将文件存储在EBS卷中，然后，其他EC2实例可以访问该文件以进行处理。<br>D. 将文件存储在S3存储捅中，并使用Amazon S3事件通知调用Lambda函数以处理该文件。</p><p>Answer: D</p><h1 id="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"><a href="#一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto-Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等-，这些文件存储在共享的Amazon-EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。" class="headerlink" title="一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。"></a>一家网站在ELB应用程序负载均衡器后面的多个EC2实例上运行。这些实例在跨多个可用区的Auto Scaling组中运行◊这些实例提供一些很大的文件（图像，PDF等)，这些文件存储在共享的Amazon EFS文件系统上。每次用户请求这些数字资产时，该公司需要避免从EC2实例中提供这些文件。</h1><p>该公司应釆取哪些措施以改进网站的用户体验？<br>A. 将数字资产移到到Amazon Glacier中。<br>B. 使用CloudFront缓存静态内容。<br>C. 调整图像以使其变小。<br>D. 使用保留的EC2实例。</p><p>Answer: B</p><h1 id="您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。"><a href="#您正在Amazon-EC2上部署一个应用程序，它必须调用AMS-API。" class="headerlink" title="您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。"></a>您正在Amazon EC2上部署一个应用程序，它必须调用AMS API。</h1><p>应使用哪种方法可将凭证安全地传送到该应用程序？<br>A. 使用实例用户数据将API凭证传送到实例。<br>B. 将API凭证作为对象存储在Amazon S3中。<br>C. 将API凭证嵌入到JAR文件中。<br>D. 将IAM角色分配给EC2实例。</p><p>Answer: D</p><h1 id="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的"><a href="#一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的" class="headerlink" title="一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:"></a>一个组织在AWS上托管着一个多语言网站◊该网站是使用CloudFront提供服务的◊语言是在HTTP请求中指定的:</h1><p>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=de" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=de</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=en" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=en</a><br>• <a href="http://dllllllabcdef8.cloudfront.net/main.html?language=es" target="_blank" rel="noopener">http://dllllllabcdef8.cloudfront.net/main.html?language=es</a><br>应如何配置CloudFront以使用正确的语言提供缓存的数据？<br>A. 将Cookie转发到原始地址。<br>B. 基于查询字符串参数。<br>C. 在原始地址中缓存对象。<br>D. 提供动态内容。</p><p>Answer: B</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonCloudFront/latest/DeveloperGuide/QueryStringParameters.html</a></p><p>一些 Web 应用程序使用查询字符串将信息发送到源。查询字符串是 Web 请求的一部分，显示在 ? 字符之后；该字符串可以包含一个或多个使用 &amp; 字符分隔的参数。在以下示例中，查询字符串包括两个参数 color=red 和 size=large：</p></blockquote><h1 id="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"><a href="#解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。" class="headerlink" title="解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。"></a>解决方案架构师正在设计一个可高度扩展的系统以跟踪记录。记录必须保留三个月以便可立即下载，然后必须删除记录。</h1><p>最适合该使用案例的决策是什么？<br>A. 将文件存储在Amazon EBS上，并创建一个生命周期策略以在三个月后删除这些文件。<br>B. 将文件存储在Amazon S3中，并创建一个生命周期策略以在三个月后删除这些文件。<br>C. 将文件存储在Amazon Glacier中，并创建一个生命周期策略以在三个月后删除这些文件。<br>D. 将文件存储在Amazon EFS上，并创建一个生命周期策略以在三个月后删除这些文件。</p><p>Answer: B</p><h1 id="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"><a href="#一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。" class="headerlink" title="一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。"></a>一个团队正在创建一个应用程序，它必须在高可用的数据存储中永久保存JSON文件并编制索引。尽管应用程序流量很高，但数据访问延迟必须保持一致。</h1><p>该团队应该选择哪种服务？<br>A. Amazon EFS<br>B. Amazon RedShift<br>C. DynamoDB<br>D. AWS CloudFormation</p><p>Answer: C</p><h1 id="一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。"><a href="#一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取-写入流量会非常高。" class="headerlink" title="(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。"></a>(*)一个应用程序在S3存储桶中读取和写入小对象。在完全部署该应用程序后，读取/写入流量会非常高。</h1><p>架构师应如何最大限度地提高Amazon S3性能？<br>A. 在每个对象名称前面添加随机字符串。<br>B. 使用STANDARD_IA存储类<br>C. 在每个对象名称前面添加当前日期。<br>D. 在S3存储桶上启用版本控制。</p><p>Answer: C</p><blockquote><p><a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>当从 Amazon S3 上传和检索存储时，您的应用程序可以轻松地实现每秒数千个事务的请求性能。Amazon S3 会自动扩展至高请求速率。例如，您的应用程序可以在存储桶中实现至少每秒每个前缀 3,500 个 PUT/COPY/POST/DELETE 请求和 5,500 个 GET/HEAD 请求。对存储桶中的前缀数量没有限制。您可以通过并行读取来增加读取或写入性能。例如，如果您在 Amazon S3 存储桶中创建 10 个前缀以并行处理读取，则可以将读取性能扩展到每秒 55,000 个读取请求。<br>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><blockquote><p>2018年7月17日 Amazon S3 宣布提高请求速率性能(<a href="https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/about-aws/whats-new/2018/07/amazon-s3-announces-increased-request-rate-performance/</a>)</p><p>Amazon S3 现在提供了更高的性能，支持每秒至少 3500 个数据添加请求、每秒 5500 个数据检索请求，而且无需额外费用，这可以节省大量处理时间。每个 S3 前缀均支持这些请求速率，因此可以轻松实现显著的性能提升。</p><p>目前在 Amazon S3 上运行的应用程序均可享受此性能改进，而无需实施任何更改；在 S3 上构建新应用程序的客户无需进行任何应用程序自定义即可享受此性能。Amazon S3 对并行请求的支持意味着您可以按照计算集群的系数扩展 S3 性能，而无需对应用程序进行任何自定义。性能按前缀扩展，因此您可以并行使用尽可能多的前缀，从而实现所需的吞吐量。前缀的数量没有限制。</p><p>在这种 S3 请求速率性能提升推出后，先前任何为加速性能而随机化对象前缀的指南均被淘汰。也就是说，您现在可以在 S3 对象命名中使用逻辑或顺序命名模式，而不会产生任何性能影响。所有 AWS 区域现在均已提供此改进。有关更多信息，请访问 Amazon S3 开发人员指南。</p></blockquote>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该模拟题出自AWS Practice，是付费后的模拟题，一共25道题，相对来说答案比较准确，答题正确率在76%，看中文的命题相对来说对理解题目内容更简单。&lt;/p&gt;
&lt;p&gt;总得分: 76%&lt;br&gt;主题得分:&lt;br&gt;1.0. Design Resilient Architectures 89%&lt;br&gt;2.0. Define Performant Architectures 71%&lt;br&gt;3.0. Specify Secure Applications and Architectures 50%&lt;br&gt;4.0. Design Cost-Optimized Architectures 100%&lt;br&gt;5.0. Define Operationally Excellent Architectures 100%&lt;/p&gt;
&lt;p&gt;个人感觉，对于网络题目还是有些晕的，因为和OpenStack的SDN还是有一些区别，特别涉及到安全组、网络ACL特别含糊；另外一类题就是服务之间的互联互通时会比较晕。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.site/tags/AWS/"/>
    
      <category term="ACA Practice" scheme="http://sunqi.site/tags/ACA-Practice/"/>
    
  </entry>
  
  <entry>
    <title>[Digitalcloud.Training]AWS CERTIFIED SOLUTIONS ARCHITECT ASSOCIATE</title>
    <link href="http://sunqi.site/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/"/>
    <id>http://sunqi.site/2020/01/14/Digitalcloud-Training-AWS-CERTIFIED-SOLUTIONS-ARCHITECT-ASSOCIATE/</id>
    <published>2020-01-14T08:49:16.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>该模拟题出自DigitalCloud Training的模拟题，一共20道题，相对来说答案比较准确，第一次答正确率只有60%，看起来还有不太扎实的知识点，并且英文多了不太爱仔细阅读也是准确率低的原因。</p><p>另外，今天在AWS培训官网上看到ACA考试在3月份会推出全新的试题，所以还需要抓紧时间考过。</p><a id="more"></a><h1 id="答题情况统计"><a href="#答题情况统计" class="headerlink" title="答题情况统计"></a>答题情况统计</h1><p>Categories<br>AWS Analytics 100%<br>AWS Application Integration 100%<br>AWS Compute 33.33%<br>AWS Database 66.67%<br>AWS Management &amp; Governance 100%<br>AWS Networking &amp; Content Delivery 33.33%<br>AWS Security, Identity, &amp; Compliance 50%<br>AWS Storage 33.33%</p><h1 id="1-Question"><a href="#1-Question" class="headerlink" title="1. Question"></a>1. Question</h1><p>A Solutions Architect has been asked to suggest a solution for analyzing data in S3 using standard SQL queries. The solution should use a serverless technology.<br>Which AWS service can the Architect use?</p><p>A. Amazon RedShift<br>B. AWS Data Pipeline<br>C. AWS Glue<br>D. Amazon Athena</p><p>Answer: D</p><p>Correct<br>Explanation:<br>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run<br>Amazon RedShift is used for analytics but cannot analyze data in S3<br>AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for customers to prepare and load their data for analytics. It is not used for analyzing data in S3<br>AWS Data Pipeline is a web service that helps you reliably process and move data between different AWS compute and storage services, as well as on-premises data sources, at specified intervals</p><p>References:<br><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a></p><h1 id="2-Question"><a href="#2-Question" class="headerlink" title="2. Question"></a>2. Question</h1><p>A systems integration company that helps customers migrate into AWS repeatedly build large, standardized architectures using several AWS services. The Solutions Architects have documented the architectural blueprints for these solutions and are looking for a method of automating the provisioning of the resources.<br>Which AWS service would satisfy this requirement?</p><p>A. AWS OpsWorks<br>B. AWS CloudFormation<br>C. AWS CodeDeploy<br>D. Elastic Beanstalk</p><p>Answer: B</p><p>Correct<br>Explanation:<br>CloudFormation allows you to use a simple text file to model and provision, in an automated and secure manner, all the resources needed for your applications across all regions and accounts<br>Elastic Beanstalk is a PaaS service that helps you to build and manage web applications<br>AWS OpsWorks is a configuration management service that helps you build and operate highly dynamic applications, and propagate changes instantly<br>AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, or serverless Lambda functions</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a></p><h1 id="3-Question"><a href="#3-Question" class="headerlink" title="3. Question"></a>3. Question</h1><p>A data-processing application runs on an i3.large EC2 instance with a single 100 GB EBS gp2 volume. The application stores temporary data in a small database (less than 30 GB) located on the EBS root volume. The application is struggling to process the data fast enough, and a Solutions Architect has determined that the I/O speed of the temporary database is the bottleneck.<br>What is the MOST cost-efficient way to improve the database response times?</p><p>A. Enable EBS optimization on the instance and keep the temporary files on the existing volume<br>B. Put the temporary database on a new 50-GB EBS gp2 volume<br>C. Move the temporary database onto instance storage<br>D. Put the temporary database on a new 50-GB EBS io1 volume with a 3000 IOPS allocation</p><p>Answer: C</p><p>Incorrect<br>Explanation:<br>EC2 Instance Stores are high-speed ephemeral storage that is physically attached to the EC2 instance. The i3.large instance type comes with a single 475GB NVMe SSD instance store so it would be a good way to lower cost and improve performance by using the attached instance store. As the files are temporary, it can be assumed that ephemeral storage (which means the data is lost when the instance is stopped) is sufficient.<br>Enabling EBS optimization will not lower cost. Also, EBS Optimization is a network traffic optimization, it does not change the I/O speed of the volume.<br>Moving the DB to a new 50-GB EBS gp2 volume will not result in a performance improvement as you get IOPS allocated per GB so a smaller volume will have lower performance.<br>Moving the DB to a new 50-GB EBS io1 volume with a 3000 IOPS allocation will improve performance but is more expensive so will not be the most cost-efficient solution.</p><p>References:<br><a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/InstanceStorage.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/amazon-ebs/</a></p><h1 id="4-Question"><a href="#4-Question" class="headerlink" title="4. Question"></a>4. Question</h1><p>An application you are designing receives and processes files. The files are typically around 4GB in size and the application extracts metadata from the files which typically takes a few seconds for each file. The pattern of updates is highly dynamic with times of little activity and then multiple uploads within a short period of time.<br>What architecture will address this workload the most cost efficiently?</p><p>A. Upload files into an S3 bucket, and use the Amazon S3 event notification to invoke a Lambda function to extract the metadata<br>B. Place the files in an SQS queue, and use a fleet of EC2 instances to extract the metadata<br>C. Store the file in an EBS volume which can then be accessed by another EC2 instance for processing<br>D. Use a Kinesis data stream to store the file, and use Lambda for processing</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Storing the file in an S3 bucket is the most cost-efficient solution, and using S3 event notifications to invoke a Lambda function works well for this unpredictable workload<br>Kinesis data streams runs on EC2 instances and you must therefore provision some capacity even when the application is not receiving files. This is not as cost-efficient as storing them in an S3 bucket prior to using Lambda for the processing<br>SQS queues have a maximum message size of 256KB. You can use the extended client library for Java to use pointers to a payload on S3 but the maximum payload size is 2GB<br>Storing the file in an EBS volume and using EC2 instances for processing is not cost efficient</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-s3/</a><br><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/NotificationHowTo.html</a></p><h1 id="5-Question"><a href="#5-Question" class="headerlink" title="5. Question"></a>5. Question</h1><p>A Solutions Architect needs to deploy an HTTP/HTTPS service on Amazon EC2 instances that will be placed behind an Elastic Load Balancer. The ELB must support WebSockets.<br>How can the Architect meet these requirements?</p><p>A. Launch an Application Load Balancer (ALB)<br>B. Launch a Network Load Balancer (NLB)<br>C. Launch a Classic Load Balancer (CLB)<br>D. Launch a Layer-4 Load Balancer</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Both the ALB and NLB support WebSockets. However, only the ALB supports HTTP/HTTPS listeners. The NLB only supports TCP, TLS, UDP, TCP_UDP.<br>The CLB does not support WebSockets.<br>A “Layer-4 Load Balancer” is not suitable, we need a layer 7 load balancer for HTTP/HTTPS.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a><br><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-listeners.html</a><br><a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-listeners.html</a></p><h1 id="6-Question"><a href="#6-Question" class="headerlink" title="6. Question"></a>6. Question</h1><p>You are building an application that will collect information about user behavior. The application will rapidly ingest large amounts of dynamic data and requires very low latency. The database must be scalable without incurring downtime. Which database would you recommend for this scenario?</p><p>A. RDS with Microsoft SQL<br>B. RedShift<br>C. DynamoDB<br>D. RDS with MySQL</p><p>Answer: C</p><p>Correct<br>Explanation:<br>Amazon Dynamo DB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability<br>Push button scaling means that you can scale the DB at any time without incurring downtime<br>DynamoDB provides low read and write latency<br>RDS uses EC2 instances so you have to change your instance type/size in order to scale compute vertically<br>RedShift uses EC2 instances as well, so you need to choose your instance type/size for scaling compute vertically, but you can also scale horizontally by adding more nodes to the cluster<br>Rapid ingestion of dynamic data is not an ideal use case for RDS or RedShift</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a></p><h1 id="7-Question"><a href="#7-Question" class="headerlink" title="7. Question"></a>7. Question</h1><p>You are running an Auto Scaling Group (ASG) with an Elastic Load Balancer (ELB) and a fleet of EC2 instances. Health checks are configured on the ASG to use EC2 status checks. The ELB has determined that an EC2 instance is unhealthy and has removed it from service. However, you noticed that the instance is still running and has not been terminated by the ASG.<br>What would be an explanation for this behavior?</p><p>A. The health check grace period has not yet expired<br>B. The ELB health check type has not been selected for the ASG and so it is unaware that the instance has been determined to be unhealthy by the ELB and has been removed from service<br>C. Connection draining is enabled and the ASG is waiting for in-flight requests to complete<br>D. The ASG is waiting for the cooldown timer to expire before terminating the instance</p><p>Answer: B</p><p>Incorrect<br>Explanation:<br>If using an ELB it is best to enable ELB health checks as otherwise EC2 status checks may show an instance as being healthy that the ELB has determined is unhealthy. In this case the instance will be removed from service by the ELB but will not be terminated by Auto Scaling<br>Connection draining is not the correct answer as the ELB has taken the instance out of service so there are no active connections<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check<br>More information on ASG health checks:<br>By default uses EC2 status checks<br>Can also use ELB health checks and custom health checks<br>ELB health checks are in addition to the EC2 status checks<br>If any health check returns an unhealthy status the instance will be terminated<br>With ELB an instance is marked as unhealthy if ELB reports it as OutOfService<br>A healthy instance enters the InService state<br>If an instance is marked as unhealthy it will be scheduled for replacement<br>If connection draining is enabled, Auto Scaling waits for in-flight requests to complete or timeout before terminating instances<br>The health check grace period allows a period of time for a new instance to warm up before performing a health check (300 seconds by default)</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/aws-auto-scaling/</a></p><h1 id="8-Question"><a href="#8-Question" class="headerlink" title="8. Question"></a>8. Question</h1><p>A solutions Architect is designing a new workload where an AWS Lambda function will access an Amazon DynamoDB table.<br>What is the MOST secure means of granting the Lambda function access to the DynamoDB table?</p><p>A. Create an identity and access management (IAM) role allowing access from AWS Lambda and assign the role to the DynamoDB table<br>B. Create an identity and access management (IAM) role with the necessary permissions to access the DynamoDB table, and assign the role to the Lambda function<br>C. Create a DynamoDB username and password and give them to the Developer to use in the Lambda function<br>D. Create an identity and access management (IAM) user and create access and secret keys for the user. Give the user the necessary permissions to access the DynamoDB table. Have the Developer use these keys to access the resources</p><p>Answer: B</p><p>Correct<br>Explanation:<br>The most secure method is to use an IAM role so you don’t need to embed any credentials in code and can tightly control the services that your Lambda function can access. You need to assign the role to the Lambda function, NOT to the DynamoDB table<br>You should not provide a username and password to the Developer to use with the function. This is insecure – always avoid using credentials in code!<br>You should not use an access key and secret ID to access DynamoDB. Again, this means embedding credentials in code which should be avoided.</p><p>References:<br><a href="https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-create-an-aws-iam-policy-to-grant-aws-lambda-access-to-an-amazon-dynamodb-table/</a></p><h1 id="9-Question"><a href="#9-Question" class="headerlink" title="9. Question"></a>9. Question</h1><p>Your company has offices in several locations around the world. Each office utilizes resources deployed in the geographically closest AWS region. You would like to implement connectivity between all of the VPCs so that you can provide full access to each other’s resources. As you are security conscious you would like to ensure the traffic is encrypted and does not traverse the public Internet. The topology should be many-to-many to enable all VPCs to access the resources in all other VPCs.<br>How can you successfully implement this connectivity using only AWS services? (choose 2)</p><p>A. Use inter-region VPC peering<br>B. Use software VPN appliances running on EC2 instances<br>C. Use VPC endpoints between VPCs<br>D. Implement a fully meshed architecture<br>E. Implement a hub and spoke architecture</p><p>Answer: AD</p><p>Incorrect<br>Explanation:<br>Peering connections can be created with VPCs in different regions (available in most regions now)<br>Data sent between VPCs in different regions is encrypted (traffic charges apply)<br>You cannot do transitive peering so a hub and spoke architecture would not allow all VPCs to communicate directly with each other. For this you need to establish a mesh topology<br>A VPC endpoint enables you to privately connect your VPC to supported AWS services and VPC endpoint services, it does not provide full VPC to VPC connectivity<br>Using software VPN appliances to connect VPCs together is not the best solution as it is cumbersome, expensive and would introduce bandwidth and latency constraints (amongst other problems)</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a></p><h1 id="10-Question"><a href="#10-Question" class="headerlink" title="10. Question"></a>10. Question</h1><p>A research company is developing a data lake solution in Amazon S3 to analyze huge datasets. The solution makes infrequent SQL queries only. In addition, the company wants to minimize infrastructure costs.<br>Which AWS service should be used to meet these requirements?</p><p>A. Amazon Athena<br>B. Amazon Redshift Spectrum<br>C. Amazon Aurora<br>D. Amazon RDS for MySQL</p><p>Answer: A</p><p>Correct<br>Explanation:<br>Amazon Athena is an interactive query service that makes it easy to analyze data in Amazon S3 using standard SQL. Athena is serverless, so there is no infrastructure to manage, and you pay only for the queries that you run – this satisfies the requirement to minimize infrastructure costs for infrequent queries.<br>Amazon RedShift Spectrum is a feature of Amazon Redshift that enables you to run queries against exabytes of unstructured data in Amazon S3, with no loading or ETL required. However, RedShift nodes run on EC2 instances, so for infrequent queries this will not minimize infrastructure costs.<br>Amazon RDS and Aurora are not suitable solutions for analyzing datasets on S3 – these are both relational databases typically used for transactional (not analytical) workloads.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-athena/</a><br><a href="https://docs.aws.amazon.com/athena/latest/ug/what-is.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/athena/latest/ug/what-is.html</a></p><h1 id="11-Question"><a href="#11-Question" class="headerlink" title="11. Question"></a>11. Question</h1><p>An Architect is designing a serverless application that will accept images uploaded by users from around the world. The application will make API calls to back-end services and save the session state data of the user to a database.<br>Which combination of services would provide a solution that is cost-effective while delivering the least latency?</p><p>A. Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>B. Amazon S3, API Gateway, AWS Lambda, Amazon RDS<br>C. API Gateway, Amazon S3, AWS Lambda, DynamoDB<br>D. Amazon CloudFront, API Gateway, Amazon S3, AWS Lambda, Amazon RDS</p><p>Answer: A</p><p>Incorrect<br>Explanation:<br>Amazon CloudFront caches content closer to users at Edge locations around the world. This is the lowest latency option for uploading content. API Gateway and AWS Lambda are present in all options. DynamoDB can be used for storing session state data<br>The option that presents API Gateway first does not offer a front-end for users to upload content to<br>Amazon RDS is not a serverless service so this option can be ruled out<br>Amazon S3 alone will not provide the least latency for users around the world unless you have many buckets in different regions and a way of directing users to the closest bucket (such as Route 3 latency based routing). However, you would then need to manage replicating the data</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-cloudfront/</a><br><a href="https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/aws/amazon-cloudfront-content-uploads-post-put-other-methods/</a></p><h1 id="12-Question"><a href="#12-Question" class="headerlink" title="12. Question"></a>12. Question</h1><p>A training provider hosts a website using Amazon API Gateway on the front end. Recently, there has been heavy traffic on the website and the company wants to control access by allowing authenticated traffic from paying students only.<br>How should the company limit access to authenticated users only? (choose 2)</p><p>A. Deploy AWS KMS to identify users<br>B. Allow X.509 certificates to authenticate traffic<br>C. Assign permissions in AWS IAM to allow users<br>D. Limit traffic through API Gateway<br>E. Allow users that are authenticated through Amazon Cognito</p><p>Answer: CE<br>Incorrect<br>Explanation:</p><p>API Gateway supports multiple mechanisms for controlling and managing access to your API. These include resource policies, standard IAM roles and policies, Lambda authorizers, and Amazon Cognito user pools.<br>Amazon Cognito user pools let you create customizable authentication and authorization solutions for your REST APIs. Amazon Cognito user pools are used to control who can invoke REST API methods.<br>IAM roles and policies offer flexible and robust access controls that can be applied to an entire API or individual methods. IAM roles and policies can be used for controlling who can create and manage your APIs as well as who can invoke them.<br>Limiting traffic through the API Gateway will not filter authenticated traffic, it will just limit overall invocations. This may prevent users from connecting who have a legitimate need.<br>X.509 certificates are not a method of authentication you can use with API Gateway.<br>AWS KMS is used for key management not user identification.</p><p>References:<br><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-to-api.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-api-gateway/</a></p><h1 id="13-Question"><a href="#13-Question" class="headerlink" title="13. Question"></a>13. Question</h1><p>An Auto Scaling Group is unable to respond quickly enough to load changes resulting in lost messages from another application tier. The messages are typically around 128KB in size.<br>What is the best design option to prevent the messages from being lost?</p><p>A. Launch an Elastic Load Balancer<br>B. Store the messages on Amazon S3<br>C. Use larger EC2 instance sizes<br>D. Store the messages on an SQS queue</p><p>Answer: D</p><p>Correct<br>Explanation:<br>In this circumstance the ASG cannot launch EC2 instances fast enough. You need to be able to store the messages somewhere so they don’t get lost whilst the EC2 instances are launched. This is a classic use case for decoupling and SQS is designed for exactly this purpose<br>Amazon Simple Queue Service (Amazon SQS) is a web service that gives you access to message queues that store messages waiting to be processed. SQS offers a reliable, highly-scalable, hosted queue for storing messages in transit between computers. An SQS queue can be used to create distributed/decoupled applications<br>Storing the messages on S3 is potentially feasible but SQS is the preferred solution as it is designed for decoupling. If the messages are over 256KB and therefore cannot be stored in SQS, you may want to consider using S3 and it can be used in combination with SQS by using the Amazon SQS Extended Client Library for Java<br>An ELB can help to distribute incoming connections to the back-end EC2 instances however if the ASG is not scaling fast enough then there aren’t enough resources for the ELB to distributed traffic to</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/application-integration/amazon-sqs/</a></p><h1 id="14-Question"><a href="#14-Question" class="headerlink" title="14. Question"></a>14. Question</h1><p>Your company would like to restrict the ability of most users to change their own passwords whilst continuing to allow a select group of users within specific user groups.<br>What is the best way to achieve this? (choose 2)</p><p>A. Under the IAM Password Policy deselect the option to allow users to change their own passwords<br>B. Create an IAM Policy that grants users the ability to change their own password and attach it to the individual user accounts<br>C. Create an IAM Policy that grants users the ability to change their own password and attach it to the groups that contain the users<br>D. Create an IAM Role that grants users the ability to change their own password and attach it to the groups that contain the users<br>E. Disable the ability for all users to change their own passwords using the AWS Security Token Service</p><p>Answer:AC</p><p>Incorrect<br>Explanation:<br>A password policy can be defined for enforcing password length, complexity etc. (applies to all users)<br>You can allow or disallow the ability to change passwords using an IAM policy and you should attach this to the group that contains the users, not to the individual users themselves<br>You cannot use an IAM role to perform this function<br>The AWS STS is not used for controlling password policies</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/security-identity-compliance/aws-iam/</a></p><h1 id="15-Question"><a href="#15-Question" class="headerlink" title="15. Question"></a>15. Question</h1><p>A Solutions Architect has created a VPC design that meets the security requirements of their organization. Any new applications that are deployed must use this VPC design.<br>How can project teams deploy, manage, and delete VPCs that meet this design with the LEAST administrative effort?</p><p>A. Deploy an AWS CloudFormation template that defines components of the VPC<br>B. Use AWS Elastic Beanstalk to deploy both the VPC and the application<br>C. Clone the existing authorized VPC for each new project<br>D. Run a script that uses the AWS Command Line interface to deploy the VPC</p><p>Answer: A</p><p>Correct<br>Explanation:<br>CloudFormation allows you to define your infrastructure through code and securely and repeatably deploy the infrastructure with minimal administrative effort. This is a perfect use case for CloudFormation.<br>You can use a script to create the VPCs using the AWS CLI however this would be a lot more work to create and manage the scripts.<br>You cannot clone VPCs.<br>You cannot deploy the VPC through Elastic Beanstalk – you need to deploy the VPC first and then deploy your application using Beanstalk.</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/management-tools/aws-cloudformation/</a><br><a href="https://aws.amazon.com/cloudformation/" target="_blank" rel="noopener">https://aws.amazon.com/cloudformation/</a></p><h1 id="16-Question"><a href="#16-Question" class="headerlink" title="16. Question"></a>16. Question</h1><p>Your organization has a data lake on S3 and you need to find a solution for performing in-place queries of the data assets in the data lake. The requirement is to perform both data discovery and SQL querying, and complex queries from a large number of concurrent users using BI tools.<br>What is the BEST combination of AWS services to use in this situation? (choose 2)</p><p>A. AWS Glue for the ad hoc SQL querying<br>B. AWS Lambda for the complex queries<br>C. RedShift Spectrum for the complex queries<br>D. Amazon Athena for the ad hoc SQL querying</p><p>Answer: CD</p><p>Incorrect<br>Explanation:<br>Performing in-place queries on a data lake allows you to run sophisticated analytics queries directly on the data in S3 without having to load it into a data warehouse<br>You can use both Athena and Redshift Spectrum against the same data assets. You would typically use Athena for ad hoc data discovery and SQL querying, and then use Redshift Spectrum for more complex queries and scenarios where a large number of data lake users want to run concurrent BI and reporting workloads<br>AWS Lambda is a serverless technology for running functions, it is not the best solution for running analytics queries<br>AWS Glue is an ETL service</p><p>References:<br><a href="https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/building-data-lakes/in-place-querying.html</a><br><a href="https://aws.amazon.com/redshift/" target="_blank" rel="noopener">https://aws.amazon.com/redshift/</a><br><a href="https://aws.amazon.com/athena/" target="_blank" rel="noopener">https://aws.amazon.com/athena/</a></p><h1 id="17-Question"><a href="#17-Question" class="headerlink" title="17. Question"></a>17. Question</h1><p>You have recently enabled Access Logs on your Application Load Balancer (ALB). One of your colleagues would like to process the log files using a hosted Hadoop service. What configuration changes and services can be leveraged to deliver this requirement?</p><p>A. Configure Access Logs to be delivered to DynamoDB and use EMR for processing the log files<br>B. Configure Access Logs to be delivered to S3 and use Kinesis for processing the log files<br>C. Configure Access Logs to be delivered to S3 and use EMR for processing the log files<br>D. Configure Access Logs to be delivered to EC2 and install Hadoop for processing the log files</p><p>Answer: C</p><p>Correct<br>Explanation:<br>Access Logs can be enabled on ALB and configured to store data in an S3 bucket. Amazon EMR is a web service that enables businesses, researchers, data analysts, and developers to easily and cost-effectively process vast amounts of data. EMR utilizes a hosted Hadoop framework running on Amazon EC2 and Amazon S3<br>Neither Kinesis or EC2 provide a hosted Hadoop service<br>You cannot configure access logs to be delivered to DynamoDB</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/analytics/amazon-emr/</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a></p><h1 id="18-Question"><a href="#18-Question" class="headerlink" title="18. Question"></a>18. Question</h1><p>A company is deploying a big data and analytics workload. The analytics will be run from a fleet of thousands of EC2 instances across multiple AZs. Data needs to be stored on a shared storage layer that can be mounted and accessed concurrently by all EC2 instances. Latency is not a concern however extremely high throughput is required.<br>What storage layer would be most suitable for this requirement?</p><p>A. Amazon EFS in General Purpose mode<br>B. Amazon EFS in Max I/O mode<br>C. Amazon S3<br>D. Amazon EBS PIOPS</p><p>Answer: B</p><p>Correct<br>Explanation:<br>Amazon EFS file systems in the Max I/O mode can scale to higher levels of aggregate throughput and operations per second with a tradeoff of slightly higher latencies for file operations<br>Amazon S3 is not a storage layer that can be mounted and accessed concurrently<br>Amazon EBS volumes cannot be shared between instances</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/storage/amazon-efs/</a><br><a href="https://docs.aws.amazon.com/efs/latest/ug/performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/efs/latest/ug/performance.html</a></p><h1 id="19-Question"><a href="#19-Question" class="headerlink" title="19. Question"></a>19. Question</h1><p>An application launched on Amazon EC2 instances needs to publish personally identifiable information (PII) about customers using Amazon SNS. The application is launched in private subnets within an Amazon VPC.<br>Which is the MOST secure way to allow the application to access service endpoints in the same region?</p><p>A. Use a proxy instance<br>B. Use a NAT gateway<br>C. Use AWS PrivateLink<br>D. Use an Internet Gateway</p><p>Answer: C</p><p>Correct<br>Explanation:<br>To publish messages to Amazon SNS topics from an Amazon VPC, create an interface VPC endpoint. Then, you can publish messages to SNS topics while keeping the traffic within the network that you manage with the VPC. This is the most secure option as traffic does not need to traverse the Internet.<br>Internet Gateways are used by instances in public subnets to access the Internet and this is less secure than an VPC endpoint.<br>A NAT Gateway is used by instances in private subnets to access the Internet and this is less secure than an VPC endpoint.<br>A proxy instance will also use the public Internet and so is less secure than a VPC endpoint.</p><p>References:<br><a href="https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/sns/latest/dg/sns-vpc-endpoint.html</a><br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/networking-and-content-delivery/amazon-vpc/</a></p><h1 id="20-Question"><a href="#20-Question" class="headerlink" title="20. Question"></a>20. Question</h1><p>A retail organization is deploying a new application that will read and write data to a database. The company wants to deploy the application in three different AWS Regions in an active-active configuration. The databases need to replicate to keep information in sync.<br>Which solution best meets these requirements?</p><p>A. Amazon Aurora Global Database<br>B. Amazon DynamoDB with global tables<br>C. Amazon Athena with Amazon S3 cross-region replication<br>D. AWS Database Migration Service with change data capture</p><p>Answer: B</p><p>Incorrect<br>Explanation:<br>Amazon DynamoDB global tables provide a fully managed solution for deploying a multi-region, multi-master database. This is the only solution presented that provides an active-active configuration where reads and writes can take place in multiple regions with full bi-directional synchronization.<br>Amazon Athena with S3 cross-region replication is not suitable. This is not a solution that provides a transactional database solution (Athena is used for analytics), or active-active synchronization.<br>Amazon Aurora Global Database provides read access to a database in multiple regions – it does not provide active-active configuration with bi-directional synchronization (though you can failover to your read-only DBs and promote them to writable).</p><p>References:<br><a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/database/amazon-dynamodb/</a><br><a href="https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/database/how-to-use-amazon-dynamodb-global-tables-to-power-multiregion-architectures/</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该模拟题出自DigitalCloud Training的模拟题，一共20道题，相对来说答案比较准确，第一次答正确率只有60%，看起来还有不太扎实的知识点，并且英文多了不太爱仔细阅读也是准确率低的原因。&lt;/p&gt;
&lt;p&gt;另外，今天在AWS培训官网上看到ACA考试在3月份会推出全新的试题，所以还需要抓紧时间考过。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.site/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.site/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q101-Q200)</title>
    <link href="http://sunqi.site/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/"/>
    <id>http://sunqi.site/2020/01/08/AWS-Certified-Solutions-Architect-Associate-Exam-Q101-Q200/</id>
    <published>2020-01-08T07:36:23.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。</p><a id="more"></a><h2 id="争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment"><a href="#争议-A-Solutions-Architect-needs-to-use-AWS-to-implement-pilot-light-disaster-recovery-for-a-three-tier-web-application-hosted-in-an-on-premises-datacenter-Which-solution-allows-rapid-provision-of-working-fully-scaled-production-environment" class="headerlink" title="(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?"></a>(争议)A Solutions Architect needs to use AWS to implement pilot light disaster recovery for a three-tier web application hosted in an on-premises datacenter. Which solution allows rapid provision of working, fully-scaled production environment?</h2><p>A. Continuously replicate the production database server to Amazon RDS. Use AWS CloudFormation to deploy the application and any additional servers if necessary.<br>B. Continuously replicate the production database server to Amazon RDS. Create one application load balancer and register on-premises servers. Configure ELB Application Load Balancer to automatically deploy Amazon EC2 instances for application and additional servers if the on-premises application is down.<br>C. Use a scheduled Lambda function to replicate the production database to AWS. Use Amazon Route 53 health checks to deploy the application automatically to Amazon S3 if production is unhealthy.<br>D. Use a scheduled Lambda function to replicate the production database to AWS. Register on-premises servers to an Auto Scaling group and deploy the application and additional servers if production is unavailable.</p><p>Answer: B</p><ul><li>分析：有人说答案是A，因为题目中的这个词pilot light(A pilot light is a small gas flame)，准确的翻译没查到，从字面理解应该就是简单轻量级的意思。A选项的方式是当出现灾难时，使用CloudFormation进行除数据库外的重建。所以很多人认为这种方式更符合题目的要求。但是从B选项看，更符合一个容灾的场景，当发生灾难时，通过B中的配置，可以做到马上接管的效果，比A选项更像是一个容灾的解决方案。</li></ul><h2 id="A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation"><a href="#A-Solutions-Architect-notices-slower-response-times-from-an-application-The-CloudWatch-metrics-on-the-MySQL-RDS-indicate-Read-IOPS-are-high-and-fluctuate-significantly-when-the-database-is-under-load-How-should-the-database-environment-be-re-designed-to-resolve-the-IOPS-fluctuation" class="headerlink" title="A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?"></a>A Solutions Architect notices slower response times from an application. The CloudWatch metrics on the MySQL RDS indicate Read IOPS are high and fluctuate significantly when the database is under load. How should the database environment be re-designed to resolve the IOPS fluctuation?</h2><p>A. Change the RDS instance type to get more RAM.<br>B. Change the storage type to Provisioned IOPS.<br>C. Scale the web server tier horizontally.<br>D. Split the DB layer into separate RDS instances.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and"><a href="#A-Solutions-Architect-is-designing-a-solution-that-can-monitor-memory-and-disk-space-utilization-of-all-Amazon-EC2-instances-running-Amazon-Linux-and" class="headerlink" title="A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and"></a>A Solutions Architect is designing a solution that can monitor memory and disk space utilization of all Amazon EC2 instances running Amazon Linux and</h2><p>Windows. Which solution meets this requirement?</p><p>A. Default Amazon CloudWatch metrics.<br>B. Custom Amazon CloudWatch metrics.<br>C. Amazon Inspector resource monitoring.<br>D. Default monitoring of Amazon EC2 instances.</p><p>Answer: B</p><ul><li>分析：这道题又是原网站给出的错题，原来给出的答案是A。我曾经对AWS两个行为比较纳闷：一个是为什么没有VNC，另外一个是为什么不提供内存监控，直到又一次和AWS的架构师聊才理解了AWS的良苦用心。AWS始终把用户安全放在第一位，但凡用户的东西我是坚决不能碰的，而无论是VNC还是内存监控无疑与这一原则相违背的。所以内存不可能是默认监控的范畴，必须通过custom脚本完成，同样磁盘利用率也是类似的方式。</li></ul><h2 id="A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use"><a href="#A-Solutions-Architect-is-creating-a-new-relational-database-The-Compliance-team-will-use-the-database-and-mandates-that-data-content-must-be-stored-across-three-different-Availability-Zones-Which-of-the-following-options-should-the-Architect-Use" class="headerlink" title="A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?"></a>A Solutions Architect is creating a new relational database. The Compliance team will use the database, and mandates that data content must be stored across three different Availability Zones. Which of the following options should the Architect Use?</h2><p>A. Amazon Aurora<br>B. Amazon RDS MySQL with Multi-AZ enabled<br>C. Amazon DynamoDB<br>D. Amazon ElastiCache</p><p>Answer: A</p><blockquote><p>问：Amazon Aurora 如何提高我的数据库对磁盘故障的容错能力？</p><p>Amazon Aurora 会将您的数据库卷分成分散在很多个磁盘上的 10GB 的区段。每 10GB 的数据库卷组块都能在三个可用区间用六种方法进行复制。Amazon Aurora 的设计可透明应对多达两个数据副本的损失，而不会影响数据库写入可用性，还能在不影响读取可用性的情况下应对多达三个副本。Amazon Aurora 存储还具有自我修复能力。可连续扫描数据块和磁盘有无出错并自动修复之。</p></blockquote><h2 id="A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design"><a href="#A-company-needs-to-quickly-ensure-that-all-files-created-in-an-Amazon-S3-bucket-in-us-east-1-are-also-available-in-another-bucket-in-ap-southeast-2-Which-option-represents-the-SIMPLIEST-way-to-implement-this-design" class="headerlink" title="A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?"></a>A company needs to quickly ensure that all files created in an Amazon S3 bucket in us-east-1 are also available in another bucket in ap-southeast-2. Which option represents the SIMPLIEST way to implement this design?</h2><p>A. Add an S3 lifecycle rule to move any files from the bucket in us-east-1 to the bucket in ap-southeast-2.<br>B. Create a Lambda function to be triggered for every new file in us-east-1 that copies the file to the bucket in ap-southeast-2.<br>C. Use SNS to notify the bucket in ap-southeast-2 to create a file whenever the file is created in the bucket in us-east-1.<br>D. Enable versioning and configure cross-region replication from the bucket in us-east-1 to the bucket in ap-southeast-2.</p><p>Answer: D</p><ul><li>分析：这道题要求的是最简单的方法，B理论上是可以的，但是与D相比过于复杂。</li></ul><h2 id="An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met"><a href="#An-organization-has-a-long-running-image-processing-application-that-runs-on-Spot-Instances-that-will-be-terminated-when-interrupted-A-highly-available-workload-must-be-designed-to-respond-to-Spot-Instance-interruption-notices-The-solution-must-include-a-two-minute-warning-when-there-is-not-enough-capacity-How-can-these-requirements-be-met" class="headerlink" title="An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?"></a>An organization has a long-running image processing application that runs on Spot Instances that will be terminated when interrupted. A highly available workload must be designed to respond to Spot Instance interruption notices. The solution must include a two-minute warning when there is not enough capacity. How can these requirements be met?</h2><p>A. Use Amazon CloudWatch Events to invoke an AWS Lambda function that can launch On-Demand Instances.<br>B. Regularly store data from the application on Amazon DynamoDB. Increase the maximum number of instances in the AWS Auto Scaling group.<br>C. Manually place a bid for additional Spot Instances at a higher price in the same AWS Region and Availability Zone.<br>D. Ensure that the Amazon Machine Image associated with the application has the latest configurations for the launch configuration.</p><p>Answer: A</p><ul><li><p>Taking Advantage of Amazon EC2 Spot Instance Interruption Notices(<a href="https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/compute/taking-advantage-of-amazon-ec2-spot-instance-interruption-notices/</a>)</p><blockquote><p>In January 2018, the Spot Instance interruption notice also became available as an event in Amazon CloudWatch Events. This allows targets such as AWS Lambda functions or Amazon SNS topics to process Spot Instance interruption notices by creating a CloudWatch Events rule to monitor for the notice.</p></blockquote></li><li><p>分析：网站给出的答案是B，但是这道题目考察的应该是合理利用Spot Instance的通知是这道题目的关键，”must include a two-minute warning” =&gt; Need CloudWatch</p></li></ul><h2 id="A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved"><a href="#A-company-has-an-Amazon-RDS-managed-online-transaction-processing-system-that-has-very-heavy-read-and-write-The-Solutions-Architect-notices-throughput-issues-with-the-system-How-can-the-responsiveness-of-the-primary-database-be-improved" class="headerlink" title="A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?"></a>A company has an Amazon RDS-managed online transaction processing system that has very heavy read and write. The Solutions Architect notices throughput issues with the system. How can the responsiveness of the primary database be improved?</h2><p>A. (争议)Use asynchronous replication for standby to maximize throughput during peak demand.<br>B. Offload SELECT queries that can tolerate stale data to READ replica.<br>C. Offload SELECT and UPDATE queries to READ replica.<br>D. Offload SELECT query that needs the most current data to READ replica.</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，大部分反对的理由是RDS之间的复制是同步的并不是异步的。</li></ul><h2 id="A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend"><a href="#A-company-is-designing-a-failover-strategy-in-Amazon-Route-53-for-its-resources-between-two-AWS-Regions-The-company-must-have-the-ability-to-route-a-user’s-traffic-to-the-region-with-least-latency-and-if-both-regions-are-healthy-Route-53-should-route-traffic-to-resources-in-both-regions-Which-strategy-should-the-Solutions-Architect-recommend" class="headerlink" title="A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?"></a>A company is designing a failover strategy in Amazon Route 53 for its resources between two AWS Regions. The company must have the ability to route a user’s traffic to the region with least latency, and if both regions are healthy, Route 53 should route traffic to resources in both regions. Which strategy should the Solutions Architect recommend?</h2><p>A. Configure active-active failover using Route 53 latency DNS records.<br>B. Configure active-passive failover using Route 53 latency DNS records.<br>C. Configure active-active failover using Route 53 failover DNS records.<br>D. Configure active-passive failover using Route 53 failover DNS records.</p><p>Answer: A</p><ul><li>分析：这道题很明显是需要AA模式的，with least latency，所以需要latency。</li></ul><blockquote><p>Active-Active Failover<br>Use this failover configuration when you want all of your resources to be available the majority of the time. When a resource becomes unavailable, Route 53 can detect that it’s unhealthy and stop including it when responding to queries.</p><p>In active-active failover, all the records that have the same name, the same type (such as A or AAAA), and the same routing policy (such as weighted or latency) are active unless Route 53 considers them unhealthy. Route 53 can respond to a DNS query using any healthy record.</p></blockquote><h2 id="A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS"><a href="#A-company-is-developing-several-critical-long-running-applications-hosted-on-Docker-How-should-a-Solutions-Architect-design-a-solution-to-meet-the-scalability-and-orchestration-requirements-on-AWS" class="headerlink" title="A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?"></a>A company is developing several critical long-running applications hosted on Docker. How should a Solutions Architect design a solution to meet the scalability and orchestration requirements on AWS?</h2><p>A. Use Amazon ECS and Service Auto Scaling.<br>B. Use Spot Instances for orchestration and for scaling containers on existing Amazon EC2 instances.<br>C. Use AWS OpsWorks to launch containers in new Amazon EC2 instances.<br>D. Use Auto Scaling groups to launch containers on existing Amazon EC2 instances.</p><p>Answer: A</p><blockquote><p>Amazon Elastic Container Service (Amazon ECS) 是用于在可扩展群集上运行 Docker 应用程序的 Amazon Web Service。在本教程中，您将了解如何在负载均衡器后面的 Amazon ECS 集群上运行支持 Docker 的示例应用程序，对该示例应用程序进行测试，然后删除您的资源以免产生费用。</p></blockquote><h2 id="争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate"><a href="#争议-A-Solutions-Architect-is-developing-a-new-web-application-on-AWS-The-Architect-expects-the-application-to-become-very-popular-so-the-application-must-scale-to-support-the-load-The-Architect-wants-to-focus-on-software-development-and-deploying-new-features-without-provisioning-or-managing-instances-What-solution-is-appropriate" class="headerlink" title="(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?"></a>(争议)A Solutions Architect is developing a new web application on AWS. The Architect expects the application to become very popular, so the application must scale to support the load. The Architect wants to focus on software development and deploying new features without provisioning or managing instances. What solution is appropriate?</h2><p>A. Amazon API Gateway and AWS Lambda<br>B. Elastic Load Balancing with Auto Scaling groups and Amazon EC2<br>C. Amazon API Gateway and Amazon EC2<br>D. Amazon CloudFront and AWS Lambda</p><p>Answer: A</p><ul><li>分析：题目的需求是：1、不想运维；2、集中精力去做开发。这是非常典型的Serverless的需求，将底层交给云原生服务，专注于业务开发。首先应该选择AWS Lambda服务，则AD作为选项。原答案给出的是D，但是CloudFront作为CDN服务，好像并没有向外提供接口的能力，AWS Lambda服务并不像阿里的函数计算提供了Http trigger，所以无法对外提供API接口的能力，从这个角度看A更合理一些。</li></ul><h2 id="A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS"><a href="#A-Solutions-Architect-is-deploying-a-new-production-MySQL-database-on-AWS-It-is-critical-that-the-database-is-highly-available-What-should-the-Architect-do-to-achieve-this-goal-with-Amazon-RDS" class="headerlink" title="A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?"></a>A Solutions Architect is deploying a new production MySQL database on AWS. It is critical that the database is highly available. What should the Architect do to achieve this goal with Amazon RDS?</h2><p>A. Create a read replica of the primary database and deploy it in a different AWS Region.<br>B. Enable multi-AZ to create a standby database in a different Availability Zone.<br>C. Enable multi-AZ to create a standby database in a different AWS Region.<br>D. Create a read replica of the primary database and deploy it in a different Availability Zone.</p><p>Answer: B</p><ul><li>分析：原题目给出的是A，但是B明显是正确答案。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/rds/ha/?nc1=h_ls" target="_blank" rel="noopener">https://aws.amazon.com/cn/rds/ha/?nc1=h_ls</a><br>Amazon Relational Database Service (Amazon RDS) supports two easy-to-use options for ensuring High Availability of your relational database.<br>For your MySQL, MariaDB, PostgreSQL, Oracle, and SQL Server database (DB) instances, you can use Amazon RDS Multi-AZ deployments. When you provision a Multi-AZ DB instance, Amazon RDS automatically creates a primary DB instance and synchronously replicates the data to a standby instance in a different Availability Zone (AZ). In case of an infrastructure failure, Amazon RDS performs an automatic failover to the standby DB instance. Since the endpoint for your DB instance remains the same after a failover, your application can resume database operation without the need for manual administrative intervention. Learn more &gt;&gt;</p></blockquote><h2 id="An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements"><a href="#An-organization-designs-a-mobile-application-for-their-customers-to-upload-photos-to-a-site-The-application-needs-a-secure-login-with-MFA-The-organization-wants-to-limit-the-initial-build-time-and-maintenance-of-the-solution-Which-solution-should-a-Solutions-Architect-recommend-to-meet-the-requirements" class="headerlink" title="An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?"></a>An organization designs a mobile application for their customers to upload photos to a site. The application needs a secure login with MFA. The organization wants to limit the initial build time and maintenance of the solution. Which solution should a Solutions Architect recommend to meet the requirements?</h2><p>A. Use Amazon Cognito Identity with SMS-based MFA.<br>B. Edit AWS IAM policies to require MFA for all users.<br>C. Federate IAM against corporate AD that requires MFA.<br>D. Use Amazon API Gateway and require SSE for photos.</p><p>Answer: A</p><h2 id="争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements"><a href="#争议-A-Solutions-Architect-is-designing-a-solution-to-monitor-weather-changes-by-the-minute-The-frontend-application-is-hosted-on-Amazon-EC2-instances-The-backend-must-be-scalable-to-a-virtually-unlimited-size-and-data-retrieval-must-occur-with-minimal-latency-Which-AWS-service-should-the-Architect-use-to-store-the-data-and-achieve-these-requirements" class="headerlink" title="(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?"></a>(争议)A Solutions Architect is designing a solution to monitor weather changes by the minute. The frontend application is hosted on Amazon EC2 instances. The backend must be scalable to a virtually unlimited size, and data retrieval must occur with minimal latency. Which AWS service should the Architect use to store the data and achieve these requirements?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon RDS<br>D. Amazon EBS</p><p>Answer: A</p><ul><li>分析：这道题很多人都认为B是正确的，但是从篇气象公司最佳实践看，确实采用的是S3方案。</li></ul><blockquote><p>AWS Case Study: The Weather Company(<a href="https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/" target="_blank" rel="noopener">https://aws.amazon.com/cn/solutions/case-studies/the-weather-company/</a>)<br>The platform ingests information from more than 100 different sources and generates close to one-half terabyte (TB) of data each time it updates. The information is mapped and processed into forecast points that can be retrieved in real time, based on queries coming into the system. All data is stored in Amazon Simple Storage Service (Amazon S3), leveraging the efficiency of cloud storage as opposed to an on-premises storage solution and eliminating the hassle of managing a storage platform.</p></blockquote><h2 id="A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency"><a href="#A-company-hosts-a-website-on-premises-The-website-has-a-mix-of-static-and-dynamic-content-but-users-experience-latency-when-loading-static-files-Which-AWS-service-can-help-reduce-latency" class="headerlink" title="A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?"></a>A company hosts a website on premises. The website has a mix of static and dynamic content, but users experience latency when loading static files. Which AWS service can help reduce latency?</h2><p>A. Amazon CloudFront with on-premises servers as the origin<br>B. ELB Application Load Balancer<br>C. Amazon Route 53 latency-based routing<br>D. Amazon EFS to store and server static files</p><p>Answer: A</p><h2 id="A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used"><a href="#A-company-wants-to-analyze-all-of-its-sales-information-aggregated-over-the-last-12-months-The-company-expects-there-to-be-over-10TB-of-data-from-multiple-sources-What-service-should-be-used" class="headerlink" title="A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?"></a>A company wants to analyze all of its sales information aggregated over the last 12 months. The company expects there to be over 10TB of data from multiple sources. What service should be used?</h2><p>A. Amazon DynamoDB<br>B. Amazon Aurora MySQL<br>C. Amazon RDS MySQL<br>D. Amazon Redshift</p><p>Answer: D</p><h2 id="A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation"><a href="#A-media-company-has-deployed-a-multi-tier-architecture-on-AWS-Web-servers-are-deployed-in-two-Availability-Zones-using-an-Auto-Scaling-group-with-a-default-Auto-Scaling-termination-policy-The-web-servers’-Auto-Scaling-group-currently-has-15-instances-running-Which-instance-will-be-terminated-first-during-a-scale-in-operation" class="headerlink" title="A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?"></a>A media company has deployed a multi-tier architecture on AWS. Web servers are deployed in two Availability Zones using an Auto Scaling group with a default Auto Scaling termination policy. The web servers’ Auto Scaling group currently has 15 instances running. Which instance will be terminated first during a scale-in operation?</h2><p>A. The instance with the oldest launch configuration.<br>B. The instance in the Availability Zone that has most instances.<br>C. The instance closest to the next billing hour.<br>D. The oldest instance in the group.</p><p>Answer: B</p><ul><li>控制在缩小过程中终止哪些 Auto Scaling 实例(<a href="https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/autoscaling/ec2/userguide/as-instance-termination.html#default-termination-policy</a>)</li></ul><blockquote><p>当达到缩减策略的阈值时，策略生效，Auto Scaling 组终止其中一个实例。如果您没有为该组分配特定的终止策略，则使用默认终止策略。它选择有两个实例的可用区，并终止从最旧启动配置启动的实例。如果这些实例是从同一启动配置启动的，则 Auto Scaling 组选择最接近下一个计费小时的实例并终止该实例。</p></blockquote><h2 id="A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements"><a href="#A-retail-company-has-sensors-placed-in-its-physical-retail-stores-The-sensors-send-messages-over-HTTP-when-customers-interact-with-in-store-product-displays-A-Solutions-Architect-needs-to-implement-a-system-for-processing-those-sensor-messages-the-results-must-be-available-for-the-Data-Analysis-team-Which-architecture-should-be-used-to-meet-these-requirements" class="headerlink" title="A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?"></a>A retail company has sensors placed in its physical retail stores. The sensors send messages over HTTP when customers interact with in-store product displays. A Solutions Architect needs to implement a system for processing those sensor messages; the results must be available for the Data Analysis team. Which architecture should be used to meet these requirements?</h2><p>A. Implement an Amazon API Gateway to server as the HTTP endpoint. Have the API Gateway trigger an AWS Lambda function to process the messages, and save the results to an Amazon DynamoDB table.<br>B. Create an Amazon EC2 instance to server as the HTTP endpoint and to process the messages. Save the results to Amazon S3 for the Data Analysis team to download.<br>C. Use Amazon Route 53 to direct incoming sensor messages to a Lambda function to process the message and save the results to a Amazon DynamoDB table.<br>D. Use AWS Direct Connect to connect sensors to DynamoDB so that data can be written directly to a DynamoDB table where it can be accessed by the Data Analysis team.</p><p>Answer: A</p><ul><li>分析：原来AWS Lambda的HTTP trigger是这么实现的</li></ul><h2 id="A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements"><a href="#A-client-is-migrating-a-legacy-web-application-to-the-AWS-Cloud-The-current-system-uses-an-Oracle-database-as-a-relational-database-management-system-solution-Backups-occur-every-night-and-the-data-is-stored-on-premises-The-Solutions-Architect-must-automate-the-backups-and-identity-a-storage-solution-while-keeping-costs-low-Which-AWS-service-will-meet-these-requirements" class="headerlink" title="A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?"></a>A client is migrating a legacy web application to the AWS Cloud. The current system uses an Oracle database as a relational database management system solution. Backups occur every night, and the data is stored on-premises. The Solutions Architect must automate the backups and identity a storage solution while keeping costs low. Which AWS service will meet these requirements?</h2><p>A. Amazon RDS<br>B. Amazon RedShift<br>C. Amazon DynamoDB Accelerator<br>D. Amazon ElastiCache</p><p>Answer: A</p><h2 id="争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met"><a href="#争议-A-company-has-an-Amazon-RDS-database-backing-its-production-website-The-Sales-team-needs-to-run-queries-against-the-database-to-track-training-program-effectiveness-Queries-against-the-production-database-cannot-impact-performance-and-the-solution-must-be-easy-to-maintain-How-can-these-requirements-be-met" class="headerlink" title="(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?"></a>(争议)A company has an Amazon RDS database backing its production website. The Sales team needs to run queries against the database to track training program effectiveness. Queries against the production database cannot impact performance, and the solution must be easy to maintain. How can these requirements be met?</h2><p>A. Use an Amazon Redshift database. Copy the product database into Redshift and allow the team to query it.<br>B. Use an Amazon RDS read replica of the production database and allow the team to query against it.<br>C. Use multiple Amazon EC2 instances running replicas of the production database, placed behind a load balancer.<br>D. Use an Amazon DynamoDB table to store a copy of the data.</p><p>Answer: B</p><ul><li>争议：原有答案给出的是A，根据题目描述看起来是一个数据仓库的需求。从easy to maintain的角度说B，更简单</li></ul><h2 id="A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two"><a href="#A-company-must-collect-temperature-data-from-thousands-of-remote-weather-devices-The-company-must-also-store-this-data-in-a-data-warehouse-to-run-aggregations-and-visualizations-Which-services-will-meet-these-requirements-Choose-two" class="headerlink" title="A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)"></a>A company must collect temperature data from thousands of remote weather devices. The company must also store this data in a data warehouse to run aggregations and visualizations. Which services will meet these requirements? (Choose two.)</h2><p>A. Amazon Kinesis Data Firehouse<br>B. Amazon SQS<br>C. Amazon Redshift<br>D. Amazon SNS<br>E. Amazon DynamoDB</p><p>Answer: AC</p><ul><li>分析：A负责接收数据并处理，C作为数据仓库存储下来</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;通过之前100道题的梳理，发现这个网站竟然有这么多争议的题目，我觉得有可能是有些题目已经跟不上AWS自身发展速度了，有了更多的方法。总之，通过这些题目的梳理，对AWS服务细节层面有了更多的了解，希望能够一次性通过ACA考试。这篇继续这个网站101到200题的学习工作，希望能提高点速度。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.site/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.site/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>AWS Certified Solutions Architect - Associate Exam(Q1-Q100)</title>
    <link href="http://sunqi.site/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/"/>
    <id>http://sunqi.site/2019/12/31/AWS-Certified-Solutions-Architect-Associate-Exam/</id>
    <published>2019-12-31T01:11:55.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>参考链接：<a href="https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/" target="_blank" rel="noopener">https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/</a></p><p>一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。</p><p>由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。</p><a id="more"></a><h2 id="A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest"><a href="#A-Solutions-Architect-is-designing-an-application-that-will-encrypt-all-data-in-an-Amazon-Redshift-cluster-Which-action-will-encrypt-the-data-at-rest" class="headerlink" title="A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?"></a>A Solutions Architect is designing an application that will encrypt all data in an Amazon Redshift cluster. Which action will encrypt the data at rest?</h2><p>A. Place the Redshift cluster in a private subnet.<br>B. Use the AWS KMS Default Customer master key.<br>C. Encrypt the Amazon EBS volumes.<br>D. Encrypt the data using SSL/TLS.</p><p>Answer: B</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/redshift/latest/mgmt/working-with-db-encryption.html</a></li><li>分析：Amazon Redshift 使用加密密钥层次结构来加密数据库。您可以使用 AWS Key Management Service (AWS KMS) 或硬件安全模块 (HSM) 来管理该层次结构中的顶级加密密钥。Amazon Redshift 用于加密的流程因您管理密钥的方式而异。Amazon Redshift 自动与 AWS KMS 集成，而不与 HSM 集成。当您使用 HSM 时，必须使用客户端和服务器证书在 Amazon Redshift 和 HSM 之间配置受信任的连接。</li></ul><h2 id="A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database"><a href="#A-website-experiences-unpredictable-traffic-During-peak-traffic-times-the-database-is-unable-to-keep-up-with-the-write-request-Which-AWS-service-will-help-decouple-the-web-application-from-the-database" class="headerlink" title="A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?"></a>A website experiences unpredictable traffic. During peak traffic times, the database is unable to keep up with the write request. Which AWS service will help decouple the web application from the database?</h2><p>A. Amazon SQS<br>B. Amazon EFS<br>C. Amazon S3<br>D. AWS Lambda</p><p>Answer: A</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/sqs/faqs/" target="_blank" rel="noopener">https://aws.amazon.com/cn/sqs/faqs/</a></li><li>分析：关键词是unpredictable traffic, keep up with write request, decouple the web application, 所以通过消息队列服务可以让写入请求排队，从而实现前端应用和后端数据库的解耦。</li></ul><h2 id="A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements"><a href="#A-legacy-application-needs-to-interact-with-local-storage-using-iSCSI-A-team-needs-to-design-a-reliable-storage-solution-to-provision-all-new-storage-on-AWS-Which-storage-solution-meets-the-legacy-application-requirements" class="headerlink" title="A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?"></a>A legacy application needs to interact with local storage using iSCSI. A team needs to design a reliable storage solution to provision all new storage on AWS. Which storage solution meets the legacy application requirements?</h2><p>A. AWS Snowball storage for the legacy application until the application can be re-architected.<br>B. AWS Storage Gateway in cached mode for the legacy application storage to write data to Amazon S3.<br>C. AWS Storage Gateway in stored mode for the legacy application storage to write data to Amazon S3.<br>D. An Amazon S3 volume mounted on the legacy application server locally using the File Gateway service.</p><p>Answer: C</p><ul><li>分析：关键词是local stroage with iSCSI, 并且需要将所有新的存储用AWS提供，所以排除A选项；因为用到了iSCSI协议，所以S3使用文件网关方式也不适用，排除D；剩下的B和C区别在于存储模式，因为需要本地应用请求，所以需要使用存储模式，而不能用缓存模式，所以最终选择C。</li></ul><h2 id="A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck"><a href="#A-Solutions-Architect-is-designing-an-architecture-for-a-mobile-gaming-application-The-application-is-expected-to-be-very-popular-The-Architect-needs-to-prevent-the-Amazon-RDS-MySQL-database-from-becoming-a-bottleneck-due-to-frequently-accessed-queries-Which-service-or-feature-should-the-Architect-add-to-prevent-a-bottleneck" class="headerlink" title="A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?"></a>A Solutions Architect is designing an architecture for a mobile gaming application. The application is expected to be very popular. The Architect needs to prevent the Amazon RDS MySQL database from becoming a bottleneck due to frequently accessed queries. Which service or feature should the Architect add to prevent a bottleneck?</h2><p>A. Multi-AZ feature on the RDS MySQL Database<br>B. ELB Classic Load Balancer in front of the web application tier<br>C. Amazon SQS in front of RDS MySQL Database<br>D. Amazon ElastiCache in front of the RDS MySQL Database</p><p>Answer: D</p><ul><li>分析：该问题的关键在于bottleneck due to frequently accessed queries，查询变成瓶颈，可以使用ElastiCache服务作为缓存，降低读取频率解决问题。</li></ul><h2 id="A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives"><a href="#A-company-is-launching-an-application-that-it-expects-to-be-very-popular-The-company-needs-a-database-that-can-scale-with-the-rest-of-the-application-The-schema-will-change-frequently-The-application-cannot-afford-any-downtime-for-database-changes-Which-AWS-service-allows-the-company-to-achieve-these-objectives" class="headerlink" title="A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?"></a>A company is launching an application that it expects to be very popular. The company needs a database that can scale with the rest of the application. The schema will change frequently. The application cannot afford any downtime for database changes. Which AWS service allows the company to achieve these objectives?</h2><p>A. Amazon Redshift<br>B. Amazon DynamoDB<br>C. Amazon RDS MySQL<br>D. Amazon Aurora</p><p>Answer: B</p><ul><li>分析：原网站给出的答案是A，但是经过分析觉得有些问题，这道题的几个关键词：scale with the rest of the application, schema will change frequently, cannot afford any downtime for database changes. 首先，schema总是变更，所以这里需要的非关系型数据库，排除C和D。Redshift是数据仓库，其实也是数据仓库，从第一点上就可以排除。另外从这个链接（<a href="http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero" target="_blank" rel="noopener">http://braindump2go.hatenablog.com/entry/2019/11/05/123057）分析上，还有一点除了DynamoDB可以真正做到scale时候zero</a> downtime，其他的都不行。所以原网站给出的答案是错误的。</li></ul><h2 id="A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements"><a href="#A-Solution-Architect-is-designing-a-disaster-recovery-solution-for-a-5-TB-Amazon-Redshift-cluster-The-recovery-site-must-be-at-least-500-miles-805-kilometers-from-the-live-site-How-should-the-Architect-meet-these-requirements" class="headerlink" title="A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?"></a>A Solution Architect is designing a disaster recovery solution for a 5 TB Amazon Redshift cluster. The recovery site must be at least 500 miles (805 kilometers) from the live site. How should the Architect meet these requirements?</h2><p>A. Use AWS CloudFormation to deploy the cluster in a second region.<br>B. Take a snapshot of the cluster and copy it to another Availability Zone.<br>C. Modify the Redshift cluster to span two regions.<br>D. Enable cross-region snapshots to a different region.</p><p>Answer: D</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/automated-cross-region-snapshot-copy-for-amazon-redshift/</a></li></ul><h2 id="A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct"><a href="#A-customer-has-written-an-application-that-uses-Amazon-S3-exclusively-as-a-data-store-The-application-works-well-until-the-customer-increases-the-rate-at-which-the-application-is-updating-information-The-customer-now-reports-that-outdated-data-occasionally-appears-when-the-application-accesses-objects-in-Amazon-S3-What-could-be-the-problem-given-that-the-application-logic-is-otherwise-correct" class="headerlink" title="A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?"></a>A customer has written an application that uses Amazon S3 exclusively as a data store. The application works well until the customer increases the rate at which the application is updating information. The customer now reports that outdated data occasionally appears when the application accesses objects in Amazon S3. What could be the problem, given that the application logic is otherwise correct?</h2><p>A. The application is reading parts of objects from Amazon S3 using a range header.<br>B. The application is reading objects from Amazon S3 using parallel object requests.<br>C. The application is updating records by writing new objects with unique keys.<br>D. The application is updating records by overwriting existing objects with the same keys.</p><p>Answer: D</p><ul><li>分析：这道题也是争论很大的一道题，原网站答案为A。问题简单描述为客户端访问不到最新的数据，发生的时间点在于应用上传信息时候速率提高导致的，所以问题应该出现在写入的时候，这样排除A和B读取的问题。因为S3同一object永远是覆盖，所以最有可能的问题是在same key的情况下，所以选择D。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-new-social-media-application-The-application-must-provide-a-secure-method-for-uploading-profile-photos-Each-user-should-be-able-to-upload-a-profile-photo-into-a-shared-storage-location-for-one-week-after-their-profile-is-created-Which-approach-will-meet-all-of-these-requirements" class="headerlink" title="A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?"></a>A Solutions Architect is designing a new social media application. The application must provide a secure method for uploading profile photos. Each user should be able to upload a profile photo into a shared storage location for one week after their profile is created. Which approach will meet all of these requirements?</h2><p>A. Use Amazon Kinesis with AWS CloudTrail for auditing the specific times when profile photos are uploaded.<br>B. Use Amazon EBS volumes with IAM policies restricting user access to specific time periods.<br>C. Use Amazon S3 with the default private access policy and generate pre-signed URLs each time a new site profile is created.<br>D. Use Amazon CloudFront with AWS CloudTrail for auditing the specific times when profile photos are uploaded.</p><p>Answer: C</p><h2 id="An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application"><a href="#An-application-requires-block-storage-for-file-updates-The-data-is-500-GB-and-must-continuously-sustain-100-MiB-s-of-aggregate-read-write-operations-Which-storage-option-is-appropriate-for-this-application" class="headerlink" title="An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?"></a>An application requires block storage for file updates. The data is 500 GB and must continuously sustain 100 MiB/s of aggregate read/write operations. Which storage option is appropriate for this application?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Amazon Glacier</p><p>Answer: C</p><ul><li>分析：没想到这道题原网站给出的答案是B，争议比较大，但是从题目描述需要Block Storage角度来看，选择C才是最合理的。这道题还需要进一步确认一下。</li></ul><h2 id="A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements"><a href="#A-mobile-application-serves-scientific-articles-from-individual-files-in-an-Amazon-S3-bucket-Articles-older-than-30-days-are-rarely-read-Articles-older-than-60-days-no-longer-need-to-be-available-through-the-application-but-the-application-owner-would-like-to-keep-them-for-historical-purposes-Which-cost-effective-solution-BEST-meets-these-requirements" class="headerlink" title="A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?"></a>A mobile application serves scientific articles from individual files in an Amazon S3 bucket. Articles older than 30 days are rarely read. Articles older than 60 days no longer need to be available through the application, but the application owner would like to keep them for historical purposes. Which cost-effective solution BEST meets these requirements?</h2><p>A. Create a Lambda function to move files older than 30 days to Amazon EBS and move files older than 60 days to Amazon Glacier.<br>B. Create a Lambda function to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon EBS.<br>C. Create lifecycle rules to move files older than 30 days to Amazon S3 Standard Infrequent Access and move files older than 60 days to Amazon Glacier.<br>D. Create lifecycle rules to move files older than 30 days to Amazon Glacier and move files older than 60 days to Amazon S3 Standard Infrequent Access.</p><p>Answer: C</p><ul><li>分析：很明显的排除A和B，S3可以自定义规则，那么问题就是30天后和60天后需要哪种存储的问题了，根据题目C是明显正确的。</li></ul><h2 id="An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs"><a href="#An-organization-is-currently-hosting-a-large-amount-of-frequently-accessed-data-consisting-of-key-value-pairs-and-semi-structured-documents-in-their-data-center-They-are-planning-to-move-this-data-to-AWS-Which-of-one-of-the-following-services-MOST-effectively-meets-their-needs" class="headerlink" title="An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?"></a>An organization is currently hosting a large amount of frequently accessed data consisting of key-value pairs and semi-structured documents in their data center. They are planning to move this data to AWS. Which of one of the following services MOST effectively meets their needs?</h2><p>A. Amazon Redshift<br>B. Amazon RDS<br>C. Amazon DynamoDB<br>D. Amazon Aurora</p><p>Answer: C</p><h2 id="A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two"><a href="#A-Lambda-function-must-execute-a-query-against-an-Amazon-RDS-database-in-a-private-subnet-Which-steps-are-required-to-allow-the-Lambda-function-to-access-the-Amazon-RDS-database-Select-two" class="headerlink" title="A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)"></a>A Lambda function must execute a query against an Amazon RDS database in a private subnet. Which steps are required to allow the Lambda function to access the Amazon RDS database? (Select two.)</h2><p>A. Create a VPC Endpoint for Amazon RDS.<br>B. Create the Lambda function within the Amazon RDS VPC.<br>C. Change the ingress rules of Lambda security group, allowing the Amazon RDS security group.<br>D. Change the ingress rules of the Amazon RDS security group, allowing the Lambda security group.<br>E. Add an Internet Gateway (IGW) to the VPC, route the private subnet to the IGW.</p><p>Answer: BD</p><ul><li>分析：又是原网站一道错题，原网站答案为AD。D选项是允许Lambda服务访问RDS，所以在进方向允许。</li><li>目前VPC支持Endpoint的服务：<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-endpoints.html</a></li></ul><blockquote><p>Amazon API Gateway<br>Amazon AppStream 2.0<br>AWS App Mesh<br>Amazon Athena<br>AWS CloudFormation<br>AWS CloudTrail<br>Amazon CloudWatch<br>Amazon CloudWatch Events<br>Amazon CloudWatch Logs<br>AWS CodeBuild<br>AWS CodeCommit<br>AWS CodePipeline<br>AWS Config<br>AWS DataSync<br>Amazon EC2 API<br>Elastic Load Balancing<br>Amazon Elastic Container Registry<br>Amazon Elastic Container Service<br>AWS Glue<br>AWS Key Management Service<br>Amazon Kinesis Data Firehose<br>Amazon Kinesis Data Streams<br>Amazon Rekognition<br>Amazon SageMaker 和 Amazon SageMaker 运行时<br>Amazon SageMaker 笔记本<br>AWS Secrets Manager<br>AWS Security Token Service<br>AWS Service Catalog<br>Amazon SNS<br>Amazon SQS<br>AWS Systems Manager<br>AWS Storage Gateway<br>AWS Transfer for SFTP<br>其他 AWS 账户托管的终端节点服务</p><p>网关终端节点是一个网关，作为您在路由表中指定的路由的目标，用于发往受支持的 AWS 服务的流量。支持以下 AWS 服务：<br>Amazon S3<br>DynamoDB</p></blockquote><h2 id="待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement"><a href="#待实际环境验证-A-Solutions-Architect-needs-to-build-a-resilient-data-warehouse-using-Amazon-Redshift-The-Architect-needs-to-rebuild-the-Redshift-cluster-in-another-region-Which-approach-can-the-Architect-take-to-address-this-requirement" class="headerlink" title="(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?"></a>(待实际环境验证)A Solutions Architect needs to build a resilient data warehouse using Amazon Redshift. The Architect needs to rebuild the Redshift cluster in another region. Which approach can the Architect take to address this requirement?</h2><p>A. Modify the Redshift cluster and configure cross-region snapshots to the other region.<br>B. Modify the Redshift cluster to take snapshots of the Amazon EBS volumes each day, sharing those snapshots with the other region.<br>C. Modify the Redshift cluster and configure the backup and specify the Amazon S3 bucket in the other region.<br>D. Modify the Redshift cluster to use AWS Snowball in export mode with data delivered to the other region.</p><p>Answer: A</p><ul><li>分析：又是一道错题，Redhift备份是通过S3实现的, 所以不存在B的情况，我个人有点倾向于C，但是A确实是Redshift在快照时默认的格式，可能是更容易恢复吧，这道题需要在实际环境进行一下验证。另外国际版本的Redshift和国内的应该比国内的高很多。</li></ul><blockquote><p>问：Amazon Redshift 如何备份数据？ 如何从备份中还原我的集群？</p><p>在加载数据时，Amazon Redshift 会复制数据仓库集群内的所有数据并将其连续备份至 S3。Amazon Redshift 始终尝试维持至少三份数据（计算节点上的正本数据、副本数据和 Amazon S3 上的备份数据）。Redshift 还能将您的快照异步复制到另一个区域的 S3 中进行灾难恢复。</p><p>默认情况下，Amazon Redshift 以一天的保留期启用数据仓库群集的自动化备份。您可将其配置为 35 天之久。</p><p>免费备份存储受限于数据仓库群集中节点上的总存储大小，并仅适用于已激活的数据仓库群集。例如，如果您有 8TB 的数据仓库总存储大小，那么我们将提供最多 8TB 的备份存储而不另外收费。如果您想将备份保留期延长为超过一天，那么您可以使用 AWS 管理控制台或 Amazon Redshift API 来实现这一目的。有关自动快照的更多信息，请参阅《Amazon Redshift 管理指南》。Amazon Redshift 仅备份已更改的数据，因此大多数快照仅占用少量的免费备份存储。</p><p>如果您需要还原备份，则可以在备份保留期内访问所有自动备份。在您选择某个要还原的备份后，我们将预置一个新的数据仓库集群并将数据还原至此集群中。</p></blockquote><h2 id="A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance"><a href="#A-popular-e-commerce-application-runs-on-AWS-The-application-encounters-performance-issues-The-database-is-unable-to-handle-the-amount-of-queries-and-load-during-peak-times-The-database-is-running-on-the-RDS-Aurora-engine-on-the-largest-instance-size-available-What-should-an-administrator-do-to-improve-performance" class="headerlink" title="A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?"></a>A popular e-commerce application runs on AWS. The application encounters performance issues. The database is unable to handle the amount of queries and load during peak times. The database is running on the RDS Aurora engine on the largest instance size available. What should an administrator do to improve performance?</h2><p>A. Convert the database to Amazon Redshift.<br>B. Create a CloudFront distribution.<br>C. Convert the database to use EBS Provisioned IOPS.<br>D. Create one or more read replicas.</p><p>Answer: C</p><ul><li>分析：这道题我最开始选择的是D，但是评论区的一种解释有一定的道理：这个网站应用类型为电商，原题中没有很清楚说明queris and load的压力有多大，很可能我们建立了read replicas只能临时性解决问题，并不是一劳永逸的方式。并且根据<a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Replication.html</a></li></ul><blockquote><p>因此，所有 Aurora 副本均返回相同的查询结果数据，且副本滞后时间非常短 - 通常远远少于主实例写入更新后的 100 毫秒。副本滞后因数据库更改速率而异。也就是说，在对数据库执行大量写入操作期间，您可能发现副本滞后时间变长。</p></blockquote><p>如果读副本在这个延时上，很可能对业务系统造成很大的影响。</p><h2 id="A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two"><a href="#A-Solutions-Architect-is-designing-the-architecture-for-a-new-three-tier-web-based-e-commerce-site-that-must-be-available-24-7-Requests-are-expected-to-range-from-100-to-10-000-each-minute-Usage-can-vary-depending-on-time-of-day-holidays-and-promotions-The-design-should-be-able-to-handle-these-volumes-with-the-ability-to-handle-higher-volumes-if-necessary-How-should-the-Architect-design-the-architecture-to-ensure-the-web-tier-is-cost-optimized-and-can-handle-the-expected-traffic-Select-two" class="headerlink" title="A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)"></a>A Solutions Architect is designing the architecture for a new three-tier web-based e-commerce site that must be available 24/7. Requests are expected to range from 100 to 10,000 each minute. Usage can vary depending on time of day, holidays, and promotions. The design should be able to handle these volumes, with the ability to handle higher volumes if necessary. How should the Architect design the architecture to ensure the web tier is cost-optimized and can handle the expected traffic? (Select two.)</h2><p>A. Launch Amazon EC2 instances in an Auto Scaling group behind an ELB.<br>B. Store all static files in a multi-AZ Amazon Aurora database.<br>C. Create an CloudFront distribution pointing to static content in Amazon S3.<br>D. Use Amazon Route 53 to route traffic to the correct region.<br>E. Use Amazon S3 multi-part uploads to improve upload times.</p><p>Answer: AC</p><ul><li>分析：A是很明显的，弹性伸缩，节约成本，其他几项和题干没有太多关系，所以选的C。</li></ul><h2 id="A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements"><a href="#A-Solution-Architect-is-designing-a-three-tier-web-application-The-Architect-wants-to-restrict-access-to-the-database-tier-to-accept-traffic-from-the-application-servers-only-However-these-application-servers-are-in-an-Auto-Scaling-group-and-may-vary-in-quantity-How-should-the-Architect-configure-the-database-servers-to-meet-the-requirements" class="headerlink" title="A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?"></a>A Solution Architect is designing a three-tier web application. The Architect wants to restrict access to the database tier to accept traffic from the application servers only. However, these application servers are in an Auto Scaling group and may vary in quantity. How should the Architect configure the database servers to meet the requirements?</h2><p>A. Configure the database security group to allow database traffic from the application server IP addresses.<br>B. Configure the database security group to allow database traffic from the application server security group.<br>C. Configure the database subnet network ACL to deny all inbound non-database traffic from the application-tier subnet.<br>D. Configure the database subnet network ACL to allow inbound database traffic from the application-tier subnet.</p><p>Answer: B</p><ul><li>分析：这好像又是一道错题，原给出的答案是C。首先要明确的一点是SG是工作在instance级别，NACL是在子网级别，SG默认全部Deny，NACL默认全部Allow。A不对的原因是insance在Auto Scaling里，IP地址是不固定的。D不对的原因是NACL默认全都是Allow的。其实本质上考察的是如何选择安全组还是网络防火墙的问题。不选择C的原因是因为配置NACL规则至少需要阻止和允许，而通过安全组只需要配置一条即可。但是也有一种声音认为题目中关键词restrict意味着需要deny流量。</li></ul><blockquote><p>问：VPC 中的安全组和 VPC 中的网络 ACL 有什么区别？</p><p>VPC 中的安全组指定允许传入或传出 Amazon EC2 实例的流量。网络 ACL 则在子网级别上运作，评估进出某个子网的流量。网络 ACL 可通过设置允许和拒绝规则来进行使用。Network ACL 不能筛选同一子网中实例之间的流量。此外，网络 ACL 执行无状态筛选，而安全组则执行有状态筛选。</p></blockquote><h2 id="An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture"><a href="#An-Internet-facing-multi-tier-web-application-must-be-highly-available-An-ELB-Classic-Load-Balancer-is-deployed-in-front-of-the-web-tier-Amazon-EC2-instances-at-the-web-application-tier-are-deployed-evenly-across-two-Availability-Zones-The-database-is-deployed-using-RDS-Multi-AZ-A-NAT-instance-is-launched-for-Amazon-EC2-instances-and-database-resources-to-access-the-Internet-These-instances-are-not-assigned-with-public-IP-addresses-Which-component-poses-a-potential-single-point-of-failure-in-this-architecture" class="headerlink" title="An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?"></a>An Internet-facing multi-tier web application must be highly available. An ELB Classic Load Balancer is deployed in front of the web tier. Amazon EC2 instances at the web application tier are deployed evenly across two Availability Zones. The database is deployed using RDS Multi-AZ. A NAT instance is launched for Amazon EC2 instances and database resources to access the Internet. These instances are not assigned with public IP addresses. Which component poses a potential single point of failure in this architecture?</h2><p>A. Amazon EC2<br>B. NAT instance<br>C. ELB Classic Load Balancer<br>D. Amazon RDS</p><p>Answer: B</p><ul><li>分析：这道题竟然给出了C的答案，很意外。</li></ul><blockquote><p><a href="https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/" target="_blank" rel="noopener">https://aws.amazon.com/articles/high-availability-for-amazon-vpc-nat-instances-an-example/</a></p><p>Instances in a private subnet can access the Internet without exposing their private IP address by routing their traffic through a Network Address Translation (NAT) instance in a public subnet. A NAT instance, however, can introduce a single point of failure to your VPC’s outbound traffic. This situation is depicted in the diagram below.</p></blockquote><h2 id="A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem"><a href="#A-call-center-application-consists-of-a-three-tier-application-using-Auto-Scaling-groups-to-automatically-scale-resources-as-needed-Users-report-that-every-morning-at-9-00-AM-the-system-becomes-very-slow-for-about-15-minutes-A-Solution-Architect-determines-that-a-large-percentage-of-the-call-center-staff-starts-work-at-9-00-AM-so-Auto-Scaling-does-not-have-enough-time-to-scale-out-to-meet-demand-How-can-the-Architect-fix-the-problem" class="headerlink" title="A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?"></a>A call center application consists of a three-tier application using Auto Scaling groups to automatically scale resources as needed. Users report that every morning at 9:00 AM the system becomes very slow for about 15 minutes. A Solution Architect determines that a large percentage of the call center staff starts work at 9:00 AM, so Auto Scaling does not have enough time to scale out to meet demand. How can the Architect fix the problem?</h2><p>A. Change the Auto Scaling group’s scale out event to scale based on network utilization.<br>B. Create an Auto Scaling scheduled action to scale out the necessary resources at 8:30 AM every morning.<br>C. Use Reserved Instances to ensure the system has reserved the right amount of capacity for the scale-up events.<br>D. Permanently keep a steady state of instances that is needed at 9:00 AM to guarantee available resources, but leverage Spot Instances.</p><p>Answer: B</p><ul><li>分析：竟然又是一道错题，记得在AWS听过这道题的分析。原答案是A，但是可能并不是由于网络引起的访问缓慢。</li></ul><h2 id="An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met"><a href="#An-e-commerce-application-is-hosted-in-AWS-The-last-time-a-new-product-was-launched-the-application-experienced-a-performance-issue-due-to-an-enormous-spike-in-traffic-Management-decided-that-capacity-must-be-doubled-the-week-after-the-product-is-launched-Which-is-the-MOST-efficient-way-for-management-to-ensure-that-capacity-requirements-are-met" class="headerlink" title="An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?"></a>An e-commerce application is hosted in AWS. The last time a new product was launched, the application experienced a performance issue due to an enormous spike in traffic. Management decided that capacity must be doubled the week after the product is launched. Which is the MOST efficient way for management to ensure that capacity requirements are met?</h2><p>A. Add a Step Scaling policy.<br>B. Add a Dynamic Scaling policy.<br>C. Add a Scheduled Scaling action.<br>D. Add Amazon EC2 Spot Instances.</p><p>Answer: B</p><ul><li>分析：又是一道争议比较大的题目，争议最大的是C选项，因为题目中有几个词在暗示时间，但是又不明确。既然现有性能上无法应对高峰访问，那么从这个角度还是通过Dynamic配置一个规则进行动态规则最为有效。所以还是选择B。</li></ul><h2 id="A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost"><a href="#A-customer-owns-a-simple-API-for-their-website-that-receives-about-1-000-requests-each-day-and-has-an-average-response-time-of-50-ms-It-is-currently-hosted-on-one-c4-large-instance-Which-changes-to-the-architecture-will-provide-high-availability-at-the-LOWEST-cost" class="headerlink" title="A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?"></a>A customer owns a simple API for their website that receives about 1,000 requests each day and has an average response time of 50 ms. It is currently hosted on one c4.large instance. Which changes to the architecture will provide high availability at the LOWEST cost?</h2><p>A. Create an Auto Scaling group with a minimum of one instance and a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>B. Recreate the API using Amazon API Gateway and use AWS Lambda as the service backend.<br>C. Create an Auto Scaling group with a maximum of two instances, then use an Application Load Balancer to balance the traffic.<br>D. Recreate the API using Amazon API Gateway and integrate the new API with the existing backend service.</p><p>Answer: A</p><ul><li>分析：这道题有个陷阱，Simple API，确实如果在不考虑开发的前提下B确实是最佳选项，但是重构也是要花成本的。所以我坚持选A。</li></ul><h2 id="A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement"><a href="#A-Solution-Architect-is-designing-an-application-that-uses-Amazon-EBS-volumes-The-volumes-must-be-backed-up-to-a-different-region-How-should-the-Architect-meet-this-requirement" class="headerlink" title="A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?"></a>A Solution Architect is designing an application that uses Amazon EBS volumes. The volumes must be backed up to a different region. How should the Architect meet this requirement?</h2><p>A. Create EBS snapshots directly from one region to another.<br>B. Move the data to an Amazon S3 bucket and enable cross-region replication.<br>C. Create EBS snapshots and then copy them to the desired region.<br>D. Use a script to copy data from the current Amazon EBS volume to the destination Amazon EBS volume.</p><p>Answer: C</p><h2 id="A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues"><a href="#A-company-is-using-an-Amazon-S3-bucket-located-in-us-west-2-to-serve-videos-to-their-customers-Their-customers-are-located-all-around-the-world-and-the-videos-are-requested-a-lot-during-peak-hours-Customers-in-Europe-complain-about-experiencing-slow-downloaded-speeds-and-during-peak-hours-customers-in-all-locations-report-experiencing-HTTP-500-errors-What-can-a-Solutions-Architect-do-to-address-these-issues" class="headerlink" title="A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?"></a>A company is using an Amazon S3 bucket located in us-west-2 to serve videos to their customers. Their customers are located all around the world and the videos are requested a lot during peak hours. Customers in Europe complain about experiencing slow downloaded speeds, and during peak hours, customers in all locations report experiencing HTTP 500 errors. What can a Solutions Architect do to address these issues?</h2><p>A. Place an elastic load balancer in front of the Amazon S3 bucket to distribute the load during peak hours.<br>B. Cache the web content with Amazon CloudFront and use all Edge locations for content delivery.<br>C. Replicate the bucket in eu-west-1 and use an Amazon Route 53 failover routing policy to determine which bucket it should serve the request to.<br>D. Use an Amazon Route 53 weighted routing policy for the CloudFront domain name to distribute the GET request between CloudFront and the Amazon S3 bucket directly.</p><p>Answer: B</p><ul><li>分析：网站给出的答案竟然是D，但是B很明显是正确的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use"><a href="#A-Solutions-Architect-is-designing-a-solution-that-includes-a-managed-VPN-connection-To-monitor-whether-the-VPN-connection-is-up-or-down-the-Architect-should-use" class="headerlink" title="A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:"></a>A Solutions Architect is designing a solution that includes a managed VPN connection. To monitor whether the VPN connection is up or down, the Architect should use:</h2><p>A. an external service to ping the VPN endpoint from outside the VPC.<br>B. AWS CloudTrail to monitor the endpoint.<br>C. the CloudWatch TunnelState Metric.<br>D. an AWS Lambda function that parses the VPN connection logs.</p><p>Answer: C</p><blockquote><p>Monitoring VPN Tunnels Using Amazon CloudWatch(<a href="https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpn/latest/s2svpn/monitoring-cloudwatch-vpn.html</a>)</p></blockquote><h2 id="A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance"><a href="#A-social-networking-portal-experiences-latency-and-throughput-issues-due-to-an-increased-number-of-users-Application-servers-use-very-large-datasets-from-an-Amazon-RDS-database-which-creates-a-performance-bottleneck-on-the-database-Which-AWS-service-should-be-used-to-improve-performance" class="headerlink" title="A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?"></a>A social networking portal experiences latency and throughput issues due to an increased number of users. Application servers use very large datasets from an Amazon RDS database, which creates a performance bottleneck on the database. Which AWS service should be used to improve performance?</h2><p>A. Auto Scaling<br>B. Amazon SQS<br>C. Amazon ElastiCache<br>D. ELB Application Load Balancer</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement"><a href="#A-Solutions-Architect-is-designing-network-architecture-for-an-application-that-has-compliance-requirements-The-application-will-be-hosted-on-Amazon-EC2-instances-in-a-private-subnet-and-will-be-using-Amazon-S3-for-storing-data-The-compliance-requirements-mandate-that-the-data-cannot-traverse-the-public-Internet-What-is-the-MOST-secure-way-to-satisfy-this-requirement" class="headerlink" title="A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?"></a>A Solutions Architect is designing network architecture for an application that has compliance requirements. The application will be hosted on Amazon EC2 instances in a private subnet and will be using Amazon S3 for storing data. The compliance requirements mandate that the data cannot traverse the public Internet. What is the MOST secure way to satisfy this requirement?</h2><p>A. Use a NAT Instance.<br>B. Use a NAT Gateway.<br>C. Use a VPC endpoint.<br>D. Use a Virtual Private Gateway.</p><p>Answer: C</p><blockquote><p>New – VPC Endpoint for Amazon S3(<a href="https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/new-vpc-endpoint-for-amazon-s3/</a>)</p></blockquote><h2 id="Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance"><a href="#Developers-are-creating-a-new-online-transaction-processing-OLTP-application-for-a-small-database-that-is-very-read-write-intensive-A-single-table-in-the-database-is-updated-continuously-throughout-the-day-and-the-developers-want-to-ensure-that-the-database-performance-is-consistent-Which-Amazon-EBS-storage-option-will-achieve-the-MOST-consistent-performance-to-help-maintain-application-performance" class="headerlink" title="Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?"></a>Developers are creating a new online transaction processing (OLTP) application for a small database that is very read-write intensive. A single table in the database is updated continuously throughout the day, and the developers want to ensure that the database performance is consistent. Which Amazon EBS storage option will achieve the MOST consistent performance to help maintain application performance?</h2><p>A. Provisioned IOPS SSD<br>B. General Purpose SSD<br>C. Cold HDD<br>D. Throughput Optimized HDD</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-log-processing-solution-that-requires-storage-that-supports-up-to-500-MB-s-throughput-The-data-is-sequentially-accessed-by-an-Amazon-EC2-instance-Which-Amazon-storage-type-satisfies-these-requirements" class="headerlink" title="A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?"></a>A Solutions Architect is designing a log-processing solution that requires storage that supports up to 500 MB/s throughput. The data is sequentially accessed by an Amazon EC2 instance. Which Amazon storage type satisfies these requirements?</h2><p>A. EBS Provisioned IOPS SSD (io1)<br>B. EBS General Purpose SSD (gp2)<br>C. EBS Throughput Optimized HDD (st1)<br>D. EBS Cold HDD (sc1)</p><p>Answer: C</p><h2 id="A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of"><a href="#A-company’s-development-team-plans-to-create-an-Amazon-S3-bucket-that-contains-millions-of-images-The-team-wants-to-maximize-the-read-performance-of" class="headerlink" title="A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of"></a>A company’s development team plans to create an Amazon S3 bucket that contains millions of images. The team wants to maximize the read performance of</h2><p>Amazon S3. Which naming scheme should the company use?</p><p>A. Add a date as the prefix.<br>B. Add a sequential id as the suffix.<br>C. Add a hexadecimal hash as the suffix.<br>D. Add a hexadecimal hash as the prefix.</p><p>Answer: A</p><ul><li>分析：这道题的旧答案是D，不过根据最新文档档案为A。</li></ul><blockquote><p><a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/optimizing-performance.html</a></p><p>For example, previously Amazon S3 performance guidelines recommended randomizing prefix naming with hashed characters to optimize performance for frequent data retrievals. You no longer have to randomize prefix naming for performance, and can use sequential date-based naming for your prefixes.</p></blockquote><ul><li>什么是Prefix?</li></ul><blockquote><p>For example, your application can achieve at least 3,500 PUT/COPY/POST/DELETE and 5,500 GET/HEAD requests per second per prefix in a bucket. There are no limits to the number of prefixes in a bucket. You can increase your read or write performance by parallelizing reads. For example, if you create 10 prefixes in an Amazon S3 bucket to parallelize reads, you could scale your read performance to 55,000 read requests per second.</p></blockquote><blockquote><p>However, if the new limits are not sufficient, prefixes would need to be used. A prefix has no fixed number of characters. It is any string between a bucket name and an object name, for example:<br>bucket/folder1/sub1/file<br>bucket/folder1/sub2/file<br>bucket/1/file<br>bucket/2/file<br>Prefixes of the object ‘file’ would be: /folder1/sub1/ , /folder1/sub2/, /1/, /2/.</p></blockquote><h2 id="A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task"><a href="#A-Solutions-Architect-needs-to-design-a-solution-that-will-enable-a-security-team-to-detect-review-and-perform-root-cause-analysis-of-security-incidents-that-occur-in-a-cloud-environment-The-Architect-must-provide-a-centralized-view-of-all-API-events-for-current-and-future-AWS-regions-How-should-the-Architect-accomplish-this-task" class="headerlink" title="A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?"></a>A Solutions Architect needs to design a solution that will enable a security team to detect, review, and perform root cause analysis of security incidents that occur in a cloud environment. The Architect must provide a centralized view of all API events for current and future AWS regions. How should the Architect accomplish this task?</h2><p>A. Enable AWS CloudTrail logging in each individual region. Repeat this for all future regions.<br>B. Enable Amazon CloudWatch logs for all AWS services across all regions and aggregate them in a single Amazon S3 bucket.<br>C. Enable AWS Trusted Advisor security checks and report all security incidents for all regions.<br>D. Enable AWS CloudTrail by creating a new trail and apply the trail to all regions.</p><p>Answer: D</p><ul><li>分析：这道题肯定使用CloudTrail，区别在于设定范围。</li></ul><blockquote><p><a href="https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/" target="_blank" rel="noopener">https://aws.amazon.com/cn/about-aws/whats-new/2015/12/turn-on-cloudtrail-across-all-regions-and-support-for-multiple-trails/</a><br>You can now turn on a trail across all regions for your AWS account. CloudTrail will deliver log files from all regions to the Amazon S3 bucket and an optional CloudWatch Logs log group you specified. Additionally, when AWS launches a new region, CloudTrail will create the same trail in the new region. As a result, you will receive log files containing API activity for the new region without taking any action. Using the CloudTrail console, you can specify that a trail applies to all regions. For more details, refer to the Applying a trail to all regions section of the CloudTrail FAQ.</p></blockquote><h2 id="A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use"><a href="#A-company-has-a-legacy-application-using-a-proprietary-file-system-and-plans-to-migrate-the-application-to-AWS-Which-storage-service-should-the-company-use" class="headerlink" title="A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?"></a>A company has a legacy application using a proprietary file system and plans to migrate the application to AWS. Which storage service should the company use?</h2><p>A. Amazon DynamoDB<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon EFS</p><p>Answer: C</p><ul><li>分析：这道题有点蒙人，关键词在proprietary（专有的），EFS未必支持，只有EBS才能100%满足。</li></ul><h2 id="A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner"><a href="#A-company-plans-to-use-AWS-for-all-new-batch-processing-workloads-The-company’s-developers-use-Docker-containers-for-the-new-batch-processing-The-system-design-must-accommodate-critical-and-non-critical-batch-processing-workloads-24-7-How-should-a-Solutions-Architect-design-this-architecture-in-a-cost-efficient-manner" class="headerlink" title="A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?"></a>A company plans to use AWS for all new batch processing workloads. The company’s developers use Docker containers for the new batch processing. The system design must accommodate critical and non-critical batch processing workloads 24/7. How should a Solutions Architect design this architecture in a cost-efficient manner?</h2><p>A. Purchase Reserved Instances to run all containers. Use Auto Scaling groups to schedule jobs.<br>B. Host a container management service on Spot Instances. Use Reserved Instances to run Docker containers.<br>C. Use Amazon ECS orchestration and Auto Scaling groups: one with Reserve Instances, one with Spot Instances.<br>D. Use Amazon ECS to manage container orchestration. Purchase Reserved Instances to run all batch workloads at the same time.</p><ul><li>分析：绕嘴，多读两遍。主要是应对两种不同类型的任务。</li></ul><h2 id="A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO"><a href="#A-company-is-evaluating-Amazon-S3-as-a-data-storage-solution-for-their-daily-analyst-reports-The-company-has-implemented-stringent-requirements-concerning-the-security-of-the-data-at-rest-Specifically-the-CISO-asked-for-the-use-of-envelope-encryption-with-separate-permissions-for-the-use-of-an-envelope-key-automated-rotation-of-the-encryption-keys-and-visibility-into-when-an-encryption-key-was-used-and-by-whom-Which-steps-should-a-Solutions-Architect-take-to-satisfy-the-security-requirements-requested-by-the-CISO" class="headerlink" title="A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?"></a>A company is evaluating Amazon S3 as a data storage solution for their daily analyst reports. The company has implemented stringent requirements concerning the security of the data at rest. Specifically, the CISO asked for the use of envelope encryption with separate permissions for the use of an envelope key, automated rotation of the encryption keys, and visibility into when an encryption key was used and by whom. Which steps should a Solutions Architect take to satisfy the security requirements requested by the CISO?</h2><p>A. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Customer-Provided Keys (SSE-C).<br>B. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).<br>C. Create an Amazon S3 bucket to store the reports and use Server-Side Encryption with AWS KMS-Managed Keys (SSE-KMS).<br>D. Create an Amazon S3 bucket to store the reports and use Amazon s3 versioning with Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3).</p><p>Answer: C</p><ul><li>分析：用S3 + KMS服务。</li></ul><h2 id="争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case"><a href="#争议-A-customer-has-a-production-application-that-frequently-overwrites-and-deletes-data-the-application-requires-the-most-up-to-date-version-of-the-data-every-time-it-is-requested-Which-storage-should-a-Solutions-Architect-recommend-to-bet-accommodate-this-use-case" class="headerlink" title="(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?"></a>(争议)A customer has a production application that frequently overwrites and deletes data, the application requires the most up-to-date version of the data every time it is requested. Which storage should a Solutions Architect recommend to bet accommodate this use case?</h2><p>A. Amazon S3<br>B. Amazon RDS<br>C. Amazon RedShift<br>D. AWS Storage Gateway</p><p>Answer: A</p><ul><li>分析：这道题的争议点在答案B，因为S3提供eventual consistency for overwirte PUTS and DELETES，可能会导致无法获取最新数据的问题。不确定该问题是否会考到，暂时没有找到更合理的解释。</li></ul><blockquote><p>Amazon S3 Data Consistency Model(<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/Introduction.html</a>)<br>Amazon S3 provides read-after-write consistency for PUTS of new objects in your S3 bucket in all Regions with one caveat. The caveat is that if you make a HEAD or GET request to the key name (to find if the object exists) before creating the object, Amazon S3 provides eventual consistency for read-after-write.</p><p>Amazon S3 offers eventual consistency for overwrite PUTS and DELETES in all Regions.</p><p>Updates to a single key are atomic. For example, if you PUT to an existing key, a subsequent read might return the old data or the updated data, but it never returns corrupted or partial data.</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a"><a href="#A-Solutions-Architect-is-designing-a-photo-application-on-AWS-Every-time-a-user-uploads-a-photo-to-Amazon-S3-the-Architect-must-insert-a-new-item-to-a" class="headerlink" title="A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a"></a>A Solutions Architect is designing a photo application on AWS. Every time a user uploads a photo to Amazon S3, the Architect must insert a new item to a</h2><p>DynamoDB table. Which AWS-managed service is the BEST fit to insert the item?</p><p>A. Lambda@Edge<br>B. AWS Lambda<br>C. Amazon API Gateway<br>D. Amazon EC2 instances</p><p>Answer: B</p><ul><li>参考链接：<a href="https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/machine-learning/build-your-own-face-recognition-service-using-amazon-rekognition/</a></li></ul><h2 id="An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used"><a href="#An-application-relies-on-messages-being-sent-and-received-in-order-The-volume-will-never-exceed-more-than-300-transactions-each-second-Which-service-should-be-used" class="headerlink" title="An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?"></a>An application relies on messages being sent and received in order. The volume will never exceed more than 300 transactions each second. Which service should be used?</h2><p>A. Amazon SQS<br>B. Amazon SNS<br>C. Amazon ECS<br>D. AWS STS</p><p>Answer: A</p><blockquote><p>问：Amazon SNS 与 Amazon SQS 有何不同？</p><p>Amazon Simple Queue Service (SQS) 和 Amazon SNS 都是 AWS 中的消息收发服务，但为开发人员提供了不同的优势。Amazon SNS 允许应用程序通过“推送”机制向多个订阅者发送时间关键型消息，并且无需定期检查或“轮询”更新。Amazon SQS 是一种供分布式应用程序使用的消息队列服务，通过轮询模式交换消息，可用于分离收发组件。Amazon SQS 使应用程序的分布式组件可以灵活地收发消息，并且不要求每个组件同时可用。</p><p>一种常见的模式是使用 SNS 将消息发布到 Amazon SQS 队列，进而以可靠的方式将消息异步发送到一个或多个系统组件。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement"><a href="#A-Solutions-Architect-is-designing-an-application-on-AWS-that-uses-persistent-block-storage-Data-must-be-encrypted-at-rest-Which-solution-meets-the-requirement" class="headerlink" title="A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?"></a>A Solutions Architect is designing an application on AWS that uses persistent block storage. Data must be encrypted at rest. Which solution meets the requirement?</h2><p>A. Enable SSL on Amazon EC2 instances.<br>B. Encrypt Amazon EBS volumes on Amazon EC2 instances.<br>C. Enable server-side encryption on Amazon S3.<br>D. Encrypt Amazon EC2 Instance Storage.</p><p>Answer: B</p><ul><li>New EBS Encryption for Additional Data Protection(<a href="https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/protect-your-data-with-new-ebs-encryption/</a>)</li></ul><h2 id="争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two"><a href="#争议-A-company-is-launching-a-static-website-using-the-zone-apex-mycompany-com-The-company-wants-to-use-Amazon-Route-53-for-DNS-Which-steps-should-the-company-perform-to-implement-a-scalable-and-cost-effective-solution-Choose-two" class="headerlink" title="(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)"></a>(争议)A company is launching a static website using the zone apex (mycompany.com). The company wants to use Amazon Route 53 for DNS. Which steps should the company perform to implement a scalable and cost-effective solution? (Choose two.)</h2><p>A. Host the website on an Amazon EC2 instance with ELB and Auto Scaling, and map a Route 53 alias record to the ELB endpoint.<br>B. Host the website using AWS Elastic Beanstalk, and map a Route 53 alias record to the Beanstalk stack.<br>C. Host the website on an Amazon EC2 instance, and map a Route 53 alias record to the public IP address of the Amazon EC2 instance.<br>D. Serve the website from an Amazon S3 bucket, and map a Route 53 alias record to the website endpoint.<br>E. Create a Route 53 hosted zone, and set the NS records of the domain to use Route 53 name servers.</p><p>Answer: DE</p><ul><li>分析：又是一道争议非常大的题，原来的答案是CD，从cost-effective的角度说C确实不够经济。参考AWS如何构建静态网站的最佳实践：<a href="https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/AmazonS3/latest/dev/website-hosting-custom-domain-walkthrough.html</a></li></ul><h2 id="争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale"><a href="#争议-A-manufacturing-company-captures-data-from-machines-running-at-customer-sites-Currently-thousands-of-machines-send-data-every-5-minutes-and-this-is-expected-to-grow-to-hundreds-of-thousands-of-machines-in-the-near-future-The-data-is-logged-with-the-intent-to-be-analyzed-in-the-future-as-needed-What-is-the-SIMPLEST-method-to-store-this-streaming-data-at-scale" class="headerlink" title="(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?"></a>(争议)A manufacturing company captures data from machines running at customer sites. Currently, thousands of machines send data every 5 minutes, and this is expected to grow to hundreds of thousands of machines in the near future. The data is logged with the intent to be analyzed in the future as needed. What is the SIMPLEST method to store this streaming data at scale?</h2><p>A. Create an Amazon Kinesis Firehouse delivery stream to store the data in Amazon S3.<br>B. Create an Auto Scaling group of Amazon EC2 servers behind ELBs to write the data into Amazon RDS.<br>C. Create an Amazon SQS queue, and have the machines write to the queue.<br>D. Create an Amazon EC2 server farm behind an ELB to store the data in Amazon EBS Cold HDD volumes.</p><p>Answer: A</p><ul><li>分析：很奇怪为什么原有答案给出B，这道题明显是暗指实时计算Kinesis服务。</li></ul><h2 id="A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database"><a href="#A-bank-is-writing-new-software-that-is-heavily-dependent-upon-the-database-transactions-for-write-consistency-The-application-will-also-occasionally-generate-reports-on-data-in-the-database-and-will-do-joins-across-multiple-tables-The-database-must-automatically-scale-as-the-amount-of-data-grows-Which-AWS-service-should-be-used-to-run-the-database" class="headerlink" title="A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?"></a>A bank is writing new software that is heavily dependent upon the database transactions for write consistency. The application will also occasionally generate reports on data in the database, and will do joins across multiple tables. The database must automatically scale as the amount of data grows. Which AWS service should be used to run the database?</h2><p>A. Amazon S3<br>B. Amazon Aurora<br>C. Amazon DynamoDB<br>D. Amazon Redshift</p><p>Answer: B</p><ul><li>分析：很明显需要关系型数据库。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost"><a href="#A-Solutions-Architect-is-designing-a-new-application-that-needs-to-access-data-in-a-different-AWS-account-located-within-the-same-region-The-data-must-not-be-accessed-over-the-Internet-Which-solution-will-meet-these-requirements-with-the-LOWEST-cost" class="headerlink" title="A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?"></a>A Solutions Architect is designing a new application that needs to access data in a different AWS account located within the same region. The data must not be accessed over the Internet. Which solution will meet these requirements with the LOWEST cost?</h2><p>A. Add rules to the security groups in each account.<br>B. Establish a VPC Peering connection between accounts.<br>C. Configure Direct Connect in each account.<br>D. Add a NAT Gateway to the data account.</p><p>Answer: B</p><ul><li>分析：B的方案是成本最低的。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3"><a href="#A-Solutions-Architect-is-designing-a-mobile-application-that-will-capture-receipt-images-to-track-expenses-The-Architect-wants-to-store-the-images-on-Amazon-S3-However-uploading-images-through-the-web-server-will-create-too-much-traffic-What-is-the-MOST-efficient-method-to-store-images-from-a-mobile-application-on-Amazon-S3" class="headerlink" title="A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?"></a>A Solutions Architect is designing a mobile application that will capture receipt images to track expenses. The Architect wants to store the images on Amazon S3. However, uploading images through the web server will create too much traffic. What is the MOST efficient method to store images from a mobile application on Amazon S3?</h2><p>A. Upload directly to S3 using a pre-signed URL.<br>B. Upload to a second bucket, and have a Lambda event copy the image to the primary bucket.<br>C. Upload to a separate Auto Scaling group of servers behind an ELB Classic Load Balancer, and have them write to the Amazon S3 bucket.<br>D. Expand the web server fleet with Spot Instances to provide the resources to handle the images.</p><p>Answer: C</p><ul><li>分析：A选项相较于题目中描述的并没有本质区别。</li></ul><blockquote><p>A presigned URL gives you access to the object identified in the URL, provided that the creator of the presigned URL has permissions to access that object. That is, if you receive a presigned URL to upload an object, you can upload the object only if the creator of the presigned URL has the necessary permissions to upload that object.</p><p>All objects and buckets by default are private. The presigned URLs are useful if you want your user/customer to be able to upload a specific object to your bucket, but you don’t require them to have AWS security credentials or permissions. When you create a presigned URL, you must provide your security credentials and then specify a bucket name, an object key, an HTTP method (PUT for uploading objects), and an expiration date and time. The presigned URLs are valid only for the specified duration.</p></blockquote><h2 id="A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal"><a href="#A-company-requires-that-the-source-destination-and-protocol-of-all-IP-packets-be-recorded-when-traversing-a-private-subnet-What-is-the-MOST-secure-and-reliable-method-of-accomplishing-this-goal" class="headerlink" title="A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal."></a>A company requires that the source, destination, and protocol of all IP packets be recorded when traversing a private subnet. What is the MOST secure and reliable method of accomplishing this goal.</h2><p>A. Create VPC flow logs on the subnet.<br>B. Enable source destination check on private Amazon EC2 instances.<br>C. Enable AWS CloudTrail logging and specify an Amazon S3 bucket for storing log files.<br>D. Create an Amazon CloudWatch log to capture packet information.</p><p>Answer: A</p><ul><li>分析：启动VPC流表日志, CloudTrail没有此能力</li></ul><h2 id="A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two"><a href="#A-Solutions-Architect-has-a-multi-layer-application-running-in-Amazon-VPC-The-application-has-an-ELB-Classic-Load-Balancer-as-the-front-end-in-a-public-subnet-and-an-Amazon-EC2-based-reverse-proxy-that-performs-content-based-routing-to-two-backend-Amazon-EC2-instances-hosted-in-a-private-subnet-The-Architect-sees-tremendous-traffic-growth-and-is-concerned-that-the-reverse-proxy-and-current-backend-set-up-will-be-insufficient-Which-actions-should-the-Architect-take-to-achieve-a-cost-effective-solution-that-ensures-the-application-automatically-scales-to-meet-traffic-demand-Select-two" class="headerlink" title="A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)"></a>A Solutions Architect has a multi-layer application running in Amazon VPC. The application has an ELB Classic Load Balancer as the front end in a public subnet, and an Amazon EC2-based reverse proxy that performs content-based routing to two backend Amazon EC2 instances hosted in a private subnet. The Architect sees tremendous traffic growth and is concerned that the reverse proxy and current backend set up will be insufficient. Which actions should the Architect take to achieve a cost-effective solution that ensures the application automatically scales to meet traffic demand? (Select two.)</h2><p>A. Replace the Amazon EC2 reverse proxy with an ELB internal Classic Load Balancer.<br>B. Add Auto Scaling to the Amazon EC2 backend fleet.<br>C. Add Auto Scaling to the Amazon EC2 reverse proxy layer.<br>D. Use t2 burstable instance types for the backend fleet.<br>E. Replace both the frontend and reverse proxy layers with an ELB Application Load Balancer.</p><p>Answer: BE</p><ul><li>分析：又是一道错题，原答案是AB。根据题目分析，出现瓶颈的地方来自于两处：反向代理和后端服务。后端服务的扩展没有什么争议，所以B很明显是正确的。最大的争议来自于是使用什么方式替代目前成为瓶颈的反向代理EC2。原题里反向代理EC2作为content-based routing，那么问题的关键就是CLB、ELB谁能做content-based routing了。根据目前最新的内容，所以需要使用Application Load Balancer来提供content-based routing了。</li></ul><blockquote><p>There are three types of Elastic Load Balancer (ELB) on AWS:</p><p>Classic Load Balancer (CLB) – this is the oldest of the three and provides basic load balancing at both layer 4 and layer 7.</p><p>Application Load Balancer (ALB) – layer 7 load balancer that routes connections based on the content of the request.</p><p>Network Load Balancer (NLB) – layer 4 load balancer that routes connections based on IP protocol data.</p><p>Note: The Classic Load Balancer may be phased out over time and Amazon are promoting the ALB and NLB for most use cases within VPC.</p></blockquote><blockquote><p>Introducing Amazon EC2 Fleet<br>Posted On: May 2, 2018</p><p>Amazon EC2 Fleet is a new feature that simplifies the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, Availability Zones and across On-Demand, Amazon EC2 Reserved Instances (RI) and Amazon EC2 Spot purchase models. With a single API call, now you can provision capacity across EC2 instance types and across purchase models to achieve desired scale, performance and cost.</p><p>You can create an EC2 Fleet specification defining target capacity, which EC2 instance types work for you, and how much of your fleet should be filled using On-Demand, RI and Spot purchase models. You can also indicate whether EC2 Fleet should take into account the number of cores and amount of memory on each instance or consider all instances equal when scaling. EC2 Fleet then launches the lowest price combination of instances to meet the target capacity based on these preferences. EC2 fleet enables you to use multiple instance types and purchase models to provision capacity cost effectively, with just a few clicks in the AWS Management Console.</p><p>Amazon EC2 Fleet is now available in all public Regions. To learn more about simplifying the provisioning of Amazon EC2 capacity across different Amazon EC2 instance types, AWS Availability Zones and across On-Demand, RI and Spot purchase models using Amazon EC2 Fleet, visit this blog. To learn more about Amazon EC2 pricing models, visit this page.</p></blockquote><ul><li>EC2 Fleet – Manage Thousands of On-Demand and Spot Instances with One Request(<a href="https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/ec2-fleet-manage-thousands-of-on-demand-and-spot-instances-with-one-request/</a>)</li><li>New – Advanced Request Routing for AWS Application Load Balancers(<a href="https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/" target="_blank" rel="noopener">https://amazonaws-china.com/blogs/aws/new-advanced-request-routing-for-aws-application-load-balancers/</a>)</li><li>ELASTIC LOAD BALANCING(<a href="https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/" target="_blank" rel="noopener">https://digitalcloud.training/certification-training/aws-solutions-architect-associate/compute/elastic-load-balancing/</a>)</li></ul><h2 id="A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers"><a href="#A-company-is-launching-a-marketing-campaign-on-their-website-tomorrow-and-expects-a-significant-increase-in-traffic-The-website-is-designed-as-a-multi-tiered-web-architecture-and-the-increase-in-traffic-could-potentially-overwhelm-the-current-design-What-should-a-Solutions-Architect-do-to-minimize-the-effects-from-a-potential-failure-in-one-or-more-of-the-tiers" class="headerlink" title="A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?"></a>A company is launching a marketing campaign on their website tomorrow and expects a significant increase in traffic. The website is designed as a multi-tiered web architecture, and the increase in traffic could potentially overwhelm the current design. What should a Solutions Architect do to minimize the effects from a potential failure in one or more of the tiers?</h2><p>A. Migrate the database to Amazon RDS.<br>B. Set up DNS failover to a statistic website.<br>C. Use Auto Scaling to keep up with the demand.<br>D. Use both a SQL and a NoSQL database in the design.</p><p>Answer: C</p><ul><li>分析：明天就上线了，改啥都来不及了，还是价格Auto scaling策略吧。</li></ul><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1"><a href="#A-web-application-experiences-high-compute-costs-due-to-serving-a-high-amount-of-static-web-content-How-should-the-web-server-architecture-be-designed-to-be-the-MOST-cost-efficient-1" class="headerlink" title="A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?"></a>A web application experiences high compute costs due to serving a high amount of static web content. How should the web server architecture be designed to be the MOST cost-efficient?</h2><p>A. Create an Auto Scaling group to scale out based on average CPU usage.<br>B. Create an Amazon CloudFront distribution to pull static content from an Amazon S3 bucket.<br>C. Leverage Reserved Instances to add additional capacity at a significantly lower price.<br>D. Create a multi-region deployment using an Amazon Route 53 geolocation routing policy.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway"><a href="#A-Solutions-Architect-plans-to-migrate-NAT-instances-to-NAT-gateway-The-Architect-has-NAT-instances-with-scripts-to-manage-high-availability-What-is-the-MOST-efficient-method-to-achieve-similar-high-availability-with-NAT-gateway" class="headerlink" title="A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?"></a>A Solutions Architect plans to migrate NAT instances to NAT gateway. The Architect has NAT instances with scripts to manage high availability. What is the MOST efficient method to achieve similar high availability with NAT gateway?</h2><p>A. Remove source/destination check on NAT instances.<br>B. Launch a NAT gateway in each Availability Zone.<br>C. Use a mix of NAT instances and NAT gateway.<br>D. Add an ELB Application Load Balancer in front of NAT gateway.</p><p>Answer: B</p><ul><li>NAT 实例与 NAT 网关的比较(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-nat-comparison.html</a>)</li></ul><blockquote><p>每个 NAT 网关都在特定可用区中创建，并在该可用区进行冗余实施。您可以在一个可用区中创建的 NAT 网关存在数量限制。有关更多信息，请参阅 Amazon VPC 限制。</p><p>注意:<br>如果您在多个可用区中拥有资源，并且它们共享一个 NAT 网关，则在该 NAT 网关的可用区不可用时，其他可用区中的资源将无法访问 Internet。要创建不依赖于可用区的架构，请在每个可用区中都创建一个 NAT 网关，并配置路由以确保这些资源使用自身可用区中的 NAT 网关。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance"><a href="#A-Solutions-Architect-is-designing-a-solution-to-store-a-large-quantity-of-event-data-in-Amazon-S3-The-Architect-anticipates-that-the-workload-will-consistently-exceed-100-requests-each-second-What-should-the-Architect-do-in-Amazon-S3-to-optimize-performance" class="headerlink" title="A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?"></a>A Solutions Architect is designing a solution to store a large quantity of event data in Amazon S3. The Architect anticipates that the workload will consistently exceed 100 requests each second. What should the Architect do in Amazon S3 to optimize performance?</h2><p>A. Randomize a key name prefix.<br>B. Store the event data in separate buckets.<br>C. Randomize the key name suffix.<br>D. Use Amazon S3 Transfer Acceleration.</p><p>Answer: A</p><ul><li>分析：这道题和上面有道题类似，目前S3建议使用时间作为prefix，原来是建议使用hash方式。</li><li>最佳实践设计模式：优化 Amazon S3 性能(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/dev/optimizing-performance.html</a>)</li></ul><blockquote><p>下面的主题介绍的最佳实践准则和设计模式用于优化使用 Amazon S3 的应用程序的性能。本指南的优先级高于之前有关优化 Amazon S3 的性能的任何指南。例如，以前的 Amazon S3 性能指南建议用哈希字符来随机化前缀命名，以便优化频繁数据检索的性能。现在，您不再需要为了提高性能随机化前缀命名，而是可以对前缀使用基于顺序日期的命名方式。有关对 Amazon S3 进行性能优化的最新信息，请参阅Amazon S3 的性能准则和Amazon S3 的性能设计模式。</p></blockquote><h2 id="A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload"><a href="#A-user-is-testing-a-new-service-that-receives-location-updates-from-3-600-rental-cars-every-hour-Which-service-will-collect-data-and-automatically-scale-to-accommodate-production-workload" class="headerlink" title="A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?"></a>A user is testing a new service that receives location updates from 3,600 rental cars every hour. Which service will collect data and automatically scale to accommodate production workload?</h2><p>A. Amazon EC2<br>B. Amazon Kinesis Firehose<br>C. Amazon EBS<br>D. Amazon API Gateway</p><p>Answer: B</p><ul><li>分析：又是一道争议题，大部分给出的答案是B，就应用场景上看Kinesis更适合实时计算场景。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required"><a href="#A-Solutions-Architect-is-designing-a-web-application-The-web-and-application-tiers-need-to-access-the-Internet-but-they-cannot-be-accessed-from-the-Internet-Which-of-the-following-steps-is-required" class="headerlink" title="A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?"></a>A Solutions Architect is designing a web application. The web and application tiers need to access the Internet, but they cannot be accessed from the Internet. Which of the following steps is required?</h2><p>A. Attach an Elastic IP address to each Amazon EC2 instance and add a route from the private subnet to the public subnet.<br>B. Launch a NAT gateway in the public subnet and add a route to it from the private subnet.<br>C. Launch Amazon EC2 instances in the public subnet and change the security group to allow outbound traffic on port 80.<br>D. Launch a NAT gateway in the private subnet and deploy a NAT instance in the private subnet.</p><p>Answer: B</p><h2 id="An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task"><a href="#An-application-stack-includes-an-Elastic-Load-Balancer-in-a-public-subnet-a-fleet-of-Amazon-EC2-instances-in-an-Auto-Scaling-group-and-an-Amazon-RDS-MySQL-cluster-Users-connect-to-the-application-from-the-Internet-The-application-servers-and-database-must-be-secure-How-should-a-Solutions-Architect-perform-this-task" class="headerlink" title="An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?"></a>An application stack includes an Elastic Load Balancer in a public subnet, a fleet of Amazon EC2 instances in an Auto Scaling group, and an Amazon RDS MySQL cluster. Users connect to the application from the Internet. The application servers and database must be secure. How should a Solutions Architect perform this task?</h2><p>A. Create a private subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.<br>B. Create a private subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>C. Create a public subnet for the Amazon EC2 instances and a private subnet for the Amazon RDS cluster.<br>D. Create a public subnet for the Amazon EC2 instances and a public subnet for the Amazon RDS cluster.</p><p>Answer: B</p><ul><li>分析：答案给出的是C，有多种方式证明这个答案是错误的。</li><li>How do I connect a public-facing load balancer to EC2 instances that have private IP addresses?(<a href="https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/" target="_blank" rel="noopener">https://amazonaws-china.com/premiumsupport/knowledge-center/public-load-balancer-private-ec2/</a>)</li><li>AWS Best Practices: 3-Tier Infrastructure(<a href="https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure" target="_blank" rel="noopener">https://blog.stratus10.com/aws-best-practices-3-tier-infrastructure</a>)</li></ul><h2 id="A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application"><a href="#A-Solutions-Architect-is-designing-a-solution-for-a-media-company-that-will-stream-large-amounts-of-data-from-an-Amazon-EC2-instance-The-data-streams-are-typically-large-and-sequential-and-must-be-able-to-support-up-to-500-MB-s-Which-storage-type-will-meet-the-performance-requirements-of-this-application" class="headerlink" title="A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?"></a>A Solutions Architect is designing a solution for a media company that will stream large amounts of data from an Amazon EC2 instance. The data streams are typically large and sequential, and must be able to support up to 500 MB/s. Which storage type will meet the performance requirements of this application?</h2><p>A. EBS Provisioned IOPS SSD<br>B. EBS General Purpose SSD<br>C. EBS Cold HDD<br>D. EBS Throughput Optimized HDD</p><p>Answer: D</p><h2 id="争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement"><a href="#争议-A-legacy-application-running-in-premises-requires-a-Solutions-Architect-to-be-able-to-open-a-firewall-to-allow-access-to-several-Amazon-S3-buckets-The-Architect-has-a-VPN-connection-to-AWS-in-place-How-should-the-Architect-meet-this-requirement" class="headerlink" title="(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?"></a>(争议)A legacy application running in premises requires a Solutions Architect to be able to open a firewall to allow access to several Amazon S3 buckets. The Architect has a VPN connection to AWS in place. How should the Architect meet this requirement?</h2><p>A. Create an IAM role that allows access from the corporate network to Amazon S3.<br>B. Configure a proxy on Amazon EC2 and use an Amazon S3 VPC endpoint.<br>C. Use Amazon API Gateway to do IP whitelisting.<br>D. Configure IP whitelisting on the customer’s gateway.</p><p>Answer: B</p><ul><li>分析：争议较大的一道题，这里采用了这个解释：<a href="https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf" target="_blank" rel="noopener">https://d0.awsstatic.com/aws-answers/Accessing_VPC_Endpoints_from_Remote_Networks.pdf</a></li></ul><h2 id="A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements"><a href="#A-Solutions-Architect-is-designing-a-database-solution-that-must-support-a-high-rate-of-random-disk-reads-and-writes-It-must-provide-consistent-performance-and-requires-long-term-persistence-Which-storage-solution-BEST-meets-these-requirements" class="headerlink" title="A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?"></a>A Solutions Architect is designing a database solution that must support a high rate of random disk reads and writes. It must provide consistent performance, and requires long-term persistence. Which storage solution BEST meets these requirements?</h2><p>A. An Amazon EBS Provisioned IOPS volume<br>B. An Amazon EBS General Purpose volume<br>C. An Amazon EBS Magnetic volume<br>D. An Amazon EC2 Instance Store</p><p>Answer: A</p><h2 id="A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way"><a href="#A-Solutions-Architect-is-designing-solution-with-AWS-Lambda-where-different-environments-require-different-database-passwords-What-should-the-Architect-do-to-accomplish-this-in-a-secure-and-scalable-way" class="headerlink" title="A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?"></a>A Solutions Architect is designing solution with AWS Lambda where different environments require different database passwords. What should the Architect do to accomplish this in a secure and scalable way?</h2><p>A. Create a Lambda function for each individual environment.<br>B. Use Amazon DynamoDB to store environmental variables.<br>C. Use encrypted AWS Lambda environmental variables.<br>D. Implement a dedicated Lambda function for distributing variables.</p><p>Answer: C</p><h2 id="A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage"><a href="#A-news-organization-plans-to-migrate-their-20-TB-video-archive-to-AWS-The-files-are-rarely-accessed-but-when-they-are-a-request-is-made-in-advance-and-a-3-to-5-hour-retrieval-time-frame-is-acceptable-However-when-there-is-a-breaking-news-story-the-editors-require-access-to-archived-footage-within-minutes-Which-storage-solution-meets-the-needs-of-this-organization-while-providing-the-LOWEST-cost-of-storage" class="headerlink" title="A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?"></a>A news organization plans to migrate their 20 TB video archive to AWS. The files are rarely accessed, but when they are, a request is made in advance and a 3 to 5-hour retrieval time frame is acceptable. However, when there is a breaking news story, the editors require access to archived footage within minutes. Which storage solution meets the needs of this organization while providing the LOWEST cost of storage?</h2><p>A. Store the archive in Amazon S3 Reduced Redundancy Storage.<br>B. Store the archive in Amazon Glacier and use standard retrieval for all content.<br>C. Store the archive in Amazon Glacier and pay the additional charge for expedited retrieval when needed.<br>D. Store the archive in Amazon S3 with a lifecycle policy to move this to S3 Infrequent Access after 30 days.</p><p>Answer: C</p><blockquote><p>问：从 Amazon S3 Glacier 检索数据如何收费？</p><p>从 Amazon S3 Glacier 检索数据的方式有三种：加急、标准和批量检索。每种方式具有不同的每 GB 检索费和每存档请求费（即请求一个存档计为一个请求）。有关不同 AWS 区域的 S3 Glacier 定价的详细信息，请访问 Amazon S3 Glacier 定价页面。</p><p>定价标准： <a href="https://amazonaws-china.com/cn/glacier/pricing/" target="_blank" rel="noopener">https://amazonaws-china.com/cn/glacier/pricing/</a></p></blockquote><h2 id="A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements"><a href="#A-Solutions-Architect-is-building-a-multi-tier-website-The-web-servers-will-be-in-a-public-subnet-and-the-database-servers-will-be-in-a-private-subnet-Only-the-web-servers-can-be-accessed-from-the-Internet-The-database-servers-must-have-Internet-access-for-software-updates-Which-solution-meets-the-requirements" class="headerlink" title="A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?"></a>A Solutions Architect is building a multi-tier website. The web servers will be in a public subnet, and the database servers will be in a private subnet. Only the web servers can be accessed from the Internet. The database servers must have Internet access for software updates. Which solution meets the requirements?</h2><p>A. Assign Elastic IP addresses to the database instances.<br>B. Allow Internet traffic on the private subnet through the network ACL.<br>C. Use a NAT Gateway.<br>D. Use an egress-only Internet Gateway.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized"><a href="#A-Solutions-Architect-is-designing-a-Lambda-function-that-calls-an-API-to-list-all-running-Amazon-RDS-instances-How-should-the-request-be-authorized" class="headerlink" title="A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?"></a>A Solutions Architect is designing a Lambda function that calls an API to list all running Amazon RDS instances. How should the request be authorized?</h2><p>A. Create an IAM access and secret key, and store it in the Lambda function.<br>B. Create an IAM role to the Lambda function with permissions to list all Amazon RDS instances.<br>C. Create an IAM role to Amazon RDS with permissions to list all Amazon RDS instances.<br>D. Create an IAM access and secret key, and store it in an encrypted RDS database.</p><p>Answer: B</p><blockquote><p>教程：配置 Lambda 函数以访问 Amazon VPC 中的 Amazon RDS</p><p>打开 IAM 控制台中的“角色”页面。<br>选择 Create role (创建角色)。<br>创建具有以下属性的角色。<br>Trusted entity (可信任的实体) – Lambda.<br>权限 – AWSLambdaVPCAccessExecutionRole。<br>角色名称 (角色名称) – lambda-vpc-role。<br>AWSLambdaVPCAccessExecutionRole 具有函数管理与 VPC 的网络连接所需的权限。</p></blockquote><h2 id="A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-on-AWS-that-will-require-20-000-IOPS-on-a-particular-volume-to-support-a-media-event-Once-the-event-ends-the-IOPS-need-is-no-longer-required-The-marketing-team-asks-the-Architect-to-build-the-platform-to-optimize-storage-without-incurring-downtime-How-should-the-Architect-design-the-platform-to-meet-these-requirements" class="headerlink" title="A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?"></a>A Solutions Architect is building an application on AWS that will require 20,000 IOPS on a particular volume to support a media event. Once the event ends, the IOPS need is no longer required. The marketing team asks the Architect to build the platform to optimize storage without incurring downtime. How should the Architect design the platform to meet these requirements?</h2><p>A. Change the Amazon EC2 instant types.<br>B. Change the EBS volume type to Provisioned IOPS.<br>C. Stop the Amazon EC2 instance and provision IOPS for the EBS volume.<br>D. Enable an API Gateway to change the endpoints for the Amazon EC2 instances.</p><p>Answer: B</p><blockquote><p>打开 Amazon EC2 控制台 <a href="https://console.aws.amazon.com/ec2/。" target="_blank" rel="noopener">https://console.aws.amazon.com/ec2/。</a></p><p>选择 Volumes，选择要修改的卷，然后依次选择 Actions、Modify Volume。</p><p>Modify Volume 窗口显示卷 ID 和卷的当前配置，包括类型、大小和 IOPS。您可以在单个操作中更改任何或所有这些设置。设置新的配置值，如下所述：</p><p>要修改类型，请为 Volume Type 选择一个值。</p><p>要修改大小，请为 Size 输入一个允许的整数值。</p><p>如果选择预配置 IOPS SSD (io1) 作为卷类型，请为 IOPS 输入一个允许的整数值。</p><p>完成更改卷设置后，请选择 Modify (修改)。当系统提示您确认时，请选择 Yes。</p><p>在扩展卷的文件系统以使用新的存储容量之前，修改卷大小没有实际效果。有关更多信息，请参阅调整卷大小后扩展 Linux 文件系统。</p></blockquote><h2 id="A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata"><a href="#A-Solutions-Architect-is-building-a-new-feature-using-a-Lambda-to-create-metadata-when-a-user-uploads-a-picture-to-Amazon-S3-All-metadata-must-be-indexed-Which-AWS-service-should-the-Architect-use-to-store-this-metadata" class="headerlink" title="A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?"></a>A Solutions Architect is building a new feature using a Lambda to create metadata when a user uploads a picture to Amazon S3. All metadata must be indexed. Which AWS service should the Architect use to store this metadata?</h2><p>A. Amazon S3<br>B. Amazon DynamoDB<br>C. Amazon Kinesis<br>D. Amazon EFC</p><p>Answer: B</p><ul><li>Building and Maintaining an Amazon S3 Metadata Index without Servers(<a href="https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers" target="_blank" rel="noopener">https://amazonaws-china.com/cn/blogs/big-data/building-and-maintaining-an-amazon-s3-metadata-index-without-servers</a>)</li></ul><blockquote><p>In this post, I walk through an approach for building such an index using Amazon DynamoDB and AWS Lambda. With these technologies, you can create a high performance, low-cost index that scales and remains highly available without the need to maintain traditional servers.</p></blockquote><h2 id="An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available"><a href="#An-interactive-dynamic-website-runs-on-Amazon-EC2-instances-in-a-single-subnet-behind-an-ELB-Classic-Load-Balancer-Which-design-changes-will-make-the-site-more-highly-available" class="headerlink" title="An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?"></a>An interactive, dynamic website runs on Amazon EC2 instances in a single subnet behind an ELB Classic Load Balancer. Which design changes will make the site more highly available?</h2><p>A. Move some Amazon EC2 instances to a subnet in a different way(different AZ).<br>B. Move the website to Amazon S3.<br>C. Change the ELB to an Application Load Balancer.<br>D. Move some Amazon EC2 instances to a subnet in the same Availability Zone.</p><p>Answer: A</p><ul><li>分析：这道题的选项A可能是写错了，根据评论区是different AZ，那么选择A就比较容易理解了。评论区有一种声音是选择C，但是从高可用性上讲，C选项并没有实质的价值。</li></ul><h2 id="A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two"><a href="#A-Solutions-Architect-is-designing-a-web-application-that-is-running-on-an-Amazon-EC2-instance-The-application-stores-data-in-DynamoDB-The-Architect-needs-to-secure-access-to-the-DynamoDB-table-What-combination-of-steps-does-AWS-recommend-to-achieve-secure-authorization-Select-two" class="headerlink" title="A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)"></a>A Solutions Architect is designing a web application that is running on an Amazon EC2 instance. The application stores data in DynamoDB. The Architect needs to secure access to the DynamoDB table. What combination of steps does AWS recommend to achieve secure authorization? (Select two.)</h2><p>A. Store an access key on the Amazon EC2 instance with rights to the Dynamo DB table.<br>B. Attach an IAM user to the Amazon EC2 instance.<br>C. Create an IAM role with permissions to write to the DynamoDB table.<br>D. Attach an IAM role to the Amazon EC2 instance.<br>E. Attach an IAM policy to the Amazon EC2 instance.</p><p>Answer: CD</p><ul><li>分析：AWS一向重视安全性，所以更推荐使用STS方式进行接口调用</li></ul><h2 id="争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements"><a href="#争议-A-Solutions-Architect-is-about-to-deploy-an-API-on-multiple-EC2-instances-in-an-Auto-Scaling-group-behind-an-ELB-The-support-team-has-the-following-operational-requirements" class="headerlink" title="(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:"></a>(争议)A Solutions Architect is about to deploy an API on multiple EC2 instances in an Auto Scaling group behind an ELB. The support team has the following operational requirements:</h2><p>1 They get an alert when the requests per second go over 50,000<br>2 They get an alert when latency goes over 5 seconds<br>3 They can validate how many times a day users call the API requesting highly-sensitive data<br>Which combination of steps does the Architect need to take to satisfy these operational requirements? (Select two.)</p><p>A. Ensure that CloudTrail is enabled.<br>B. Create a custom CloudWatch metric to monitor the API for data access.<br>C. Configure CloudWatch alarms for any metrics the support team requires.<br>D. Ensure that detailed monitoring for the EC2 instances is enabled.<br>E. Create an application to export and save CloudWatch metrics for longer term trending analysis.</p><p>Answer: BC</p><ul><li>分析：原题给出的答案是BD，但是EC2的详细监控其实并没有包含API级别的监控，ELB的监控才包含了API访问的监控。</li></ul><h2 id="争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement"><a href="#争议-A-Solutions-Architect-is-designing-a-highly-available-website-that-is-served-by-multiple-web-servers-hosted-outside-of-AWS-If-an-instance-becomes-unresponsive-the-Architect-needs-to-remove-it-from-the-rotation-What-is-the-MOST-efficient-way-to-fulfill-this-requirement" class="headerlink" title="(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?"></a>(争议)A Solutions Architect is designing a highly-available website that is served by multiple web servers hosted outside of AWS. If an instance becomes unresponsive, the Architect needs to remove it from the rotation. What is the MOST efficient way to fulfill this requirement?</h2><p>A. Use Amazon CloudWatch to monitor utilization.<br>B. Use Amazon API Gateway to monitor availability.<br>C. Use an Amazon Elastic Load Balancer.<br>D. Use Amazon Route 53 health checks.</p><p>Answer: A</p><ul><li>分析：不同网站给出不同答案，原网站给出的答案是C，但是从题目分析关键词是the Architect needs to remove it，所以看起来A更合理一些。但是ELB增加health check之后应该可以自动的将不可用节点移除掉。</li></ul><h2 id="A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications"><a href="#A-company-hosts-a-popular-web-application-The-web-application-connects-to-a-database-running-in-a-private-VPC-subnet-The-web-servers-must-be-accessible-only-to-customers-on-an-SSL-connection-The-RDS-MySQL-database-server-must-be-accessible-only-from-the-web-servers-How-should-the-Architect-design-a-solution-to-meet-the-requirements-without-impacting-running-applications" class="headerlink" title="A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?"></a>A company hosts a popular web application. The web application connects to a database running in a private VPC subnet. The web servers must be accessible only to customers on an SSL connection. The RDS MySQL database server must be accessible only from the web servers. How should the Architect design a solution to meet the requirements without impacting running applications?</h2><p>A. Create a network ACL on the web server’s subnet, and allow HTTPS inbound and MySQL outbound. Place both database and web servers on the same subnet.<br>B. Open an HTTPS port on the security group for web servers and set the source to 0.0.0.0/0. Open the MySQL port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.<br>C. Create a network ACL on the web server’s subnet, and allow HTTPS inbound, and specify the source as 0.0.0.0/0. Create a network ACL on a database subnet, allow MySQL port inbound for web servers, and deny all outbound traffic.<br>D. Open the MySQL port on the security group for web servers and set the source to 0.0.0.0/0. Open the HTTPS port on the database security group and attach it to the MySQL instance. Set the source to Web Server Security Group.</p><p>Answer: B</p><h2 id="Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx"><a href="#Which-service-should-an-organization-use-if-it-requires-an-easily-managed-and-scalable-platform-to-host-its-web-application-running-on-Nginx" class="headerlink" title="Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?"></a>Which service should an organization use if it requires an easily managed and scalable platform to host its web application running on Nginx?</h2><p>A. AWS Lambda<br>B. Auto Scaling<br>C. AWS Elastic Beanstalk<br>D. Elastic Load Balancing</p><p>Answer: C</p><blockquote><p>AWS Elastic Beanstalk 是一项易于使用的服务，用于在熟悉的服务器（例如 Apache 、Nginx、Passenger 和 IIS ）上部署和扩展使用 Java、.NET、PHP、Node.js、Python、Ruby、GO 和 Docker 开发的 Web 应用程序和服务。<br>您只需上传代码，Elastic Beanstalk 即可自动处理包括容量预配置、负载均衡、自动扩展和应用程序运行状况监控在内的部署工作。同时，您能够完全控制为应用程序提供支持的 AWS 资源，并可以随时访问底层资源。<br>Elastic Beanstalk 不额外收费 – 您只需为存储和运行应用程序所需的 AWS 资源付费。</p></blockquote><h2 id="An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances"><a href="#An-Administrator-is-hosting-an-application-on-a-single-Amazon-EC2-instance-which-users-can-access-by-the-public-hostname-The-administrator-is-adding-a-second-instance-but-does-not-want-users-to-have-to-decide-between-many-public-hostnames-Which-AWS-service-will-decouple-the-users-from-specific-Amazon-EC2-instances" class="headerlink" title="An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?"></a>An Administrator is hosting an application on a single Amazon EC2 instance, which users can access by the public hostname. The administrator is adding a second instance, but does not want users to have to decide between many public hostnames. Which AWS service will decouple the users from specific Amazon EC2 instances?</h2><p>A. Amazon SQS<br>B. Auto Scaling group<br>C. Amazon EC2 security group<br>D. Amazon ELB</p><p>Answer: D</p><ul><li>分析：这道题原网站给出答案是B，但是明显应该是D</li></ul><h2 id="A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload"><a href="#A-Solutions-Architect-is-designing-a-microservices-based-application-using-Amazon-ECS-The-application-includes-a-WebSocket-component-and-the-traffic-needs-to-be-distributed-between-microservices-based-on-the-URL-Which-service-should-the-Architect-choose-to-distribute-the-workload" class="headerlink" title="A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?"></a>A Solutions Architect is designing a microservices-based application using Amazon ECS. The application includes a WebSocket component, and the traffic needs to be distributed between microservices based on the URL. Which service should the Architect choose to distribute the workload?</h2><p>A. ELB Classic Load Balancer<br>B. Amazon Route 53 DNS<br>C. ELB Application Load Balancer<br>D. Amazon CloudFront</p><p>Answer: C</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page" target="_blank" rel="noopener">https://docs.aws.amazon.com/aws-technical-content/latest/microservices-on-aws/microservices-on-aws.pdf?icmpid=link_from_whitepapers_page</a></li></ul><h2 id="A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements"><a href="#A-Solutions-Architect-is-designing-the-storage-layer-for-a-production-relational-database-The-database-will-run-on-Amazon-EC2-The-database-is-accessed-by-an-application-that-performs-intensive-reads-and-writes-so-the-database-requires-the-LOWEST-random-I-O-latency-Which-data-storage-method-fulfills-the-above-requirements" class="headerlink" title="A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?"></a>A Solutions Architect is designing the storage layer for a production relational database. The database will run on Amazon EC2. The database is accessed by an application that performs intensive reads and writes, so the database requires the LOWEST random I/O latency. Which data storage method fulfills the above requirements?</h2><p>A. Store data in a filesystem backed by Amazon Elastic File System (EFS).<br>B. Store data in Amazon S3 and use a third-party solution to expose Amazon S3 as a filesystem to the database server.<br>C. Store data in Amazon Dynamo DB and emulate relational database semantics.<br>D. Stripe data across multiple Amazon EBS volumes using RAID 0.</p><p>Answer: D</p><h2 id="A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with"><a href="#A-Solutions-Architect-is-designing-a-VPC-Instances-in-a-private-subnet-must-be-able-to-establish-IPv6-traffic-to-the-Internet-The-design-must-scale-automatically-and-not-incur-any-additional-cost-This-can-be-accomplished-with" class="headerlink" title="A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:"></a>A Solutions Architect is designing a VPC. Instances in a private subnet must be able to establish IPv6 traffic to the Internet. The design must scale automatically and not incur any additional cost. This can be accomplished with:</h2><p>A. an egress-only internet gateway<br>B. a NAT gateway<br>C. a custom NAT instance<br>D. a VPC endpoint</p><p>Answer: A</p><ul><li>参考链接：<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Scenario2.html</a><blockquote><p>An egress-only Internet gateway. This enables instances in the private subnet to send requests to the Internet over IPv6 (for example, for software updates). An egress-only Internet gateway is necessary if you want instances in the private subnet to be able to initiate communication with the Internet over IPv6. For more information, see Egress-Only Internet Gateways.</p></blockquote></li></ul><h2 id="A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application"><a href="#A-web-application-stores-all-data-in-an-Amazon-RDS-Aurora-database-instance-A-Solutions-Architect-wants-to-provide-access-to-the-data-for-a-detailed-report-for-the-Marketing-team-but-is-concerned-that-the-additional-load-on-the-database-will-affect-the-performance-of-the-web-application-How-can-the-report-be-created-without-affecting-the-performance-of-the-application" class="headerlink" title="A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?"></a>A web application stores all data in an Amazon RDS Aurora database instance. A Solutions Architect wants to provide access to the data for a detailed report for the Marketing team, but is concerned that the additional load on the database will affect the performance of the web application. How can the report be created without affecting the performance of the application?</h2><p>A. Create a read replica of the database.<br>B. Provision a new RDS instance as a secondary master.<br>C. Configure the database to be in multiple regions.<br>D. Increase the number of provisioned storage IOPS.</p><p>Answer: A</p><ul><li>分析：原有网站给出的答案是B，明显是A，搞这么复杂干啥</li></ul><h2 id="A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement"><a href="#A-company-has-an-application-that-stores-sensitive-data-The-company-is-required-by-government-regulations-to-store-multiple-copies-of-its-data-What-would-be-the-MOST-resilient-and-cost-effective-option-to-meet-this-requirement" class="headerlink" title="A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?"></a>A company has an application that stores sensitive data. The company is required by government regulations to store multiple copies of its data. What would be the MOST resilient and cost-effective option to meet this requirement?</h2><p>A. Amazon EFS<br>B. Amazon RDS<br>C. AWS Storage Gateway<br>D. Amazon S3</p><p>Answer: D</p><h2 id="A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution"><a href="#A-company-is-using-AWS-Key-Management-Service-AWS-KMS-to-secure-their-Amazon-RDS-databases-An-auditor-has-recommended-that-the-company-log-all-use-of-their-AWS-KMS-keys-What-is-the-SIMPLEST-solution" class="headerlink" title="A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?"></a>A company is using AWS Key Management Service (AWS KMS) to secure their Amazon RDS databases. An auditor has recommended that the company log all use of their AWS KMS keys. What is the SIMPLEST solution?</h2><p>A. Associate AWS KMS metrics with Amazon CloudWatch.<br>B. Use AWS CloudTrail to log AWS KMS key usage.<br>C. Deploy a monitoring agent on the RDS instances.<br>D. Poll AWS KMS periodically with a scheduled job.</p><p>Answer: B</p><h2 id="A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform"><a href="#A-Solutions-Architect-is-designing-a-stateful-web-application-that-will-run-for-one-year-24-7-and-then-be-decommissioned-Load-on-this-platform-will-be-constant-using-a-number-of-r4-8xlarge-instances-Key-drivers-for-this-system-include-high-availability-but-elasticity-is-not-required-What-is-the-MOST-cost-effective-way-to-purchase-compute-for-this-platform" class="headerlink" title="A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?"></a>A Solutions Architect is designing a stateful web application that will run for one year (24/7) and then be decommissioned. Load on this platform will be constant, using a number of r4.8xlarge instances. Key drivers for this system include high availability, but elasticity is not required. What is the MOST cost-effective way to purchase compute for this platform?</h2><p>A. Scheduled Reserved Instances<br>B. Convertible Reserved Instances<br>C. Standard Reserved Instances<br>D. Spot Instances</p><p>Answer: C</p><ul><li>分析：根据题目要求7*24小时不停机，所以需要排除A和D两个选项, B选项在这个场景下并不需要，所以选C</li></ul><blockquote><p>Exchanging Convertible Reserved Instances: You can exchange one or more Convertible Reserved Instances for another Convertible Reserved Instance with a different configuration, including instance family, operating system, and tenancy. There are no limits to how many times you perform an exchange, as long as the target Convertible Reserved Instance is of an equal or higher value than the Convertible Reserved Instances that you are exchanging.</p></blockquote><h2 id="A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements"><a href="#A-media-company-asked-a-Solutions-Architect-to-design-a-highly-available-storage-solution-to-serve-as-a-centralized-document-store-for-their-Amazon-EC2-instances-The-storage-solution-needs-to-be-POSIX-compliant-scale-dynamically-and-be-able-to-serve-up-to-100-concurrent-EC2-instances-Which-solution-meets-these-requirements" class="headerlink" title="A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?"></a>A media company asked a Solutions Architect to design a highly available storage solution to serve as a centralized document store for their Amazon EC2 instances. The storage solution needs to be POSIX-compliant, scale dynamically, and be able to serve up to 100 concurrent EC2 instances. Which solution meets these requirements?</h2><p>A. Create an Amazon S3 bucket and store all of the documents in this bucket.<br>B. Create an Amazon EBS volume and allow multiple users to mount that volume to their EC2 instance(s).<br>C. Use Amazon Glacier to store all of the documents.<br>D. Create an Amazon Elastic File System (Amazon EFS) to store and share the documents.</p><p>Answer: D</p><ul><li>分析：需要文件接口，并且同时访问，那么只有EFS能够满足</li></ul><h2 id="A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two"><a href="#A-Solution-Architect-has-a-two-tier-application-with-a-single-Amazon-EC2-instance-web-server-and-Amazon-RDS-MySQL-Multi-AZ-DB-instances-The-Architect-is-re-architecting-the-application-for-high-availability-by-adding-instances-in-a-second-Availability-Zone-Which-additional-services-will-improve-the-availability-of-the-application-Choose-two" class="headerlink" title="A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)"></a>A Solution Architect has a two-tier application with a single Amazon EC2 instance web server and Amazon RDS MySQL Multi-AZ DB instances. The Architect is re-architecting the application for high availability by adding instances in a second Availability Zone. Which additional services will improve the availability of the application? (Choose two.)</h2><p>A. Auto Scaling group<br>B. AWS CloudTrail<br>C. ELB Classic Load Balancer<br>D. Amazon DynamoDB<br>E. Amazon ElastiCache</p><p>Answer: AC</p><ul><li>分析：原网站给出的答案是AE，E显然没什么用对于目标</li></ul><h2 id="A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application"><a href="#A-company-is-migrating-its-data-center-to-AWS-As-part-of-this-migration-there-is-a-three-tier-web-application-that-has-strict-data-at-rest-encryption-requirements-The-customer-deploys-this-application-on-Amazon-EC2-using-Amazon-EBS-and-now-must-provide-encryption-at-rest-How-can-this-requirement-be-met-without-changing-the-application" class="headerlink" title="A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?"></a>A company is migrating its data center to AWS. As part of this migration, there is a three-tier web application that has strict data-at-rest encryption requirements. The customer deploys this application on Amazon EC2 using Amazon EBS, and now must provide encryption at-rest. How can this requirement be met without changing the application?</h2><p>A. Use AWS Key Management Service and move the encrypted data to Amazon S3.<br>B. Use an application-specific encryption API with AWS server-side encryption.<br>C. Use encrypted EBS storage volumes with AWS-managed keys.<br>D. Use third-party tools to encrypt the EBS data volumes with Key Management Service Bring Your Own Keys.</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements"><a href="#A-Solutions-Architect-is-developing-software-on-AWS-that-requires-access-to-multiple-AWS-services-including-an-Amazon-EC2-instance-This-is-a-security-sensitive-application-and-AWS-credentials-such-as-Access-Key-ID-and-Secret-Access-Key-need-to-be-protected-and-cannot-be-exposed-anywhere-in-the-system-What-security-measure-would-satisfy-these-requirements" class="headerlink" title="A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?"></a>A Solutions Architect is developing software on AWS that requires access to multiple AWS services, including an Amazon EC2 instance. This is a security sensitive application, and AWS credentials such as Access Key ID and Secret Access Key need to be protected and cannot be exposed anywhere in the system. What security measure would satisfy these requirements?</h2><p>A. Store the AWS Access Key ID/Secret Access Key combination in software comments.<br>B. Assign an IAM user to the Amazon EC2 instance.<br>C. Assign an IAM role to the Amazon EC2 instance.<br>D. Enable multi-factor authentication for the AWS root account.</p><p>Answer: C</p><h2 id="An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage"><a href="#An-AWS-workload-in-a-VPC-is-running-a-legacy-database-on-an-Amazon-EC2-instance-Data-is-stored-on-a-200GB-Amazon-EBS-gp2-volume-At-peak-load-times-logs-show-excessive-wait-time-What-solution-should-be-implemented-to-improve-database-performance-using-persistent-storage" class="headerlink" title="An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?"></a>An AWS workload in a VPC is running a legacy database on an Amazon EC2 instance. Data is stored on a 200GB Amazon EBS (gp2) volume. At peak load times, logs show excessive wait time. What solution should be implemented to improve database performance using persistent storage?</h2><p>A. Migrate the data on the Amazon EBS volume to an SSD-backed volume.<br>B. Change the EC2 instance type to one with EC2 instance store volumes.<br>C. Migrate the data on the EBS volume to provisioned IOPS SSD (io1).<br>D. Change the EC2 instance type to one with burstable performance.</p><p>Answer: C</p><ul><li>分析：原有答案给出的是D，但是从性能角度看C明显是正确的</li></ul><h2 id="A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user"><a href="#A-company’s-website-receives-50-000-requests-each-second-and-the-company-wants-to-use-multiple-applications-to-analyze-the-navigation-patterns-of-the-users-on-their-website-so-that-the-experience-can-be-personalized-What-can-a-Solutions-Architect-use-to-collect-page-clicks-for-the-website-and-process-them-sequentially-for-each-user" class="headerlink" title="A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?"></a>A company’s website receives 50,000 requests each second, and the company wants to use multiple applications to analyze the navigation patterns of the users on their website so that the experience can be personalized. What can a Solutions Architect use to collect page clicks for the website and process them sequentially for each user?</h2><p>A. Amazon Kinesis Stream<br>B. Amazon SQS standard queue<br>C. Amazon SQS FIFO queue<br>D. AWS CloudTrail trail</p><p>Answer: A</p><ul><li>Create real-time clickstream sessions and run analytics with Amazon Kinesis Data Analytics, AWS Glue, and Amazon Athena(<a href="https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/big-data/create-real-time-clickstream-sessions-and-run-analytics-with-amazon-kinesis-data-analytics-aws-glue-and-amazon-athena/</a>)</li><li>Amazon Kinesis – Real-Time Processing of Streaming Big Data(<a href="https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/aws/amazon-kinesis-real-time-processing-of-streamed-data/</a>)</li></ul><h2 id="A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend"><a href="#A-company-wants-to-migrate-a-highly-transactional-database-to-AWS-Requirements-state-that-the-database-has-more-than-6-TB-of-data-and-will-grow-exponentially-Which-solution-should-a-Solutions-Architect-recommend" class="headerlink" title="A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?"></a>A company wants to migrate a highly transactional database to AWS. Requirements state that the database has more than 6 TB of data and will grow exponentially. Which solution should a Solutions Architect recommend?</h2><p>A. Amazon Aurora<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon RDS MySQL</p><p>Answer: A</p><ul><li>分析：A和D的区别没有找到合适的解释，Aurora的扩展性更好，而且是AWS云原生的。</li></ul><h2 id="争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two"><a href="#争议-A-company-hosts-a-two-tier-application-that-consists-of-a-publicly-accessible-web-server-that-communicates-with-a-private-database-Only-HTTPS-port-443-traffic-to-the-web-server-must-be-allowed-from-the-Internet-Which-of-the-following-options-will-achieve-these-requirements-Choose-two" class="headerlink" title="(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)"></a>(争议)A company hosts a two-tier application that consists of a publicly accessible web server that communicates with a private database. Only HTTPS port 443 traffic to the web server must be allowed from the Internet. Which of the following options will achieve these requirements? (Choose two.)</h2><p>A. Security group rule that allows inbound Internet traffic for port 443.<br>B. Security group rule that denies all inbound Internet traffic except port 443.<br>C. Network ACL rule that allows port 443 inbound and all ports outbound for Internet traffic.<br>D. Security group rule that allows Internet traffic for port 443 in both inbound and outbound.<br>E. Network ACL rule that allows port 443 for both inbound and outbound for all Internet traffic.</p><p>Answer: AC</p><ul><li>分析：答案给出的是AE，根据Network ACL的描述，默认情况为白名单，是无状态性的，返回的端口不会自动允许，所以需要C选项打开所有返回的端口。</li><li>临时端口(<a href="https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/vpc/latest/userguide/vpc-network-acls.html#nacl-ephemeral-ports</a>)</li></ul><blockquote><p>临时端口<br>上一个部分中的网络 ACL 实例使用了临时端口范围 32768-65535。但是，您可能需要根据自己使用的或作为通信目标的客户端的类型为网络 ACL 使用不同的范围。<br>发起请求的客户端会选择临时端口范围。根据客户端的操作系统不同，范围也随之更改。<br>许多 Linux 内核（包括 Amazon Linux 内核）使用端口 32768-61000。<br>生成自 Elastic Load Balancing 的请求使用端口 1024-65535。<br>Windows 操作系统通过 Windows Server 2003 使用端口 1025-5000。<br>Windows Server 2008 及更高版本使用端口 49152-65535。<br>NAT 网关使用端口 1024 - 65535。<br>AWS Lambda 函数使用端口 1024-65535。<br>例如，如果一个来自 Internet 上的 Windows XP 客户端的请求到达您的 VPC 中的 Web 服务器，则您的网络 ACL 必须有相应的出站规则，以支持目标为端口 1025-5000 的数据流。<br>如果您的 VPC 中的一个实例是发起请求的客户端，则您的网络 ACL 必须有入站规则来支持发送到实例类型（Amazon Linux、Windows Server 2008 等）特有的临时端口的数据流。<br>在实际中，为使不同客户端类型可以启动流量进入您 VPC 中的公有实例，您可以开放临时端口 1024-65535。但是，您也可以在 ACL 中添加规则以拒绝任何在此范围内的来自恶意端口的数据流。请务必将拒绝 规则放在表的较前端，先于开放一系列临时端口的允许 规则。</p></blockquote><h2 id="A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through"><a href="#A-Solutions-Architect-is-designing-an-Amazon-VPC-Applications-in-the-VPC-must-have-private-connectivity-to-Amazon-DynamoDB-in-the-same-AWS-Region-The-design-should-route-DynamoDB-traffic-through" class="headerlink" title="A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:"></a>A Solutions Architect is designing an Amazon VPC. Applications in the VPC must have private connectivity to Amazon DynamoDB in the same AWS Region. The design should route DynamoDB traffic through:</h2><p>A. VPC peering connection.<br>B. NAT gateway<br>C. VPC endpoint<br>D. AWS Direct Connect</p><p>Answer: C</p><h2 id="A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement"><a href="#A-Solutions-Architect-is-architecting-a-workload-that-requires-a-performant-object-based-storage-system-that-must-be-shared-with-multiple-Amazon-EC2-instances-Which-AWS-service-meets-this-requirement" class="headerlink" title="A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?"></a>A Solutions Architect is architecting a workload that requires a performant object-based storage system that must be shared with multiple Amazon EC2 instances. Which AWS service meets this requirement?</h2><p>A. Amazon EFS<br>B. Amazon S3<br>C. Amazon EBS<br>D. Amazon ElastiCache</p><p>Answer: B</p><ul><li>分析：这道题给出的答案竟然是A，不明白这个网站是不是专门负责坑人的。object-based storage system，很明显是S3.</li></ul><h2 id="A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case"><a href="#A-Solutions-Architect-is-developing-a-solution-for-sharing-files-in-an-organization-The-solution-must-allow-multiple-users-to-access-the-storage-service-at-once-from-different-virtual-machines-and-scale-automatically-It-must-also-support-file-level-locking-Which-storage-service-meets-the-requirements-of-this-use-case" class="headerlink" title="A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?"></a>A Solutions Architect is developing a solution for sharing files in an organization. The solution must allow multiple users to access the storage service at once from different virtual machines and scale automatically. It must also support file-level locking. Which storage service meets the requirements of this use case?</h2><p>A. Amazon S3<br>B. Amazon EFS<br>C. Amazon EBS<br>D. Cached Volumes</p><p>Answer: B</p><h2 id="A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload"><a href="#A-company-runs-a-legacy-application-with-a-single-tier-architecture-on-an-Amazon-EC2-instance-Disk-I-O-is-low-with-occasional-small-spikes-during-business-hours-The-company-requires-the-instance-to-be-stopped-from-8-PM-to-8-AM-daily-Which-storage-option-is-MOST-appropriate-for-this-workload" class="headerlink" title="A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?"></a>A company runs a legacy application with a single-tier architecture on an Amazon EC2 instance. Disk I/O is low, with occasional small spikes during business hours. The company requires the instance to be stopped from 8 PM to 8 AM daily. Which storage option is MOST appropriate for this workload?</h2><p>A. Amazon EC2 instance storage<br>B. Amazon EBS General Purpose SSD (gp2) storage<br>C. Amazon S3<br>D. Amazon EBS Provision IOPS SSD (io1) storage</p><p>Answer: B</p><ul><li>分析：原始答案给出的是C，一个legcy application为什么会用S3呢？这可是需要应用改造的。</li></ul><h2 id="争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements"><a href="#争议-As-part-of-securing-an-API-layer-built-on-Amazon-API-gateway-a-Solutions-Architect-has-to-authorize-users-who-are-currently-authenticated-by-an-existing-identity-provider-The-users-must-be-denied-access-for-a-period-of-one-hour-after-three-unsuccessful-attempts-How-can-the-Solutions-Architect-meet-these-requirements" class="headerlink" title="(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?"></a>(争议)As part of securing an API layer built on Amazon API gateway, a Solutions Architect has to authorize users who are currently authenticated by an existing identity provider. The users must be denied access for a period of one hour after three unsuccessful attempts. How can the Solutions Architect meet these requirements?</h2><p>A. Use AWS IAM authorization and add least-privileged permissions to each respective IAM role.<br>B. Use an API Gateway custom authorizer to invoke an AWS Lambda function to validate each user’s identity.<br>C. Use Amazon Cognito user pools to provide built-in user management.<br>D. Use Amazon Cognito user pools to integrate with external identity providers.</p><p>Answer: B</p><ul><li>分析：正义点在答案D，参考链接：<a href="https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html" target="_blank" rel="noopener">https://serverless-stack.com/chapters/cognito-user-pool-vs-identity-pool.html</a></li></ul><h2 id="An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution"><a href="#An-organization-runs-an-online-media-site-hosted-on-premises-An-employee-posted-a-product-review-that-contained-videos-and-pictures-The-review-went-viral-and-the-organization-needs-to-handle-the-resulting-spike-in-website-traffic-What-action-would-provide-an-immediate-solution" class="headerlink" title="An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?"></a>An organization runs an online media site, hosted on-premises. An employee posted a product review that contained videos and pictures. The review went viral and the organization needs to handle the resulting spike in website traffic. What action would provide an immediate solution?</h2><p>A. Redesign the website to use Amazon API Gateway, and use AWS Lambda to deliver content.<br>B. Add server instances using Amazon EC2 and use Amazon Route 53 with a failover routing policy.<br>C. Serve the images and videos via an Amazon CloudFront distribution created using the news site as the origin.<br>D. Use Amazon ElasticCache for Redis for caching and reducing the load requests from the origin.</p><p>Answer: C</p><h2 id="A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process"><a href="#A-client-notices-that-their-engineers-often-make-mistakes-when-creating-Amazon-SQS-queues-for-their-backend-system-Which-action-should-a-Solutions-Architect-recommend-to-improve-this-process" class="headerlink" title="A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?"></a>A client notices that their engineers often make mistakes when creating Amazon SQS queues for their backend system. Which action should a Solutions Architect recommend to improve this process?</h2><p>A. Use the AWS CLI to create queues using AWS IAM Access Keys.<br>B. Write a script to create the Amazon SQS queue using AWS Lambda.<br>C. Use AWS Elastic Beanstalk to automatically create the Amazon SQS queues.<br>D. Use AWS CloudFormation Templates to manage the Amazon SQS queue creation.</p><p>Answer: D</p><ul><li>教程：创建 Amazon SQS 队列(<a href="https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-create-queue.html#create-queue-cloudformation</a>)</li></ul><h2 id="争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone"><a href="#争议-A-development-team-is-building-an-application-with-front-end-and-backend-application-tiers-Each-tier-consists-of-Amazon-EC2-instances-behind-an-ELB-Classic-Load-Balancer-The-instances-run-in-Auto-Scaling-groups-across-multiple-Availability-Zones-The-network-team-has-allocated-the-10-0-0-0-24-address-space-for-this-application-Only-the-front-end-load-balancer-should-be-exposed-to-the-Internet-There-are-concerns-about-the-limited-size-of-the-address-space-and-the-ability-of-each-tier-to-scale-What-should-the-VPC-subnet-design-be-in-each-Availability-Zone" class="headerlink" title="(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?"></a>(争议)A development team is building an application with front-end and backend application tiers. Each tier consists of Amazon EC2 instances behind an ELB Classic Load Balancer. The instances run in Auto Scaling groups across multiple Availability Zones. The network team has allocated the 10.0.0.0/24 address space for this application. Only the front-end load balancer should be exposed to the Internet. There are concerns about the limited size of the address space and the ability of each tier to scale. What should the VPC subnet design be in each Availability Zone?</h2><p>A. One public subnet for the load balancer tier, one public subnet for the front-end tier, and one private subnet for the backend tier.<br>B. One shared public subnet for all tiers of the application.<br>C. One public subnet for the load balancer tier and one shared private subnet for the application tiers.<br>D. One shared private subnet for all tiers of the application.</p><p>Answer: C</p><ul><li>分析：答案给出的是A，但是题目中说道only the front-end load balancer should be exposed to the internet，所以A答案中为front-end tier一个公网subnet有点多余了</li></ul><h2 id="A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application"><a href="#A-Solutions-Architect-must-select-the-storage-type-for-a-big-data-application-that-requires-very-high-sequential-I-O-The-data-must-persist-if-the-instance-is-stopped-Which-of-the-following-storage-types-will-provide-the-best-fit-at-the-LOWEST-cost-for-the-application" class="headerlink" title="A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?"></a>A Solutions Architect must select the storage type for a big data application that requires very high sequential I/O. The data must persist if the instance is stopped. Which of the following storage types will provide the best fit at the LOWEST cost for the application?</h2><p>A. An Amazon EC2 instance store local SSD volume.<br>B. An Amazon EBS provisioned IOPS SSD volume.<br>C. An Amazon EBS throughput optimized HDD volume.<br>D. An Amazon EBS general purpose SSD volume.</p><p>Answer: C</p><ul><li>分析：这道题需要高顺序I/O和低成本，显然C正确</li></ul><h2 id="Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy"><a href="#Two-Auto-Scaling-applications-Application-A-and-Application-B-currently-run-within-a-shared-set-of-subnets-A-Solutions-Architect-wants-to-make-sure-that-Application-A-can-make-requests-to-Application-B-but-Application-B-should-be-denied-from-making-requests-to-Application-A-Which-is-the-SIMPLEST-solution-to-achieve-this-policy" class="headerlink" title="Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?"></a>Two Auto Scaling applications, Application A and Application B, currently run within a shared set of subnets. A Solutions Architect wants to make sure that Application A can make requests to Application B, but Application B should be denied from making requests to Application A. Which is the SIMPLEST solution to achieve this policy?</h2><p>A. Using security groups that reference the security groups of the other application<br>B. Using security groups that reference the application server’s IP addresses<br>C. Using Network Access Control Lists to allow/deny traffic based on application IP addresses<br>D. Migrating the applications to separate subnets from each other</p><p>Answer: A</p><h2 id="Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two"><a href="#Legacy-applications-currently-send-messages-through-a-single-Amazon-EC2-instance-which-then-routes-the-messages-to-the-appropriate-destinations-The-Amazon-EC2-instance-is-a-bottleneck-and-single-point-of-failure-so-the-company-would-like-to-address-these-issues-Which-services-could-address-this-architectural-use-case-Choose-two" class="headerlink" title="Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)"></a>Legacy applications currently send messages through a single Amazon EC2 instance, which then routes the messages to the appropriate destinations. The Amazon EC2 instance is a bottleneck and single point of failure, so the company would like to address these issues. Which services could address this architectural use case? (Choose two.)</h2><p>A. Amazon SNS<br>B. AWS STS<br>C. Amazon SQS<br>D. Amazon Route 53<br>E. AWS Glue</p><p>Answer: AC</p><ul><li>分析：根据题目是要解决消息的问题，消息服务有两种SNS和SQS，因为题目里并没有说消息模式，所以这两种也许能解决需求。</li></ul><h2 id="A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model"><a href="#A-Solutions-Architect-needs-to-design-an-architecture-for-a-new-mission-critical-batch-processing-billing-application-The-application-is-required-to-run-Monday-Wednesday-and-Friday-from-5-AM-to-11-AM-Which-is-the-MOST-cost-effective-Amazon-EC2-pricing-model" class="headerlink" title="A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?"></a>A Solutions Architect needs to design an architecture for a new, mission-critical batch processing billing application. The application is required to run Monday, Wednesday, and Friday from 5 AM to 11 AM. Which is the MOST cost-effective Amazon EC2 pricing model?</h2><p>A. Amazon EC2 Spot Instances<br>B. On-Demand Amazon EC2 Instances<br>C. Scheduled Reserved Instances<br>D. Dedicated Amazon EC2 Instances</p><p>Answer: C</p><h2 id="A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available"><a href="#A-workload-consists-of-downloading-an-image-from-an-Amazon-S3-bucket-processing-the-image-and-moving-it-to-another-Amazon-S3-bucket-An-Amazon-EC2-instance-runs-a-scheduled-task-every-hour-to-perform-the-operation-How-should-a-Solutions-Architect-redesign-the-process-so-that-it-is-highly-available" class="headerlink" title="A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?"></a>A workload consists of downloading an image from an Amazon S3 bucket, processing the image, and moving it to another Amazon S3 bucket. An Amazon EC2 instance runs a scheduled task every hour to perform the operation. How should a Solutions Architect redesign the process so that it is highly available?</h2><p>A. Change the Amazon EC2 instance to compute optimized.<br>B. Launch a second Amazon EC2 instance to monitor the health of the first.<br>C. Trigger a Lambda function when a new object is uploaded.<br>D. Initially copy the images to an attached Amazon EBS volume.</p><p>Answer: C</p><ul><li>分析：Lambda的触发器很适合做这个</li></ul><h2 id="An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met"><a href="#An-application-is-running-on-an-Amazon-EC2-instance-in-a-private-subnet-The-application-needs-to-read-and-write-data-onto-Amazon-Kinesis-Data-Streams-and-corporate-policy-requires-that-this-traffic-should-not-go-to-the-internet-How-can-these-requirements-be-met" class="headerlink" title="An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?"></a>An application is running on an Amazon EC2 instance in a private subnet. The application needs to read and write data onto Amazon Kinesis Data Streams, and corporate policy requires that this traffic should not go to the internet. How can these requirements be met?</h2><p>A. Configure a NAT gateway in a public subnet and route all traffic to Amazon Kinesis through the NAT gateway.<br>B. Configure a gateway VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>C. Configure an interface VPC endpoint for Kinesis and route all traffic to Kinesis through the gateway VPC endpoint.<br>D. Configure an AWS Direct Connect private virtual interface for Kinesis and route all traffic to Kinesis through the virtual interface.</p><p>Answer: C</p><ul><li>分析：误选了B，从题目说是Kinesis需要读取EC2的数据，所以应该是在VPC interface上建立一个endpoint</li></ul><h2 id="A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements"><a href="#A-Solutions-Architect-is-building-an-application-that-stores-object-data-Compliance-requirements-state-that-the-data-stored-is-immutable-Which-service-meets-these-requirements" class="headerlink" title="A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?"></a>A Solutions Architect is building an application that stores object data. Compliance requirements state that the data stored is immutable. Which service meets these requirements?</h2><p>A. Amazon S3<br>B. Amazon Glacier<br>C. Amazon EFS<br>D. AWS Storage Gateway</p><p>Answer: B</p><blockquote><p>Data stored in Amazon Glacier is immutable, meaning that after an archive is created it cannot be updated. This ensures that data such as compliance and regulatory records cannot be altered after they have been archived.</p></blockquote><ul><li>分析：从这道题我们可以看出，考试的时候选中文的重要性，不会因为一个单词意思不明确导致整个题目判断失误，关键词是immutalbe，不可改变的</li></ul><h2 id="争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted"><a href="#争议-A-Solutions-Architect-is-defining-a-shared-Amazon-S3-bucket-where-corporate-applications-will-save-objects-How-can-the-Architect-ensure-that-when-an-application-uploads-an-object-to-the-Amazon-S3-bucket-the-object-is-encrypted" class="headerlink" title="(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?"></a>(争议)A Solutions Architect is defining a shared Amazon S3 bucket where corporate applications will save objects. How can the Architect ensure that when an application uploads an object to the Amazon S3 bucket, the object is encrypted?</h2><p>A. Set a CORS configuration.<br>B. Set a bucket policy to encrypt all Amazon S3 objects.<br>C. Enable default encryption on the bucket.<br>D. Set permission for users.</p><p>Answer: B</p><ul><li>分析：争议点在于答案C，从界面操作上看BC好像是在做同一件事情</li><li>如何为 Amazon S3 存储桶启用默认加密？(<a href="https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html" target="_blank" rel="noopener">https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/default-bucket-encryption.html</a>)</li><li>How to Prevent Uploads of Unencrypted Objects to Amazon S3(<a href="https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/" target="_blank" rel="noopener">https://aws.amazon.com/cn/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/</a>)</li></ul><h2 id="An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path"><a href="#An-application-tier-currently-hosts-two-web-services-on-the-same-set-of-instances-listening-on-different-ports-Which-AWS-service-should-a-Solutions-Architect-use-to-route-traffic-to-the-service-based-on-the-incoming-request-path" class="headerlink" title="An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?"></a>An application tier currently hosts two web services on the same set of instances, listening on different ports. Which AWS service should a Solutions Architect use to route traffic to the service based on the incoming request path?</h2><p>A. AWS Application Load Balancer<br>B. Amazon CloudFront<br>C. Amazon Classic Load Balancer<br>D. Amazon Route 53</p><p>Answer: A</p><h2 id="A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend"><a href="#A-data-analytics-startup-company-asks-a-Solutions-Architect-to-recommend-an-AWS-data-store-options-for-indexed-data-The-data-processing-engine-will-generate-and-input-more-than-64-TB-of-processed-data-every-day-with-item-sizes-reaching-up-to-300-KB-The-startup-is-flexible-with-data-storage-and-is-more-interested-in-a-database-that-requires-minimal-effort-to-scale-with-a-growing-dataset-size-Which-AWS-data-store-service-should-the-Architect-recommend" class="headerlink" title="A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?"></a>A data analytics startup company asks a Solutions Architect to recommend an AWS data store options for indexed data. The data processing engine will generate and input more than 64 TB of processed data every day, with item sizes reaching up to 300 KB. The startup is flexible with data storage and is more interested in a database that requires minimal effort to scale with a growing dataset size. Which AWS data store service should the Architect recommend?</h2><p>A. Amazon RDS<br>B. Amazon Redshift<br>C. Amazon DynamoDB<br>D. Amazon S3</p><p>Answer: C</p><h2 id="争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows"><a href="#争议-A-Solutions-Architect-needs-to-allow-developers-to-have-SSH-connectivity-to-web-servers-The-requirements-are-as-follows" class="headerlink" title="(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:"></a>(争议)A Solutions Architect needs to allow developers to have SSH connectivity to web servers. The requirements are as follows:</h2><p>✑ Limit access to users origination from the corporate network.<br>✑ Web servers cannot have SSH access directly from the Internet.<br>✑ Web servers reside in a private subnet.<br>Which combination of steps must the Architect complete to meet these requirements? (Choose two.)</p><p>A. Create a bastion host that authenticates users against the corporate directory.<br>B. Create a bastion host with security group rules that only allow traffic from the corporate network.<br>C. Attach an IAM role to the bastion host with relevant permissions.<br>D. Configure the web servers’ security group to allow SSH traffic from a bastion host.<br>E. Deny all SSH traffic from the corporate network in the inbound network ACL.</p><p>Answer: BD</p><ul><li>分析：原有答案给出的是AC，感觉并不能完全解决该问题</li><li>How to Record SSH Sessions Established Through a Bastion Host(<a href="https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/" target="_blank" rel="noopener">https://aws.amazon.com/blogs/security/how-to-record-ssh-sessions-established-through-a-bastion-host/</a>)</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;参考链接：&lt;a href=&quot;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate/view/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;一直对AWS情有独钟，也想尝试考取最高认证，但是苦于无法集中精力学习。2019年由于和AWS合作的原因，所以痛下决心一定要考取AWS各种认证。另外，在AWS的学习过程中，也逐渐帮我梳理了以前在OpenStack开发过程中不是很清晰的设计理念。并且AWS的文档和最佳实践堪称各个公有云的典范，非常具有学习价值。考试不是最终的目的，学以致用才是。&lt;/p&gt;
&lt;p&gt;由于备考AWS ACA考试，所以从网上看到这套模拟试题，在学习过程中对试题进行系统性分析和记录。发现有很多问题答案并非十分准确，所以也尝试做出分析和更正。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="AWS" scheme="http://sunqi.site/tags/AWS/"/>
    
      <category term="ACA Exam" scheme="http://sunqi.site/tags/ACA-Exam/"/>
    
  </entry>
  
  <entry>
    <title>使用阿里云函数计算构建小程序</title>
    <link href="http://sunqi.site/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/"/>
    <id>http://sunqi.site/2019/12/19/how-to-use-aliyun-function-service-to-implement-mini-program/</id>
    <published>2019-12-19T15:19:16.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<h1 id="1、需求"><a href="#1、需求" class="headerlink" title="1、需求"></a>1、需求</h1><p>在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。</p><a id="more"></a><img src="/images/blogs/2019-12-19/architecture.png" class=""><p>在原有设计中，使用了Python Flask提供WEB界面，后台使用Celery异步的将用户请求发送至钉钉中，之后采用轮询方式监控审批工单状态，当工单完成审批后，将生成好的License发送至客户提供的邮箱中。</p><p>实现的效果：</p><img src="/images/blogs/2019-12-19/UI.jpeg" class=""><p>这种方式虽然可以满足需求，但是在使用过程中也发现有如下痛点：<br>1、由于对于可用性要求比较高，所以将整套应用以容器化方式部署在云主机上，程序高可用性依赖于底层的平台，基于成本考虑并没有在多可用区进行部署。<br>2、当业务变化时，需要专人将容器从本地容器库上传后进行更新，更新速度慢，敏捷性低。<br>3、需要专人对操作系统层进行维护，并且由于该云主机还运行了其他程序，所以管控上也存在安全风险。</p><p>基于以上出现的问题，决定对原有二维码程序进行重构，并重新部署在阿里云函数计算服务上。<br>1、第一阶段的改造主要是将二维码扫描程序移植到函数计算服务中。<br>2、第二阶段的改造主要是将发送二维码程序改造为函数计算服务，使用钉钉流程接口中的Callback方法调用该接口，在审批结束后触发发送License流程。</p><h1 id="2、函数计算服务——无服务，零运维"><a href="#2、函数计算服务——无服务，零运维" class="headerlink" title="2、函数计算服务——无服务，零运维"></a>2、函数计算服务——无服务，零运维</h1><p>最早接触Serverless的雏形是在2011年开发Cloud Foundry项目时，当时留下一个非常深的印象就是把写好的应用直接上传就完成了部署、扩展等。但是当时Cloud Foundry有一个非常大的局限性，受限于几种开发语言和框架。记得当时的Cloud Foundry只支持Node.js、Python、Java、PHP、Ruby on Rails等，脱离了这个范围则就无法支持，所以当时我其实对这种形态的应用场景存在很大的疑问。<br>这种困惑直到2013年Docker的出现而逐步解开，Docker的出现让开发语言、框架不再是问题，巧妙的解决了Cloud Foundry上述局限性。但是Docker毕竟只是一种工具形态，还不能称得上是平台，紧接着k8s的出现弥补了这一空白，使得Docker从游击队变成了正规军。<br>在这个发展过程中我们不难看出，软件领域发展出现了重大变革，从服务器为王逐渐演进到应用为王的阶段。如果说虚拟化改变了整个物理机的格局，那么无服务化的出现则改变了整个软件开发行业。<br>由于网上各种文档太多了，这里就不对Serverless基本概念进行介绍了，借用一张图说明下。另外还有一点，我们从这里面看到IT行业里的某些岗位，注定要消失的，比如传统运维。</p><img src="/images/blogs/2019-12-19/compare.png" class=""><h1 id="3、应用架构"><a href="#3、应用架构" class="headerlink" title="3、应用架构"></a>3、应用架构</h1><p>整个架构上，分为两个函数计算服务完成：</p><ul><li>二维码前端：主要用于显示页面，并承担HTTP请求转发代理的角色，将请求转发至二维码后端，发给钉钉，采用HTTP触发器，允许公网访问。</li><li>二维码后端：用于将用户请求发送给钉钉，该部分服务仍然采用HTTP触发器，不同于前端，该服务是不允许公网直接访问的，但是需要配置NAT网关，通过网关访问钉钉，实现固定IP访问钉钉的效果。</li></ul><img src="/images/blogs/2019-12-19/new_architecture.png" class=""><p>从逻辑上讲，整个应用并不复杂，但是在实际使用时遇到最大的问题来自钉钉白名单。由于函数服务对外连接的IP并不固定，所以无法在钉钉中添加，那么就要求函数服务对外连接的IP地址一定要固定。社区中提供的方法主要分为：</p><ul><li>ECI（运行Nginx充当Proxy），优势是便宜，劣势是高可用性需要自己维护</li><li>NAT网关，优势是高可用性，劣势是比ECI贵</li></ul><h1 id="4、构建过程"><a href="#4、构建过程" class="headerlink" title="4、构建过程"></a>4、构建过程</h1><p>由于篇幅原因，这里只介绍关键步骤。</p><h2 id="4-1-构建模板"><a href="#4-1-构建模板" class="headerlink" title="4.1 构建模板"></a>4.1 构建模板</h2><p>为了后续管理和扩展方便，选用了阿里云函数计算中使用flask-web模板进行构建，同时可以将前端静态文件模板存放于项目下（出于统一管理的需要，也可以存放于阿里云的OSS中，作为静态网站发布）。</p><p>前端我们使用flask-web作为模板创建函数，后端我们直接采用最简单的HTTP函数。</p><img src="/images/blogs/2019-12-19/create_function_template.png" class=""><p>函数入口配置，及触发器配置：</p><img src="/images/blogs/2019-12-19/create_function.png" class=""><p>服务配置，包含公网访问权限，专有网络配置，日志配置，权限配置。</p><ul><li>前端服务需要公网访问权限，不需要专有网络配置，需要的权限为：AliyunLogFullAccess。</li><li>后端服务不需要公网访问权限，但是需要配置好的NAT映射的专有网络，由于函数服务在北京2区中在cn-beijing-c和cn-beijing-f，所以在新建交换机时需要使用这两个区。还需要选择安全组，由于出方向并没有明确禁止，所以不需要特别的安全组规则设定。需要的权限为：AliyunLogFullAccess/AliyunECSNetworkInterfaceManagementAccess。</li></ul><p>配置好后，通过导出功能，分别下载前端和后端代码和配置，在本地进行开发调试。</p><img src="/images/blogs/2019-12-19/export_function.png" class=""><h2 id="4-2-前端开发"><a href="#4-2-前端开发" class="headerlink" title="4.2 前端开发"></a>4.2 前端开发</h2><p>我们的前端采用Vue.js进行开发，在main.py同级新建templates目录。Vue编译好的静态文件可以放入该目录中，后续Flask会加载该文件作为入口文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">├── templates</span><br><span class="line">│   ├── index.html</span><br><span class="line">│   ├── static</span><br><span class="line">├── main.py</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># main.py sample</span><br><span class="line">from flask import render_template</span><br><span class="line"></span><br><span class="line">LICENSE_URL &#x3D; &quot;https:&#x2F;&#x2F;[x](https:&#x2F;&#x2F;.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license)x[x](https:&#x2F;&#x2F;xxxxxx.cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license).cn-beijing-internal.fc.aliyuncs.com&#x2F;2016-08-15&#x2F;proxy&#x2F;QR_code&#x2F;apply_license&#x2F;license&quot;</span><br><span class="line"></span><br><span class="line">@app.route(&#39;&#x2F;qr_code&#39;, methods&#x3D;[&#39;GET&#39;])</span><br><span class="line">def index():</span><br><span class="line">      return render_template(&#39;index.html&#39;)</span><br><span class="line"></span><br><span class="line">      @app.route(&#39;&#x2F;qr_code&#x2F;license&#39;, methods&#x3D;[&#39;POST&#39;])</span><br><span class="line">      def create():</span><br><span class="line">            payload &#x3D; request.json</span><br><span class="line">                resp &#x3D; requests.post(LICENSE_URL,</span><br><span class="line">                                                 json&#x3D;payload,</span><br><span class="line">                                                                              headers&#x3D;DEFAULT_HEADERS)</span><br><span class="line">                return make_response(resp.text, resp.status_code)</span><br></pre></td></tr></table></figure><h2 id="4-3-后端开发"><a href="#4-3-后端开发" class="headerlink" title="4.3 后端开发"></a>4.3 后端开发</h2><p>后端的开发较为简单，实现一个函数支持POST请求，将转发的结果发送至钉钉即可。</p><h2 id="4-4-本地调试"><a href="#4-4-本地调试" class="headerlink" title="4.4 本地调试"></a>4.4 本地调试</h2><p>阿里云在本地开发时提供了fun应用部署和开发工具，详细使用方法见：<a href="https://help.aliyun.com/document_detail/64204.html" target="_blank" rel="noopener">https://help.aliyun.com/document_detail/64204.html</a>。</p><h3 id="安装fun"><a href="#安装fun" class="headerlink" title="安装fun"></a>安装fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">npm config set registry [https:&#x2F;&#x2F;registry.npm.taobao.org](https:&#x2F;&#x2F;registry.npm.taobao.org&#x2F;) --global</span><br><span class="line">npm config set disturl [https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist](https:&#x2F;&#x2F;npm.taobao.org&#x2F;dist) --global</span><br><span class="line"></span><br><span class="line">npm install @alicloud&#x2F;fun -g</span><br></pre></td></tr></table></figure><h3 id="配置fun"><a href="#配置fun" class="headerlink" title="配置fun"></a>配置fun</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">fun config</span><br><span class="line"></span><br><span class="line">(venv) [root@ray-dev test_func]# fun config</span><br><span class="line">? Aliyun Account ID xxxxxxxx</span><br><span class="line">? Aliyun Access Key ID ***********r5Qd</span><br><span class="line">? Aliyun Access Key Secret ***********kCCi</span><br><span class="line">? Default region name cn-beijing</span><br><span class="line">? The timeout in seconds for each SDK client invoking 10</span><br><span class="line">? The maximum number of retries for each SDK client 3</span><br><span class="line">? Allow to anonymously report usage statistics to improve the tool over time? Yes</span><br></pre></td></tr></table></figure><h3 id="Http-Trigger本地运行"><a href="#Http-Trigger本地运行" class="headerlink" title="Http Trigger本地运行"></a>Http Trigger本地运行</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun local start</span><br></pre></td></tr></table></figure><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">fun deploy</span><br></pre></td></tr></table></figure><h2 id="4-5-配置域名解析"><a href="#4-5-配置域名解析" class="headerlink" title="4.5 配置域名解析"></a>4.5 配置域名解析</h2><p>部署完成后有一点需要特别注意，必须要绑定域名，并且设定必要的路由。如果在没有绑定域名的情况下，服务端会为 response header中强制添加 content-disposition: attachment字段，此字段会使得返回结果在浏览器中以附件的方式打开。（<a href="https://www.alibabacloud.com/help/zh/doc-detail/56103.htm" target="_blank" rel="noopener">https://www.alibabacloud.com/help/zh/doc-detail/56103.htm</a>）</p><h1 id="5、总结"><a href="#5、总结" class="headerlink" title="5、总结"></a>5、总结</h1><ul><li>灵活使用函数计算对开发成本和运行成本具有“双降”的效果</li><li>函数计算除了Http Trigger外，还包含了Event Trigger。Event Trigger中包含了连接各个服务之间的作用，在一些服务衔接上的作用越来越明显</li><li>函数计算在线开发时比较麻烦，并且查看日志不方便，所以尽量在本地开发好在上传的方式</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;1、需求&quot;&gt;&lt;a href=&quot;#1、需求&quot; class=&quot;headerlink&quot; title=&quot;1、需求&quot;&gt;&lt;/a&gt;1、需求&lt;/h1&gt;&lt;p&gt;在用户使用HyperMotion产品过程中，用户可以通过扫描产品中二维码方式，自助进行Licnese申请。用户提交申请后，请求将发送到钉钉流程中。完成审批后，后台服务将自动根据用户的特征码、申请的数量、可使用的时间将生成好的正式Licnese发送到客户的邮箱中。&lt;/p&gt;
    
    </summary>
    
    
    
      <category term="阿里云" scheme="http://sunqi.site/tags/%E9%98%BF%E9%87%8C%E4%BA%91/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.site/tags/Cloud-Computing/"/>
    
      <category term="Serverless" scheme="http://sunqi.site/tags/Serverless/"/>
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Newton国内代码贡献</title>
    <link href="http://sunqi.site/2016/09/30/contricution-in-newton/"/>
    <id>http://sunqi.site/2016/09/30/contricution-in-newton/</id>
    <published>2016-09-30T17:00:49.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。</p><p>这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。</p><p>最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。</p><p>还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><p>OpenStack Mitaka深度解读请见：<a href="http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>下面是最近三个版本Release的详细对比：</p><img src="/images/blogs/contribution-in-newton-projects.png" class="center"><p>让我们来关注这次Release中的三个新项目：</p><h3 id="Panko-计量服务事件消息存储"><a href="#Panko-计量服务事件消息存储" class="headerlink" title="Panko(计量服务事件消息存储)"></a>Panko(计量服务事件消息存储)</h3><p>Panko是计量模块中的一部分，主要是为了计量模块提供事件消息存储，众所周知，在上一个OpenStack Release中，Ceilometer被一分为三，分别为aodh(告警服务)/Gnocchi(基于时间的数据库服务)/Ceilometer，为了解决当前Ceilometer中存在的性能问题，提高更好的扩展性。</p><p>现在Panko的文档并不是很丰富，如果有需要了解更多详细内容的，可以关注Developer的文档：<a href="http://docs.openstack.org/developer/panko/" target="_blank" rel="noopener">http://docs.openstack.org/developer/panko/</a></p><h3 id="Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务"><a href="#Vitrage-广大OpenStack管理员的福音，平台问题定位分析服务" class="headerlink" title="Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)"></a>Vitrage(广大OpenStack管理员的福音，平台问题定位分析服务)</h3><p>Vitrage是一个OpenStack RCA(Root Cause Analysis)服务，用于组织、分析和扩展OpenStack的告警和事件，在真正的问题发生前找到根本原因。</p><p>众所周知，OpenStack平台最大的优势来自于架构的可扩展性，这也是OpenStack能够在基础架构曾一枝独秀的重要原因。分布式架构最大的优势在于扩展，但是过于灵活的扩展性为运维带来的极大的困难，所以Vitrage的出现在一定程度上缓解了OpenStack运维上的痛点。</p><p>我们来简单看一下他的架构，更多详细的介绍请查看WIKI：<a href="https://wiki.openstack.org/wiki/Vitrage" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Vitrage</a></p><img src="/images/blogs/contribution-in-newton-vitrage-architecture.png" class="center"><h3 id="Watcher-OpenStack平台优化服务"><a href="#Watcher-OpenStack平台优化服务" class="headerlink" title="Watcher(OpenStack平台优化服务)"></a>Watcher(OpenStack平台优化服务)</h3><p>从名字上看，我们并不能理解这个模块的具体左右，我们通过文档中用户应用场景来了解一下Watcher的作用：</p><p>作为一名云平台的管理员在云平台使用一段时间后，想根据一些物理特性对云平台虚拟机的分布进行重新平衡，例如服务器的温度、电源的状态等信息，那么这时候就可以通过watcher，利用Nova虚拟机的在线迁移对整个数据中心云平台的虚拟机进行一些优化处理，从而达到某种平衡。我认为这其实类似于VMWare的DRS功能。</p><p>当然Watcher还有更多的应用场景，更多详细的介绍请查看：<a href="https://wiki.openstack.org/wiki/Watcher" target="_blank" rel="noopener">https://wiki.openstack.org/wiki/Watcher</a></p><p>我们来简单看一下他的架构，更多架构方面的详细的介绍请查看：<a href="http://docs.openstack.org/developer/watcher/architecture.html" target="_blank" rel="noopener">http://docs.openstack.org/developer/watcher/architecture.html</a></p><img src="/images/blogs/contribution-in-newton-watcher-architecture.svg" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits和blueprints的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司和贡献者来说，都有所上升，这也不难理解，随着OpenStack模块增加，势必涉及更多的领域，所以更多的公司加入了这个生态圈。</p><img src="/images/blogs/contribution-in-newton-companies-contributors.png" class="center"><p>从commits角度进行分析，传统几大好强几乎没有变化，日本的Fujitsu在commits上挤掉了华为，进入了前十名的位置。模块方面，核心模块的贡献仍然位于前十名，也说明是应用最多的模块，所以才会不断的发现问题。本次统计的总项目数量为629个，可能stackalytics在统计策略上有所调整。</p><img src="/images/blogs/contribution-in-newton-companies-modules-commits.png" class="center"><p>单从commits角度统计其实有失偏颇，真正能够体现公司在OpenStack实力的指标应该是Blueprints。我认为完成Blueprints至少具备三个必要条件：英语要好、在社区有一定的影响力、架构设计能力。这些都是需要不断在社区进行积累和沉淀的。</p><p>本次release周期内，能够完成Blueprints的公司为64个，国内的华为和九州云均进入前10名，排名比较靠前的国内企业还包括：Easystack、中兴。</p><p>完成Blueprints最多的仍然是核心模块，排在第二名的是kolla，看来在上一个周期中，kolla项目的活跃程度是较高的。</p><img src="/images/blogs/contribution-in-newton-companies-modules-blueprints.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完总体的状况，再来关注一些国内的贡献情况，与去年相比，今年上榜的国内企业达到了21家，创历年之最，比去年的15家企业整整多了7家，并且我们发现在这些新增企业中大部分都是提供企业服务的公司，说明OpenStack在国内的企业级市场开始站稳脚跟。下面我们来做一个详细的分析：</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在最近的三个版本连续对社区有贡献的企业包括：华为，Easystack，九州云，海云捷迅，华三，Unitedstack，乐视，中国移动和北京休伦科技(Huron)。</p><p>本次爬升最快的企业：中兴，从108位攀升至13位。</p><p>本次统计新增的7家企业：云途腾(t2cloud)，大唐高鸿数据(GohighSec)，华云数据，烽火通信，爱数，北京国电通，云英，中国银联，赛特斯信息。</p><p>本次排名中OpenStack的直接用户：中国移动和中国银联。中国移动更是参选了OpenStack SuperAward的评比，预祝他们能顺利当选。</p><img src="/images/blogs/contribution-in-newton-china-companies.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>我们再来从人员投入来分析贡献情况一下：</p><ul><li>投入人数最多的仍然是华为，有65名工程师贡献了本次的commits</li><li>中兴无疑是本次人员投入增长最快的，从6名工程师一下子扩张到61名，也是唯一能和华为抗衡的</li><li>超过2位数人员投入的包括，Easystack，九州云和Unitedstack，另外海云捷迅有9人，华三有8人，中国移动有7人参与社区贡献</li></ul><img src="/images/blogs/contribution-in-newton-companies-effort.png" class="center"><h3 id="模块贡献分析"><a href="#模块贡献分析" class="headerlink" title="模块贡献分析"></a>模块贡献分析</h3><p>从模块贡献角度来分析，国内企业的贡献仍然没有出现一个统一的趋势，与Mitaka Release相比，贡献涉及模块的总量从192个增加至Newton Release的246个，一方面说明OpenStack本身模块的增加，也说明国内企业使用或开发OpenStack在方向上的多元化。</p><p>从贡献的模块来看，华为主导的dargonflow高居榜首，紧随其后的是手册和clients两个项目，随后的贡献集中在OpenStack的核心模块，与Docker相关的几个模块中。Kolla项目无疑是最近关注的热点，随着Docker的快速发展，OpenStack和Docker不断碰撞出新的火花。</p><img src="/images/blogs/contribution-in-newton-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这个问题仍然是比较敏感的问题，只有每个公司的CEO能够回答这个问题，这里面我从融资的角度来回顾一下2015至2016年之间在OpenStack领域发生过什么。</p><ul><li>2015年9月17日，英特尔投资部门披露了此前投资的中国8家公司名单。投资总额达6700万美元，领域覆盖了新材料、智能设备、物联网、云服务等领域。其中包含九州云和海云捷迅两家OpenStack企业。(<a href="http://tech.qq.com/a/20150917/038604.htm" target="_blank" rel="noopener">http://tech.qq.com/a/20150917/038604.htm</a>)</li><li>2015年10月17日，中国最大的独立公有云提供商UCloud和全球领先的OpenStack厂商Mirantis在东京的OpenStack峰会上正式宣布成立合资公司UMCloud，以求更好的在中国做OpenStack。(<a href="http://www.doit.com.cn/article/1027290510.html" target="_blank" rel="noopener">http://www.doit.com.cn/article/1027290510.html</a>)</li><li>2015年12月16日，UnitedStack有云宣布完成C轮融资，该轮融资由思科和红杉资本投资，具体数额未公布(<a href="http://www.infoq.com/cn/news/2015/12/unitedstack-financing" target="_blank" rel="noopener">http://www.infoq.com/cn/news/2015/12/unitedstack-financing</a>)</li><li>2016年5月20日，云途腾(T2Cloud)完成A轮3650万融资(<a href="http://iimedia.cn/42262.html" target="_blank" rel="noopener">http://iimedia.cn/42262.html</a>)</li><li>2016年9月21日，腾讯与海云捷迅昨日下午在京共同宣布达成战略投资合作关系，海云捷迅接受腾讯的战略投资(<a href="http://www.36dsj.com/archives/62353" target="_blank" rel="noopener">http://www.36dsj.com/archives/62353</a>)</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>回到开篇的那句话，OpenStack贡献量只能反应中国企业对于开源项目的重视程度，无法反应真实的用户需求。VMWare花了将近10年的时间教育用户，说服用户把应用从物理机迁移至虚拟机。OpenStack从2011年出生到现在也仅仅短短的5年，可见OpenStack还有很长的路要走。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;今天是十一黄金周开始的第一天，在2016年10月6日，OpenStack马上要迎来第14个版本的发布，也是Big Tent后的第三个版本，计划Release项目达到32个，比Mitaka版本多了3个。&lt;/p&gt;
&lt;p&gt;这是继OpenStack Liberty贡献分析后的第三篇系列文章，我们很欣喜的看到在每次的OpenStack Release之后，我们总是可以发现有很多新的中国企业投身于OpenStack生态圈中，无论如何，随着时间的推移，像OpenStack这样的开源软件势必在企业市场中有越来越多的应用。在当今房价飞速增长的今天，整个的社会充满了浮躁，能出现一个像OpenStack一样的项目实属不易。我们的国家、我们的民族太需要一些脚踏实地的人做一些真正的“自主可控”的技术积累，否则我们的未来仍然摆脱不了表面强大的现实。&lt;/p&gt;
&lt;p&gt;最近一段时间一直在接触客户，也在思考为什么OpenStack无法像苹果手机那样轻松落地、供不应求，当然这个对比并不恰当。记得寄云科技的时博士曾经说过：越接近于用户底层的应用越难落地。现实也的确如此，就好像用户盖了一栋大楼，这时候你告诉用户，我这有个地基比你原来的好，来我给你换了；又或者你告诉用户说，我这个地基比你以前的好，我给你重新搭个地基，你再盖个楼。我想如果我是用户，我也不会答应的。所以，在用户基础架构已经非常成熟的企业中，OpenStack在落地过程中势必会遇到痛点不痛，落地困难的问题。我觉得解决这个问题无外乎几个方面：第一，有一位高瞻远瞩的领导，像携程的叶总、恒丰银行的张总；第二，把OpenStack的解决方案做的像VMWare一样完整，比如用户原来的业务系统怎么无缝迁移过来，用户原有资产怎么重新利用，怎么让OpenStack适用用户现有的网络架构，怎么让OpenStack适用用户现有的管理流程；第三，将OpenStack和刺中用户痛点的应用结合起来，进而推进OpenStack在企业中的应用，这也是我一直在寻找的方向。这仅仅是我在从事四年多OpenStack研发、销售过程中的一点点思考，也欢迎各位一起进行讨论。&lt;/p&gt;
&lt;p&gt;还是那句话，排名并不是这篇文章的真正目的。我们希望能有更多的用户看到，我们中国企业在OpenStack上的影响力，让更多的用户了解OpenStack，从而能够在未来的应用中使用OpenStack，形成真正的OpenStack的生态圈。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;OpenStack Mitaka深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2016/04/07/contribution-in-mitaka/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.site/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.site/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
  <entry>
    <title>使用国内源部署Ceph</title>
    <link href="http://sunqi.site/2016/06/19/deploy-ceph-using-china-mirror/"/>
    <id>http://sunqi.site/2016/06/19/deploy-ceph-using-china-mirror/</id>
    <published>2016-06-19T01:25:37.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。</p><a id="more"></a><h2 id="关于国内源"><a href="#关于国内源" class="headerlink" title="关于国内源"></a>关于国内源</h2><p>根据Ceph中国社区的<a href="http://bbs.ceph.org.cn/?/page/image" target="_blank" rel="noopener">统计</a>，国内已经有四家网站开始同步Ceph源，分别是：</p><ul><li>网易镜像源<a href="http://mirrors.163.com/ceph" target="_blank" rel="noopener">http://mirrors.163.com/ceph</a></li><li>阿里镜像源<a href="http://mirrors.aliyun.com/ceph" target="_blank" rel="noopener">http://mirrors.aliyun.com/ceph</a></li><li>中科大镜像源<a href="http://mirrors.ustc.edu.cn/ceph" target="_blank" rel="noopener">http://mirrors.ustc.edu.cn/ceph</a></li><li>宝德镜像源 <a href="http://mirrors.plcloud.com/ceph" target="_blank" rel="noopener">http://mirrors.plcloud.com/ceph</a></li></ul><h2 id="国内源分析"><a href="#国内源分析" class="headerlink" title="国内源分析"></a>国内源分析</h2><p>以163为例，是以天为单位向回同步Ceph源，完全可以满足大多数场景的需求，同步的源也非常全，包含了calamari，debian和rpm的全部源，最近几个版本的源也能从中找到。</p><h2 id="安装指定版本的Ceph"><a href="#安装指定版本的Ceph" class="headerlink" title="安装指定版本的Ceph"></a>安装指定版本的Ceph</h2><p>这里以安装最新版本的Jewel为例，由于Jewel版本中已经不提供el6的镜像源，所以只能使用CentOS 7以上版本进行安装。我们并不需要在repos里增加相应的源，只需要设置环境变量，即可让ceph-deploy使用国内源，具体过程如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export CEPH_DEPLOY_REPO_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;rpm-jewel&#x2F;el7</span><br><span class="line">export CEPH_DEPLOY_GPG_URL&#x3D;http:&#x2F;&#x2F;mirrors.163.com&#x2F;ceph&#x2F;keys&#x2F;release.asc</span><br></pre></td></tr></table></figure><p>之后的过程就没有任何区别了：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># Create monitor node</span><br><span class="line">ceph-deploy new node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Software Installation</span><br><span class="line">ceph-deploy install deploy node1 node2 node3</span><br><span class="line"></span><br><span class="line"># Gather keys</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line"></span><br><span class="line"># Ceph deploy parepare and activate</span><br><span class="line">ceph-deploy osd prepare node1:&#x2F;dev&#x2F;sdb node2:&#x2F;dev&#x2F;sdb node3:&#x2F;dev&#x2F;sdb</span><br><span class="line">ceph-deploy osd activate node1:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-0 node2:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-1 node3:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd&#x2F;ceph-2</span><br><span class="line"></span><br><span class="line"># Make 3 copies by default</span><br><span class="line">echo &quot;osd pool default size &#x3D; 3&quot; | tee -a $HOME&#x2F;ceph.conf</span><br><span class="line"></span><br><span class="line"># Copy admin keys and configuration files</span><br><span class="line">ceph-deploy --overwrite-conf admin deploy node1 node2 node3</span><br></pre></td></tr></table></figure><p>这样就可以很快速的使用国内源创建出Ceph集群，希望能对大家日常的使用提供便捷。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;由于网络方面的原因，Ceph的部署经常受到干扰，通常为了加速部署，基本上大家都是将Ceph的源同步到本地进行安装。根据Ceph中国社区的统计，当前已经有国内的网站定期将Ceph安装源同步，极大的方便了我们的测试。本文就是介绍如何使用国内源，加速ceph-deploy部署Ceph集群。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.site/categories/Ceph/"/>
    
    
  </entry>
  
  <entry>
    <title>使用Docker部署Ceph</title>
    <link href="http://sunqi.site/2016/06/12/bootstrap-your-ceph-cluster-in-docker/"/>
    <id>http://sunqi.site/2016/06/12/bootstrap-your-ceph-cluster-in-docker/</id>
    <published>2016-06-12T23:20:50.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章是根据Sébastien Han的<a href="https://www.youtube.com/watch?v=FUSTjTBA8f8&feature=youtu.be" target="_blank" rel="noopener">演示视频</a>进行整理的，对过程中有问题的部分进行了修复。</p><p>Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。</p><a id="more"></a><h2 id="部署环境"><a href="#部署环境" class="headerlink" title="部署环境"></a>部署环境</h2><ul><li>至少需要三台虚拟机或者物理机，每台虚拟机或者物理机至少有两块硬盘，这里我是在一台物理机上用vagrant模拟出三台CentOS 6.6虚拟机进行的实验</li><li>三台虚拟机需要安装docker，本文附带Docker加速方案</li><li>获取ceph/daemon镜像</li></ul><h2 id="部署流程"><a href="#部署流程" class="headerlink" title="部署流程"></a>部署流程</h2><img src="/images/blogs/bootstrap-ceph-docker-flow.png" class="center"><h2 id="部署架构"><a href="#部署架构" class="headerlink" title="部署架构"></a>部署架构</h2><p>主机名和集群的对应关系如下：</p><ul><li>node1 -&gt; 192.168.33.11</li><li>node2 -&gt; 192.168.33.12</li><li>node3 -&gt; 192.168.33.13</li></ul><img src="/images/blogs/bootstrap-ceph-docker-architecture.png" class="center"><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><h3 id="安装Docker，下载镜像"><a href="#安装Docker，下载镜像" class="headerlink" title="安装Docker，下载镜像"></a>安装Docker，下载镜像</h3><p>国内安装Dcoker还是速度很慢的，这里推荐使用daocloud的加速方案。不但docker安装速度提高了，pull镜像的速度也大幅度提高。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -sSL https:&#x2F;&#x2F;get.daocloud.io&#x2F;docker | sh</span><br></pre></td></tr></table></figure><p>我是在CentOS系统上进行的测试，将docker加入自动启动，并启动docker，接下来pull ceph daemon镜像，该镜像包含了所有的ceph服务和entrypoint。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">chkconfig docker</span><br><span class="line">service docker start</span><br><span class="line">docker pull ceph&#x2F;daemon</span><br></pre></td></tr></table></figure><h3 id="启动第一个Monitor"><a href="#启动第一个Monitor" class="headerlink" title="启动第一个Monitor"></a>启动第一个Monitor</h3><p>在node1上启动第一个Monitor，注意，如果你的环境中IP和我不同，请修改MON_IP。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.11 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>验证一下效果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker ps</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CONTAINER ID        IMAGE               COMMAND                CREATED             STATUS              PORTS               NAMES</span><br><span class="line">7babea544ef1        ceph&#x2F;daemon         &quot;&#x2F;entrypoint.sh mon&quot;   3 seconds ago       Up 2 seconds                            backstabbing_brattain</span><br></pre></td></tr></table></figure><p>查看一下集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker exec 7babea544ef1 ceph -s</span><br></pre></td></tr></table></figure><p>当前集群状态，能看到当前已经有一个mon启动起来了。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e1: 1 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 2, quorum 0 node1.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="复制配置文件"><a href="#复制配置文件" class="headerlink" title="复制配置文件"></a>复制配置文件</h3><p>接下来需要将node1的配置文件复制到node2和node3上，复制的路径包含/etc/ceph和/var/lib/ceph/bootstrap-*下的所有内容。这些配置文件非常重要，如果没有这些配置文件的存在，我们在其他节点启动新的docker ceph daemon的时候会被认为是一个新的集群。<br>我们在node1执行以下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ssh root@node2 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node2:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node2:&#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line"></span><br><span class="line">ssh root@node3 mkdir -p &#x2F;var&#x2F;lib&#x2F;ceph</span><br><span class="line">scp -r &#x2F;etc&#x2F;ceph root@node3:&#x2F;etc</span><br><span class="line">scp -r &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;bootstrap* root@node3:&#x2F;var&#x2F;lib&#x2F;ceph</span><br></pre></td></tr></table></figure><h3 id="启动第二个和第三个Monitor"><a href="#启动第二个和第三个Monitor" class="headerlink" title="启动第二个和第三个Monitor"></a>启动第二个和第三个Monitor</h3><p>在node2上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.12 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node3上执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">     --net&#x3D;host \</span><br><span class="line">     -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">     -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">     -e MON_IP&#x3D;192.168.33.13 \</span><br><span class="line">     -e CEPH_PUBLIC_NETWORK&#x3D;192.168.33.0&#x2F;24 \</span><br><span class="line">     ceph&#x2F;daemon mon</span><br></pre></td></tr></table></figure><p>在node1上查看集群状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_ERR</span><br><span class="line">        64 pgs stuck inactive</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        no osds</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e1: 0 osds: 0 up, 0 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        0 kB used, 0 kB &#x2F; 0 kB avail</span><br><span class="line">              64 creating</span><br></pre></td></tr></table></figure><h3 id="启动OSD的遇到的问题"><a href="#启动OSD的遇到的问题" class="headerlink" title="启动OSD的遇到的问题"></a>启动OSD的遇到的问题</h3><p>按照原视频的介绍的方法，启动OSD可以直接指定某个分区，然后用osd_ceph_disk作为启动ceph/daemon的参数，之后docker镜像会自动的进行分区等动作。但是经过实际验证却发现在mkjournal创建错误，OSD无法启动。</p><p>经过和社区确认，发现这个Bug在之前版本中得到过修复，但是之后的版本又出现了。根据社区的建议使用jewel版本的ceph daemon进行了再次验证，发现问题依旧，所以这里介绍的方法只能退而求其次，采用手动方式分区、格式化，之后用osd_directory启动ceph/daemon。</p><p>这是github上的相关讨论：<a href="https://github.com/ceph/ceph-docker/issues/171" target="_blank" rel="noopener">https://github.com/ceph/ceph-docker/issues/171</a></p><p>这是用osd_ceph_disk方式启动后的错误日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">command_check_call: Running command: &#x2F;usr&#x2F;bin&#x2F;ceph-osd --cluster ceph --mkfs --mkkey -i 4 --monmap &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;activate.monmap --osd-data &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG --osd-journal &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal --osd-uuid 89e240e1-17e9-4d6c-8d4f-f1a3e0278b91 --keyring &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;keyring --setuser ceph --setgroup disk</span><br><span class="line">2016-06-12 23:37:26.180610 7f8889654800 -1 filestore(&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG) mkjournal error creating journal on &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG&#x2F;journal: (2) No such file or directory</span><br><span class="line">2016-06-12 23:37:26.180752 7f8889654800 -1 OSD::mkfs: ObjectStore::mkfs failed with error -2</span><br><span class="line">2016-06-12 23:37:26.180918 7f8889654800 -1 ** ERROR: error creating empty object store in &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG: (2) No such file or directory</span><br><span class="line">mount_activate: Failed to activate</span><br><span class="line">unmount: Unmounting &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br><span class="line">command_check_call: Running command: &#x2F;bin&#x2F;umount -- &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;tmp&#x2F;mnt.BT8FXG</span><br></pre></td></tr></table></figure><h3 id="启动OSD"><a href="#启动OSD" class="headerlink" title="启动OSD"></a>启动OSD</h3><p>第一步先进行分区和格式化，这里只给出node1的操作方式，其他两个节点的方式类似。</p><p>先来安装必要的工具：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install -y parted xfsprogs</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# parted &#x2F;dev&#x2F;sdb</span><br><span class="line">GNU Parted 2.1</span><br><span class="line">Using &#x2F;dev&#x2F;sdb</span><br><span class="line">(parted) mklabel</span><br><span class="line">New disk label type? gpt</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start  End  Size  File system  Name  Flags</span><br><span class="line"></span><br><span class="line">(parted) mkpart</span><br><span class="line">Partition name?  []? &quot;Linux filesystem&quot;</span><br><span class="line">File system type?  [ext2]? xfs</span><br><span class="line">Start? 0G</span><br><span class="line">End? 107GB</span><br><span class="line">(parted) p</span><br><span class="line">Model: ATA VBOX HARDDISK (scsi)</span><br><span class="line">Disk &#x2F;dev&#x2F;sdb: 107GB</span><br><span class="line">Sector size (logical&#x2F;physical): 512B&#x2F;512B</span><br><span class="line">Partition Table: gpt</span><br><span class="line"></span><br><span class="line">Number  Start   End    Size   File system  Name              Flags</span><br><span class="line"> 1      1049kB  107GB  107GB               Linux filesystem</span><br><span class="line"></span><br><span class="line">(parted) quit</span><br></pre></td></tr></table></figure><p>格式化：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@node1 vagrant]# mkfs.xfs &#x2F;dev&#x2F;sdb1</span><br><span class="line">meta-data&#x3D;&#x2F;dev&#x2F;sdb1              isize&#x3D;256    agcount&#x3D;4, agsize&#x3D;6553472 blks</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   attr&#x3D;2, projid32bit&#x3D;1</span><br><span class="line">         &#x3D;                       crc&#x3D;0        finobt&#x3D;0</span><br><span class="line">data     &#x3D;                       bsize&#x3D;4096   blocks&#x3D;26213888, imaxpct&#x3D;25</span><br><span class="line">         &#x3D;                       sunit&#x3D;0      swidth&#x3D;0 blks</span><br><span class="line">naming   &#x3D;version 2              bsize&#x3D;4096   ascii-ci&#x3D;0 ftype&#x3D;0</span><br><span class="line">log      &#x3D;internal log           bsize&#x3D;4096   blocks&#x3D;12799, version&#x3D;2</span><br><span class="line">         &#x3D;                       sectsz&#x3D;512   sunit&#x3D;0 blks, lazy-count&#x3D;1</span><br><span class="line">realtime &#x3D;none                   extsz&#x3D;4096   blocks&#x3D;0, rtextents&#x3D;0</span><br></pre></td></tr></table></figure><p>我们把目录在node1上进行挂载。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p &#x2F;ceph&#x2F;sdb</span><br><span class="line">mount &#x2F;dev&#x2F;sdb1 &#x2F;ceph&#x2F;sdb</span><br></pre></td></tr></table></figure><p>最后启动OSD，这里最重要的就是把我们刚刚挂载好的OSD的实际路径透传给Docker内部的/var/lib/ceph/osd，如果每个节点有多个OSD的情况下，只需要在Host上映射到不同的目录，启动Docker的时候变更和/var/lib/ceph/osd的映射关系即可。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -v &#x2F;dev&#x2F;:&#x2F;dev&#x2F; \</span><br><span class="line">    -v &#x2F;ceph&#x2F;sdb:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F;osd \</span><br><span class="line">    --privileged&#x3D;true \</span><br><span class="line">    ceph&#x2F;daemon osd_directory</span><br></pre></td></tr></table></figure><p>按照同样的方法，将node2和node3的OSD也加入到集群，最终的效果如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        64 pgs degraded</span><br><span class="line">        64 pgs stuck unclean</span><br><span class="line">        64 pgs undersized</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> osdmap e13: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v18: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">        4551 MB used, 11306 MB &#x2F; 16720 MB avail</span><br><span class="line">              64 active+undersized+degraded</span><br></pre></td></tr></table></figure><h3 id="创建MDS"><a href="#创建MDS" class="headerlink" title="创建MDS"></a>创建MDS</h3><p>创建好基本的环境，其他的就容易了很多，下面来启动MDS。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    --net&#x3D;host \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    -e CEPHFS_CREATE&#x3D;1 \</span><br><span class="line">    ceph&#x2F;daemon mds</span><br></pre></td></tr></table></figure><h3 id="启动RGW，并且映射80端口"><a href="#启动RGW，并且映射80端口" class="headerlink" title="启动RGW，并且映射80端口"></a>启动RGW，并且映射80端口</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo docker run -d \</span><br><span class="line">    -p 80:80 \</span><br><span class="line">    -v &#x2F;etc&#x2F;ceph:&#x2F;etc&#x2F;ceph \</span><br><span class="line">    -v &#x2F;var&#x2F;lib&#x2F;ceph&#x2F;:&#x2F;var&#x2F;lib&#x2F;ceph&#x2F; \</span><br><span class="line">    ceph&#x2F;daemon rgw</span><br></pre></td></tr></table></figure><h3 id="最终的集群状态"><a href="#最终的集群状态" class="headerlink" title="最终的集群状态"></a>最终的集群状态</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">cluster 0de1fc5a-084d-4396-bb0b-59db72a9a439</span><br><span class="line"> health HEALTH_WARN</span><br><span class="line">        clock skew detected on mon.node2.docker.com</span><br><span class="line">        48 pgs stuck inactive</span><br><span class="line">        48 pgs stuck unclean</span><br><span class="line">        Monitor clock skew detected</span><br><span class="line"> monmap e3: 3 mons at &#123;node1.docker.com&#x3D;192.168.33.11:6789&#x2F;0,node2.docker.com&#x3D;192.168.33.12:6789&#x2F;0,node3.docker.com&#x3D;192.168.33.13:6789&#x2F;0&#125;</span><br><span class="line">        election epoch 6, quorum 0,1,2 node1.docker.com,node2.docker.com,node3.docker.com</span><br><span class="line"> mdsmap e5: 1&#x2F;1&#x2F;1 up &#123;0&#x3D;mds-node1.docker.com&#x3D;up:active&#125;</span><br><span class="line"> osdmap e25: 3 osds: 3 up, 3 in</span><br><span class="line">        flags sortbitwise</span><br><span class="line">  pgmap v38: 128 pgs, 9 pools, 588 bytes data, 11 objects</span><br><span class="line">        6791 MB used, 16996 MB &#x2F; 25081 MB avail</span><br><span class="line">              80 active+clean</span><br><span class="line">              45 creating</span><br><span class="line">               3 creating+activating</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>在Docker中部署Ceph并没有想象中的那么顺利，社区的版本中仍然有Bug需要解决。</p><p>Docker作为一种快捷的部署方式，的确可以大幅度提高Ceph的部署效率，提高扩展的速度。但是从另一个角度我们应该注意到，随着Docker的引入也改变了Ceph的运维方式，比如在OSD增减的时候，需要到容器中对Ceph集群进行维护。再比如配置文件变更后的重启问题等。</p><p>但是无论如何，我相信这些问题都会得到完美的解决，用Docker部署Ceph作为一种新的尝试，值得推广。<br>之后还会为大家带来，如何使用Ansible结合Docker更快速的部署Ceph集群，敬请期待。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章是根据Sébastien Han的&lt;a href=&quot;https://www.youtube.com/watch?v=FUSTjTBA8f8&amp;feature=youtu.be&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;演示视频&lt;/a&gt;进行整理的，对过程中有问题的部分进行了修复。&lt;/p&gt;
&lt;p&gt;Docker作为持久化集成的最佳工具，特别是在部署中有着得天独厚的优势。Ceph作为开源的分布式存储得到越来越多的使用，但是作为分布式系统，Ceph在部署和运维上仍然有不小的难度,本文重点介绍利用Docker快速的进行Ceph集群的创建，以及各个组件的安装。&lt;/p&gt;
    
    </summary>
    
    
      <category term="Ceph" scheme="http://sunqi.site/categories/Ceph/"/>
    
      <category term="Docker" scheme="http://sunqi.site/categories/Ceph/Docker/"/>
    
    
  </entry>
  
  <entry>
    <title>深度解读OpenStack Mitaka国内代码贡献</title>
    <link href="http://sunqi.site/2016/04/07/contribution-in-mitaka/"/>
    <id>http://sunqi.site/2016/04/07/contribution-in-mitaka/</id>
    <published>2016-04-07T07:19:39.000Z</published>
    <updated>2020-11-01T07:20:36.404Z</updated>
    
    <content type="html"><![CDATA[<p>转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。</p><p>去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。</p><p>这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。</p><p>对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。</p><p>OpenStack Liberty深度解读请见：<a href="http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/" target="_blank" rel="noopener">http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/</a></p><a id="more"></a><h2 id="Release项目简介"><a href="#Release项目简介" class="headerlink" title="Release项目简介"></a>Release项目简介</h2><p>Openstack官方的Release的网站已经更新为：<a href="http://releases.openstack.org/" target="_blank" rel="noopener">http://releases.openstack.org/</a></p><p>在Big Tent公布之后，OpenStack的项目被分为Core Projects和Big Tent Projects。</p><img src="/images/blogs/contribution-in-mitaka-big-tent.jpg" class="center"><p>让我们来看一下在Mitaka版本中，多了哪些新项目。</p><ul><li>几个与Docker相关的项目被发布出来，magnum, senlin, solum</li><li>数据备份容灾的项目：freezer</li><li>计费的项目：cloudkitty</li><li>NFV相关的项目：tracker</li><li>监控相关的项目：monasca</li></ul><p>关于这些新项目的一些介绍，我将放在另外一篇博客里，敬请关注。</p><img src="/images/blogs/contribution-in-mitaka-projects.png" class="center"><h2 id="社区贡献总体分析"><a href="#社区贡献总体分析" class="headerlink" title="社区贡献总体分析"></a>社区贡献总体分析</h2><p>本次统计的方法仍然为commits的方式，统计范围为stackalystatics默认统计的全部项目。</p><p>从总体参与的公司数量来看，Mitaka版本略有下降，但是参与的人数多了100多人。</p><img src="/images/blogs/contribution-in-mitaka-companies-contributors.png" class="center"><p>整个社区的公司贡献排名上没有明显的变化，传统的几大豪强仍然霸占公司排名的前十位，华为表现依然强劲，是中国区唯一能进入前十名的公司。</p><p>在模块方面，整体统计的绝大部分比例已经被others所占据，说明在Big Tent计划下，OpenStack正在朝更多元化的方向演进。在Mitaka排名前十位的项目中，fuel相关的两个项目都进入了前十，说明fuel在OpenStack部署的地位已经越来越重要了。同时，核心项目中的nova，neutron，cinder项目仍然在前十名的范围内，贡献量基本保持不变。值得一提的是，在Mitaka统计的项目数量已经从Liberty的708个增长到了829个，可见在短短的6个月内，OpenStack社区的蓬勃发展。</p><img src="/images/blogs/contribution-in-mitaka-companies-modules.png" class="center"><h2 id="OpenStack国内社区分析"><a href="#OpenStack国内社区分析" class="headerlink" title="OpenStack国内社区分析"></a>OpenStack国内社区分析</h2><p>看完了整体统计，我们再回到国内，因为已经有文章做了我在Liberty时候的分析，所以这里我换个角度来看国内的社区贡献，首先是统计排名的变化。</p><h3 id="贡献企业"><a href="#贡献企业" class="headerlink" title="贡献企业"></a>贡献企业</h3><p>在Liberty中，有13家国内企业为社区做了贡献，在Mitaka中这个数量增加到了15家企业，这里简单的将这些企业做了一下分类：</p><ul><li>互联网用户：乐视、新浪、网易</li><li>电信用户：中国移动</li><li>传统IT服务商：华为、中兴、华三</li><li>私有云服务商：Easystack、九州云、海云捷迅、北京有云、麒麟云、UMCloud、象云、Huron(休伦科技)</li></ul><img src="/images/blogs/contribution-in-mitaka-china-companies.png" class="center"><h3 id="行业分析"><a href="#行业分析" class="headerlink" title="行业分析"></a>行业分析</h3><p>通过行业的分析我们可以看出，国内的主要贡献仍然来自私有云服务商和传统IT服务商，换言之来自于以OpenStack提供产品或者服务的公司。厂商们贡献的目的很明确，主要为了展示自身在开源项目中的积累和专家形象。而用户的贡献主要来自平时在使用OpenStack时候遇到Bug，就是在实际应用过程中出现的问题。</p><img src="/images/blogs/contribution-in-mitaka-china-by-industry.png" class="center"><h3 id="人员投入分析"><a href="#人员投入分析" class="headerlink" title="人员投入分析"></a>人员投入分析</h3><p>单纯的社区贡献排名的比较仅仅是一个维度，下面我们来看一下各个公司的人员投入情况：</p><ul><li>排名前几位的公司对社区投入的人力基本都是两位数，相对于Liberty版本，人员均有所增加</li><li>在人均贡献投入上，99cloud是国内最高的，平均达到了59天，甚至超过了华为，这个统计不仅仅包含了代码贡献，还包含了邮件、Review、Blueprint的时间，基本可以衡量每个公司在OpenStack社区贡献方面的投入力量</li><li>人员投入来看，Easystack和中国移动无疑是最下本的两家，Easystack从Liberty的3人，增长到了23人，一下子增加了20人；中国移动也从最初的4个人，增加到了13个人，可见中国移动未来对OpenStack的野心</li></ul><img src="/images/blogs/contribution-in-mitaka-companies-effort.png" class="center"><h3 id="贡献模块分析"><a href="#贡献模块分析" class="headerlink" title="贡献模块分析"></a>贡献模块分析</h3><p>从模块的角度进行统计，国内企业的贡献情况并未出现一个统一的趋势，总体的贡献项目为193个，项目几乎涉及OpenStack所有最活跃的项目，从排名前十的项目来看：</p><ul><li>得益于华为的主导，dargonflow项目的贡献量超高</li><li>紧随其后的，也是当下的热点，容器相关的两个项目</li><li>几大OpenStack老模块贡献量也高居前十位，说明这些模块是在解决方案中使用频率较高的</li></ul><img src="/images/blogs/contribution-in-mitaka-modules.png" class="center"><h3 id="投入产出比"><a href="#投入产出比" class="headerlink" title="投入产出比"></a>投入产出比</h3><p>这是一个很敏感的话题，每个公司对社区的投入到底换来多少项目上的回报呢？可能这只有每个公司的CEO能够回答的问题了。我在这里就不多做过多的分析，留给大家充分讨论的空间吧。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>刚刚结束在南京的OpenStack开发培训，也了解到5G的通信网络上已经确定引入了OpenStack，虽然我说不清楚他的具体用途，但是我相信这对OpenStack这个项目、社区是一个重大的利好消息。我也相信，通过国内企业的集体努力，一定能让OpenStack在中国遍地开花结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;转眼间，OpenStack又迎来了新版本发布的日子，这是OpenStack第13个版本，也是Big Tent后的第二个版本，秉承“公开公正”的原则，OpenStack Release的项目达到了29个，比Liberty多出了8个。&lt;/p&gt;
&lt;p&gt;去年的时候，对国内的OpenStack Liberty贡献进行了深度解读后引起了广泛的关注，在今年Mitaka版本发布之后，类似的解读已经遍布朋友圈，但是在看过后，发现并非国内贡献的全部统计，所以决定还是自己写一篇完整的深度解读系列文章，来帮助国内用户对国内OpenStack的现状有一个全面的了解和认识。&lt;/p&gt;
&lt;p&gt;这几天一直在思考写这篇文章的目的和意义，我们搞分析也好，搞排名也罢，到底是为了什么？Mitaka版本更新后，各个公司也以排名作为企业宣传的最好的武器，我觉得这些都无可厚非。但是我觉得更重要的一点是在当前去IOEV的大形势下，我们应该告诉国内的企业用户，有一批热衷于追求Geek精神的年轻人在为中国未来的IT产业变革做着不懈的努力，他们用数字证明了国外公司能做到的我们国内公司也能做到，这个世界上不仅有IOEV，还有中国制造的OpenStack。&lt;/p&gt;
&lt;p&gt;对于友商们已经分析的数据，这里不再赘述，本文主要通过stackalytics.com提供的API对国内社区贡献进行一次深度挖掘和整理。&lt;/p&gt;
&lt;p&gt;OpenStack Liberty深度解读请见：&lt;a href=&quot;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;http://xiaoquqi.github.io/blog/2015/10/29/contribution-in-liberty/&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="OpenStack" scheme="http://sunqi.site/categories/OpenStack/"/>
    
      <category term="Cloud Computing" scheme="http://sunqi.site/categories/OpenStack/Cloud-Computing/"/>
    
    
  </entry>
  
</feed>
